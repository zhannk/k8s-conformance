I0504 11:44:27.463738      21 e2e.go:116] Starting e2e run "d107b977-ad83-4d56-8961-eff1e125655a" on Ginkgo node 1
May  4 11:44:27.502: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1683200667 - will randomize all specs

Will run 362 of 7066 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
{"msg":"Test Suite starting","completed":0,"skipped":0,"failed":0}
May  4 11:44:27.879: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
E0504 11:44:27.880851      21 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
E0504 11:44:27.880851      21 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
May  4 11:44:27.881: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
May  4 11:44:27.905: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
May  4 11:44:27.959: INFO: 21 / 21 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
May  4 11:44:27.959: INFO: expected 10 pod replicas in namespace 'kube-system', 10 are Running and Ready.
May  4 11:44:27.959: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
May  4 11:44:27.966: INFO: 8 / 8 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
May  4 11:44:27.966: INFO: e2e test version: v1.25.5
May  4 11:44:27.967: INFO: kube-apiserver version: v1.25.5
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
May  4 11:44:27.967: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
May  4 11:44:27.973: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.094 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    May  4 11:44:27.879: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    E0504 11:44:27.880851      21 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    May  4 11:44:27.881: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    May  4 11:44:27.905: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    May  4 11:44:27.959: INFO: 21 / 21 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    May  4 11:44:27.959: INFO: expected 10 pod replicas in namespace 'kube-system', 10 are Running and Ready.
    May  4 11:44:27.959: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    May  4 11:44:27.966: INFO: 8 / 8 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
    May  4 11:44:27.966: INFO: e2e test version: v1.25.5
    May  4 11:44:27.967: INFO: kube-apiserver version: v1.25.5
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    May  4 11:44:27.967: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    May  4 11:44:27.973: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:44:28.004
May  4 11:44:28.004: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename services 05/04/23 11:44:28.005
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:44:28.028
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:44:28.032
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
STEP: creating service in namespace services-7882 05/04/23 11:44:28.035
STEP: creating service affinity-nodeport in namespace services-7882 05/04/23 11:44:28.035
STEP: creating replication controller affinity-nodeport in namespace services-7882 05/04/23 11:44:28.058
I0504 11:44:28.070424      21 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-7882, replica count: 3
I0504 11:44:31.121494      21 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0504 11:44:34.122759      21 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  4 11:44:34.151: INFO: Creating new exec pod
May  4 11:44:34.162: INFO: Waiting up to 5m0s for pod "execpod-affinityb5mbw" in namespace "services-7882" to be "running"
May  4 11:44:34.169: INFO: Pod "execpod-affinityb5mbw": Phase="Pending", Reason="", readiness=false. Elapsed: 6.187939ms
May  4 11:44:36.174: INFO: Pod "execpod-affinityb5mbw": Phase="Running", Reason="", readiness=true. Elapsed: 2.011740325s
May  4 11:44:36.174: INFO: Pod "execpod-affinityb5mbw" satisfied condition "running"
May  4 11:44:37.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-7882 exec execpod-affinityb5mbw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
May  4 11:44:37.389: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
May  4 11:44:37.389: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  4 11:44:37.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-7882 exec execpod-affinityb5mbw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.21.106.184 80'
May  4 11:44:37.571: INFO: stderr: "+ + echonc -v hostName -t\n -w 2 10.21.106.184 80\nConnection to 10.21.106.184 80 port [tcp/http] succeeded!\n"
May  4 11:44:37.572: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  4 11:44:37.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-7882 exec execpod-affinityb5mbw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.1.189 31523'
May  4 11:44:37.755: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.1.189 31523\nConnection to 10.0.1.189 31523 port [tcp/*] succeeded!\n"
May  4 11:44:37.755: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  4 11:44:37.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-7882 exec execpod-affinityb5mbw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.1.224 31523'
May  4 11:44:37.928: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.1.224 31523\nConnection to 10.0.1.224 31523 port [tcp/*] succeeded!\n"
May  4 11:44:37.928: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  4 11:44:37.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-7882 exec execpod-affinityb5mbw -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.1.189:31523/ ; done'
May  4 11:44:38.158: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:31523/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:31523/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:31523/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:31523/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:31523/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:31523/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:31523/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:31523/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:31523/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:31523/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:31523/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:31523/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:31523/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:31523/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:31523/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:31523/\n"
May  4 11:44:38.158: INFO: stdout: "\naffinity-nodeport-fqh2k\naffinity-nodeport-fqh2k\naffinity-nodeport-fqh2k\naffinity-nodeport-fqh2k\naffinity-nodeport-fqh2k\naffinity-nodeport-fqh2k\naffinity-nodeport-fqh2k\naffinity-nodeport-fqh2k\naffinity-nodeport-fqh2k\naffinity-nodeport-fqh2k\naffinity-nodeport-fqh2k\naffinity-nodeport-fqh2k\naffinity-nodeport-fqh2k\naffinity-nodeport-fqh2k\naffinity-nodeport-fqh2k\naffinity-nodeport-fqh2k"
May  4 11:44:38.158: INFO: Received response from host: affinity-nodeport-fqh2k
May  4 11:44:38.158: INFO: Received response from host: affinity-nodeport-fqh2k
May  4 11:44:38.158: INFO: Received response from host: affinity-nodeport-fqh2k
May  4 11:44:38.158: INFO: Received response from host: affinity-nodeport-fqh2k
May  4 11:44:38.158: INFO: Received response from host: affinity-nodeport-fqh2k
May  4 11:44:38.158: INFO: Received response from host: affinity-nodeport-fqh2k
May  4 11:44:38.158: INFO: Received response from host: affinity-nodeport-fqh2k
May  4 11:44:38.158: INFO: Received response from host: affinity-nodeport-fqh2k
May  4 11:44:38.158: INFO: Received response from host: affinity-nodeport-fqh2k
May  4 11:44:38.158: INFO: Received response from host: affinity-nodeport-fqh2k
May  4 11:44:38.158: INFO: Received response from host: affinity-nodeport-fqh2k
May  4 11:44:38.158: INFO: Received response from host: affinity-nodeport-fqh2k
May  4 11:44:38.158: INFO: Received response from host: affinity-nodeport-fqh2k
May  4 11:44:38.158: INFO: Received response from host: affinity-nodeport-fqh2k
May  4 11:44:38.158: INFO: Received response from host: affinity-nodeport-fqh2k
May  4 11:44:38.158: INFO: Received response from host: affinity-nodeport-fqh2k
May  4 11:44:38.158: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-7882, will wait for the garbage collector to delete the pods 05/04/23 11:44:38.182
May  4 11:44:38.246: INFO: Deleting ReplicationController affinity-nodeport took: 8.305504ms
May  4 11:44:38.346: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.177086ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  4 11:44:45.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7882" for this suite. 05/04/23 11:44:45.933
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","completed":1,"skipped":4,"failed":0}
------------------------------
• [SLOW TEST] [17.944 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:44:28.004
    May  4 11:44:28.004: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename services 05/04/23 11:44:28.005
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:44:28.028
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:44:28.032
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2204
    STEP: creating service in namespace services-7882 05/04/23 11:44:28.035
    STEP: creating service affinity-nodeport in namespace services-7882 05/04/23 11:44:28.035
    STEP: creating replication controller affinity-nodeport in namespace services-7882 05/04/23 11:44:28.058
    I0504 11:44:28.070424      21 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-7882, replica count: 3
    I0504 11:44:31.121494      21 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0504 11:44:34.122759      21 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  4 11:44:34.151: INFO: Creating new exec pod
    May  4 11:44:34.162: INFO: Waiting up to 5m0s for pod "execpod-affinityb5mbw" in namespace "services-7882" to be "running"
    May  4 11:44:34.169: INFO: Pod "execpod-affinityb5mbw": Phase="Pending", Reason="", readiness=false. Elapsed: 6.187939ms
    May  4 11:44:36.174: INFO: Pod "execpod-affinityb5mbw": Phase="Running", Reason="", readiness=true. Elapsed: 2.011740325s
    May  4 11:44:36.174: INFO: Pod "execpod-affinityb5mbw" satisfied condition "running"
    May  4 11:44:37.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-7882 exec execpod-affinityb5mbw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
    May  4 11:44:37.389: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    May  4 11:44:37.389: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  4 11:44:37.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-7882 exec execpod-affinityb5mbw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.21.106.184 80'
    May  4 11:44:37.571: INFO: stderr: "+ + echonc -v hostName -t\n -w 2 10.21.106.184 80\nConnection to 10.21.106.184 80 port [tcp/http] succeeded!\n"
    May  4 11:44:37.572: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  4 11:44:37.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-7882 exec execpod-affinityb5mbw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.1.189 31523'
    May  4 11:44:37.755: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.1.189 31523\nConnection to 10.0.1.189 31523 port [tcp/*] succeeded!\n"
    May  4 11:44:37.755: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  4 11:44:37.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-7882 exec execpod-affinityb5mbw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.1.224 31523'
    May  4 11:44:37.928: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.1.224 31523\nConnection to 10.0.1.224 31523 port [tcp/*] succeeded!\n"
    May  4 11:44:37.928: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  4 11:44:37.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-7882 exec execpod-affinityb5mbw -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.1.189:31523/ ; done'
    May  4 11:44:38.158: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:31523/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:31523/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:31523/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:31523/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:31523/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:31523/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:31523/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:31523/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:31523/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:31523/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:31523/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:31523/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:31523/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:31523/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:31523/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:31523/\n"
    May  4 11:44:38.158: INFO: stdout: "\naffinity-nodeport-fqh2k\naffinity-nodeport-fqh2k\naffinity-nodeport-fqh2k\naffinity-nodeport-fqh2k\naffinity-nodeport-fqh2k\naffinity-nodeport-fqh2k\naffinity-nodeport-fqh2k\naffinity-nodeport-fqh2k\naffinity-nodeport-fqh2k\naffinity-nodeport-fqh2k\naffinity-nodeport-fqh2k\naffinity-nodeport-fqh2k\naffinity-nodeport-fqh2k\naffinity-nodeport-fqh2k\naffinity-nodeport-fqh2k\naffinity-nodeport-fqh2k"
    May  4 11:44:38.158: INFO: Received response from host: affinity-nodeport-fqh2k
    May  4 11:44:38.158: INFO: Received response from host: affinity-nodeport-fqh2k
    May  4 11:44:38.158: INFO: Received response from host: affinity-nodeport-fqh2k
    May  4 11:44:38.158: INFO: Received response from host: affinity-nodeport-fqh2k
    May  4 11:44:38.158: INFO: Received response from host: affinity-nodeport-fqh2k
    May  4 11:44:38.158: INFO: Received response from host: affinity-nodeport-fqh2k
    May  4 11:44:38.158: INFO: Received response from host: affinity-nodeport-fqh2k
    May  4 11:44:38.158: INFO: Received response from host: affinity-nodeport-fqh2k
    May  4 11:44:38.158: INFO: Received response from host: affinity-nodeport-fqh2k
    May  4 11:44:38.158: INFO: Received response from host: affinity-nodeport-fqh2k
    May  4 11:44:38.158: INFO: Received response from host: affinity-nodeport-fqh2k
    May  4 11:44:38.158: INFO: Received response from host: affinity-nodeport-fqh2k
    May  4 11:44:38.158: INFO: Received response from host: affinity-nodeport-fqh2k
    May  4 11:44:38.158: INFO: Received response from host: affinity-nodeport-fqh2k
    May  4 11:44:38.158: INFO: Received response from host: affinity-nodeport-fqh2k
    May  4 11:44:38.158: INFO: Received response from host: affinity-nodeport-fqh2k
    May  4 11:44:38.158: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-7882, will wait for the garbage collector to delete the pods 05/04/23 11:44:38.182
    May  4 11:44:38.246: INFO: Deleting ReplicationController affinity-nodeport took: 8.305504ms
    May  4 11:44:38.346: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.177086ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  4 11:44:45.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7882" for this suite. 05/04/23 11:44:45.933
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:44:45.949
May  4 11:44:45.949: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename custom-resource-definition 05/04/23 11:44:45.95
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:44:45.987
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:44:45.996
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
May  4 11:44:46.002: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  4 11:44:47.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1418" for this suite. 05/04/23 11:44:47.052
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","completed":2,"skipped":8,"failed":0}
------------------------------
• [1.112 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:44:45.949
    May  4 11:44:45.949: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename custom-resource-definition 05/04/23 11:44:45.95
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:44:45.987
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:44:45.996
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    May  4 11:44:46.002: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  4 11:44:47.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-1418" for this suite. 05/04/23 11:44:47.052
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:44:47.063
May  4 11:44:47.063: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename projected 05/04/23 11:44:47.064
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:44:47.158
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:44:47.162
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
STEP: Creating secret with name projected-secret-test-29131903-9eb6-4972-b0de-7802eb3edf29 05/04/23 11:44:47.164
STEP: Creating a pod to test consume secrets 05/04/23 11:44:47.171
May  4 11:44:47.184: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9e2b367f-ce6b-4322-88af-3a52114bf28c" in namespace "projected-1688" to be "Succeeded or Failed"
May  4 11:44:47.189: INFO: Pod "pod-projected-secrets-9e2b367f-ce6b-4322-88af-3a52114bf28c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.789005ms
May  4 11:44:49.194: INFO: Pod "pod-projected-secrets-9e2b367f-ce6b-4322-88af-3a52114bf28c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010428347s
May  4 11:44:51.198: INFO: Pod "pod-projected-secrets-9e2b367f-ce6b-4322-88af-3a52114bf28c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014424769s
STEP: Saw pod success 05/04/23 11:44:51.199
May  4 11:44:51.199: INFO: Pod "pod-projected-secrets-9e2b367f-ce6b-4322-88af-3a52114bf28c" satisfied condition "Succeeded or Failed"
May  4 11:44:51.207: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-projected-secrets-9e2b367f-ce6b-4322-88af-3a52114bf28c container secret-volume-test: <nil>
STEP: delete the pod 05/04/23 11:44:51.228
May  4 11:44:51.256: INFO: Waiting for pod pod-projected-secrets-9e2b367f-ce6b-4322-88af-3a52114bf28c to disappear
May  4 11:44:51.261: INFO: Pod pod-projected-secrets-9e2b367f-ce6b-4322-88af-3a52114bf28c no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
May  4 11:44:51.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1688" for this suite. 05/04/23 11:44:51.277
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":3,"skipped":18,"failed":0}
------------------------------
• [4.232 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:44:47.063
    May  4 11:44:47.063: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename projected 05/04/23 11:44:47.064
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:44:47.158
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:44:47.162
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:118
    STEP: Creating secret with name projected-secret-test-29131903-9eb6-4972-b0de-7802eb3edf29 05/04/23 11:44:47.164
    STEP: Creating a pod to test consume secrets 05/04/23 11:44:47.171
    May  4 11:44:47.184: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9e2b367f-ce6b-4322-88af-3a52114bf28c" in namespace "projected-1688" to be "Succeeded or Failed"
    May  4 11:44:47.189: INFO: Pod "pod-projected-secrets-9e2b367f-ce6b-4322-88af-3a52114bf28c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.789005ms
    May  4 11:44:49.194: INFO: Pod "pod-projected-secrets-9e2b367f-ce6b-4322-88af-3a52114bf28c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010428347s
    May  4 11:44:51.198: INFO: Pod "pod-projected-secrets-9e2b367f-ce6b-4322-88af-3a52114bf28c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014424769s
    STEP: Saw pod success 05/04/23 11:44:51.199
    May  4 11:44:51.199: INFO: Pod "pod-projected-secrets-9e2b367f-ce6b-4322-88af-3a52114bf28c" satisfied condition "Succeeded or Failed"
    May  4 11:44:51.207: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-projected-secrets-9e2b367f-ce6b-4322-88af-3a52114bf28c container secret-volume-test: <nil>
    STEP: delete the pod 05/04/23 11:44:51.228
    May  4 11:44:51.256: INFO: Waiting for pod pod-projected-secrets-9e2b367f-ce6b-4322-88af-3a52114bf28c to disappear
    May  4 11:44:51.261: INFO: Pod pod-projected-secrets-9e2b367f-ce6b-4322-88af-3a52114bf28c no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    May  4 11:44:51.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1688" for this suite. 05/04/23 11:44:51.277
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:44:51.301
May  4 11:44:51.301: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename emptydir 05/04/23 11:44:51.302
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:44:51.337
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:44:51.342
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
STEP: Creating a pod to test emptydir 0644 on node default medium 05/04/23 11:44:51.35
May  4 11:44:51.382: INFO: Waiting up to 5m0s for pod "pod-6df90f54-715a-41f8-837b-3afcf0bf471d" in namespace "emptydir-5507" to be "Succeeded or Failed"
May  4 11:44:51.402: INFO: Pod "pod-6df90f54-715a-41f8-837b-3afcf0bf471d": Phase="Pending", Reason="", readiness=false. Elapsed: 19.930685ms
May  4 11:44:53.407: INFO: Pod "pod-6df90f54-715a-41f8-837b-3afcf0bf471d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02532024s
May  4 11:44:55.407: INFO: Pod "pod-6df90f54-715a-41f8-837b-3afcf0bf471d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024823302s
STEP: Saw pod success 05/04/23 11:44:55.407
May  4 11:44:55.407: INFO: Pod "pod-6df90f54-715a-41f8-837b-3afcf0bf471d" satisfied condition "Succeeded or Failed"
May  4 11:44:55.412: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod pod-6df90f54-715a-41f8-837b-3afcf0bf471d container test-container: <nil>
STEP: delete the pod 05/04/23 11:44:55.435
May  4 11:44:55.458: INFO: Waiting for pod pod-6df90f54-715a-41f8-837b-3afcf0bf471d to disappear
May  4 11:44:55.464: INFO: Pod pod-6df90f54-715a-41f8-837b-3afcf0bf471d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May  4 11:44:55.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5507" for this suite. 05/04/23 11:44:55.479
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":4,"skipped":65,"failed":0}
------------------------------
• [4.192 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:44:51.301
    May  4 11:44:51.301: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename emptydir 05/04/23 11:44:51.302
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:44:51.337
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:44:51.342
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:166
    STEP: Creating a pod to test emptydir 0644 on node default medium 05/04/23 11:44:51.35
    May  4 11:44:51.382: INFO: Waiting up to 5m0s for pod "pod-6df90f54-715a-41f8-837b-3afcf0bf471d" in namespace "emptydir-5507" to be "Succeeded or Failed"
    May  4 11:44:51.402: INFO: Pod "pod-6df90f54-715a-41f8-837b-3afcf0bf471d": Phase="Pending", Reason="", readiness=false. Elapsed: 19.930685ms
    May  4 11:44:53.407: INFO: Pod "pod-6df90f54-715a-41f8-837b-3afcf0bf471d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02532024s
    May  4 11:44:55.407: INFO: Pod "pod-6df90f54-715a-41f8-837b-3afcf0bf471d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024823302s
    STEP: Saw pod success 05/04/23 11:44:55.407
    May  4 11:44:55.407: INFO: Pod "pod-6df90f54-715a-41f8-837b-3afcf0bf471d" satisfied condition "Succeeded or Failed"
    May  4 11:44:55.412: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod pod-6df90f54-715a-41f8-837b-3afcf0bf471d container test-container: <nil>
    STEP: delete the pod 05/04/23 11:44:55.435
    May  4 11:44:55.458: INFO: Waiting for pod pod-6df90f54-715a-41f8-837b-3afcf0bf471d to disappear
    May  4 11:44:55.464: INFO: Pod pod-6df90f54-715a-41f8-837b-3afcf0bf471d no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May  4 11:44:55.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5507" for this suite. 05/04/23 11:44:55.479
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:44:55.494
May  4 11:44:55.494: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename projected 05/04/23 11:44:55.495
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:44:55.518
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:44:55.523
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
STEP: Creating configMap with name projected-configmap-test-volume-map-c31c3625-f62f-4f28-a682-10e14ded4b62 05/04/23 11:44:55.525
STEP: Creating a pod to test consume configMaps 05/04/23 11:44:55.531
May  4 11:44:55.542: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3c36e698-5dc3-4463-b86e-1c2819c396ad" in namespace "projected-1060" to be "Succeeded or Failed"
May  4 11:44:55.546: INFO: Pod "pod-projected-configmaps-3c36e698-5dc3-4463-b86e-1c2819c396ad": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053774ms
May  4 11:44:57.552: INFO: Pod "pod-projected-configmaps-3c36e698-5dc3-4463-b86e-1c2819c396ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009329659s
May  4 11:44:59.552: INFO: Pod "pod-projected-configmaps-3c36e698-5dc3-4463-b86e-1c2819c396ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009740311s
STEP: Saw pod success 05/04/23 11:44:59.552
May  4 11:44:59.552: INFO: Pod "pod-projected-configmaps-3c36e698-5dc3-4463-b86e-1c2819c396ad" satisfied condition "Succeeded or Failed"
May  4 11:44:59.557: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod pod-projected-configmaps-3c36e698-5dc3-4463-b86e-1c2819c396ad container agnhost-container: <nil>
STEP: delete the pod 05/04/23 11:44:59.567
May  4 11:44:59.586: INFO: Waiting for pod pod-projected-configmaps-3c36e698-5dc3-4463-b86e-1c2819c396ad to disappear
May  4 11:44:59.595: INFO: Pod pod-projected-configmaps-3c36e698-5dc3-4463-b86e-1c2819c396ad no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
May  4 11:44:59.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1060" for this suite. 05/04/23 11:44:59.605
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":5,"skipped":71,"failed":0}
------------------------------
• [4.120 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:44:55.494
    May  4 11:44:55.494: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename projected 05/04/23 11:44:55.495
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:44:55.518
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:44:55.523
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:108
    STEP: Creating configMap with name projected-configmap-test-volume-map-c31c3625-f62f-4f28-a682-10e14ded4b62 05/04/23 11:44:55.525
    STEP: Creating a pod to test consume configMaps 05/04/23 11:44:55.531
    May  4 11:44:55.542: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3c36e698-5dc3-4463-b86e-1c2819c396ad" in namespace "projected-1060" to be "Succeeded or Failed"
    May  4 11:44:55.546: INFO: Pod "pod-projected-configmaps-3c36e698-5dc3-4463-b86e-1c2819c396ad": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053774ms
    May  4 11:44:57.552: INFO: Pod "pod-projected-configmaps-3c36e698-5dc3-4463-b86e-1c2819c396ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009329659s
    May  4 11:44:59.552: INFO: Pod "pod-projected-configmaps-3c36e698-5dc3-4463-b86e-1c2819c396ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009740311s
    STEP: Saw pod success 05/04/23 11:44:59.552
    May  4 11:44:59.552: INFO: Pod "pod-projected-configmaps-3c36e698-5dc3-4463-b86e-1c2819c396ad" satisfied condition "Succeeded or Failed"
    May  4 11:44:59.557: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod pod-projected-configmaps-3c36e698-5dc3-4463-b86e-1c2819c396ad container agnhost-container: <nil>
    STEP: delete the pod 05/04/23 11:44:59.567
    May  4 11:44:59.586: INFO: Waiting for pod pod-projected-configmaps-3c36e698-5dc3-4463-b86e-1c2819c396ad to disappear
    May  4 11:44:59.595: INFO: Pod pod-projected-configmaps-3c36e698-5dc3-4463-b86e-1c2819c396ad no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    May  4 11:44:59.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1060" for this suite. 05/04/23 11:44:59.605
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:44:59.618
May  4 11:44:59.618: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename pod-network-test 05/04/23 11:44:59.619
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:44:59.641
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:44:59.644
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-6124 05/04/23 11:44:59.648
STEP: creating a selector 05/04/23 11:44:59.648
STEP: Creating the service pods in kubernetes 05/04/23 11:44:59.648
May  4 11:44:59.648: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May  4 11:44:59.732: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6124" to be "running and ready"
May  4 11:44:59.745: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 13.126657ms
May  4 11:44:59.745: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May  4 11:45:01.751: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019509647s
May  4 11:45:01.751: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May  4 11:45:03.751: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.018884135s
May  4 11:45:03.751: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  4 11:45:05.751: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.019602809s
May  4 11:45:05.751: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  4 11:45:07.751: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.018976065s
May  4 11:45:07.751: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  4 11:45:09.752: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.020603512s
May  4 11:45:09.752: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  4 11:45:11.751: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.019287805s
May  4 11:45:11.751: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  4 11:45:13.751: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.019306185s
May  4 11:45:13.751: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  4 11:45:15.752: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.02043008s
May  4 11:45:15.752: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  4 11:45:17.752: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.020199944s
May  4 11:45:17.752: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  4 11:45:19.750: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.018751472s
May  4 11:45:19.751: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  4 11:45:21.751: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.018800297s
May  4 11:45:21.751: INFO: The phase of Pod netserver-0 is Running (Ready = true)
May  4 11:45:21.751: INFO: Pod "netserver-0" satisfied condition "running and ready"
May  4 11:45:21.755: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6124" to be "running and ready"
May  4 11:45:21.760: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.649487ms
May  4 11:45:21.760: INFO: The phase of Pod netserver-1 is Running (Ready = true)
May  4 11:45:21.760: INFO: Pod "netserver-1" satisfied condition "running and ready"
May  4 11:45:21.768: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6124" to be "running and ready"
May  4 11:45:21.772: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.769932ms
May  4 11:45:21.772: INFO: The phase of Pod netserver-2 is Running (Ready = true)
May  4 11:45:21.772: INFO: Pod "netserver-2" satisfied condition "running and ready"
May  4 11:45:21.778: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-6124" to be "running and ready"
May  4 11:45:21.783: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 5.409364ms
May  4 11:45:21.783: INFO: The phase of Pod netserver-3 is Running (Ready = true)
May  4 11:45:21.783: INFO: Pod "netserver-3" satisfied condition "running and ready"
May  4 11:45:21.789: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-6124" to be "running and ready"
May  4 11:45:21.794: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 4.901386ms
May  4 11:45:21.794: INFO: The phase of Pod netserver-4 is Running (Ready = true)
May  4 11:45:21.794: INFO: Pod "netserver-4" satisfied condition "running and ready"
STEP: Creating test pods 05/04/23 11:45:21.799
May  4 11:45:21.813: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6124" to be "running"
May  4 11:45:21.822: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.3398ms
May  4 11:45:23.827: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.014120346s
May  4 11:45:23.827: INFO: Pod "test-container-pod" satisfied condition "running"
May  4 11:45:23.831: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
May  4 11:45:23.831: INFO: Breadth first check of 10.20.11.196 on host 10.0.1.189...
May  4 11:45:23.836: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.83.9:9080/dial?request=hostname&protocol=http&host=10.20.11.196&port=8083&tries=1'] Namespace:pod-network-test-6124 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  4 11:45:23.836: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
May  4 11:45:23.836: INFO: ExecWithOptions: Clientset creation
May  4 11:45:23.836: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-6124/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.20.83.9%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.20.11.196%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May  4 11:45:23.924: INFO: Waiting for responses: map[]
May  4 11:45:23.924: INFO: reached 10.20.11.196 after 0/1 tries
May  4 11:45:23.924: INFO: Breadth first check of 10.20.142.134 on host 10.0.1.216...
May  4 11:45:23.929: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.83.9:9080/dial?request=hostname&protocol=http&host=10.20.142.134&port=8083&tries=1'] Namespace:pod-network-test-6124 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  4 11:45:23.929: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
May  4 11:45:23.929: INFO: ExecWithOptions: Clientset creation
May  4 11:45:23.930: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-6124/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.20.83.9%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.20.142.134%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May  4 11:45:24.021: INFO: Waiting for responses: map[]
May  4 11:45:24.021: INFO: reached 10.20.142.134 after 0/1 tries
May  4 11:45:24.021: INFO: Breadth first check of 10.20.83.8 on host 10.0.1.224...
May  4 11:45:24.025: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.83.9:9080/dial?request=hostname&protocol=http&host=10.20.83.8&port=8083&tries=1'] Namespace:pod-network-test-6124 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  4 11:45:24.025: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
May  4 11:45:24.026: INFO: ExecWithOptions: Clientset creation
May  4 11:45:24.026: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-6124/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.20.83.9%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.20.83.8%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May  4 11:45:24.108: INFO: Waiting for responses: map[]
May  4 11:45:24.108: INFO: reached 10.20.83.8 after 0/1 tries
May  4 11:45:24.108: INFO: Breadth first check of 10.20.92.134 on host 10.0.1.232...
May  4 11:45:24.113: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.83.9:9080/dial?request=hostname&protocol=http&host=10.20.92.134&port=8083&tries=1'] Namespace:pod-network-test-6124 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  4 11:45:24.113: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
May  4 11:45:24.113: INFO: ExecWithOptions: Clientset creation
May  4 11:45:24.113: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-6124/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.20.83.9%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.20.92.134%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May  4 11:45:24.188: INFO: Waiting for responses: map[]
May  4 11:45:24.189: INFO: reached 10.20.92.134 after 0/1 tries
May  4 11:45:24.189: INFO: Breadth first check of 10.20.1.132 on host 10.0.1.253...
May  4 11:45:24.194: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.83.9:9080/dial?request=hostname&protocol=http&host=10.20.1.132&port=8083&tries=1'] Namespace:pod-network-test-6124 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  4 11:45:24.194: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
May  4 11:45:24.195: INFO: ExecWithOptions: Clientset creation
May  4 11:45:24.196: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-6124/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.20.83.9%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.20.1.132%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May  4 11:45:24.278: INFO: Waiting for responses: map[]
May  4 11:45:24.278: INFO: reached 10.20.1.132 after 0/1 tries
May  4 11:45:24.278: INFO: Going to retry 0 out of 5 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
May  4 11:45:24.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6124" for this suite. 05/04/23 11:45:24.293
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","completed":6,"skipped":112,"failed":0}
------------------------------
• [SLOW TEST] [24.685 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:44:59.618
    May  4 11:44:59.618: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename pod-network-test 05/04/23 11:44:59.619
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:44:59.641
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:44:59.644
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-6124 05/04/23 11:44:59.648
    STEP: creating a selector 05/04/23 11:44:59.648
    STEP: Creating the service pods in kubernetes 05/04/23 11:44:59.648
    May  4 11:44:59.648: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    May  4 11:44:59.732: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6124" to be "running and ready"
    May  4 11:44:59.745: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 13.126657ms
    May  4 11:44:59.745: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    May  4 11:45:01.751: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019509647s
    May  4 11:45:01.751: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    May  4 11:45:03.751: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.018884135s
    May  4 11:45:03.751: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  4 11:45:05.751: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.019602809s
    May  4 11:45:05.751: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  4 11:45:07.751: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.018976065s
    May  4 11:45:07.751: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  4 11:45:09.752: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.020603512s
    May  4 11:45:09.752: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  4 11:45:11.751: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.019287805s
    May  4 11:45:11.751: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  4 11:45:13.751: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.019306185s
    May  4 11:45:13.751: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  4 11:45:15.752: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.02043008s
    May  4 11:45:15.752: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  4 11:45:17.752: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.020199944s
    May  4 11:45:17.752: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  4 11:45:19.750: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.018751472s
    May  4 11:45:19.751: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  4 11:45:21.751: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.018800297s
    May  4 11:45:21.751: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    May  4 11:45:21.751: INFO: Pod "netserver-0" satisfied condition "running and ready"
    May  4 11:45:21.755: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6124" to be "running and ready"
    May  4 11:45:21.760: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.649487ms
    May  4 11:45:21.760: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    May  4 11:45:21.760: INFO: Pod "netserver-1" satisfied condition "running and ready"
    May  4 11:45:21.768: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6124" to be "running and ready"
    May  4 11:45:21.772: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.769932ms
    May  4 11:45:21.772: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    May  4 11:45:21.772: INFO: Pod "netserver-2" satisfied condition "running and ready"
    May  4 11:45:21.778: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-6124" to be "running and ready"
    May  4 11:45:21.783: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 5.409364ms
    May  4 11:45:21.783: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    May  4 11:45:21.783: INFO: Pod "netserver-3" satisfied condition "running and ready"
    May  4 11:45:21.789: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-6124" to be "running and ready"
    May  4 11:45:21.794: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 4.901386ms
    May  4 11:45:21.794: INFO: The phase of Pod netserver-4 is Running (Ready = true)
    May  4 11:45:21.794: INFO: Pod "netserver-4" satisfied condition "running and ready"
    STEP: Creating test pods 05/04/23 11:45:21.799
    May  4 11:45:21.813: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6124" to be "running"
    May  4 11:45:21.822: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.3398ms
    May  4 11:45:23.827: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.014120346s
    May  4 11:45:23.827: INFO: Pod "test-container-pod" satisfied condition "running"
    May  4 11:45:23.831: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
    May  4 11:45:23.831: INFO: Breadth first check of 10.20.11.196 on host 10.0.1.189...
    May  4 11:45:23.836: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.83.9:9080/dial?request=hostname&protocol=http&host=10.20.11.196&port=8083&tries=1'] Namespace:pod-network-test-6124 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  4 11:45:23.836: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    May  4 11:45:23.836: INFO: ExecWithOptions: Clientset creation
    May  4 11:45:23.836: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-6124/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.20.83.9%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.20.11.196%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May  4 11:45:23.924: INFO: Waiting for responses: map[]
    May  4 11:45:23.924: INFO: reached 10.20.11.196 after 0/1 tries
    May  4 11:45:23.924: INFO: Breadth first check of 10.20.142.134 on host 10.0.1.216...
    May  4 11:45:23.929: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.83.9:9080/dial?request=hostname&protocol=http&host=10.20.142.134&port=8083&tries=1'] Namespace:pod-network-test-6124 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  4 11:45:23.929: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    May  4 11:45:23.929: INFO: ExecWithOptions: Clientset creation
    May  4 11:45:23.930: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-6124/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.20.83.9%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.20.142.134%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May  4 11:45:24.021: INFO: Waiting for responses: map[]
    May  4 11:45:24.021: INFO: reached 10.20.142.134 after 0/1 tries
    May  4 11:45:24.021: INFO: Breadth first check of 10.20.83.8 on host 10.0.1.224...
    May  4 11:45:24.025: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.83.9:9080/dial?request=hostname&protocol=http&host=10.20.83.8&port=8083&tries=1'] Namespace:pod-network-test-6124 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  4 11:45:24.025: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    May  4 11:45:24.026: INFO: ExecWithOptions: Clientset creation
    May  4 11:45:24.026: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-6124/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.20.83.9%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.20.83.8%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May  4 11:45:24.108: INFO: Waiting for responses: map[]
    May  4 11:45:24.108: INFO: reached 10.20.83.8 after 0/1 tries
    May  4 11:45:24.108: INFO: Breadth first check of 10.20.92.134 on host 10.0.1.232...
    May  4 11:45:24.113: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.83.9:9080/dial?request=hostname&protocol=http&host=10.20.92.134&port=8083&tries=1'] Namespace:pod-network-test-6124 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  4 11:45:24.113: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    May  4 11:45:24.113: INFO: ExecWithOptions: Clientset creation
    May  4 11:45:24.113: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-6124/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.20.83.9%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.20.92.134%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May  4 11:45:24.188: INFO: Waiting for responses: map[]
    May  4 11:45:24.189: INFO: reached 10.20.92.134 after 0/1 tries
    May  4 11:45:24.189: INFO: Breadth first check of 10.20.1.132 on host 10.0.1.253...
    May  4 11:45:24.194: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.83.9:9080/dial?request=hostname&protocol=http&host=10.20.1.132&port=8083&tries=1'] Namespace:pod-network-test-6124 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  4 11:45:24.194: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    May  4 11:45:24.195: INFO: ExecWithOptions: Clientset creation
    May  4 11:45:24.196: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-6124/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.20.83.9%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.20.1.132%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May  4 11:45:24.278: INFO: Waiting for responses: map[]
    May  4 11:45:24.278: INFO: reached 10.20.1.132 after 0/1 tries
    May  4 11:45:24.278: INFO: Going to retry 0 out of 5 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    May  4 11:45:24.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-6124" for this suite. 05/04/23 11:45:24.293
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:45:24.304
May  4 11:45:24.304: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename projected 05/04/23 11:45:24.305
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:45:24.323
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:45:24.33
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
STEP: Creating the pod 05/04/23 11:45:24.334
May  4 11:45:24.344: INFO: Waiting up to 5m0s for pod "labelsupdatebf56465e-8bf6-4a70-95ba-0183c34d088d" in namespace "projected-6827" to be "running and ready"
May  4 11:45:24.348: INFO: Pod "labelsupdatebf56465e-8bf6-4a70-95ba-0183c34d088d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.849987ms
May  4 11:45:24.348: INFO: The phase of Pod labelsupdatebf56465e-8bf6-4a70-95ba-0183c34d088d is Pending, waiting for it to be Running (with Ready = true)
May  4 11:45:26.353: INFO: Pod "labelsupdatebf56465e-8bf6-4a70-95ba-0183c34d088d": Phase="Running", Reason="", readiness=true. Elapsed: 2.008694315s
May  4 11:45:26.353: INFO: The phase of Pod labelsupdatebf56465e-8bf6-4a70-95ba-0183c34d088d is Running (Ready = true)
May  4 11:45:26.353: INFO: Pod "labelsupdatebf56465e-8bf6-4a70-95ba-0183c34d088d" satisfied condition "running and ready"
May  4 11:45:26.890: INFO: Successfully updated pod "labelsupdatebf56465e-8bf6-4a70-95ba-0183c34d088d"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
May  4 11:45:28.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6827" for this suite. 05/04/23 11:45:28.917
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","completed":7,"skipped":118,"failed":0}
------------------------------
• [4.621 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:45:24.304
    May  4 11:45:24.304: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename projected 05/04/23 11:45:24.305
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:45:24.323
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:45:24.33
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:129
    STEP: Creating the pod 05/04/23 11:45:24.334
    May  4 11:45:24.344: INFO: Waiting up to 5m0s for pod "labelsupdatebf56465e-8bf6-4a70-95ba-0183c34d088d" in namespace "projected-6827" to be "running and ready"
    May  4 11:45:24.348: INFO: Pod "labelsupdatebf56465e-8bf6-4a70-95ba-0183c34d088d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.849987ms
    May  4 11:45:24.348: INFO: The phase of Pod labelsupdatebf56465e-8bf6-4a70-95ba-0183c34d088d is Pending, waiting for it to be Running (with Ready = true)
    May  4 11:45:26.353: INFO: Pod "labelsupdatebf56465e-8bf6-4a70-95ba-0183c34d088d": Phase="Running", Reason="", readiness=true. Elapsed: 2.008694315s
    May  4 11:45:26.353: INFO: The phase of Pod labelsupdatebf56465e-8bf6-4a70-95ba-0183c34d088d is Running (Ready = true)
    May  4 11:45:26.353: INFO: Pod "labelsupdatebf56465e-8bf6-4a70-95ba-0183c34d088d" satisfied condition "running and ready"
    May  4 11:45:26.890: INFO: Successfully updated pod "labelsupdatebf56465e-8bf6-4a70-95ba-0183c34d088d"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    May  4 11:45:28.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6827" for this suite. 05/04/23 11:45:28.917
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:45:28.927
May  4 11:45:28.927: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename watch 05/04/23 11:45:28.928
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:45:28.947
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:45:28.95
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 05/04/23 11:45:28.953
STEP: starting a background goroutine to produce watch events 05/04/23 11:45:28.96
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 05/04/23 11:45:28.96
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
May  4 11:45:31.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9472" for this suite. 05/04/23 11:45:31.791
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","completed":8,"skipped":143,"failed":0}
------------------------------
• [2.914 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:45:28.927
    May  4 11:45:28.927: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename watch 05/04/23 11:45:28.928
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:45:28.947
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:45:28.95
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 05/04/23 11:45:28.953
    STEP: starting a background goroutine to produce watch events 05/04/23 11:45:28.96
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 05/04/23 11:45:28.96
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    May  4 11:45:31.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-9472" for this suite. 05/04/23 11:45:31.791
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:45:31.841
May  4 11:45:31.841: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename cronjob 05/04/23 11:45:31.842
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:45:31.862
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:45:31.866
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 05/04/23 11:45:31.871
STEP: creating 05/04/23 11:45:31.871
STEP: getting 05/04/23 11:45:31.879
STEP: listing 05/04/23 11:45:31.888
STEP: watching 05/04/23 11:45:31.894
May  4 11:45:31.894: INFO: starting watch
STEP: cluster-wide listing 05/04/23 11:45:31.896
STEP: cluster-wide watching 05/04/23 11:45:31.901
May  4 11:45:31.901: INFO: starting watch
STEP: patching 05/04/23 11:45:31.902
STEP: updating 05/04/23 11:45:31.917
May  4 11:45:31.928: INFO: waiting for watch events with expected annotations
May  4 11:45:31.928: INFO: saw patched and updated annotations
STEP: patching /status 05/04/23 11:45:31.928
STEP: updating /status 05/04/23 11:45:31.938
STEP: get /status 05/04/23 11:45:31.948
STEP: deleting 05/04/23 11:45:31.953
STEP: deleting a collection 05/04/23 11:45:31.975
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
May  4 11:45:31.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8707" for this suite. 05/04/23 11:45:31.999
{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","completed":9,"skipped":153,"failed":0}
------------------------------
• [0.169 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:45:31.841
    May  4 11:45:31.841: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename cronjob 05/04/23 11:45:31.842
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:45:31.862
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:45:31.866
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 05/04/23 11:45:31.871
    STEP: creating 05/04/23 11:45:31.871
    STEP: getting 05/04/23 11:45:31.879
    STEP: listing 05/04/23 11:45:31.888
    STEP: watching 05/04/23 11:45:31.894
    May  4 11:45:31.894: INFO: starting watch
    STEP: cluster-wide listing 05/04/23 11:45:31.896
    STEP: cluster-wide watching 05/04/23 11:45:31.901
    May  4 11:45:31.901: INFO: starting watch
    STEP: patching 05/04/23 11:45:31.902
    STEP: updating 05/04/23 11:45:31.917
    May  4 11:45:31.928: INFO: waiting for watch events with expected annotations
    May  4 11:45:31.928: INFO: saw patched and updated annotations
    STEP: patching /status 05/04/23 11:45:31.928
    STEP: updating /status 05/04/23 11:45:31.938
    STEP: get /status 05/04/23 11:45:31.948
    STEP: deleting 05/04/23 11:45:31.953
    STEP: deleting a collection 05/04/23 11:45:31.975
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    May  4 11:45:31.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-8707" for this suite. 05/04/23 11:45:31.999
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:45:32.014
May  4 11:45:32.014: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename emptydir 05/04/23 11:45:32.015
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:45:32.034
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:45:32.038
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
STEP: Creating a pod to test emptydir volume type on node default medium 05/04/23 11:45:32.041
May  4 11:45:32.053: INFO: Waiting up to 5m0s for pod "pod-08caaac9-fe1e-4103-bfff-3fa76ef98183" in namespace "emptydir-4567" to be "Succeeded or Failed"
May  4 11:45:32.066: INFO: Pod "pod-08caaac9-fe1e-4103-bfff-3fa76ef98183": Phase="Pending", Reason="", readiness=false. Elapsed: 12.476225ms
May  4 11:45:34.074: INFO: Pod "pod-08caaac9-fe1e-4103-bfff-3fa76ef98183": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020440097s
May  4 11:45:36.071: INFO: Pod "pod-08caaac9-fe1e-4103-bfff-3fa76ef98183": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017765245s
STEP: Saw pod success 05/04/23 11:45:36.071
May  4 11:45:36.071: INFO: Pod "pod-08caaac9-fe1e-4103-bfff-3fa76ef98183" satisfied condition "Succeeded or Failed"
May  4 11:45:36.075: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-08caaac9-fe1e-4103-bfff-3fa76ef98183 container test-container: <nil>
STEP: delete the pod 05/04/23 11:45:36.083
May  4 11:45:36.102: INFO: Waiting for pod pod-08caaac9-fe1e-4103-bfff-3fa76ef98183 to disappear
May  4 11:45:36.106: INFO: Pod pod-08caaac9-fe1e-4103-bfff-3fa76ef98183 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May  4 11:45:36.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4567" for this suite. 05/04/23 11:45:36.119
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":10,"skipped":186,"failed":0}
------------------------------
• [4.115 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:45:32.014
    May  4 11:45:32.014: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename emptydir 05/04/23 11:45:32.015
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:45:32.034
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:45:32.038
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:156
    STEP: Creating a pod to test emptydir volume type on node default medium 05/04/23 11:45:32.041
    May  4 11:45:32.053: INFO: Waiting up to 5m0s for pod "pod-08caaac9-fe1e-4103-bfff-3fa76ef98183" in namespace "emptydir-4567" to be "Succeeded or Failed"
    May  4 11:45:32.066: INFO: Pod "pod-08caaac9-fe1e-4103-bfff-3fa76ef98183": Phase="Pending", Reason="", readiness=false. Elapsed: 12.476225ms
    May  4 11:45:34.074: INFO: Pod "pod-08caaac9-fe1e-4103-bfff-3fa76ef98183": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020440097s
    May  4 11:45:36.071: INFO: Pod "pod-08caaac9-fe1e-4103-bfff-3fa76ef98183": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017765245s
    STEP: Saw pod success 05/04/23 11:45:36.071
    May  4 11:45:36.071: INFO: Pod "pod-08caaac9-fe1e-4103-bfff-3fa76ef98183" satisfied condition "Succeeded or Failed"
    May  4 11:45:36.075: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-08caaac9-fe1e-4103-bfff-3fa76ef98183 container test-container: <nil>
    STEP: delete the pod 05/04/23 11:45:36.083
    May  4 11:45:36.102: INFO: Waiting for pod pod-08caaac9-fe1e-4103-bfff-3fa76ef98183 to disappear
    May  4 11:45:36.106: INFO: Pod pod-08caaac9-fe1e-4103-bfff-3fa76ef98183 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May  4 11:45:36.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4567" for this suite. 05/04/23 11:45:36.119
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:45:36.128
May  4 11:45:36.128: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename configmap 05/04/23 11:45:36.13
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:45:36.148
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:45:36.151
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
STEP: Creating configMap with name configmap-test-volume-bfa38fea-14dc-40bf-91ca-d1797ff91adb 05/04/23 11:45:36.155
STEP: Creating a pod to test consume configMaps 05/04/23 11:45:36.161
May  4 11:45:36.214: INFO: Waiting up to 5m0s for pod "pod-configmaps-fe51cd32-9fa7-4186-9eeb-5094b1dab490" in namespace "configmap-3665" to be "Succeeded or Failed"
May  4 11:45:36.220: INFO: Pod "pod-configmaps-fe51cd32-9fa7-4186-9eeb-5094b1dab490": Phase="Pending", Reason="", readiness=false. Elapsed: 5.960085ms
May  4 11:45:38.224: INFO: Pod "pod-configmaps-fe51cd32-9fa7-4186-9eeb-5094b1dab490": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010322033s
May  4 11:45:40.226: INFO: Pod "pod-configmaps-fe51cd32-9fa7-4186-9eeb-5094b1dab490": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012219777s
STEP: Saw pod success 05/04/23 11:45:40.226
May  4 11:45:40.226: INFO: Pod "pod-configmaps-fe51cd32-9fa7-4186-9eeb-5094b1dab490" satisfied condition "Succeeded or Failed"
May  4 11:45:40.230: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-configmaps-fe51cd32-9fa7-4186-9eeb-5094b1dab490 container agnhost-container: <nil>
STEP: delete the pod 05/04/23 11:45:40.238
May  4 11:45:40.255: INFO: Waiting for pod pod-configmaps-fe51cd32-9fa7-4186-9eeb-5094b1dab490 to disappear
May  4 11:45:40.259: INFO: Pod pod-configmaps-fe51cd32-9fa7-4186-9eeb-5094b1dab490 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
May  4 11:45:40.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3665" for this suite. 05/04/23 11:45:40.278
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":11,"skipped":186,"failed":0}
------------------------------
• [4.159 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:45:36.128
    May  4 11:45:36.128: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename configmap 05/04/23 11:45:36.13
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:45:36.148
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:45:36.151
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:56
    STEP: Creating configMap with name configmap-test-volume-bfa38fea-14dc-40bf-91ca-d1797ff91adb 05/04/23 11:45:36.155
    STEP: Creating a pod to test consume configMaps 05/04/23 11:45:36.161
    May  4 11:45:36.214: INFO: Waiting up to 5m0s for pod "pod-configmaps-fe51cd32-9fa7-4186-9eeb-5094b1dab490" in namespace "configmap-3665" to be "Succeeded or Failed"
    May  4 11:45:36.220: INFO: Pod "pod-configmaps-fe51cd32-9fa7-4186-9eeb-5094b1dab490": Phase="Pending", Reason="", readiness=false. Elapsed: 5.960085ms
    May  4 11:45:38.224: INFO: Pod "pod-configmaps-fe51cd32-9fa7-4186-9eeb-5094b1dab490": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010322033s
    May  4 11:45:40.226: INFO: Pod "pod-configmaps-fe51cd32-9fa7-4186-9eeb-5094b1dab490": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012219777s
    STEP: Saw pod success 05/04/23 11:45:40.226
    May  4 11:45:40.226: INFO: Pod "pod-configmaps-fe51cd32-9fa7-4186-9eeb-5094b1dab490" satisfied condition "Succeeded or Failed"
    May  4 11:45:40.230: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-configmaps-fe51cd32-9fa7-4186-9eeb-5094b1dab490 container agnhost-container: <nil>
    STEP: delete the pod 05/04/23 11:45:40.238
    May  4 11:45:40.255: INFO: Waiting for pod pod-configmaps-fe51cd32-9fa7-4186-9eeb-5094b1dab490 to disappear
    May  4 11:45:40.259: INFO: Pod pod-configmaps-fe51cd32-9fa7-4186-9eeb-5094b1dab490 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    May  4 11:45:40.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3665" for this suite. 05/04/23 11:45:40.278
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:45:40.29
May  4 11:45:40.290: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename downward-api 05/04/23 11:45:40.291
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:45:40.313
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:45:40.318
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
STEP: Creating a pod to test downward api env vars 05/04/23 11:45:40.323
May  4 11:45:40.335: INFO: Waiting up to 5m0s for pod "downward-api-76c5832c-dea9-45f9-9fde-820d60c80455" in namespace "downward-api-6079" to be "Succeeded or Failed"
May  4 11:45:40.343: INFO: Pod "downward-api-76c5832c-dea9-45f9-9fde-820d60c80455": Phase="Pending", Reason="", readiness=false. Elapsed: 7.346887ms
May  4 11:45:42.349: INFO: Pod "downward-api-76c5832c-dea9-45f9-9fde-820d60c80455": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014210519s
May  4 11:45:44.349: INFO: Pod "downward-api-76c5832c-dea9-45f9-9fde-820d60c80455": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01390658s
May  4 11:45:46.350: INFO: Pod "downward-api-76c5832c-dea9-45f9-9fde-820d60c80455": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014339829s
STEP: Saw pod success 05/04/23 11:45:46.35
May  4 11:45:46.350: INFO: Pod "downward-api-76c5832c-dea9-45f9-9fde-820d60c80455" satisfied condition "Succeeded or Failed"
May  4 11:45:46.354: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod downward-api-76c5832c-dea9-45f9-9fde-820d60c80455 container dapi-container: <nil>
STEP: delete the pod 05/04/23 11:45:46.363
May  4 11:45:46.380: INFO: Waiting for pod downward-api-76c5832c-dea9-45f9-9fde-820d60c80455 to disappear
May  4 11:45:46.385: INFO: Pod downward-api-76c5832c-dea9-45f9-9fde-820d60c80455 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
May  4 11:45:46.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6079" for this suite. 05/04/23 11:45:46.398
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","completed":12,"skipped":194,"failed":0}
------------------------------
• [SLOW TEST] [6.119 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:45:40.29
    May  4 11:45:40.290: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename downward-api 05/04/23 11:45:40.291
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:45:40.313
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:45:40.318
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:165
    STEP: Creating a pod to test downward api env vars 05/04/23 11:45:40.323
    May  4 11:45:40.335: INFO: Waiting up to 5m0s for pod "downward-api-76c5832c-dea9-45f9-9fde-820d60c80455" in namespace "downward-api-6079" to be "Succeeded or Failed"
    May  4 11:45:40.343: INFO: Pod "downward-api-76c5832c-dea9-45f9-9fde-820d60c80455": Phase="Pending", Reason="", readiness=false. Elapsed: 7.346887ms
    May  4 11:45:42.349: INFO: Pod "downward-api-76c5832c-dea9-45f9-9fde-820d60c80455": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014210519s
    May  4 11:45:44.349: INFO: Pod "downward-api-76c5832c-dea9-45f9-9fde-820d60c80455": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01390658s
    May  4 11:45:46.350: INFO: Pod "downward-api-76c5832c-dea9-45f9-9fde-820d60c80455": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014339829s
    STEP: Saw pod success 05/04/23 11:45:46.35
    May  4 11:45:46.350: INFO: Pod "downward-api-76c5832c-dea9-45f9-9fde-820d60c80455" satisfied condition "Succeeded or Failed"
    May  4 11:45:46.354: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod downward-api-76c5832c-dea9-45f9-9fde-820d60c80455 container dapi-container: <nil>
    STEP: delete the pod 05/04/23 11:45:46.363
    May  4 11:45:46.380: INFO: Waiting for pod downward-api-76c5832c-dea9-45f9-9fde-820d60c80455 to disappear
    May  4 11:45:46.385: INFO: Pod downward-api-76c5832c-dea9-45f9-9fde-820d60c80455 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    May  4 11:45:46.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6079" for this suite. 05/04/23 11:45:46.398
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:45:46.409
May  4 11:45:46.409: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename downward-api 05/04/23 11:45:46.41
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:45:46.434
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:45:46.438
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
STEP: Creating the pod 05/04/23 11:45:46.441
May  4 11:45:46.453: INFO: Waiting up to 5m0s for pod "annotationupdate7a20db89-9c5c-43d3-9a21-39634e1ab9df" in namespace "downward-api-5475" to be "running and ready"
May  4 11:45:46.462: INFO: Pod "annotationupdate7a20db89-9c5c-43d3-9a21-39634e1ab9df": Phase="Pending", Reason="", readiness=false. Elapsed: 9.41849ms
May  4 11:45:46.462: INFO: The phase of Pod annotationupdate7a20db89-9c5c-43d3-9a21-39634e1ab9df is Pending, waiting for it to be Running (with Ready = true)
May  4 11:45:48.467: INFO: Pod "annotationupdate7a20db89-9c5c-43d3-9a21-39634e1ab9df": Phase="Running", Reason="", readiness=true. Elapsed: 2.014255136s
May  4 11:45:48.467: INFO: The phase of Pod annotationupdate7a20db89-9c5c-43d3-9a21-39634e1ab9df is Running (Ready = true)
May  4 11:45:48.467: INFO: Pod "annotationupdate7a20db89-9c5c-43d3-9a21-39634e1ab9df" satisfied condition "running and ready"
May  4 11:45:48.997: INFO: Successfully updated pod "annotationupdate7a20db89-9c5c-43d3-9a21-39634e1ab9df"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
May  4 11:45:53.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5475" for this suite. 05/04/23 11:45:53.037
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","completed":13,"skipped":195,"failed":0}
------------------------------
• [SLOW TEST] [6.637 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:45:46.409
    May  4 11:45:46.409: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename downward-api 05/04/23 11:45:46.41
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:45:46.434
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:45:46.438
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:161
    STEP: Creating the pod 05/04/23 11:45:46.441
    May  4 11:45:46.453: INFO: Waiting up to 5m0s for pod "annotationupdate7a20db89-9c5c-43d3-9a21-39634e1ab9df" in namespace "downward-api-5475" to be "running and ready"
    May  4 11:45:46.462: INFO: Pod "annotationupdate7a20db89-9c5c-43d3-9a21-39634e1ab9df": Phase="Pending", Reason="", readiness=false. Elapsed: 9.41849ms
    May  4 11:45:46.462: INFO: The phase of Pod annotationupdate7a20db89-9c5c-43d3-9a21-39634e1ab9df is Pending, waiting for it to be Running (with Ready = true)
    May  4 11:45:48.467: INFO: Pod "annotationupdate7a20db89-9c5c-43d3-9a21-39634e1ab9df": Phase="Running", Reason="", readiness=true. Elapsed: 2.014255136s
    May  4 11:45:48.467: INFO: The phase of Pod annotationupdate7a20db89-9c5c-43d3-9a21-39634e1ab9df is Running (Ready = true)
    May  4 11:45:48.467: INFO: Pod "annotationupdate7a20db89-9c5c-43d3-9a21-39634e1ab9df" satisfied condition "running and ready"
    May  4 11:45:48.997: INFO: Successfully updated pod "annotationupdate7a20db89-9c5c-43d3-9a21-39634e1ab9df"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    May  4 11:45:53.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5475" for this suite. 05/04/23 11:45:53.037
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:45:53.047
May  4 11:45:53.047: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename pods 05/04/23 11:45:53.048
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:45:53.079
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:45:53.084
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
STEP: creating a Pod with a static label 05/04/23 11:45:53.122
STEP: watching for Pod to be ready 05/04/23 11:45:53.146
May  4 11:45:53.149: INFO: observed Pod pod-test in namespace pods-1473 in phase Pending with labels: map[test-pod-static:true] & conditions []
May  4 11:45:53.155: INFO: observed Pod pod-test in namespace pods-1473 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 11:45:53 +0000 UTC  }]
May  4 11:45:53.176: INFO: observed Pod pod-test in namespace pods-1473 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 11:45:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 11:45:53 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 11:45:53 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 11:45:53 +0000 UTC  }]
May  4 11:45:53.722: INFO: observed Pod pod-test in namespace pods-1473 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 11:45:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 11:45:53 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 11:45:53 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 11:45:53 +0000 UTC  }]
May  4 11:45:55.029: INFO: Found Pod pod-test in namespace pods-1473 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 11:45:53 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 11:45:55 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 11:45:55 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 11:45:53 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 05/04/23 11:45:55.036
STEP: getting the Pod and ensuring that it's patched 05/04/23 11:45:55.059
STEP: replacing the Pod's status Ready condition to False 05/04/23 11:45:55.065
STEP: check the Pod again to ensure its Ready conditions are False 05/04/23 11:45:55.083
STEP: deleting the Pod via a Collection with a LabelSelector 05/04/23 11:45:55.083
STEP: watching for the Pod to be deleted 05/04/23 11:45:55.099
May  4 11:45:55.102: INFO: observed event type MODIFIED
May  4 11:45:57.030: INFO: observed event type MODIFIED
May  4 11:45:57.361: INFO: observed event type MODIFIED
May  4 11:45:58.034: INFO: observed event type MODIFIED
May  4 11:45:58.043: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
May  4 11:45:58.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1473" for this suite. 05/04/23 11:45:58.066
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","completed":14,"skipped":220,"failed":0}
------------------------------
• [SLOW TEST] [5.030 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:45:53.047
    May  4 11:45:53.047: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename pods 05/04/23 11:45:53.048
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:45:53.079
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:45:53.084
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:895
    STEP: creating a Pod with a static label 05/04/23 11:45:53.122
    STEP: watching for Pod to be ready 05/04/23 11:45:53.146
    May  4 11:45:53.149: INFO: observed Pod pod-test in namespace pods-1473 in phase Pending with labels: map[test-pod-static:true] & conditions []
    May  4 11:45:53.155: INFO: observed Pod pod-test in namespace pods-1473 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 11:45:53 +0000 UTC  }]
    May  4 11:45:53.176: INFO: observed Pod pod-test in namespace pods-1473 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 11:45:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 11:45:53 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 11:45:53 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 11:45:53 +0000 UTC  }]
    May  4 11:45:53.722: INFO: observed Pod pod-test in namespace pods-1473 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 11:45:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 11:45:53 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 11:45:53 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 11:45:53 +0000 UTC  }]
    May  4 11:45:55.029: INFO: Found Pod pod-test in namespace pods-1473 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 11:45:53 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 11:45:55 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 11:45:55 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 11:45:53 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 05/04/23 11:45:55.036
    STEP: getting the Pod and ensuring that it's patched 05/04/23 11:45:55.059
    STEP: replacing the Pod's status Ready condition to False 05/04/23 11:45:55.065
    STEP: check the Pod again to ensure its Ready conditions are False 05/04/23 11:45:55.083
    STEP: deleting the Pod via a Collection with a LabelSelector 05/04/23 11:45:55.083
    STEP: watching for the Pod to be deleted 05/04/23 11:45:55.099
    May  4 11:45:55.102: INFO: observed event type MODIFIED
    May  4 11:45:57.030: INFO: observed event type MODIFIED
    May  4 11:45:57.361: INFO: observed event type MODIFIED
    May  4 11:45:58.034: INFO: observed event type MODIFIED
    May  4 11:45:58.043: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    May  4 11:45:58.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-1473" for this suite. 05/04/23 11:45:58.066
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:45:58.081
May  4 11:45:58.081: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename secrets 05/04/23 11:45:58.083
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:45:58.106
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:45:58.11
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
STEP: Creating secret with name secret-test-e2a7bb29-4e5a-4bab-b40b-8b73489a5424 05/04/23 11:45:58.112
STEP: Creating a pod to test consume secrets 05/04/23 11:45:58.121
May  4 11:45:58.141: INFO: Waiting up to 5m0s for pod "pod-secrets-cb8b23a0-dffe-46d7-b44a-74b9f64c7743" in namespace "secrets-2641" to be "Succeeded or Failed"
May  4 11:45:58.156: INFO: Pod "pod-secrets-cb8b23a0-dffe-46d7-b44a-74b9f64c7743": Phase="Pending", Reason="", readiness=false. Elapsed: 15.219284ms
May  4 11:46:00.162: INFO: Pod "pod-secrets-cb8b23a0-dffe-46d7-b44a-74b9f64c7743": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020637377s
May  4 11:46:02.162: INFO: Pod "pod-secrets-cb8b23a0-dffe-46d7-b44a-74b9f64c7743": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020628971s
STEP: Saw pod success 05/04/23 11:46:02.162
May  4 11:46:02.162: INFO: Pod "pod-secrets-cb8b23a0-dffe-46d7-b44a-74b9f64c7743" satisfied condition "Succeeded or Failed"
May  4 11:46:02.167: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod pod-secrets-cb8b23a0-dffe-46d7-b44a-74b9f64c7743 container secret-volume-test: <nil>
STEP: delete the pod 05/04/23 11:46:02.175
May  4 11:46:02.192: INFO: Waiting for pod pod-secrets-cb8b23a0-dffe-46d7-b44a-74b9f64c7743 to disappear
May  4 11:46:02.198: INFO: Pod pod-secrets-cb8b23a0-dffe-46d7-b44a-74b9f64c7743 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
May  4 11:46:02.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2641" for this suite. 05/04/23 11:46:02.213
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","completed":15,"skipped":276,"failed":0}
------------------------------
• [4.140 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:45:58.081
    May  4 11:45:58.081: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename secrets 05/04/23 11:45:58.083
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:45:58.106
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:45:58.11
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:46
    STEP: Creating secret with name secret-test-e2a7bb29-4e5a-4bab-b40b-8b73489a5424 05/04/23 11:45:58.112
    STEP: Creating a pod to test consume secrets 05/04/23 11:45:58.121
    May  4 11:45:58.141: INFO: Waiting up to 5m0s for pod "pod-secrets-cb8b23a0-dffe-46d7-b44a-74b9f64c7743" in namespace "secrets-2641" to be "Succeeded or Failed"
    May  4 11:45:58.156: INFO: Pod "pod-secrets-cb8b23a0-dffe-46d7-b44a-74b9f64c7743": Phase="Pending", Reason="", readiness=false. Elapsed: 15.219284ms
    May  4 11:46:00.162: INFO: Pod "pod-secrets-cb8b23a0-dffe-46d7-b44a-74b9f64c7743": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020637377s
    May  4 11:46:02.162: INFO: Pod "pod-secrets-cb8b23a0-dffe-46d7-b44a-74b9f64c7743": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020628971s
    STEP: Saw pod success 05/04/23 11:46:02.162
    May  4 11:46:02.162: INFO: Pod "pod-secrets-cb8b23a0-dffe-46d7-b44a-74b9f64c7743" satisfied condition "Succeeded or Failed"
    May  4 11:46:02.167: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod pod-secrets-cb8b23a0-dffe-46d7-b44a-74b9f64c7743 container secret-volume-test: <nil>
    STEP: delete the pod 05/04/23 11:46:02.175
    May  4 11:46:02.192: INFO: Waiting for pod pod-secrets-cb8b23a0-dffe-46d7-b44a-74b9f64c7743 to disappear
    May  4 11:46:02.198: INFO: Pod pod-secrets-cb8b23a0-dffe-46d7-b44a-74b9f64c7743 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    May  4 11:46:02.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-2641" for this suite. 05/04/23 11:46:02.213
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:46:02.222
May  4 11:46:02.222: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename container-probe 05/04/23 11:46:02.228
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:46:02.247
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:46:02.261
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
STEP: Creating pod liveness-215adf6a-a1d7-4849-8f6a-871b86ce432b in namespace container-probe-6210 05/04/23 11:46:02.268
May  4 11:46:02.281: INFO: Waiting up to 5m0s for pod "liveness-215adf6a-a1d7-4849-8f6a-871b86ce432b" in namespace "container-probe-6210" to be "not pending"
May  4 11:46:02.292: INFO: Pod "liveness-215adf6a-a1d7-4849-8f6a-871b86ce432b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.542517ms
May  4 11:46:04.297: INFO: Pod "liveness-215adf6a-a1d7-4849-8f6a-871b86ce432b": Phase="Running", Reason="", readiness=true. Elapsed: 2.016190045s
May  4 11:46:04.297: INFO: Pod "liveness-215adf6a-a1d7-4849-8f6a-871b86ce432b" satisfied condition "not pending"
May  4 11:46:04.297: INFO: Started pod liveness-215adf6a-a1d7-4849-8f6a-871b86ce432b in namespace container-probe-6210
STEP: checking the pod's current state and verifying that restartCount is present 05/04/23 11:46:04.297
May  4 11:46:04.301: INFO: Initial restart count of pod liveness-215adf6a-a1d7-4849-8f6a-871b86ce432b is 0
May  4 11:46:24.373: INFO: Restart count of pod container-probe-6210/liveness-215adf6a-a1d7-4849-8f6a-871b86ce432b is now 1 (20.071198217s elapsed)
May  4 11:46:44.440: INFO: Restart count of pod container-probe-6210/liveness-215adf6a-a1d7-4849-8f6a-871b86ce432b is now 2 (40.138773901s elapsed)
May  4 11:47:04.500: INFO: Restart count of pod container-probe-6210/liveness-215adf6a-a1d7-4849-8f6a-871b86ce432b is now 3 (1m0.198685032s elapsed)
May  4 11:47:24.562: INFO: Restart count of pod container-probe-6210/liveness-215adf6a-a1d7-4849-8f6a-871b86ce432b is now 4 (1m20.260450871s elapsed)
May  4 11:48:34.772: INFO: Restart count of pod container-probe-6210/liveness-215adf6a-a1d7-4849-8f6a-871b86ce432b is now 5 (2m30.47091741s elapsed)
STEP: deleting the pod 05/04/23 11:48:34.772
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
May  4 11:48:34.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6210" for this suite. 05/04/23 11:48:34.802
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","completed":16,"skipped":297,"failed":0}
------------------------------
• [SLOW TEST] [152.588 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:46:02.222
    May  4 11:46:02.222: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename container-probe 05/04/23 11:46:02.228
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:46:02.247
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:46:02.261
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:195
    STEP: Creating pod liveness-215adf6a-a1d7-4849-8f6a-871b86ce432b in namespace container-probe-6210 05/04/23 11:46:02.268
    May  4 11:46:02.281: INFO: Waiting up to 5m0s for pod "liveness-215adf6a-a1d7-4849-8f6a-871b86ce432b" in namespace "container-probe-6210" to be "not pending"
    May  4 11:46:02.292: INFO: Pod "liveness-215adf6a-a1d7-4849-8f6a-871b86ce432b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.542517ms
    May  4 11:46:04.297: INFO: Pod "liveness-215adf6a-a1d7-4849-8f6a-871b86ce432b": Phase="Running", Reason="", readiness=true. Elapsed: 2.016190045s
    May  4 11:46:04.297: INFO: Pod "liveness-215adf6a-a1d7-4849-8f6a-871b86ce432b" satisfied condition "not pending"
    May  4 11:46:04.297: INFO: Started pod liveness-215adf6a-a1d7-4849-8f6a-871b86ce432b in namespace container-probe-6210
    STEP: checking the pod's current state and verifying that restartCount is present 05/04/23 11:46:04.297
    May  4 11:46:04.301: INFO: Initial restart count of pod liveness-215adf6a-a1d7-4849-8f6a-871b86ce432b is 0
    May  4 11:46:24.373: INFO: Restart count of pod container-probe-6210/liveness-215adf6a-a1d7-4849-8f6a-871b86ce432b is now 1 (20.071198217s elapsed)
    May  4 11:46:44.440: INFO: Restart count of pod container-probe-6210/liveness-215adf6a-a1d7-4849-8f6a-871b86ce432b is now 2 (40.138773901s elapsed)
    May  4 11:47:04.500: INFO: Restart count of pod container-probe-6210/liveness-215adf6a-a1d7-4849-8f6a-871b86ce432b is now 3 (1m0.198685032s elapsed)
    May  4 11:47:24.562: INFO: Restart count of pod container-probe-6210/liveness-215adf6a-a1d7-4849-8f6a-871b86ce432b is now 4 (1m20.260450871s elapsed)
    May  4 11:48:34.772: INFO: Restart count of pod container-probe-6210/liveness-215adf6a-a1d7-4849-8f6a-871b86ce432b is now 5 (2m30.47091741s elapsed)
    STEP: deleting the pod 05/04/23 11:48:34.772
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    May  4 11:48:34.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-6210" for this suite. 05/04/23 11:48:34.802
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:48:34.814
May  4 11:48:34.814: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename deployment 05/04/23 11:48:34.814
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:48:34.836
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:48:34.842
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
May  4 11:48:34.866: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 05/04/23 11:48:34.866
May  4 11:48:34.866: INFO: Waiting up to 5m0s for pod "test-cleanup-controller-vmnwm" in namespace "deployment-1362" to be "running"
May  4 11:48:34.876: INFO: Pod "test-cleanup-controller-vmnwm": Phase="Pending", Reason="", readiness=false. Elapsed: 9.382073ms
May  4 11:48:36.881: INFO: Pod "test-cleanup-controller-vmnwm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014444048s
May  4 11:48:38.881: INFO: Pod "test-cleanup-controller-vmnwm": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015186446s
May  4 11:48:40.883: INFO: Pod "test-cleanup-controller-vmnwm": Phase="Running", Reason="", readiness=true. Elapsed: 6.016715932s
May  4 11:48:40.883: INFO: Pod "test-cleanup-controller-vmnwm" satisfied condition "running"
May  4 11:48:40.883: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 05/04/23 11:48:40.899
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May  4 11:48:42.923: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-1362  e3398b69-69ca-4b67-9d70-0ec5588722b4 7371 1 2023-05-04 11:48:40 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-05-04 11:48:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 11:48:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00357a168 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-04 11:48:40 +0000 UTC,LastTransitionTime:2023-05-04 11:48:40 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-69cb9c5497" has successfully progressed.,LastUpdateTime:2023-05-04 11:48:42 +0000 UTC,LastTransitionTime:2023-05-04 11:48:40 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May  4 11:48:42.928: INFO: New ReplicaSet "test-cleanup-deployment-69cb9c5497" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-69cb9c5497  deployment-1362  e06a5040-e487-463b-bd77-17e3993a96d5 7361 1 2023-05-04 11:48:40 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment e3398b69-69ca-4b67-9d70-0ec5588722b4 0xc0038fa337 0xc0038fa338}] [] [{kube-controller-manager Update apps/v1 2023-05-04 11:48:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3398b69-69ca-4b67-9d70-0ec5588722b4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 11:48:42 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 69cb9c5497,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038fa3e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May  4 11:48:42.932: INFO: Pod "test-cleanup-deployment-69cb9c5497-p8dv4" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-69cb9c5497-p8dv4 test-cleanup-deployment-69cb9c5497- deployment-1362  6eaed5cb-d3e4-46d6-bf58-1e52187364fc 7360 0 2023-05-04 11:48:40 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[cni.projectcalico.org/containerID:f8b25d181dd3dc84890e5a96d07e0259c9c03ca2ccbe9f2d5ca2867f223fe186 cni.projectcalico.org/podIP:10.20.142.139/32 cni.projectcalico.org/podIPs:10.20.142.139/32] [{apps/v1 ReplicaSet test-cleanup-deployment-69cb9c5497 e06a5040-e487-463b-bd77-17e3993a96d5 0xc003a05687 0xc003a05688}] [] [{kube-controller-manager Update v1 2023-05-04 11:48:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e06a5040-e487-463b-bd77-17e3993a96d5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-04 11:48:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-04 11:48:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.142.139\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9smzs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9smzs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-216.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 11:48:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 11:48:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 11:48:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 11:48:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.216,PodIP:10.20.142.139,StartTime:2023-05-04 11:48:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-04 11:48:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://f23f132ae317a13b0550e3f7988405dd2ca7a74833cb0b03df6d5d9fb3949b0b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.142.139,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
May  4 11:48:42.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1362" for this suite. 05/04/23 11:48:42.943
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","completed":17,"skipped":334,"failed":0}
------------------------------
• [SLOW TEST] [8.141 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:48:34.814
    May  4 11:48:34.814: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename deployment 05/04/23 11:48:34.814
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:48:34.836
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:48:34.842
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    May  4 11:48:34.866: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 05/04/23 11:48:34.866
    May  4 11:48:34.866: INFO: Waiting up to 5m0s for pod "test-cleanup-controller-vmnwm" in namespace "deployment-1362" to be "running"
    May  4 11:48:34.876: INFO: Pod "test-cleanup-controller-vmnwm": Phase="Pending", Reason="", readiness=false. Elapsed: 9.382073ms
    May  4 11:48:36.881: INFO: Pod "test-cleanup-controller-vmnwm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014444048s
    May  4 11:48:38.881: INFO: Pod "test-cleanup-controller-vmnwm": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015186446s
    May  4 11:48:40.883: INFO: Pod "test-cleanup-controller-vmnwm": Phase="Running", Reason="", readiness=true. Elapsed: 6.016715932s
    May  4 11:48:40.883: INFO: Pod "test-cleanup-controller-vmnwm" satisfied condition "running"
    May  4 11:48:40.883: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 05/04/23 11:48:40.899
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May  4 11:48:42.923: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-1362  e3398b69-69ca-4b67-9d70-0ec5588722b4 7371 1 2023-05-04 11:48:40 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-05-04 11:48:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 11:48:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00357a168 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-04 11:48:40 +0000 UTC,LastTransitionTime:2023-05-04 11:48:40 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-69cb9c5497" has successfully progressed.,LastUpdateTime:2023-05-04 11:48:42 +0000 UTC,LastTransitionTime:2023-05-04 11:48:40 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    May  4 11:48:42.928: INFO: New ReplicaSet "test-cleanup-deployment-69cb9c5497" of Deployment "test-cleanup-deployment":
    &ReplicaSet{ObjectMeta:{test-cleanup-deployment-69cb9c5497  deployment-1362  e06a5040-e487-463b-bd77-17e3993a96d5 7361 1 2023-05-04 11:48:40 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment e3398b69-69ca-4b67-9d70-0ec5588722b4 0xc0038fa337 0xc0038fa338}] [] [{kube-controller-manager Update apps/v1 2023-05-04 11:48:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3398b69-69ca-4b67-9d70-0ec5588722b4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 11:48:42 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 69cb9c5497,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038fa3e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    May  4 11:48:42.932: INFO: Pod "test-cleanup-deployment-69cb9c5497-p8dv4" is available:
    &Pod{ObjectMeta:{test-cleanup-deployment-69cb9c5497-p8dv4 test-cleanup-deployment-69cb9c5497- deployment-1362  6eaed5cb-d3e4-46d6-bf58-1e52187364fc 7360 0 2023-05-04 11:48:40 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[cni.projectcalico.org/containerID:f8b25d181dd3dc84890e5a96d07e0259c9c03ca2ccbe9f2d5ca2867f223fe186 cni.projectcalico.org/podIP:10.20.142.139/32 cni.projectcalico.org/podIPs:10.20.142.139/32] [{apps/v1 ReplicaSet test-cleanup-deployment-69cb9c5497 e06a5040-e487-463b-bd77-17e3993a96d5 0xc003a05687 0xc003a05688}] [] [{kube-controller-manager Update v1 2023-05-04 11:48:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e06a5040-e487-463b-bd77-17e3993a96d5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-04 11:48:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-04 11:48:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.142.139\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9smzs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9smzs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-216.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 11:48:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 11:48:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 11:48:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 11:48:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.216,PodIP:10.20.142.139,StartTime:2023-05-04 11:48:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-04 11:48:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://f23f132ae317a13b0550e3f7988405dd2ca7a74833cb0b03df6d5d9fb3949b0b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.142.139,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    May  4 11:48:42.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-1362" for this suite. 05/04/23 11:48:42.943
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:48:42.955
May  4 11:48:42.955: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename sched-pred 05/04/23 11:48:42.956
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:48:42.976
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:48:42.98
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
May  4 11:48:42.983: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May  4 11:48:43.002: INFO: Waiting for terminating namespaces to be deleted...
May  4 11:48:43.009: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-189.us-west-2.compute.internal before test
May  4 11:48:43.069: INFO: calico-node-r8wl6 from kube-system started at 2023-05-04 11:14:50 +0000 UTC (1 container statuses recorded)
May  4 11:48:43.069: INFO: 	Container calico-node ready: true, restart count 0
May  4 11:48:43.069: INFO: calico-typha-85dbcd447f-6tg8g from kube-system started at 2023-05-04 11:15:25 +0000 UTC (1 container statuses recorded)
May  4 11:48:43.069: INFO: 	Container calico-typha ready: true, restart count 0
May  4 11:48:43.069: INFO: metrics-server-57bbf8548c-qkgc6 from kube-system started at 2023-05-04 11:16:29 +0000 UTC (1 container statuses recorded)
May  4 11:48:43.069: INFO: 	Container metrics-server ready: true, restart count 0
May  4 11:48:43.069: INFO: node-exporter-tsxz4 from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
May  4 11:48:43.069: INFO: 	Container node-exporter ready: true, restart count 0
May  4 11:48:43.069: INFO: prometheus-operator-85498c86bb-gdnsq from pf9-operators started at 2023-05-04 11:16:25 +0000 UTC (1 container statuses recorded)
May  4 11:48:43.069: INFO: 	Container prometheus-operator ready: true, restart count 0
May  4 11:48:43.069: INFO: sonobuoy-e2e-job-a8c15777d8b94166 from sonobuoy started at 2023-05-04 11:44:14 +0000 UTC (2 container statuses recorded)
May  4 11:48:43.069: INFO: 	Container e2e ready: true, restart count 0
May  4 11:48:43.069: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  4 11:48:43.069: INFO: sonobuoy-systemd-logs-daemon-set-d34856018c814171-f2qjv from sonobuoy started at 2023-05-04 11:44:14 +0000 UTC (2 container statuses recorded)
May  4 11:48:43.069: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  4 11:48:43.069: INFO: 	Container systemd-logs ready: true, restart count 0
May  4 11:48:43.069: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-216.us-west-2.compute.internal before test
May  4 11:48:43.119: INFO: test-cleanup-deployment-69cb9c5497-p8dv4 from deployment-1362 started at 2023-05-04 11:48:40 +0000 UTC (1 container statuses recorded)
May  4 11:48:43.119: INFO: 	Container agnhost ready: true, restart count 0
May  4 11:48:43.119: INFO: calico-node-ll8r2 from kube-system started at 2023-05-04 11:15:13 +0000 UTC (1 container statuses recorded)
May  4 11:48:43.119: INFO: 	Container calico-node ready: true, restart count 0
May  4 11:48:43.119: INFO: calico-typha-85dbcd447f-qw96l from kube-system started at 2023-05-04 11:15:29 +0000 UTC (1 container statuses recorded)
May  4 11:48:43.119: INFO: 	Container calico-typha ready: true, restart count 0
May  4 11:48:43.119: INFO: grafana-84c9c8d6bd-rsbn8 from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (2 container statuses recorded)
May  4 11:48:43.119: INFO: 	Container grafana ready: true, restart count 0
May  4 11:48:43.119: INFO: 	Container proxy ready: true, restart count 0
May  4 11:48:43.119: INFO: node-exporter-pnw8x from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
May  4 11:48:43.119: INFO: 	Container node-exporter ready: true, restart count 0
May  4 11:48:43.119: INFO: sonobuoy from sonobuoy started at 2023-05-04 11:44:11 +0000 UTC (1 container statuses recorded)
May  4 11:48:43.119: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May  4 11:48:43.119: INFO: sonobuoy-systemd-logs-daemon-set-d34856018c814171-44cxg from sonobuoy started at 2023-05-04 11:44:14 +0000 UTC (2 container statuses recorded)
May  4 11:48:43.119: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  4 11:48:43.119: INFO: 	Container systemd-logs ready: true, restart count 0
May  4 11:48:43.119: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-224.us-west-2.compute.internal before test
May  4 11:48:43.222: INFO: calico-node-bb7nh from kube-system started at 2023-05-04 11:14:53 +0000 UTC (1 container statuses recorded)
May  4 11:48:43.222: INFO: 	Container calico-node ready: true, restart count 0
May  4 11:48:43.222: INFO: coredns-c7944df6b-gkv4x from kube-system started at 2023-05-04 11:16:34 +0000 UTC (1 container statuses recorded)
May  4 11:48:43.222: INFO: 	Container coredns ready: true, restart count 0
May  4 11:48:43.222: INFO: kube-dns-autoscaler-558cdf6846-vvkfg from kube-system started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
May  4 11:48:43.222: INFO: 	Container autoscaler ready: true, restart count 0
May  4 11:48:43.222: INFO: dashboard-metrics-scraper-7564894f4b-cpm4c from kubernetes-dashboard started at 2023-05-04 11:16:29 +0000 UTC (1 container statuses recorded)
May  4 11:48:43.222: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
May  4 11:48:43.222: INFO: kube-state-metrics-5bf549bd69-vwzfr from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
May  4 11:48:43.222: INFO: 	Container kube-state-metrics ready: true, restart count 0
May  4 11:48:43.222: INFO: node-exporter-qn2kc from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
May  4 11:48:43.222: INFO: 	Container node-exporter ready: true, restart count 0
May  4 11:48:43.222: INFO: sonobuoy-systemd-logs-daemon-set-d34856018c814171-lss27 from sonobuoy started at 2023-05-04 11:44:14 +0000 UTC (2 container statuses recorded)
May  4 11:48:43.222: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  4 11:48:43.222: INFO: 	Container systemd-logs ready: true, restart count 0
May  4 11:48:43.222: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-232.us-west-2.compute.internal before test
May  4 11:48:43.268: INFO: calico-kube-controllers-654d9ff976-ddwnz from kube-system started at 2023-05-04 11:15:10 +0000 UTC (1 container statuses recorded)
May  4 11:48:43.268: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May  4 11:48:43.268: INFO: calico-node-48gv4 from kube-system started at 2023-05-04 11:14:51 +0000 UTC (1 container statuses recorded)
May  4 11:48:43.268: INFO: 	Container calico-node ready: true, restart count 0
May  4 11:48:43.268: INFO: calico-typha-85dbcd447f-2s7df from kube-system started at 2023-05-04 11:15:10 +0000 UTC (1 container statuses recorded)
May  4 11:48:43.268: INFO: 	Container calico-typha ready: true, restart count 0
May  4 11:48:43.268: INFO: calico-typha-autoscaler-795696b9bd-qhx26 from kube-system started at 2023-05-04 11:15:10 +0000 UTC (1 container statuses recorded)
May  4 11:48:43.268: INFO: 	Container autoscaler ready: true, restart count 0
May  4 11:48:43.268: INFO: kube-state-metrics-857f6bcbbb-krrw9 from kube-system started at 2023-05-04 11:15:10 +0000 UTC (1 container statuses recorded)
May  4 11:48:43.268: INFO: 	Container kube-state-metrics ready: true, restart count 0
May  4 11:48:43.268: INFO: alertmanager-sysalert-0 from pf9-monitoring started at 2023-05-04 11:16:29 +0000 UTC (2 container statuses recorded)
May  4 11:48:43.268: INFO: 	Container alertmanager ready: true, restart count 1
May  4 11:48:43.268: INFO: 	Container config-reloader ready: true, restart count 0
May  4 11:48:43.268: INFO: node-exporter-49wcf from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
May  4 11:48:43.268: INFO: 	Container node-exporter ready: true, restart count 0
May  4 11:48:43.268: INFO: sonobuoy-systemd-logs-daemon-set-d34856018c814171-thbzc from sonobuoy started at 2023-05-04 11:44:14 +0000 UTC (2 container statuses recorded)
May  4 11:48:43.268: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  4 11:48:43.268: INFO: 	Container systemd-logs ready: true, restart count 0
May  4 11:48:43.268: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-253.us-west-2.compute.internal before test
May  4 11:48:43.293: INFO: calico-node-69h7w from kube-system started at 2023-05-04 11:14:49 +0000 UTC (1 container statuses recorded)
May  4 11:48:43.293: INFO: 	Container calico-node ready: true, restart count 0
May  4 11:48:43.293: INFO: coredns-c7944df6b-zhglb from kube-system started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
May  4 11:48:43.293: INFO: 	Container coredns ready: true, restart count 0
May  4 11:48:43.293: INFO: node-exporter-q8qnn from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
May  4 11:48:43.293: INFO: 	Container node-exporter ready: true, restart count 0
May  4 11:48:43.293: INFO: prometheus-system-0 from pf9-monitoring started at 2023-05-04 11:16:30 +0000 UTC (2 container statuses recorded)
May  4 11:48:43.293: INFO: 	Container config-reloader ready: true, restart count 0
May  4 11:48:43.293: INFO: 	Container prometheus ready: true, restart count 0
May  4 11:48:43.293: INFO: monhelper-57744bf759-6nndz from pf9-operators started at 2023-05-04 11:16:25 +0000 UTC (1 container statuses recorded)
May  4 11:48:43.293: INFO: 	Container monhelper ready: true, restart count 0
May  4 11:48:43.293: INFO: sonobuoy-systemd-logs-daemon-set-d34856018c814171-rx5lv from sonobuoy started at 2023-05-04 11:44:14 +0000 UTC (2 container statuses recorded)
May  4 11:48:43.293: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  4 11:48:43.293: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
STEP: Trying to launch a pod without a label to get a node which can launch it. 05/04/23 11:48:43.293
May  4 11:48:43.308: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-3066" to be "running"
May  4 11:48:43.313: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.698766ms
May  4 11:48:45.319: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010688711s
May  4 11:48:47.319: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.011269824s
May  4 11:48:47.319: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 05/04/23 11:48:47.327
STEP: Trying to apply a random label on the found node. 05/04/23 11:48:47.349
STEP: verifying the node has the label kubernetes.io/e2e-fd0e7b7f-de45-49e0-a87b-a58445a3da13 42 05/04/23 11:48:47.36
STEP: Trying to relaunch the pod, now with labels. 05/04/23 11:48:47.369
May  4 11:48:47.377: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-3066" to be "not pending"
May  4 11:48:47.383: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 6.051927ms
May  4 11:48:49.389: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.012775762s
May  4 11:48:49.389: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-fd0e7b7f-de45-49e0-a87b-a58445a3da13 off the node ip-10-0-1-224.us-west-2.compute.internal 05/04/23 11:48:49.394
STEP: verifying the node doesn't have the label kubernetes.io/e2e-fd0e7b7f-de45-49e0-a87b-a58445a3da13 05/04/23 11:48:49.413
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
May  4 11:48:49.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3066" for this suite. 05/04/23 11:48:49.426
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","completed":18,"skipped":334,"failed":0}
------------------------------
• [SLOW TEST] [6.482 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:48:42.955
    May  4 11:48:42.955: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename sched-pred 05/04/23 11:48:42.956
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:48:42.976
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:48:42.98
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    May  4 11:48:42.983: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    May  4 11:48:43.002: INFO: Waiting for terminating namespaces to be deleted...
    May  4 11:48:43.009: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-1-189.us-west-2.compute.internal before test
    May  4 11:48:43.069: INFO: calico-node-r8wl6 from kube-system started at 2023-05-04 11:14:50 +0000 UTC (1 container statuses recorded)
    May  4 11:48:43.069: INFO: 	Container calico-node ready: true, restart count 0
    May  4 11:48:43.069: INFO: calico-typha-85dbcd447f-6tg8g from kube-system started at 2023-05-04 11:15:25 +0000 UTC (1 container statuses recorded)
    May  4 11:48:43.069: INFO: 	Container calico-typha ready: true, restart count 0
    May  4 11:48:43.069: INFO: metrics-server-57bbf8548c-qkgc6 from kube-system started at 2023-05-04 11:16:29 +0000 UTC (1 container statuses recorded)
    May  4 11:48:43.069: INFO: 	Container metrics-server ready: true, restart count 0
    May  4 11:48:43.069: INFO: node-exporter-tsxz4 from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
    May  4 11:48:43.069: INFO: 	Container node-exporter ready: true, restart count 0
    May  4 11:48:43.069: INFO: prometheus-operator-85498c86bb-gdnsq from pf9-operators started at 2023-05-04 11:16:25 +0000 UTC (1 container statuses recorded)
    May  4 11:48:43.069: INFO: 	Container prometheus-operator ready: true, restart count 0
    May  4 11:48:43.069: INFO: sonobuoy-e2e-job-a8c15777d8b94166 from sonobuoy started at 2023-05-04 11:44:14 +0000 UTC (2 container statuses recorded)
    May  4 11:48:43.069: INFO: 	Container e2e ready: true, restart count 0
    May  4 11:48:43.069: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  4 11:48:43.069: INFO: sonobuoy-systemd-logs-daemon-set-d34856018c814171-f2qjv from sonobuoy started at 2023-05-04 11:44:14 +0000 UTC (2 container statuses recorded)
    May  4 11:48:43.069: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  4 11:48:43.069: INFO: 	Container systemd-logs ready: true, restart count 0
    May  4 11:48:43.069: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-1-216.us-west-2.compute.internal before test
    May  4 11:48:43.119: INFO: test-cleanup-deployment-69cb9c5497-p8dv4 from deployment-1362 started at 2023-05-04 11:48:40 +0000 UTC (1 container statuses recorded)
    May  4 11:48:43.119: INFO: 	Container agnhost ready: true, restart count 0
    May  4 11:48:43.119: INFO: calico-node-ll8r2 from kube-system started at 2023-05-04 11:15:13 +0000 UTC (1 container statuses recorded)
    May  4 11:48:43.119: INFO: 	Container calico-node ready: true, restart count 0
    May  4 11:48:43.119: INFO: calico-typha-85dbcd447f-qw96l from kube-system started at 2023-05-04 11:15:29 +0000 UTC (1 container statuses recorded)
    May  4 11:48:43.119: INFO: 	Container calico-typha ready: true, restart count 0
    May  4 11:48:43.119: INFO: grafana-84c9c8d6bd-rsbn8 from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (2 container statuses recorded)
    May  4 11:48:43.119: INFO: 	Container grafana ready: true, restart count 0
    May  4 11:48:43.119: INFO: 	Container proxy ready: true, restart count 0
    May  4 11:48:43.119: INFO: node-exporter-pnw8x from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
    May  4 11:48:43.119: INFO: 	Container node-exporter ready: true, restart count 0
    May  4 11:48:43.119: INFO: sonobuoy from sonobuoy started at 2023-05-04 11:44:11 +0000 UTC (1 container statuses recorded)
    May  4 11:48:43.119: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    May  4 11:48:43.119: INFO: sonobuoy-systemd-logs-daemon-set-d34856018c814171-44cxg from sonobuoy started at 2023-05-04 11:44:14 +0000 UTC (2 container statuses recorded)
    May  4 11:48:43.119: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  4 11:48:43.119: INFO: 	Container systemd-logs ready: true, restart count 0
    May  4 11:48:43.119: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-1-224.us-west-2.compute.internal before test
    May  4 11:48:43.222: INFO: calico-node-bb7nh from kube-system started at 2023-05-04 11:14:53 +0000 UTC (1 container statuses recorded)
    May  4 11:48:43.222: INFO: 	Container calico-node ready: true, restart count 0
    May  4 11:48:43.222: INFO: coredns-c7944df6b-gkv4x from kube-system started at 2023-05-04 11:16:34 +0000 UTC (1 container statuses recorded)
    May  4 11:48:43.222: INFO: 	Container coredns ready: true, restart count 0
    May  4 11:48:43.222: INFO: kube-dns-autoscaler-558cdf6846-vvkfg from kube-system started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
    May  4 11:48:43.222: INFO: 	Container autoscaler ready: true, restart count 0
    May  4 11:48:43.222: INFO: dashboard-metrics-scraper-7564894f4b-cpm4c from kubernetes-dashboard started at 2023-05-04 11:16:29 +0000 UTC (1 container statuses recorded)
    May  4 11:48:43.222: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    May  4 11:48:43.222: INFO: kube-state-metrics-5bf549bd69-vwzfr from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
    May  4 11:48:43.222: INFO: 	Container kube-state-metrics ready: true, restart count 0
    May  4 11:48:43.222: INFO: node-exporter-qn2kc from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
    May  4 11:48:43.222: INFO: 	Container node-exporter ready: true, restart count 0
    May  4 11:48:43.222: INFO: sonobuoy-systemd-logs-daemon-set-d34856018c814171-lss27 from sonobuoy started at 2023-05-04 11:44:14 +0000 UTC (2 container statuses recorded)
    May  4 11:48:43.222: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  4 11:48:43.222: INFO: 	Container systemd-logs ready: true, restart count 0
    May  4 11:48:43.222: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-1-232.us-west-2.compute.internal before test
    May  4 11:48:43.268: INFO: calico-kube-controllers-654d9ff976-ddwnz from kube-system started at 2023-05-04 11:15:10 +0000 UTC (1 container statuses recorded)
    May  4 11:48:43.268: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    May  4 11:48:43.268: INFO: calico-node-48gv4 from kube-system started at 2023-05-04 11:14:51 +0000 UTC (1 container statuses recorded)
    May  4 11:48:43.268: INFO: 	Container calico-node ready: true, restart count 0
    May  4 11:48:43.268: INFO: calico-typha-85dbcd447f-2s7df from kube-system started at 2023-05-04 11:15:10 +0000 UTC (1 container statuses recorded)
    May  4 11:48:43.268: INFO: 	Container calico-typha ready: true, restart count 0
    May  4 11:48:43.268: INFO: calico-typha-autoscaler-795696b9bd-qhx26 from kube-system started at 2023-05-04 11:15:10 +0000 UTC (1 container statuses recorded)
    May  4 11:48:43.268: INFO: 	Container autoscaler ready: true, restart count 0
    May  4 11:48:43.268: INFO: kube-state-metrics-857f6bcbbb-krrw9 from kube-system started at 2023-05-04 11:15:10 +0000 UTC (1 container statuses recorded)
    May  4 11:48:43.268: INFO: 	Container kube-state-metrics ready: true, restart count 0
    May  4 11:48:43.268: INFO: alertmanager-sysalert-0 from pf9-monitoring started at 2023-05-04 11:16:29 +0000 UTC (2 container statuses recorded)
    May  4 11:48:43.268: INFO: 	Container alertmanager ready: true, restart count 1
    May  4 11:48:43.268: INFO: 	Container config-reloader ready: true, restart count 0
    May  4 11:48:43.268: INFO: node-exporter-49wcf from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
    May  4 11:48:43.268: INFO: 	Container node-exporter ready: true, restart count 0
    May  4 11:48:43.268: INFO: sonobuoy-systemd-logs-daemon-set-d34856018c814171-thbzc from sonobuoy started at 2023-05-04 11:44:14 +0000 UTC (2 container statuses recorded)
    May  4 11:48:43.268: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  4 11:48:43.268: INFO: 	Container systemd-logs ready: true, restart count 0
    May  4 11:48:43.268: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-1-253.us-west-2.compute.internal before test
    May  4 11:48:43.293: INFO: calico-node-69h7w from kube-system started at 2023-05-04 11:14:49 +0000 UTC (1 container statuses recorded)
    May  4 11:48:43.293: INFO: 	Container calico-node ready: true, restart count 0
    May  4 11:48:43.293: INFO: coredns-c7944df6b-zhglb from kube-system started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
    May  4 11:48:43.293: INFO: 	Container coredns ready: true, restart count 0
    May  4 11:48:43.293: INFO: node-exporter-q8qnn from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
    May  4 11:48:43.293: INFO: 	Container node-exporter ready: true, restart count 0
    May  4 11:48:43.293: INFO: prometheus-system-0 from pf9-monitoring started at 2023-05-04 11:16:30 +0000 UTC (2 container statuses recorded)
    May  4 11:48:43.293: INFO: 	Container config-reloader ready: true, restart count 0
    May  4 11:48:43.293: INFO: 	Container prometheus ready: true, restart count 0
    May  4 11:48:43.293: INFO: monhelper-57744bf759-6nndz from pf9-operators started at 2023-05-04 11:16:25 +0000 UTC (1 container statuses recorded)
    May  4 11:48:43.293: INFO: 	Container monhelper ready: true, restart count 0
    May  4 11:48:43.293: INFO: sonobuoy-systemd-logs-daemon-set-d34856018c814171-rx5lv from sonobuoy started at 2023-05-04 11:44:14 +0000 UTC (2 container statuses recorded)
    May  4 11:48:43.293: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  4 11:48:43.293: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:461
    STEP: Trying to launch a pod without a label to get a node which can launch it. 05/04/23 11:48:43.293
    May  4 11:48:43.308: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-3066" to be "running"
    May  4 11:48:43.313: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.698766ms
    May  4 11:48:45.319: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010688711s
    May  4 11:48:47.319: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.011269824s
    May  4 11:48:47.319: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 05/04/23 11:48:47.327
    STEP: Trying to apply a random label on the found node. 05/04/23 11:48:47.349
    STEP: verifying the node has the label kubernetes.io/e2e-fd0e7b7f-de45-49e0-a87b-a58445a3da13 42 05/04/23 11:48:47.36
    STEP: Trying to relaunch the pod, now with labels. 05/04/23 11:48:47.369
    May  4 11:48:47.377: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-3066" to be "not pending"
    May  4 11:48:47.383: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 6.051927ms
    May  4 11:48:49.389: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.012775762s
    May  4 11:48:49.389: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-fd0e7b7f-de45-49e0-a87b-a58445a3da13 off the node ip-10-0-1-224.us-west-2.compute.internal 05/04/23 11:48:49.394
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-fd0e7b7f-de45-49e0-a87b-a58445a3da13 05/04/23 11:48:49.413
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    May  4 11:48:49.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-3066" for this suite. 05/04/23 11:48:49.426
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:48:49.438
May  4 11:48:49.438: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename secrets 05/04/23 11:48:49.439
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:48:49.457
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:48:49.461
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
STEP: Creating secret with name secret-test-d0a4a955-40ea-4273-81bb-07ac847f4ea5 05/04/23 11:48:49.464
STEP: Creating a pod to test consume secrets 05/04/23 11:48:49.477
May  4 11:48:49.491: INFO: Waiting up to 5m0s for pod "pod-secrets-7f71ef1a-07d1-487a-8d7e-1add1e94a687" in namespace "secrets-6847" to be "Succeeded or Failed"
May  4 11:48:49.499: INFO: Pod "pod-secrets-7f71ef1a-07d1-487a-8d7e-1add1e94a687": Phase="Pending", Reason="", readiness=false. Elapsed: 7.159947ms
May  4 11:48:51.505: INFO: Pod "pod-secrets-7f71ef1a-07d1-487a-8d7e-1add1e94a687": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013504813s
May  4 11:48:53.505: INFO: Pod "pod-secrets-7f71ef1a-07d1-487a-8d7e-1add1e94a687": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013049453s
STEP: Saw pod success 05/04/23 11:48:53.505
May  4 11:48:53.505: INFO: Pod "pod-secrets-7f71ef1a-07d1-487a-8d7e-1add1e94a687" satisfied condition "Succeeded or Failed"
May  4 11:48:53.509: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod pod-secrets-7f71ef1a-07d1-487a-8d7e-1add1e94a687 container secret-volume-test: <nil>
STEP: delete the pod 05/04/23 11:48:53.545
May  4 11:48:53.567: INFO: Waiting for pod pod-secrets-7f71ef1a-07d1-487a-8d7e-1add1e94a687 to disappear
May  4 11:48:53.572: INFO: Pod pod-secrets-7f71ef1a-07d1-487a-8d7e-1add1e94a687 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
May  4 11:48:53.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6847" for this suite. 05/04/23 11:48:53.583
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":19,"skipped":351,"failed":0}
------------------------------
• [4.154 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:48:49.438
    May  4 11:48:49.438: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename secrets 05/04/23 11:48:49.439
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:48:49.457
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:48:49.461
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:124
    STEP: Creating secret with name secret-test-d0a4a955-40ea-4273-81bb-07ac847f4ea5 05/04/23 11:48:49.464
    STEP: Creating a pod to test consume secrets 05/04/23 11:48:49.477
    May  4 11:48:49.491: INFO: Waiting up to 5m0s for pod "pod-secrets-7f71ef1a-07d1-487a-8d7e-1add1e94a687" in namespace "secrets-6847" to be "Succeeded or Failed"
    May  4 11:48:49.499: INFO: Pod "pod-secrets-7f71ef1a-07d1-487a-8d7e-1add1e94a687": Phase="Pending", Reason="", readiness=false. Elapsed: 7.159947ms
    May  4 11:48:51.505: INFO: Pod "pod-secrets-7f71ef1a-07d1-487a-8d7e-1add1e94a687": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013504813s
    May  4 11:48:53.505: INFO: Pod "pod-secrets-7f71ef1a-07d1-487a-8d7e-1add1e94a687": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013049453s
    STEP: Saw pod success 05/04/23 11:48:53.505
    May  4 11:48:53.505: INFO: Pod "pod-secrets-7f71ef1a-07d1-487a-8d7e-1add1e94a687" satisfied condition "Succeeded or Failed"
    May  4 11:48:53.509: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod pod-secrets-7f71ef1a-07d1-487a-8d7e-1add1e94a687 container secret-volume-test: <nil>
    STEP: delete the pod 05/04/23 11:48:53.545
    May  4 11:48:53.567: INFO: Waiting for pod pod-secrets-7f71ef1a-07d1-487a-8d7e-1add1e94a687 to disappear
    May  4 11:48:53.572: INFO: Pod pod-secrets-7f71ef1a-07d1-487a-8d7e-1add1e94a687 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    May  4 11:48:53.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-6847" for this suite. 05/04/23 11:48:53.583
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:48:53.594
May  4 11:48:53.594: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename kubelet-test 05/04/23 11:48:53.595
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:48:53.624
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:48:53.639
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
May  4 11:48:53.656: INFO: Waiting up to 5m0s for pod "busybox-scheduling-575729aa-2617-4884-a1bc-99b68bc11328" in namespace "kubelet-test-5627" to be "running and ready"
May  4 11:48:53.665: INFO: Pod "busybox-scheduling-575729aa-2617-4884-a1bc-99b68bc11328": Phase="Pending", Reason="", readiness=false. Elapsed: 8.806754ms
May  4 11:48:53.665: INFO: The phase of Pod busybox-scheduling-575729aa-2617-4884-a1bc-99b68bc11328 is Pending, waiting for it to be Running (with Ready = true)
May  4 11:48:55.671: INFO: Pod "busybox-scheduling-575729aa-2617-4884-a1bc-99b68bc11328": Phase="Running", Reason="", readiness=true. Elapsed: 2.014065904s
May  4 11:48:55.671: INFO: The phase of Pod busybox-scheduling-575729aa-2617-4884-a1bc-99b68bc11328 is Running (Ready = true)
May  4 11:48:55.671: INFO: Pod "busybox-scheduling-575729aa-2617-4884-a1bc-99b68bc11328" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
May  4 11:48:55.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5627" for this suite. 05/04/23 11:48:55.698
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","completed":20,"skipped":363,"failed":0}
------------------------------
• [2.111 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:48:53.594
    May  4 11:48:53.594: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename kubelet-test 05/04/23 11:48:53.595
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:48:53.624
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:48:53.639
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    May  4 11:48:53.656: INFO: Waiting up to 5m0s for pod "busybox-scheduling-575729aa-2617-4884-a1bc-99b68bc11328" in namespace "kubelet-test-5627" to be "running and ready"
    May  4 11:48:53.665: INFO: Pod "busybox-scheduling-575729aa-2617-4884-a1bc-99b68bc11328": Phase="Pending", Reason="", readiness=false. Elapsed: 8.806754ms
    May  4 11:48:53.665: INFO: The phase of Pod busybox-scheduling-575729aa-2617-4884-a1bc-99b68bc11328 is Pending, waiting for it to be Running (with Ready = true)
    May  4 11:48:55.671: INFO: Pod "busybox-scheduling-575729aa-2617-4884-a1bc-99b68bc11328": Phase="Running", Reason="", readiness=true. Elapsed: 2.014065904s
    May  4 11:48:55.671: INFO: The phase of Pod busybox-scheduling-575729aa-2617-4884-a1bc-99b68bc11328 is Running (Ready = true)
    May  4 11:48:55.671: INFO: Pod "busybox-scheduling-575729aa-2617-4884-a1bc-99b68bc11328" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    May  4 11:48:55.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-5627" for this suite. 05/04/23 11:48:55.698
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:48:55.706
May  4 11:48:55.706: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename kubectl 05/04/23 11:48:55.707
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:48:55.732
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:48:55.736
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
May  4 11:48:55.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-7554 create -f -'
May  4 11:48:57.387: INFO: stderr: ""
May  4 11:48:57.387: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
May  4 11:48:57.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-7554 create -f -'
May  4 11:48:58.810: INFO: stderr: ""
May  4 11:48:58.810: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 05/04/23 11:48:58.811
May  4 11:48:59.817: INFO: Selector matched 1 pods for map[app:agnhost]
May  4 11:48:59.817: INFO: Found 1 / 1
May  4 11:48:59.817: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May  4 11:48:59.822: INFO: Selector matched 1 pods for map[app:agnhost]
May  4 11:48:59.822: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May  4 11:48:59.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-7554 describe pod agnhost-primary-nmdxs'
May  4 11:48:59.900: INFO: stderr: ""
May  4 11:48:59.900: INFO: stdout: "Name:             agnhost-primary-nmdxs\nNamespace:        kubectl-7554\nPriority:         0\nService Account:  default\nNode:             ip-10-0-1-224.us-west-2.compute.internal/10.0.1.224\nStart Time:       Thu, 04 May 2023 11:48:57 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 22bd765c3af736d08a8c9acc8b3e925d06f2ef2507a8afe69f959f283de012e1\n                  cni.projectcalico.org/podIP: 10.20.83.17/32\n                  cni.projectcalico.org/podIPs: 10.20.83.17/32\nStatus:           Running\nIP:               10.20.83.17\nIPs:\n  IP:           10.20.83.17\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://c1f45b218eaa05863739e4c3034c73060d99854588c5fae8443f9f57a128e6e4\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 04 May 2023 11:48:58 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hmb6k (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-hmb6k:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-7554/agnhost-primary-nmdxs to ip-10-0-1-224.us-west-2.compute.internal\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
May  4 11:48:59.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-7554 describe rc agnhost-primary'
May  4 11:48:59.988: INFO: stderr: ""
May  4 11:48:59.988: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-7554\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-nmdxs\n"
May  4 11:48:59.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-7554 describe service agnhost-primary'
May  4 11:49:00.090: INFO: stderr: ""
May  4 11:49:00.090: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-7554\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.21.55.118\nIPs:               10.21.55.118\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.20.83.17:6379\nSession Affinity:  None\nEvents:            <none>\n"
May  4 11:49:00.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-7554 describe node ip-10-0-1-168.us-west-2.compute.internal'
May  4 11:49:00.398: INFO: stderr: ""
May  4 11:49:00.398: INFO: stdout: "Name:               ip-10-0-1-168.us-west-2.compute.internal\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=t3.xlarge\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-west-2\n                    failure-domain.beta.kubernetes.io/zone=us-west-2b\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-10-0-1-168.us-west-2.compute.internal\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\n                    node.kubernetes.io/instance-type=t3.xlarge\n                    topology.kubernetes.io/region=us-west-2\n                    topology.kubernetes.io/zone=us-west-2b\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.0.1.168/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.20.112.64\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 04 May 2023 11:13:50 +0000\nTaints:             node-role.kubernetes.io/master=true:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-10-0-1-168.us-west-2.compute.internal\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 04 May 2023 11:48:53 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Thu, 04 May 2023 11:14:24 +0000   Thu, 04 May 2023 11:14:24 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Thu, 04 May 2023 11:44:59 +0000   Thu, 04 May 2023 11:13:50 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 04 May 2023 11:44:59 +0000   Thu, 04 May 2023 11:13:50 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 04 May 2023 11:44:59 +0000   Thu, 04 May 2023 11:13:50 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 04 May 2023 11:44:59 +0000   Thu, 04 May 2023 11:14:20 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:   10.0.1.168\n  ExternalIP:   54.202.40.255\n  Hostname:     ip-10-0-1-168.us-west-2.compute.internal\n  InternalDNS:  ip-10-0-1-168.us-west-2.compute.internal\n  ExternalDNS:  ec2-54-202-40-255.us-west-2.compute.amazonaws.com\nCapacity:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         4\n  ephemeral-storage:           50758604Ki\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      16197968Ki\n  pods:                        200\nAllocatable:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         4\n  ephemeral-storage:           46779129369\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      16095568Ki\n  pods:                        200\nSystem Info:\n  Machine ID:                 ec2f01927bea3e1de85f3346ede35d13\n  System UUID:                ec2f0192-7bea-3e1d-e85f-3346ede35d13\n  Boot ID:                    23fc1f1a-6fbc-4122-89d2-0c55b7e35baa\n  Kernel Version:             5.11.0-1028-aws\n  OS Image:                   Ubuntu 20.04.3 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.6\n  Kubelet Version:            v1.25.5\n  Kube-Proxy Version:         v1.25.5\nPodCIDR:                      10.20.1.0/24\nPodCIDRs:                     10.20.1.0/24\nProviderID:                   aws:///us-west-2b/i-087a4372e640f465d\nNon-terminated Pods:          (4 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-kdmh2                                          250m (6%)     1 (25%)     70Mi (0%)        1000Mi (6%)    35m\n  kube-system                 k8s-master-ip-10-0-1-168.us-west-2.compute.internal        0 (0%)        0 (0%)      0 (0%)           0 (0%)         34m\n  pf9-monitoring              node-exporter-fns5h                                        102m (2%)     250m (6%)   180Mi (1%)       180Mi (1%)     32m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-d34856018c814171-bwsv4    0 (0%)        0 (0%)      0 (0%)           0 (0%)         4m46s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests    Limits\n  --------                    --------    ------\n  cpu                         352m (8%)   1250m (31%)\n  memory                      250Mi (1%)  1180Mi (7%)\n  ephemeral-storage           0 (0%)      0 (0%)\n  hugepages-1Gi               0 (0%)      0 (0%)\n  hugepages-2Mi               0 (0%)      0 (0%)\n  attachable-volumes-aws-ebs  0           0\nEvents:\n  Type    Reason                   Age                From             Message\n  ----    ------                   ----               ----             -------\n  Normal  Starting                 34m                kube-proxy       \n  Normal  NodeAllocatableEnforced  35m                kubelet          Updated Node Allocatable limit across pods\n  Normal  NodeHasSufficientMemory  35m (x7 over 35m)  kubelet          Node ip-10-0-1-168.us-west-2.compute.internal status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    35m (x7 over 35m)  kubelet          Node ip-10-0-1-168.us-west-2.compute.internal status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     35m (x7 over 35m)  kubelet          Node ip-10-0-1-168.us-west-2.compute.internal status is now: NodeHasSufficientPID\n  Normal  RegisteredNode           35m                node-controller  Node ip-10-0-1-168.us-west-2.compute.internal event: Registered Node ip-10-0-1-168.us-west-2.compute.internal in Controller\n"
May  4 11:49:00.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-7554 describe namespace kubectl-7554'
May  4 11:49:00.502: INFO: stderr: ""
May  4 11:49:00.502: INFO: stdout: "Name:         kubectl-7554\nLabels:       e2e-framework=kubectl\n              e2e-run=d107b977-ad83-4d56-8961-eff1e125655a\n              kubernetes.io/metadata.name=kubectl-7554\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  4 11:49:00.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7554" for this suite. 05/04/23 11:49:00.516
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","completed":21,"skipped":371,"failed":0}
------------------------------
• [4.825 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1268
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1274

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:48:55.706
    May  4 11:48:55.706: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename kubectl 05/04/23 11:48:55.707
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:48:55.732
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:48:55.736
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1274
    May  4 11:48:55.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-7554 create -f -'
    May  4 11:48:57.387: INFO: stderr: ""
    May  4 11:48:57.387: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    May  4 11:48:57.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-7554 create -f -'
    May  4 11:48:58.810: INFO: stderr: ""
    May  4 11:48:58.810: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 05/04/23 11:48:58.811
    May  4 11:48:59.817: INFO: Selector matched 1 pods for map[app:agnhost]
    May  4 11:48:59.817: INFO: Found 1 / 1
    May  4 11:48:59.817: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    May  4 11:48:59.822: INFO: Selector matched 1 pods for map[app:agnhost]
    May  4 11:48:59.822: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    May  4 11:48:59.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-7554 describe pod agnhost-primary-nmdxs'
    May  4 11:48:59.900: INFO: stderr: ""
    May  4 11:48:59.900: INFO: stdout: "Name:             agnhost-primary-nmdxs\nNamespace:        kubectl-7554\nPriority:         0\nService Account:  default\nNode:             ip-10-0-1-224.us-west-2.compute.internal/10.0.1.224\nStart Time:       Thu, 04 May 2023 11:48:57 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 22bd765c3af736d08a8c9acc8b3e925d06f2ef2507a8afe69f959f283de012e1\n                  cni.projectcalico.org/podIP: 10.20.83.17/32\n                  cni.projectcalico.org/podIPs: 10.20.83.17/32\nStatus:           Running\nIP:               10.20.83.17\nIPs:\n  IP:           10.20.83.17\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://c1f45b218eaa05863739e4c3034c73060d99854588c5fae8443f9f57a128e6e4\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 04 May 2023 11:48:58 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hmb6k (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-hmb6k:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-7554/agnhost-primary-nmdxs to ip-10-0-1-224.us-west-2.compute.internal\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
    May  4 11:48:59.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-7554 describe rc agnhost-primary'
    May  4 11:48:59.988: INFO: stderr: ""
    May  4 11:48:59.988: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-7554\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-nmdxs\n"
    May  4 11:48:59.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-7554 describe service agnhost-primary'
    May  4 11:49:00.090: INFO: stderr: ""
    May  4 11:49:00.090: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-7554\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.21.55.118\nIPs:               10.21.55.118\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.20.83.17:6379\nSession Affinity:  None\nEvents:            <none>\n"
    May  4 11:49:00.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-7554 describe node ip-10-0-1-168.us-west-2.compute.internal'
    May  4 11:49:00.398: INFO: stderr: ""
    May  4 11:49:00.398: INFO: stdout: "Name:               ip-10-0-1-168.us-west-2.compute.internal\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=t3.xlarge\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-west-2\n                    failure-domain.beta.kubernetes.io/zone=us-west-2b\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-10-0-1-168.us-west-2.compute.internal\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\n                    node.kubernetes.io/instance-type=t3.xlarge\n                    topology.kubernetes.io/region=us-west-2\n                    topology.kubernetes.io/zone=us-west-2b\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.0.1.168/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.20.112.64\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 04 May 2023 11:13:50 +0000\nTaints:             node-role.kubernetes.io/master=true:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-10-0-1-168.us-west-2.compute.internal\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 04 May 2023 11:48:53 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Thu, 04 May 2023 11:14:24 +0000   Thu, 04 May 2023 11:14:24 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Thu, 04 May 2023 11:44:59 +0000   Thu, 04 May 2023 11:13:50 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 04 May 2023 11:44:59 +0000   Thu, 04 May 2023 11:13:50 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 04 May 2023 11:44:59 +0000   Thu, 04 May 2023 11:13:50 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 04 May 2023 11:44:59 +0000   Thu, 04 May 2023 11:14:20 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:   10.0.1.168\n  ExternalIP:   54.202.40.255\n  Hostname:     ip-10-0-1-168.us-west-2.compute.internal\n  InternalDNS:  ip-10-0-1-168.us-west-2.compute.internal\n  ExternalDNS:  ec2-54-202-40-255.us-west-2.compute.amazonaws.com\nCapacity:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         4\n  ephemeral-storage:           50758604Ki\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      16197968Ki\n  pods:                        200\nAllocatable:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         4\n  ephemeral-storage:           46779129369\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      16095568Ki\n  pods:                        200\nSystem Info:\n  Machine ID:                 ec2f01927bea3e1de85f3346ede35d13\n  System UUID:                ec2f0192-7bea-3e1d-e85f-3346ede35d13\n  Boot ID:                    23fc1f1a-6fbc-4122-89d2-0c55b7e35baa\n  Kernel Version:             5.11.0-1028-aws\n  OS Image:                   Ubuntu 20.04.3 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.6\n  Kubelet Version:            v1.25.5\n  Kube-Proxy Version:         v1.25.5\nPodCIDR:                      10.20.1.0/24\nPodCIDRs:                     10.20.1.0/24\nProviderID:                   aws:///us-west-2b/i-087a4372e640f465d\nNon-terminated Pods:          (4 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-kdmh2                                          250m (6%)     1 (25%)     70Mi (0%)        1000Mi (6%)    35m\n  kube-system                 k8s-master-ip-10-0-1-168.us-west-2.compute.internal        0 (0%)        0 (0%)      0 (0%)           0 (0%)         34m\n  pf9-monitoring              node-exporter-fns5h                                        102m (2%)     250m (6%)   180Mi (1%)       180Mi (1%)     32m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-d34856018c814171-bwsv4    0 (0%)        0 (0%)      0 (0%)           0 (0%)         4m46s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests    Limits\n  --------                    --------    ------\n  cpu                         352m (8%)   1250m (31%)\n  memory                      250Mi (1%)  1180Mi (7%)\n  ephemeral-storage           0 (0%)      0 (0%)\n  hugepages-1Gi               0 (0%)      0 (0%)\n  hugepages-2Mi               0 (0%)      0 (0%)\n  attachable-volumes-aws-ebs  0           0\nEvents:\n  Type    Reason                   Age                From             Message\n  ----    ------                   ----               ----             -------\n  Normal  Starting                 34m                kube-proxy       \n  Normal  NodeAllocatableEnforced  35m                kubelet          Updated Node Allocatable limit across pods\n  Normal  NodeHasSufficientMemory  35m (x7 over 35m)  kubelet          Node ip-10-0-1-168.us-west-2.compute.internal status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    35m (x7 over 35m)  kubelet          Node ip-10-0-1-168.us-west-2.compute.internal status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     35m (x7 over 35m)  kubelet          Node ip-10-0-1-168.us-west-2.compute.internal status is now: NodeHasSufficientPID\n  Normal  RegisteredNode           35m                node-controller  Node ip-10-0-1-168.us-west-2.compute.internal event: Registered Node ip-10-0-1-168.us-west-2.compute.internal in Controller\n"
    May  4 11:49:00.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-7554 describe namespace kubectl-7554'
    May  4 11:49:00.502: INFO: stderr: ""
    May  4 11:49:00.502: INFO: stdout: "Name:         kubectl-7554\nLabels:       e2e-framework=kubectl\n              e2e-run=d107b977-ad83-4d56-8961-eff1e125655a\n              kubernetes.io/metadata.name=kubectl-7554\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  4 11:49:00.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7554" for this suite. 05/04/23 11:49:00.516
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:49:00.531
May  4 11:49:00.531: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename namespaces 05/04/23 11:49:00.533
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:49:00.553
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:49:00.556
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
STEP: creating a Namespace 05/04/23 11:49:00.56
STEP: patching the Namespace 05/04/23 11:49:00.582
STEP: get the Namespace and ensuring it has the label 05/04/23 11:49:00.59
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
May  4 11:49:00.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2566" for this suite. 05/04/23 11:49:00.607
STEP: Destroying namespace "nspatchtest-4787baea-0d82-461a-88dd-516fafde2695-248" for this suite. 05/04/23 11:49:00.626
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","completed":22,"skipped":379,"failed":0}
------------------------------
• [0.106 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:49:00.531
    May  4 11:49:00.531: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename namespaces 05/04/23 11:49:00.533
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:49:00.553
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:49:00.556
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:267
    STEP: creating a Namespace 05/04/23 11:49:00.56
    STEP: patching the Namespace 05/04/23 11:49:00.582
    STEP: get the Namespace and ensuring it has the label 05/04/23 11:49:00.59
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    May  4 11:49:00.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-2566" for this suite. 05/04/23 11:49:00.607
    STEP: Destroying namespace "nspatchtest-4787baea-0d82-461a-88dd-516fafde2695-248" for this suite. 05/04/23 11:49:00.626
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:49:00.638
May  4 11:49:00.638: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename events 05/04/23 11:49:00.639
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:49:00.655
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:49:00.657
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 05/04/23 11:49:00.66
STEP: listing events in all namespaces 05/04/23 11:49:00.679
STEP: listing events in test namespace 05/04/23 11:49:00.709
STEP: listing events with field selection filtering on source 05/04/23 11:49:00.714
STEP: listing events with field selection filtering on reportingController 05/04/23 11:49:00.719
STEP: getting the test event 05/04/23 11:49:00.725
STEP: patching the test event 05/04/23 11:49:00.729
STEP: getting the test event 05/04/23 11:49:00.748
STEP: updating the test event 05/04/23 11:49:00.752
STEP: getting the test event 05/04/23 11:49:00.764
STEP: deleting the test event 05/04/23 11:49:00.768
STEP: listing events in all namespaces 05/04/23 11:49:00.781
STEP: listing events in test namespace 05/04/23 11:49:00.821
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
May  4 11:49:00.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4086" for this suite. 05/04/23 11:49:00.833
{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","completed":23,"skipped":387,"failed":0}
------------------------------
• [0.203 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:49:00.638
    May  4 11:49:00.638: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename events 05/04/23 11:49:00.639
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:49:00.655
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:49:00.657
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 05/04/23 11:49:00.66
    STEP: listing events in all namespaces 05/04/23 11:49:00.679
    STEP: listing events in test namespace 05/04/23 11:49:00.709
    STEP: listing events with field selection filtering on source 05/04/23 11:49:00.714
    STEP: listing events with field selection filtering on reportingController 05/04/23 11:49:00.719
    STEP: getting the test event 05/04/23 11:49:00.725
    STEP: patching the test event 05/04/23 11:49:00.729
    STEP: getting the test event 05/04/23 11:49:00.748
    STEP: updating the test event 05/04/23 11:49:00.752
    STEP: getting the test event 05/04/23 11:49:00.764
    STEP: deleting the test event 05/04/23 11:49:00.768
    STEP: listing events in all namespaces 05/04/23 11:49:00.781
    STEP: listing events in test namespace 05/04/23 11:49:00.821
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    May  4 11:49:00.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-4086" for this suite. 05/04/23 11:49:00.833
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:49:00.845
May  4 11:49:00.845: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename endpointslice 05/04/23 11:49:00.846
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:49:00.867
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:49:00.871
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
STEP: referencing a single matching pod 05/04/23 11:49:06.018
STEP: referencing matching pods with named port 05/04/23 11:49:11.036
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 05/04/23 11:49:16.05
STEP: recreating EndpointSlices after they've been deleted 05/04/23 11:49:21.068
May  4 11:49:21.113: INFO: EndpointSlice for Service endpointslice-3447/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
May  4 11:49:31.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-3447" for this suite. 05/04/23 11:49:31.134
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","completed":24,"skipped":401,"failed":0}
------------------------------
• [SLOW TEST] [30.298 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:49:00.845
    May  4 11:49:00.845: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename endpointslice 05/04/23 11:49:00.846
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:49:00.867
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:49:00.871
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:204
    STEP: referencing a single matching pod 05/04/23 11:49:06.018
    STEP: referencing matching pods with named port 05/04/23 11:49:11.036
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 05/04/23 11:49:16.05
    STEP: recreating EndpointSlices after they've been deleted 05/04/23 11:49:21.068
    May  4 11:49:21.113: INFO: EndpointSlice for Service endpointslice-3447/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    May  4 11:49:31.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-3447" for this suite. 05/04/23 11:49:31.134
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:49:31.143
May  4 11:49:31.143: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename projected 05/04/23 11:49:31.144
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:49:31.163
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:49:31.166
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
STEP: Creating configMap with name projected-configmap-test-volume-55c4abcf-09cc-4837-808f-98680ae3c1cf 05/04/23 11:49:31.171
STEP: Creating a pod to test consume configMaps 05/04/23 11:49:31.185
May  4 11:49:31.204: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-280714b8-2726-4515-aae3-a2578ec7d2bf" in namespace "projected-5830" to be "Succeeded or Failed"
May  4 11:49:31.210: INFO: Pod "pod-projected-configmaps-280714b8-2726-4515-aae3-a2578ec7d2bf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.936618ms
May  4 11:49:33.215: INFO: Pod "pod-projected-configmaps-280714b8-2726-4515-aae3-a2578ec7d2bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010828134s
May  4 11:49:35.219: INFO: Pod "pod-projected-configmaps-280714b8-2726-4515-aae3-a2578ec7d2bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014253283s
STEP: Saw pod success 05/04/23 11:49:35.219
May  4 11:49:35.219: INFO: Pod "pod-projected-configmaps-280714b8-2726-4515-aae3-a2578ec7d2bf" satisfied condition "Succeeded or Failed"
May  4 11:49:35.224: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-projected-configmaps-280714b8-2726-4515-aae3-a2578ec7d2bf container agnhost-container: <nil>
STEP: delete the pod 05/04/23 11:49:35.242
May  4 11:49:35.259: INFO: Waiting for pod pod-projected-configmaps-280714b8-2726-4515-aae3-a2578ec7d2bf to disappear
May  4 11:49:35.264: INFO: Pod pod-projected-configmaps-280714b8-2726-4515-aae3-a2578ec7d2bf no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
May  4 11:49:35.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5830" for this suite. 05/04/23 11:49:35.275
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":25,"skipped":402,"failed":0}
------------------------------
• [4.144 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:49:31.143
    May  4 11:49:31.143: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename projected 05/04/23 11:49:31.144
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:49:31.163
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:49:31.166
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:73
    STEP: Creating configMap with name projected-configmap-test-volume-55c4abcf-09cc-4837-808f-98680ae3c1cf 05/04/23 11:49:31.171
    STEP: Creating a pod to test consume configMaps 05/04/23 11:49:31.185
    May  4 11:49:31.204: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-280714b8-2726-4515-aae3-a2578ec7d2bf" in namespace "projected-5830" to be "Succeeded or Failed"
    May  4 11:49:31.210: INFO: Pod "pod-projected-configmaps-280714b8-2726-4515-aae3-a2578ec7d2bf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.936618ms
    May  4 11:49:33.215: INFO: Pod "pod-projected-configmaps-280714b8-2726-4515-aae3-a2578ec7d2bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010828134s
    May  4 11:49:35.219: INFO: Pod "pod-projected-configmaps-280714b8-2726-4515-aae3-a2578ec7d2bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014253283s
    STEP: Saw pod success 05/04/23 11:49:35.219
    May  4 11:49:35.219: INFO: Pod "pod-projected-configmaps-280714b8-2726-4515-aae3-a2578ec7d2bf" satisfied condition "Succeeded or Failed"
    May  4 11:49:35.224: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-projected-configmaps-280714b8-2726-4515-aae3-a2578ec7d2bf container agnhost-container: <nil>
    STEP: delete the pod 05/04/23 11:49:35.242
    May  4 11:49:35.259: INFO: Waiting for pod pod-projected-configmaps-280714b8-2726-4515-aae3-a2578ec7d2bf to disappear
    May  4 11:49:35.264: INFO: Pod pod-projected-configmaps-280714b8-2726-4515-aae3-a2578ec7d2bf no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    May  4 11:49:35.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5830" for this suite. 05/04/23 11:49:35.275
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:49:35.288
May  4 11:49:35.288: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename var-expansion 05/04/23 11:49:35.289
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:49:35.31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:49:35.314
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
May  4 11:49:35.332: INFO: Waiting up to 2m0s for pod "var-expansion-1706c652-a67a-4105-a3e0-45c372b6be63" in namespace "var-expansion-1729" to be "container 0 failed with reason CreateContainerConfigError"
May  4 11:49:35.339: INFO: Pod "var-expansion-1706c652-a67a-4105-a3e0-45c372b6be63": Phase="Pending", Reason="", readiness=false. Elapsed: 6.649752ms
May  4 11:49:37.344: INFO: Pod "var-expansion-1706c652-a67a-4105-a3e0-45c372b6be63": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012097279s
May  4 11:49:37.344: INFO: Pod "var-expansion-1706c652-a67a-4105-a3e0-45c372b6be63" satisfied condition "container 0 failed with reason CreateContainerConfigError"
May  4 11:49:37.344: INFO: Deleting pod "var-expansion-1706c652-a67a-4105-a3e0-45c372b6be63" in namespace "var-expansion-1729"
May  4 11:49:37.355: INFO: Wait up to 5m0s for pod "var-expansion-1706c652-a67a-4105-a3e0-45c372b6be63" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
May  4 11:49:41.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1729" for this suite. 05/04/23 11:49:41.376
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","completed":26,"skipped":428,"failed":0}
------------------------------
• [SLOW TEST] [6.102 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:49:35.288
    May  4 11:49:35.288: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename var-expansion 05/04/23 11:49:35.289
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:49:35.31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:49:35.314
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:151
    May  4 11:49:35.332: INFO: Waiting up to 2m0s for pod "var-expansion-1706c652-a67a-4105-a3e0-45c372b6be63" in namespace "var-expansion-1729" to be "container 0 failed with reason CreateContainerConfigError"
    May  4 11:49:35.339: INFO: Pod "var-expansion-1706c652-a67a-4105-a3e0-45c372b6be63": Phase="Pending", Reason="", readiness=false. Elapsed: 6.649752ms
    May  4 11:49:37.344: INFO: Pod "var-expansion-1706c652-a67a-4105-a3e0-45c372b6be63": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012097279s
    May  4 11:49:37.344: INFO: Pod "var-expansion-1706c652-a67a-4105-a3e0-45c372b6be63" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    May  4 11:49:37.344: INFO: Deleting pod "var-expansion-1706c652-a67a-4105-a3e0-45c372b6be63" in namespace "var-expansion-1729"
    May  4 11:49:37.355: INFO: Wait up to 5m0s for pod "var-expansion-1706c652-a67a-4105-a3e0-45c372b6be63" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    May  4 11:49:41.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-1729" for this suite. 05/04/23 11:49:41.376
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:49:41.391
May  4 11:49:41.391: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename ingress 05/04/23 11:49:41.392
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:49:41.409
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:49:41.412
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 05/04/23 11:49:41.417
STEP: getting /apis/networking.k8s.io 05/04/23 11:49:41.42
STEP: getting /apis/networking.k8s.iov1 05/04/23 11:49:41.421
STEP: creating 05/04/23 11:49:41.424
STEP: getting 05/04/23 11:49:41.445
STEP: listing 05/04/23 11:49:41.449
STEP: watching 05/04/23 11:49:41.454
May  4 11:49:41.454: INFO: starting watch
STEP: cluster-wide listing 05/04/23 11:49:41.455
STEP: cluster-wide watching 05/04/23 11:49:41.459
May  4 11:49:41.459: INFO: starting watch
STEP: patching 05/04/23 11:49:41.461
STEP: updating 05/04/23 11:49:41.466
May  4 11:49:41.476: INFO: waiting for watch events with expected annotations
May  4 11:49:41.476: INFO: saw patched and updated annotations
STEP: patching /status 05/04/23 11:49:41.476
STEP: updating /status 05/04/23 11:49:41.483
STEP: get /status 05/04/23 11:49:41.494
STEP: deleting 05/04/23 11:49:41.497
STEP: deleting a collection 05/04/23 11:49:41.517
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
May  4 11:49:41.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-5305" for this suite. 05/04/23 11:49:41.544
{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","completed":27,"skipped":431,"failed":0}
------------------------------
• [0.162 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:49:41.391
    May  4 11:49:41.391: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename ingress 05/04/23 11:49:41.392
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:49:41.409
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:49:41.412
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 05/04/23 11:49:41.417
    STEP: getting /apis/networking.k8s.io 05/04/23 11:49:41.42
    STEP: getting /apis/networking.k8s.iov1 05/04/23 11:49:41.421
    STEP: creating 05/04/23 11:49:41.424
    STEP: getting 05/04/23 11:49:41.445
    STEP: listing 05/04/23 11:49:41.449
    STEP: watching 05/04/23 11:49:41.454
    May  4 11:49:41.454: INFO: starting watch
    STEP: cluster-wide listing 05/04/23 11:49:41.455
    STEP: cluster-wide watching 05/04/23 11:49:41.459
    May  4 11:49:41.459: INFO: starting watch
    STEP: patching 05/04/23 11:49:41.461
    STEP: updating 05/04/23 11:49:41.466
    May  4 11:49:41.476: INFO: waiting for watch events with expected annotations
    May  4 11:49:41.476: INFO: saw patched and updated annotations
    STEP: patching /status 05/04/23 11:49:41.476
    STEP: updating /status 05/04/23 11:49:41.483
    STEP: get /status 05/04/23 11:49:41.494
    STEP: deleting 05/04/23 11:49:41.497
    STEP: deleting a collection 05/04/23 11:49:41.517
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:187
    May  4 11:49:41.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingress-5305" for this suite. 05/04/23 11:49:41.544
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:49:41.554
May  4 11:49:41.554: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename statefulset 05/04/23 11:49:41.554
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:49:41.574
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:49:41.577
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-5657 05/04/23 11:49:41.58
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
STEP: Creating a new StatefulSet 05/04/23 11:49:41.59
May  4 11:49:41.605: INFO: Found 0 stateful pods, waiting for 3
May  4 11:49:51.611: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May  4 11:49:51.611: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May  4 11:49:51.611: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
May  4 11:50:01.622: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May  4 11:50:01.622: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May  4 11:50:01.622: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
May  4 11:50:01.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=statefulset-5657 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  4 11:50:01.933: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  4 11:50:01.933: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  4 11:50:01.933: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 05/04/23 11:50:11.952
May  4 11:50:11.976: INFO: Updating stateful set ss2
STEP: Creating a new revision 05/04/23 11:50:11.976
STEP: Updating Pods in reverse ordinal order 05/04/23 11:50:22.012
May  4 11:50:22.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=statefulset-5657 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  4 11:50:22.195: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  4 11:50:22.195: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  4 11:50:22.195: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  4 11:50:42.229: INFO: Waiting for StatefulSet statefulset-5657/ss2 to complete update
STEP: Rolling back to a previous revision 05/04/23 11:50:52.24
May  4 11:50:52.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=statefulset-5657 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  4 11:50:52.477: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  4 11:50:52.477: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  4 11:50:52.477: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  4 11:51:02.521: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 05/04/23 11:51:12.546
May  4 11:51:12.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=statefulset-5657 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  4 11:51:12.744: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  4 11:51:12.744: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  4 11:51:12.744: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
May  4 11:51:22.776: INFO: Deleting all statefulset in ns statefulset-5657
May  4 11:51:22.781: INFO: Scaling statefulset ss2 to 0
May  4 11:51:32.806: INFO: Waiting for statefulset status.replicas updated to 0
May  4 11:51:32.810: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
May  4 11:51:32.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5657" for this suite. 05/04/23 11:51:32.839
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","completed":28,"skipped":442,"failed":0}
------------------------------
• [SLOW TEST] [111.306 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:304

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:49:41.554
    May  4 11:49:41.554: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename statefulset 05/04/23 11:49:41.554
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:49:41.574
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:49:41.577
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-5657 05/04/23 11:49:41.58
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:304
    STEP: Creating a new StatefulSet 05/04/23 11:49:41.59
    May  4 11:49:41.605: INFO: Found 0 stateful pods, waiting for 3
    May  4 11:49:51.611: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    May  4 11:49:51.611: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    May  4 11:49:51.611: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
    May  4 11:50:01.622: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    May  4 11:50:01.622: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    May  4 11:50:01.622: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    May  4 11:50:01.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=statefulset-5657 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  4 11:50:01.933: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  4 11:50:01.933: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  4 11:50:01.933: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 05/04/23 11:50:11.952
    May  4 11:50:11.976: INFO: Updating stateful set ss2
    STEP: Creating a new revision 05/04/23 11:50:11.976
    STEP: Updating Pods in reverse ordinal order 05/04/23 11:50:22.012
    May  4 11:50:22.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=statefulset-5657 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  4 11:50:22.195: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May  4 11:50:22.195: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May  4 11:50:22.195: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May  4 11:50:42.229: INFO: Waiting for StatefulSet statefulset-5657/ss2 to complete update
    STEP: Rolling back to a previous revision 05/04/23 11:50:52.24
    May  4 11:50:52.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=statefulset-5657 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  4 11:50:52.477: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  4 11:50:52.477: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  4 11:50:52.477: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May  4 11:51:02.521: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 05/04/23 11:51:12.546
    May  4 11:51:12.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=statefulset-5657 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  4 11:51:12.744: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May  4 11:51:12.744: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May  4 11:51:12.744: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    May  4 11:51:22.776: INFO: Deleting all statefulset in ns statefulset-5657
    May  4 11:51:22.781: INFO: Scaling statefulset ss2 to 0
    May  4 11:51:32.806: INFO: Waiting for statefulset status.replicas updated to 0
    May  4 11:51:32.810: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    May  4 11:51:32.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-5657" for this suite. 05/04/23 11:51:32.839
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:51:32.861
May  4 11:51:32.862: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename downward-api 05/04/23 11:51:32.862
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:51:32.883
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:51:32.886
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
STEP: Creating a pod to test downward API volume plugin 05/04/23 11:51:32.89
May  4 11:51:32.901: INFO: Waiting up to 5m0s for pod "downwardapi-volume-238bb416-88fd-4597-8f34-fad5506ce605" in namespace "downward-api-6801" to be "Succeeded or Failed"
May  4 11:51:32.908: INFO: Pod "downwardapi-volume-238bb416-88fd-4597-8f34-fad5506ce605": Phase="Pending", Reason="", readiness=false. Elapsed: 7.204258ms
May  4 11:51:34.914: INFO: Pod "downwardapi-volume-238bb416-88fd-4597-8f34-fad5506ce605": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012786324s
May  4 11:51:36.915: INFO: Pod "downwardapi-volume-238bb416-88fd-4597-8f34-fad5506ce605": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014112767s
May  4 11:51:38.914: INFO: Pod "downwardapi-volume-238bb416-88fd-4597-8f34-fad5506ce605": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012878516s
STEP: Saw pod success 05/04/23 11:51:38.914
May  4 11:51:38.914: INFO: Pod "downwardapi-volume-238bb416-88fd-4597-8f34-fad5506ce605" satisfied condition "Succeeded or Failed"
May  4 11:51:38.919: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod downwardapi-volume-238bb416-88fd-4597-8f34-fad5506ce605 container client-container: <nil>
STEP: delete the pod 05/04/23 11:51:38.934
May  4 11:51:38.951: INFO: Waiting for pod downwardapi-volume-238bb416-88fd-4597-8f34-fad5506ce605 to disappear
May  4 11:51:38.956: INFO: Pod downwardapi-volume-238bb416-88fd-4597-8f34-fad5506ce605 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
May  4 11:51:38.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6801" for this suite. 05/04/23 11:51:38.97
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","completed":29,"skipped":487,"failed":0}
------------------------------
• [SLOW TEST] [6.118 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:51:32.861
    May  4 11:51:32.862: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename downward-api 05/04/23 11:51:32.862
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:51:32.883
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:51:32.886
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:220
    STEP: Creating a pod to test downward API volume plugin 05/04/23 11:51:32.89
    May  4 11:51:32.901: INFO: Waiting up to 5m0s for pod "downwardapi-volume-238bb416-88fd-4597-8f34-fad5506ce605" in namespace "downward-api-6801" to be "Succeeded or Failed"
    May  4 11:51:32.908: INFO: Pod "downwardapi-volume-238bb416-88fd-4597-8f34-fad5506ce605": Phase="Pending", Reason="", readiness=false. Elapsed: 7.204258ms
    May  4 11:51:34.914: INFO: Pod "downwardapi-volume-238bb416-88fd-4597-8f34-fad5506ce605": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012786324s
    May  4 11:51:36.915: INFO: Pod "downwardapi-volume-238bb416-88fd-4597-8f34-fad5506ce605": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014112767s
    May  4 11:51:38.914: INFO: Pod "downwardapi-volume-238bb416-88fd-4597-8f34-fad5506ce605": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012878516s
    STEP: Saw pod success 05/04/23 11:51:38.914
    May  4 11:51:38.914: INFO: Pod "downwardapi-volume-238bb416-88fd-4597-8f34-fad5506ce605" satisfied condition "Succeeded or Failed"
    May  4 11:51:38.919: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod downwardapi-volume-238bb416-88fd-4597-8f34-fad5506ce605 container client-container: <nil>
    STEP: delete the pod 05/04/23 11:51:38.934
    May  4 11:51:38.951: INFO: Waiting for pod downwardapi-volume-238bb416-88fd-4597-8f34-fad5506ce605 to disappear
    May  4 11:51:38.956: INFO: Pod downwardapi-volume-238bb416-88fd-4597-8f34-fad5506ce605 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    May  4 11:51:38.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6801" for this suite. 05/04/23 11:51:38.97
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:51:38.98
May  4 11:51:38.980: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename resourcequota 05/04/23 11:51:38.982
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:51:39.01
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:51:39.016
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
STEP: Counting existing ResourceQuota 05/04/23 11:51:39.019
STEP: Creating a ResourceQuota 05/04/23 11:51:44.024
STEP: Ensuring resource quota status is calculated 05/04/23 11:51:44.032
STEP: Creating a ReplicaSet 05/04/23 11:51:46.037
STEP: Ensuring resource quota status captures replicaset creation 05/04/23 11:51:46.062
STEP: Deleting a ReplicaSet 05/04/23 11:51:48.068
STEP: Ensuring resource quota status released usage 05/04/23 11:51:48.076
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
May  4 11:51:50.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-948" for this suite. 05/04/23 11:51:50.095
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","completed":30,"skipped":490,"failed":0}
------------------------------
• [SLOW TEST] [11.125 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:51:38.98
    May  4 11:51:38.980: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename resourcequota 05/04/23 11:51:38.982
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:51:39.01
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:51:39.016
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:438
    STEP: Counting existing ResourceQuota 05/04/23 11:51:39.019
    STEP: Creating a ResourceQuota 05/04/23 11:51:44.024
    STEP: Ensuring resource quota status is calculated 05/04/23 11:51:44.032
    STEP: Creating a ReplicaSet 05/04/23 11:51:46.037
    STEP: Ensuring resource quota status captures replicaset creation 05/04/23 11:51:46.062
    STEP: Deleting a ReplicaSet 05/04/23 11:51:48.068
    STEP: Ensuring resource quota status released usage 05/04/23 11:51:48.076
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    May  4 11:51:50.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-948" for this suite. 05/04/23 11:51:50.095
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:51:50.107
May  4 11:51:50.108: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename limitrange 05/04/23 11:51:50.109
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:51:50.139
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:51:50.152
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
STEP: Creating a LimitRange 05/04/23 11:51:50.158
STEP: Setting up watch 05/04/23 11:51:50.159
STEP: Submitting a LimitRange 05/04/23 11:51:50.271
STEP: Verifying LimitRange creation was observed 05/04/23 11:51:50.278
STEP: Fetching the LimitRange to ensure it has proper values 05/04/23 11:51:50.278
May  4 11:51:50.283: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
May  4 11:51:50.283: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 05/04/23 11:51:50.283
STEP: Ensuring Pod has resource requirements applied from LimitRange 05/04/23 11:51:50.291
May  4 11:51:50.296: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
May  4 11:51:50.296: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 05/04/23 11:51:50.296
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 05/04/23 11:51:50.307
May  4 11:51:50.314: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
May  4 11:51:50.314: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 05/04/23 11:51:50.314
STEP: Failing to create a Pod with more than max resources 05/04/23 11:51:50.319
STEP: Updating a LimitRange 05/04/23 11:51:50.324
STEP: Verifying LimitRange updating is effective 05/04/23 11:51:50.38
STEP: Creating a Pod with less than former min resources 05/04/23 11:51:52.389
STEP: Failing to create a Pod with more than max resources 05/04/23 11:51:52.399
STEP: Deleting a LimitRange 05/04/23 11:51:52.401
STEP: Verifying the LimitRange was deleted 05/04/23 11:51:52.418
May  4 11:51:57.427: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 05/04/23 11:51:57.427
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
May  4 11:51:57.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-6813" for this suite. 05/04/23 11:51:57.447
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","completed":31,"skipped":527,"failed":0}
------------------------------
• [SLOW TEST] [7.421 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:51:50.107
    May  4 11:51:50.108: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename limitrange 05/04/23 11:51:50.109
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:51:50.139
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:51:50.152
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:57
    STEP: Creating a LimitRange 05/04/23 11:51:50.158
    STEP: Setting up watch 05/04/23 11:51:50.159
    STEP: Submitting a LimitRange 05/04/23 11:51:50.271
    STEP: Verifying LimitRange creation was observed 05/04/23 11:51:50.278
    STEP: Fetching the LimitRange to ensure it has proper values 05/04/23 11:51:50.278
    May  4 11:51:50.283: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    May  4 11:51:50.283: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 05/04/23 11:51:50.283
    STEP: Ensuring Pod has resource requirements applied from LimitRange 05/04/23 11:51:50.291
    May  4 11:51:50.296: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    May  4 11:51:50.296: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 05/04/23 11:51:50.296
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 05/04/23 11:51:50.307
    May  4 11:51:50.314: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    May  4 11:51:50.314: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 05/04/23 11:51:50.314
    STEP: Failing to create a Pod with more than max resources 05/04/23 11:51:50.319
    STEP: Updating a LimitRange 05/04/23 11:51:50.324
    STEP: Verifying LimitRange updating is effective 05/04/23 11:51:50.38
    STEP: Creating a Pod with less than former min resources 05/04/23 11:51:52.389
    STEP: Failing to create a Pod with more than max resources 05/04/23 11:51:52.399
    STEP: Deleting a LimitRange 05/04/23 11:51:52.401
    STEP: Verifying the LimitRange was deleted 05/04/23 11:51:52.418
    May  4 11:51:57.427: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 05/04/23 11:51:57.427
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:187
    May  4 11:51:57.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "limitrange-6813" for this suite. 05/04/23 11:51:57.447
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:51:57.529
May  4 11:51:57.529: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename runtimeclass 05/04/23 11:51:57.53
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:51:57.547
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:51:57.551
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 05/04/23 11:51:57.556
STEP: getting /apis/node.k8s.io 05/04/23 11:51:57.559
STEP: getting /apis/node.k8s.io/v1 05/04/23 11:51:57.56
STEP: creating 05/04/23 11:51:57.562
STEP: watching 05/04/23 11:51:57.587
May  4 11:51:57.587: INFO: starting watch
STEP: getting 05/04/23 11:51:57.595
STEP: listing 05/04/23 11:51:57.599
STEP: patching 05/04/23 11:51:57.603
STEP: updating 05/04/23 11:51:57.609
May  4 11:51:57.614: INFO: waiting for watch events with expected annotations
STEP: deleting 05/04/23 11:51:57.614
STEP: deleting a collection 05/04/23 11:51:57.636
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
May  4 11:51:57.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-1480" for this suite. 05/04/23 11:51:57.678
{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","completed":32,"skipped":528,"failed":0}
------------------------------
• [0.158 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:51:57.529
    May  4 11:51:57.529: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename runtimeclass 05/04/23 11:51:57.53
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:51:57.547
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:51:57.551
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 05/04/23 11:51:57.556
    STEP: getting /apis/node.k8s.io 05/04/23 11:51:57.559
    STEP: getting /apis/node.k8s.io/v1 05/04/23 11:51:57.56
    STEP: creating 05/04/23 11:51:57.562
    STEP: watching 05/04/23 11:51:57.587
    May  4 11:51:57.587: INFO: starting watch
    STEP: getting 05/04/23 11:51:57.595
    STEP: listing 05/04/23 11:51:57.599
    STEP: patching 05/04/23 11:51:57.603
    STEP: updating 05/04/23 11:51:57.609
    May  4 11:51:57.614: INFO: waiting for watch events with expected annotations
    STEP: deleting 05/04/23 11:51:57.614
    STEP: deleting a collection 05/04/23 11:51:57.636
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    May  4 11:51:57.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-1480" for this suite. 05/04/23 11:51:57.678
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:51:57.69
May  4 11:51:57.690: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename crd-publish-openapi 05/04/23 11:51:57.691
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:51:57.708
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:51:57.712
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 05/04/23 11:51:57.716
May  4 11:51:57.716: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
May  4 11:52:03.722: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  4 11:52:25.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9528" for this suite. 05/04/23 11:52:25.447
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","completed":33,"skipped":556,"failed":0}
------------------------------
• [SLOW TEST] [27.766 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:51:57.69
    May  4 11:51:57.690: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename crd-publish-openapi 05/04/23 11:51:57.691
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:51:57.708
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:51:57.712
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:356
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 05/04/23 11:51:57.716
    May  4 11:51:57.716: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    May  4 11:52:03.722: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  4 11:52:25.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-9528" for this suite. 05/04/23 11:52:25.447
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:52:25.46
May  4 11:52:25.460: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename configmap 05/04/23 11:52:25.462
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:52:25.482
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:52:25.49
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
STEP: Creating configMap with name configmap-test-volume-2006e71b-a715-413d-a232-846a65f2c3a4 05/04/23 11:52:25.492
STEP: Creating a pod to test consume configMaps 05/04/23 11:52:25.499
May  4 11:52:25.512: INFO: Waiting up to 5m0s for pod "pod-configmaps-e9e16950-c9c5-484a-8da2-46de223fe7cf" in namespace "configmap-9684" to be "Succeeded or Failed"
May  4 11:52:25.516: INFO: Pod "pod-configmaps-e9e16950-c9c5-484a-8da2-46de223fe7cf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.765917ms
May  4 11:52:27.523: INFO: Pod "pod-configmaps-e9e16950-c9c5-484a-8da2-46de223fe7cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011499141s
May  4 11:52:29.522: INFO: Pod "pod-configmaps-e9e16950-c9c5-484a-8da2-46de223fe7cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009778221s
STEP: Saw pod success 05/04/23 11:52:29.522
May  4 11:52:29.522: INFO: Pod "pod-configmaps-e9e16950-c9c5-484a-8da2-46de223fe7cf" satisfied condition "Succeeded or Failed"
May  4 11:52:29.527: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod pod-configmaps-e9e16950-c9c5-484a-8da2-46de223fe7cf container configmap-volume-test: <nil>
STEP: delete the pod 05/04/23 11:52:29.548
May  4 11:52:29.569: INFO: Waiting for pod pod-configmaps-e9e16950-c9c5-484a-8da2-46de223fe7cf to disappear
May  4 11:52:29.576: INFO: Pod pod-configmaps-e9e16950-c9c5-484a-8da2-46de223fe7cf no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
May  4 11:52:29.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9684" for this suite. 05/04/23 11:52:29.587
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":34,"skipped":589,"failed":0}
------------------------------
• [4.137 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:52:25.46
    May  4 11:52:25.460: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename configmap 05/04/23 11:52:25.462
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:52:25.482
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:52:25.49
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:422
    STEP: Creating configMap with name configmap-test-volume-2006e71b-a715-413d-a232-846a65f2c3a4 05/04/23 11:52:25.492
    STEP: Creating a pod to test consume configMaps 05/04/23 11:52:25.499
    May  4 11:52:25.512: INFO: Waiting up to 5m0s for pod "pod-configmaps-e9e16950-c9c5-484a-8da2-46de223fe7cf" in namespace "configmap-9684" to be "Succeeded or Failed"
    May  4 11:52:25.516: INFO: Pod "pod-configmaps-e9e16950-c9c5-484a-8da2-46de223fe7cf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.765917ms
    May  4 11:52:27.523: INFO: Pod "pod-configmaps-e9e16950-c9c5-484a-8da2-46de223fe7cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011499141s
    May  4 11:52:29.522: INFO: Pod "pod-configmaps-e9e16950-c9c5-484a-8da2-46de223fe7cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009778221s
    STEP: Saw pod success 05/04/23 11:52:29.522
    May  4 11:52:29.522: INFO: Pod "pod-configmaps-e9e16950-c9c5-484a-8da2-46de223fe7cf" satisfied condition "Succeeded or Failed"
    May  4 11:52:29.527: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod pod-configmaps-e9e16950-c9c5-484a-8da2-46de223fe7cf container configmap-volume-test: <nil>
    STEP: delete the pod 05/04/23 11:52:29.548
    May  4 11:52:29.569: INFO: Waiting for pod pod-configmaps-e9e16950-c9c5-484a-8da2-46de223fe7cf to disappear
    May  4 11:52:29.576: INFO: Pod pod-configmaps-e9e16950-c9c5-484a-8da2-46de223fe7cf no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    May  4 11:52:29.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9684" for this suite. 05/04/23 11:52:29.587
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:52:29.598
May  4 11:52:29.598: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename resourcequota 05/04/23 11:52:29.599
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:52:29.62
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:52:29.626
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
STEP: Counting existing ResourceQuota 05/04/23 11:52:29.631
STEP: Creating a ResourceQuota 05/04/23 11:52:34.636
STEP: Ensuring resource quota status is calculated 05/04/23 11:52:34.644
STEP: Creating a Service 05/04/23 11:52:36.651
STEP: Creating a NodePort Service 05/04/23 11:52:36.683
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 05/04/23 11:52:36.73
STEP: Ensuring resource quota status captures service creation 05/04/23 11:52:36.775
STEP: Deleting Services 05/04/23 11:52:38.782
STEP: Ensuring resource quota status released usage 05/04/23 11:52:38.884
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
May  4 11:52:40.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1626" for this suite. 05/04/23 11:52:40.898
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","completed":35,"skipped":618,"failed":0}
------------------------------
• [SLOW TEST] [11.309 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:52:29.598
    May  4 11:52:29.598: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename resourcequota 05/04/23 11:52:29.599
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:52:29.62
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:52:29.626
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:90
    STEP: Counting existing ResourceQuota 05/04/23 11:52:29.631
    STEP: Creating a ResourceQuota 05/04/23 11:52:34.636
    STEP: Ensuring resource quota status is calculated 05/04/23 11:52:34.644
    STEP: Creating a Service 05/04/23 11:52:36.651
    STEP: Creating a NodePort Service 05/04/23 11:52:36.683
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 05/04/23 11:52:36.73
    STEP: Ensuring resource quota status captures service creation 05/04/23 11:52:36.775
    STEP: Deleting Services 05/04/23 11:52:38.782
    STEP: Ensuring resource quota status released usage 05/04/23 11:52:38.884
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    May  4 11:52:40.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-1626" for this suite. 05/04/23 11:52:40.898
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:52:40.91
May  4 11:52:40.910: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename var-expansion 05/04/23 11:52:40.912
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:52:40.936
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:52:40.946
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
STEP: creating the pod 05/04/23 11:52:40.954
STEP: waiting for pod running 05/04/23 11:52:40.977
May  4 11:52:40.977: INFO: Waiting up to 2m0s for pod "var-expansion-a8d21f2d-843e-48ee-99d2-8cbfce4ec2b6" in namespace "var-expansion-7799" to be "running"
May  4 11:52:40.995: INFO: Pod "var-expansion-a8d21f2d-843e-48ee-99d2-8cbfce4ec2b6": Phase="Pending", Reason="", readiness=false. Elapsed: 18.021279ms
May  4 11:52:42.999: INFO: Pod "var-expansion-a8d21f2d-843e-48ee-99d2-8cbfce4ec2b6": Phase="Running", Reason="", readiness=true. Elapsed: 2.022592098s
May  4 11:52:43.000: INFO: Pod "var-expansion-a8d21f2d-843e-48ee-99d2-8cbfce4ec2b6" satisfied condition "running"
STEP: creating a file in subpath 05/04/23 11:52:43
May  4 11:52:43.004: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-7799 PodName:var-expansion-a8d21f2d-843e-48ee-99d2-8cbfce4ec2b6 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  4 11:52:43.004: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
May  4 11:52:43.007: INFO: ExecWithOptions: Clientset creation
May  4 11:52:43.008: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/var-expansion-7799/pods/var-expansion-a8d21f2d-843e-48ee-99d2-8cbfce4ec2b6/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 05/04/23 11:52:43.111
May  4 11:52:43.115: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-7799 PodName:var-expansion-a8d21f2d-843e-48ee-99d2-8cbfce4ec2b6 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  4 11:52:43.115: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
May  4 11:52:43.116: INFO: ExecWithOptions: Clientset creation
May  4 11:52:43.116: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/var-expansion-7799/pods/var-expansion-a8d21f2d-843e-48ee-99d2-8cbfce4ec2b6/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 05/04/23 11:52:43.199
May  4 11:52:43.712: INFO: Successfully updated pod "var-expansion-a8d21f2d-843e-48ee-99d2-8cbfce4ec2b6"
STEP: waiting for annotated pod running 05/04/23 11:52:43.712
May  4 11:52:43.712: INFO: Waiting up to 2m0s for pod "var-expansion-a8d21f2d-843e-48ee-99d2-8cbfce4ec2b6" in namespace "var-expansion-7799" to be "running"
May  4 11:52:43.716: INFO: Pod "var-expansion-a8d21f2d-843e-48ee-99d2-8cbfce4ec2b6": Phase="Running", Reason="", readiness=true. Elapsed: 3.754146ms
May  4 11:52:43.716: INFO: Pod "var-expansion-a8d21f2d-843e-48ee-99d2-8cbfce4ec2b6" satisfied condition "running"
STEP: deleting the pod gracefully 05/04/23 11:52:43.716
May  4 11:52:43.716: INFO: Deleting pod "var-expansion-a8d21f2d-843e-48ee-99d2-8cbfce4ec2b6" in namespace "var-expansion-7799"
May  4 11:52:43.726: INFO: Wait up to 5m0s for pod "var-expansion-a8d21f2d-843e-48ee-99d2-8cbfce4ec2b6" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
May  4 11:53:17.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7799" for this suite. 05/04/23 11:53:17.753
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","completed":36,"skipped":669,"failed":0}
------------------------------
• [SLOW TEST] [36.851 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:52:40.91
    May  4 11:52:40.910: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename var-expansion 05/04/23 11:52:40.912
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:52:40.936
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:52:40.946
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:296
    STEP: creating the pod 05/04/23 11:52:40.954
    STEP: waiting for pod running 05/04/23 11:52:40.977
    May  4 11:52:40.977: INFO: Waiting up to 2m0s for pod "var-expansion-a8d21f2d-843e-48ee-99d2-8cbfce4ec2b6" in namespace "var-expansion-7799" to be "running"
    May  4 11:52:40.995: INFO: Pod "var-expansion-a8d21f2d-843e-48ee-99d2-8cbfce4ec2b6": Phase="Pending", Reason="", readiness=false. Elapsed: 18.021279ms
    May  4 11:52:42.999: INFO: Pod "var-expansion-a8d21f2d-843e-48ee-99d2-8cbfce4ec2b6": Phase="Running", Reason="", readiness=true. Elapsed: 2.022592098s
    May  4 11:52:43.000: INFO: Pod "var-expansion-a8d21f2d-843e-48ee-99d2-8cbfce4ec2b6" satisfied condition "running"
    STEP: creating a file in subpath 05/04/23 11:52:43
    May  4 11:52:43.004: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-7799 PodName:var-expansion-a8d21f2d-843e-48ee-99d2-8cbfce4ec2b6 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  4 11:52:43.004: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    May  4 11:52:43.007: INFO: ExecWithOptions: Clientset creation
    May  4 11:52:43.008: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/var-expansion-7799/pods/var-expansion-a8d21f2d-843e-48ee-99d2-8cbfce4ec2b6/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 05/04/23 11:52:43.111
    May  4 11:52:43.115: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-7799 PodName:var-expansion-a8d21f2d-843e-48ee-99d2-8cbfce4ec2b6 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  4 11:52:43.115: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    May  4 11:52:43.116: INFO: ExecWithOptions: Clientset creation
    May  4 11:52:43.116: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/var-expansion-7799/pods/var-expansion-a8d21f2d-843e-48ee-99d2-8cbfce4ec2b6/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 05/04/23 11:52:43.199
    May  4 11:52:43.712: INFO: Successfully updated pod "var-expansion-a8d21f2d-843e-48ee-99d2-8cbfce4ec2b6"
    STEP: waiting for annotated pod running 05/04/23 11:52:43.712
    May  4 11:52:43.712: INFO: Waiting up to 2m0s for pod "var-expansion-a8d21f2d-843e-48ee-99d2-8cbfce4ec2b6" in namespace "var-expansion-7799" to be "running"
    May  4 11:52:43.716: INFO: Pod "var-expansion-a8d21f2d-843e-48ee-99d2-8cbfce4ec2b6": Phase="Running", Reason="", readiness=true. Elapsed: 3.754146ms
    May  4 11:52:43.716: INFO: Pod "var-expansion-a8d21f2d-843e-48ee-99d2-8cbfce4ec2b6" satisfied condition "running"
    STEP: deleting the pod gracefully 05/04/23 11:52:43.716
    May  4 11:52:43.716: INFO: Deleting pod "var-expansion-a8d21f2d-843e-48ee-99d2-8cbfce4ec2b6" in namespace "var-expansion-7799"
    May  4 11:52:43.726: INFO: Wait up to 5m0s for pod "var-expansion-a8d21f2d-843e-48ee-99d2-8cbfce4ec2b6" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    May  4 11:53:17.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-7799" for this suite. 05/04/23 11:53:17.753
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:53:17.762
May  4 11:53:17.762: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename secrets 05/04/23 11:53:17.763
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:53:17.782
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:53:17.785
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
STEP: Creating secret with name secret-test-dffb80e8-14aa-4d2a-8e18-a9fb7a718386 05/04/23 11:53:17.788
STEP: Creating a pod to test consume secrets 05/04/23 11:53:17.793
May  4 11:53:17.813: INFO: Waiting up to 5m0s for pod "pod-secrets-b14a5248-775d-4938-bcc8-1812bf4c45ef" in namespace "secrets-4808" to be "Succeeded or Failed"
May  4 11:53:17.818: INFO: Pod "pod-secrets-b14a5248-775d-4938-bcc8-1812bf4c45ef": Phase="Pending", Reason="", readiness=false. Elapsed: 5.254774ms
May  4 11:53:19.824: INFO: Pod "pod-secrets-b14a5248-775d-4938-bcc8-1812bf4c45ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010973129s
May  4 11:53:21.823: INFO: Pod "pod-secrets-b14a5248-775d-4938-bcc8-1812bf4c45ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010007585s
STEP: Saw pod success 05/04/23 11:53:21.823
May  4 11:53:21.823: INFO: Pod "pod-secrets-b14a5248-775d-4938-bcc8-1812bf4c45ef" satisfied condition "Succeeded or Failed"
May  4 11:53:21.828: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod pod-secrets-b14a5248-775d-4938-bcc8-1812bf4c45ef container secret-env-test: <nil>
STEP: delete the pod 05/04/23 11:53:21.838
May  4 11:53:21.859: INFO: Waiting for pod pod-secrets-b14a5248-775d-4938-bcc8-1812bf4c45ef to disappear
May  4 11:53:21.866: INFO: Pod pod-secrets-b14a5248-775d-4938-bcc8-1812bf4c45ef no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
May  4 11:53:21.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4808" for this suite. 05/04/23 11:53:21.882
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","completed":37,"skipped":682,"failed":0}
------------------------------
• [4.127 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:53:17.762
    May  4 11:53:17.762: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename secrets 05/04/23 11:53:17.763
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:53:17.782
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:53:17.785
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:45
    STEP: Creating secret with name secret-test-dffb80e8-14aa-4d2a-8e18-a9fb7a718386 05/04/23 11:53:17.788
    STEP: Creating a pod to test consume secrets 05/04/23 11:53:17.793
    May  4 11:53:17.813: INFO: Waiting up to 5m0s for pod "pod-secrets-b14a5248-775d-4938-bcc8-1812bf4c45ef" in namespace "secrets-4808" to be "Succeeded or Failed"
    May  4 11:53:17.818: INFO: Pod "pod-secrets-b14a5248-775d-4938-bcc8-1812bf4c45ef": Phase="Pending", Reason="", readiness=false. Elapsed: 5.254774ms
    May  4 11:53:19.824: INFO: Pod "pod-secrets-b14a5248-775d-4938-bcc8-1812bf4c45ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010973129s
    May  4 11:53:21.823: INFO: Pod "pod-secrets-b14a5248-775d-4938-bcc8-1812bf4c45ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010007585s
    STEP: Saw pod success 05/04/23 11:53:21.823
    May  4 11:53:21.823: INFO: Pod "pod-secrets-b14a5248-775d-4938-bcc8-1812bf4c45ef" satisfied condition "Succeeded or Failed"
    May  4 11:53:21.828: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod pod-secrets-b14a5248-775d-4938-bcc8-1812bf4c45ef container secret-env-test: <nil>
    STEP: delete the pod 05/04/23 11:53:21.838
    May  4 11:53:21.859: INFO: Waiting for pod pod-secrets-b14a5248-775d-4938-bcc8-1812bf4c45ef to disappear
    May  4 11:53:21.866: INFO: Pod pod-secrets-b14a5248-775d-4938-bcc8-1812bf4c45ef no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    May  4 11:53:21.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4808" for this suite. 05/04/23 11:53:21.882
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:53:21.89
May  4 11:53:21.890: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename projected 05/04/23 11:53:21.891
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:53:21.908
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:53:21.913
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
STEP: Creating configMap with name projected-configmap-test-volume-40a27d8a-e3d3-46d6-b519-fcce37d764ac 05/04/23 11:53:21.92
STEP: Creating a pod to test consume configMaps 05/04/23 11:53:21.938
May  4 11:53:21.949: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-21b10fcf-e7de-4970-893a-91bef43e9b5f" in namespace "projected-1663" to be "Succeeded or Failed"
May  4 11:53:21.957: INFO: Pod "pod-projected-configmaps-21b10fcf-e7de-4970-893a-91bef43e9b5f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.308948ms
May  4 11:53:23.968: INFO: Pod "pod-projected-configmaps-21b10fcf-e7de-4970-893a-91bef43e9b5f": Phase="Running", Reason="", readiness=true. Elapsed: 2.018828537s
May  4 11:53:25.963: INFO: Pod "pod-projected-configmaps-21b10fcf-e7de-4970-893a-91bef43e9b5f": Phase="Running", Reason="", readiness=false. Elapsed: 4.014096833s
May  4 11:53:27.968: INFO: Pod "pod-projected-configmaps-21b10fcf-e7de-4970-893a-91bef43e9b5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01880326s
STEP: Saw pod success 05/04/23 11:53:27.968
May  4 11:53:27.968: INFO: Pod "pod-projected-configmaps-21b10fcf-e7de-4970-893a-91bef43e9b5f" satisfied condition "Succeeded or Failed"
May  4 11:53:27.974: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod pod-projected-configmaps-21b10fcf-e7de-4970-893a-91bef43e9b5f container agnhost-container: <nil>
STEP: delete the pod 05/04/23 11:53:27.983
May  4 11:53:28.001: INFO: Waiting for pod pod-projected-configmaps-21b10fcf-e7de-4970-893a-91bef43e9b5f to disappear
May  4 11:53:28.007: INFO: Pod pod-projected-configmaps-21b10fcf-e7de-4970-893a-91bef43e9b5f no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
May  4 11:53:28.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1663" for this suite. 05/04/23 11:53:28.016
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":38,"skipped":695,"failed":0}
------------------------------
• [SLOW TEST] [6.135 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:53:21.89
    May  4 11:53:21.890: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename projected 05/04/23 11:53:21.891
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:53:21.908
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:53:21.913
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:46
    STEP: Creating configMap with name projected-configmap-test-volume-40a27d8a-e3d3-46d6-b519-fcce37d764ac 05/04/23 11:53:21.92
    STEP: Creating a pod to test consume configMaps 05/04/23 11:53:21.938
    May  4 11:53:21.949: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-21b10fcf-e7de-4970-893a-91bef43e9b5f" in namespace "projected-1663" to be "Succeeded or Failed"
    May  4 11:53:21.957: INFO: Pod "pod-projected-configmaps-21b10fcf-e7de-4970-893a-91bef43e9b5f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.308948ms
    May  4 11:53:23.968: INFO: Pod "pod-projected-configmaps-21b10fcf-e7de-4970-893a-91bef43e9b5f": Phase="Running", Reason="", readiness=true. Elapsed: 2.018828537s
    May  4 11:53:25.963: INFO: Pod "pod-projected-configmaps-21b10fcf-e7de-4970-893a-91bef43e9b5f": Phase="Running", Reason="", readiness=false. Elapsed: 4.014096833s
    May  4 11:53:27.968: INFO: Pod "pod-projected-configmaps-21b10fcf-e7de-4970-893a-91bef43e9b5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01880326s
    STEP: Saw pod success 05/04/23 11:53:27.968
    May  4 11:53:27.968: INFO: Pod "pod-projected-configmaps-21b10fcf-e7de-4970-893a-91bef43e9b5f" satisfied condition "Succeeded or Failed"
    May  4 11:53:27.974: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod pod-projected-configmaps-21b10fcf-e7de-4970-893a-91bef43e9b5f container agnhost-container: <nil>
    STEP: delete the pod 05/04/23 11:53:27.983
    May  4 11:53:28.001: INFO: Waiting for pod pod-projected-configmaps-21b10fcf-e7de-4970-893a-91bef43e9b5f to disappear
    May  4 11:53:28.007: INFO: Pod pod-projected-configmaps-21b10fcf-e7de-4970-893a-91bef43e9b5f no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    May  4 11:53:28.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1663" for this suite. 05/04/23 11:53:28.016
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:53:28.026
May  4 11:53:28.026: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename projected 05/04/23 11:53:28.027
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:53:28.044
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:53:28.048
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
STEP: Creating the pod 05/04/23 11:53:28.051
May  4 11:53:28.065: INFO: Waiting up to 5m0s for pod "annotationupdate597a1f9b-d67d-4abd-950c-3293772a61c8" in namespace "projected-4592" to be "running and ready"
May  4 11:53:28.069: INFO: Pod "annotationupdate597a1f9b-d67d-4abd-950c-3293772a61c8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.960598ms
May  4 11:53:28.069: INFO: The phase of Pod annotationupdate597a1f9b-d67d-4abd-950c-3293772a61c8 is Pending, waiting for it to be Running (with Ready = true)
May  4 11:53:30.074: INFO: Pod "annotationupdate597a1f9b-d67d-4abd-950c-3293772a61c8": Phase="Running", Reason="", readiness=true. Elapsed: 2.009298811s
May  4 11:53:30.074: INFO: The phase of Pod annotationupdate597a1f9b-d67d-4abd-950c-3293772a61c8 is Running (Ready = true)
May  4 11:53:30.074: INFO: Pod "annotationupdate597a1f9b-d67d-4abd-950c-3293772a61c8" satisfied condition "running and ready"
May  4 11:53:30.612: INFO: Successfully updated pod "annotationupdate597a1f9b-d67d-4abd-950c-3293772a61c8"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
May  4 11:53:34.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4592" for this suite. 05/04/23 11:53:34.657
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","completed":39,"skipped":704,"failed":0}
------------------------------
• [SLOW TEST] [6.639 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:53:28.026
    May  4 11:53:28.026: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename projected 05/04/23 11:53:28.027
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:53:28.044
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:53:28.048
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:161
    STEP: Creating the pod 05/04/23 11:53:28.051
    May  4 11:53:28.065: INFO: Waiting up to 5m0s for pod "annotationupdate597a1f9b-d67d-4abd-950c-3293772a61c8" in namespace "projected-4592" to be "running and ready"
    May  4 11:53:28.069: INFO: Pod "annotationupdate597a1f9b-d67d-4abd-950c-3293772a61c8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.960598ms
    May  4 11:53:28.069: INFO: The phase of Pod annotationupdate597a1f9b-d67d-4abd-950c-3293772a61c8 is Pending, waiting for it to be Running (with Ready = true)
    May  4 11:53:30.074: INFO: Pod "annotationupdate597a1f9b-d67d-4abd-950c-3293772a61c8": Phase="Running", Reason="", readiness=true. Elapsed: 2.009298811s
    May  4 11:53:30.074: INFO: The phase of Pod annotationupdate597a1f9b-d67d-4abd-950c-3293772a61c8 is Running (Ready = true)
    May  4 11:53:30.074: INFO: Pod "annotationupdate597a1f9b-d67d-4abd-950c-3293772a61c8" satisfied condition "running and ready"
    May  4 11:53:30.612: INFO: Successfully updated pod "annotationupdate597a1f9b-d67d-4abd-950c-3293772a61c8"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    May  4 11:53:34.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4592" for this suite. 05/04/23 11:53:34.657
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:53:34.668
May  4 11:53:34.668: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename replicaset 05/04/23 11:53:34.669
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:53:34.694
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:53:34.697
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 05/04/23 11:53:34.7
STEP: Verify that the required pods have come up 05/04/23 11:53:34.708
May  4 11:53:34.713: INFO: Pod name sample-pod: Found 0 pods out of 3
May  4 11:53:39.720: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 05/04/23 11:53:39.72
May  4 11:53:39.725: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 05/04/23 11:53:39.725
STEP: DeleteCollection of the ReplicaSets 05/04/23 11:53:39.737
STEP: After DeleteCollection verify that ReplicaSets have been deleted 05/04/23 11:53:39.764
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
May  4 11:53:39.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-220" for this suite. 05/04/23 11:53:39.788
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","completed":40,"skipped":729,"failed":0}
------------------------------
• [SLOW TEST] [5.134 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:53:34.668
    May  4 11:53:34.668: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename replicaset 05/04/23 11:53:34.669
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:53:34.694
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:53:34.697
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 05/04/23 11:53:34.7
    STEP: Verify that the required pods have come up 05/04/23 11:53:34.708
    May  4 11:53:34.713: INFO: Pod name sample-pod: Found 0 pods out of 3
    May  4 11:53:39.720: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 05/04/23 11:53:39.72
    May  4 11:53:39.725: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 05/04/23 11:53:39.725
    STEP: DeleteCollection of the ReplicaSets 05/04/23 11:53:39.737
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 05/04/23 11:53:39.764
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    May  4 11:53:39.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-220" for this suite. 05/04/23 11:53:39.788
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:53:39.803
May  4 11:53:39.803: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename subpath 05/04/23 11:53:39.804
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:53:39.838
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:53:39.845
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 05/04/23 11:53:39.848
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-h49n 05/04/23 11:53:39.868
STEP: Creating a pod to test atomic-volume-subpath 05/04/23 11:53:39.868
May  4 11:53:39.883: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-h49n" in namespace "subpath-2357" to be "Succeeded or Failed"
May  4 11:53:39.891: INFO: Pod "pod-subpath-test-configmap-h49n": Phase="Pending", Reason="", readiness=false. Elapsed: 8.442033ms
May  4 11:53:41.897: INFO: Pod "pod-subpath-test-configmap-h49n": Phase="Running", Reason="", readiness=true. Elapsed: 2.014277735s
May  4 11:53:43.900: INFO: Pod "pod-subpath-test-configmap-h49n": Phase="Running", Reason="", readiness=true. Elapsed: 4.016707411s
May  4 11:53:45.902: INFO: Pod "pod-subpath-test-configmap-h49n": Phase="Running", Reason="", readiness=true. Elapsed: 6.019523345s
May  4 11:53:47.897: INFO: Pod "pod-subpath-test-configmap-h49n": Phase="Running", Reason="", readiness=true. Elapsed: 8.013684603s
May  4 11:53:49.896: INFO: Pod "pod-subpath-test-configmap-h49n": Phase="Running", Reason="", readiness=true. Elapsed: 10.01351437s
May  4 11:53:51.898: INFO: Pod "pod-subpath-test-configmap-h49n": Phase="Running", Reason="", readiness=true. Elapsed: 12.015296621s
May  4 11:53:53.914: INFO: Pod "pod-subpath-test-configmap-h49n": Phase="Running", Reason="", readiness=true. Elapsed: 14.030587909s
May  4 11:53:55.896: INFO: Pod "pod-subpath-test-configmap-h49n": Phase="Running", Reason="", readiness=true. Elapsed: 16.01293337s
May  4 11:53:57.896: INFO: Pod "pod-subpath-test-configmap-h49n": Phase="Running", Reason="", readiness=true. Elapsed: 18.013345342s
May  4 11:53:59.898: INFO: Pod "pod-subpath-test-configmap-h49n": Phase="Running", Reason="", readiness=true. Elapsed: 20.014898782s
May  4 11:54:01.899: INFO: Pod "pod-subpath-test-configmap-h49n": Phase="Running", Reason="", readiness=false. Elapsed: 22.015775691s
May  4 11:54:03.905: INFO: Pod "pod-subpath-test-configmap-h49n": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.02240045s
STEP: Saw pod success 05/04/23 11:54:03.905
May  4 11:54:03.905: INFO: Pod "pod-subpath-test-configmap-h49n" satisfied condition "Succeeded or Failed"
May  4 11:54:03.922: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod pod-subpath-test-configmap-h49n container test-container-subpath-configmap-h49n: <nil>
STEP: delete the pod 05/04/23 11:54:03.934
May  4 11:54:03.991: INFO: Waiting for pod pod-subpath-test-configmap-h49n to disappear
May  4 11:54:03.998: INFO: Pod pod-subpath-test-configmap-h49n no longer exists
STEP: Deleting pod pod-subpath-test-configmap-h49n 05/04/23 11:54:03.998
May  4 11:54:03.998: INFO: Deleting pod "pod-subpath-test-configmap-h49n" in namespace "subpath-2357"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
May  4 11:54:04.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2357" for this suite. 05/04/23 11:54:04.022
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","completed":41,"skipped":747,"failed":0}
------------------------------
• [SLOW TEST] [24.228 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:53:39.803
    May  4 11:53:39.803: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename subpath 05/04/23 11:53:39.804
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:53:39.838
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:53:39.845
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 05/04/23 11:53:39.848
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-h49n 05/04/23 11:53:39.868
    STEP: Creating a pod to test atomic-volume-subpath 05/04/23 11:53:39.868
    May  4 11:53:39.883: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-h49n" in namespace "subpath-2357" to be "Succeeded or Failed"
    May  4 11:53:39.891: INFO: Pod "pod-subpath-test-configmap-h49n": Phase="Pending", Reason="", readiness=false. Elapsed: 8.442033ms
    May  4 11:53:41.897: INFO: Pod "pod-subpath-test-configmap-h49n": Phase="Running", Reason="", readiness=true. Elapsed: 2.014277735s
    May  4 11:53:43.900: INFO: Pod "pod-subpath-test-configmap-h49n": Phase="Running", Reason="", readiness=true. Elapsed: 4.016707411s
    May  4 11:53:45.902: INFO: Pod "pod-subpath-test-configmap-h49n": Phase="Running", Reason="", readiness=true. Elapsed: 6.019523345s
    May  4 11:53:47.897: INFO: Pod "pod-subpath-test-configmap-h49n": Phase="Running", Reason="", readiness=true. Elapsed: 8.013684603s
    May  4 11:53:49.896: INFO: Pod "pod-subpath-test-configmap-h49n": Phase="Running", Reason="", readiness=true. Elapsed: 10.01351437s
    May  4 11:53:51.898: INFO: Pod "pod-subpath-test-configmap-h49n": Phase="Running", Reason="", readiness=true. Elapsed: 12.015296621s
    May  4 11:53:53.914: INFO: Pod "pod-subpath-test-configmap-h49n": Phase="Running", Reason="", readiness=true. Elapsed: 14.030587909s
    May  4 11:53:55.896: INFO: Pod "pod-subpath-test-configmap-h49n": Phase="Running", Reason="", readiness=true. Elapsed: 16.01293337s
    May  4 11:53:57.896: INFO: Pod "pod-subpath-test-configmap-h49n": Phase="Running", Reason="", readiness=true. Elapsed: 18.013345342s
    May  4 11:53:59.898: INFO: Pod "pod-subpath-test-configmap-h49n": Phase="Running", Reason="", readiness=true. Elapsed: 20.014898782s
    May  4 11:54:01.899: INFO: Pod "pod-subpath-test-configmap-h49n": Phase="Running", Reason="", readiness=false. Elapsed: 22.015775691s
    May  4 11:54:03.905: INFO: Pod "pod-subpath-test-configmap-h49n": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.02240045s
    STEP: Saw pod success 05/04/23 11:54:03.905
    May  4 11:54:03.905: INFO: Pod "pod-subpath-test-configmap-h49n" satisfied condition "Succeeded or Failed"
    May  4 11:54:03.922: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod pod-subpath-test-configmap-h49n container test-container-subpath-configmap-h49n: <nil>
    STEP: delete the pod 05/04/23 11:54:03.934
    May  4 11:54:03.991: INFO: Waiting for pod pod-subpath-test-configmap-h49n to disappear
    May  4 11:54:03.998: INFO: Pod pod-subpath-test-configmap-h49n no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-h49n 05/04/23 11:54:03.998
    May  4 11:54:03.998: INFO: Deleting pod "pod-subpath-test-configmap-h49n" in namespace "subpath-2357"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    May  4 11:54:04.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-2357" for this suite. 05/04/23 11:54:04.022
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:54:04.038
May  4 11:54:04.038: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename deployment 05/04/23 11:54:04.039
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:54:04.068
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:54:04.074
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
May  4 11:54:04.081: INFO: Creating simple deployment test-new-deployment
May  4 11:54:04.128: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 05/04/23 11:54:06.15
STEP: updating a scale subresource 05/04/23 11:54:06.154
STEP: verifying the deployment Spec.Replicas was modified 05/04/23 11:54:06.165
STEP: Patch a scale subresource 05/04/23 11:54:06.171
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May  4 11:54:06.245: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-5869  d2e3d952-d079-4544-8a98-fd5baf7c166d 9425 3 2023-05-04 11:54:04 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-05-04 11:54:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 11:54:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc008630218 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:2,UpdatedReplicas:2,AvailableReplicas:1,UnavailableReplicas:3,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-05-04 11:54:05 +0000 UTC,LastTransitionTime:2023-05-04 11:54:04 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-04 11:54:06 +0000 UTC,LastTransitionTime:2023-05-04 11:54:06 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May  4 11:54:06.253: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-5869  4c3cd081-6fea-4d66-9f41-2a70e5cd7753 9432 3 2023-05-04 11:54:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment d2e3d952-d079-4544-8a98-fd5baf7c166d 0xc00871ffc0 0xc00871ffc1}] [] [{kube-controller-manager Update apps/v1 2023-05-04 11:54:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d2e3d952-d079-4544-8a98-fd5baf7c166d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 11:54:06 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc008758048 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:4,FullyLabeledReplicas:4,ObservedGeneration:3,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May  4 11:54:06.260: INFO: Pod "test-new-deployment-845c8977d9-pt2td" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-pt2td test-new-deployment-845c8977d9- deployment-5869  887af3d0-a349-4660-a885-acce844d64f2 9433 0 2023-05-04 11:54:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 4c3cd081-6fea-4d66-9f41-2a70e5cd7753 0xc008758410 0xc008758411}] [] [{kube-controller-manager Update v1 2023-05-04 11:54:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4c3cd081-6fea-4d66-9f41-2a70e5cd7753\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-04 11:54:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hfxg9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hfxg9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-189.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 11:54:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 11:54:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 11:54:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 11:54:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.189,PodIP:,StartTime:2023-05-04 11:54:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  4 11:54:06.261: INFO: Pod "test-new-deployment-845c8977d9-rjdh7" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-rjdh7 test-new-deployment-845c8977d9- deployment-5869  a622f228-3c51-4b77-8d43-4407c329d79a 9421 0 2023-05-04 11:54:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 4c3cd081-6fea-4d66-9f41-2a70e5cd7753 0xc0087585c0 0xc0087585c1}] [] [{kube-controller-manager Update v1 2023-05-04 11:54:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4c3cd081-6fea-4d66-9f41-2a70e5cd7753\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-04 11:54:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qswk8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qswk8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-216.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 11:54:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 11:54:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 11:54:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 11:54:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.216,PodIP:,StartTime:2023-05-04 11:54:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  4 11:54:06.261: INFO: Pod "test-new-deployment-845c8977d9-srqlj" is available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-srqlj test-new-deployment-845c8977d9- deployment-5869  07bf7201-9127-4858-a044-06bb858e64ae 9400 0 2023-05-04 11:54:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:a2426d6ca86b420922c63c7dbf2939b62f32cd642298f0426bc3b2833fec84dc cni.projectcalico.org/podIP:10.20.83.25/32 cni.projectcalico.org/podIPs:10.20.83.25/32] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 4c3cd081-6fea-4d66-9f41-2a70e5cd7753 0xc008758790 0xc008758791}] [] [{calico Update v1 2023-05-04 11:54:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-04 11:54:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4c3cd081-6fea-4d66-9f41-2a70e5cd7753\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-04 11:54:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.83.25\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6mgnp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6mgnp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-224.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 11:54:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 11:54:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 11:54:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 11:54:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.224,PodIP:10.20.83.25,StartTime:2023-05-04 11:54:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-04 11:54:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://06177145c2d03185a1913fe0591ad27562e4db57daddb168c6b1c6f3ce577423,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.83.25,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  4 11:54:06.261: INFO: Pod "test-new-deployment-845c8977d9-v46mm" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-v46mm test-new-deployment-845c8977d9- deployment-5869  d32a5eeb-e037-4912-873a-eadc5070ae16 9430 0 2023-05-04 11:54:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 4c3cd081-6fea-4d66-9f41-2a70e5cd7753 0xc008758980 0xc008758981}] [] [{kube-controller-manager Update v1 2023-05-04 11:54:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4c3cd081-6fea-4d66-9f41-2a70e5cd7753\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-04 11:54:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hm65l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hm65l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-232.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 11:54:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 11:54:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 11:54:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 11:54:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.232,PodIP:,StartTime:2023-05-04 11:54:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
May  4 11:54:06.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5869" for this suite. 05/04/23 11:54:06.271
{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","completed":42,"skipped":781,"failed":0}
------------------------------
• [2.241 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:54:04.038
    May  4 11:54:04.038: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename deployment 05/04/23 11:54:04.039
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:54:04.068
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:54:04.074
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    May  4 11:54:04.081: INFO: Creating simple deployment test-new-deployment
    May  4 11:54:04.128: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 05/04/23 11:54:06.15
    STEP: updating a scale subresource 05/04/23 11:54:06.154
    STEP: verifying the deployment Spec.Replicas was modified 05/04/23 11:54:06.165
    STEP: Patch a scale subresource 05/04/23 11:54:06.171
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May  4 11:54:06.245: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-5869  d2e3d952-d079-4544-8a98-fd5baf7c166d 9425 3 2023-05-04 11:54:04 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-05-04 11:54:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 11:54:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc008630218 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:2,UpdatedReplicas:2,AvailableReplicas:1,UnavailableReplicas:3,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-05-04 11:54:05 +0000 UTC,LastTransitionTime:2023-05-04 11:54:04 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-04 11:54:06 +0000 UTC,LastTransitionTime:2023-05-04 11:54:06 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    May  4 11:54:06.253: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-5869  4c3cd081-6fea-4d66-9f41-2a70e5cd7753 9432 3 2023-05-04 11:54:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment d2e3d952-d079-4544-8a98-fd5baf7c166d 0xc00871ffc0 0xc00871ffc1}] [] [{kube-controller-manager Update apps/v1 2023-05-04 11:54:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d2e3d952-d079-4544-8a98-fd5baf7c166d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 11:54:06 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc008758048 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:4,FullyLabeledReplicas:4,ObservedGeneration:3,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    May  4 11:54:06.260: INFO: Pod "test-new-deployment-845c8977d9-pt2td" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-pt2td test-new-deployment-845c8977d9- deployment-5869  887af3d0-a349-4660-a885-acce844d64f2 9433 0 2023-05-04 11:54:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 4c3cd081-6fea-4d66-9f41-2a70e5cd7753 0xc008758410 0xc008758411}] [] [{kube-controller-manager Update v1 2023-05-04 11:54:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4c3cd081-6fea-4d66-9f41-2a70e5cd7753\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-04 11:54:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hfxg9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hfxg9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-189.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 11:54:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 11:54:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 11:54:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 11:54:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.189,PodIP:,StartTime:2023-05-04 11:54:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  4 11:54:06.261: INFO: Pod "test-new-deployment-845c8977d9-rjdh7" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-rjdh7 test-new-deployment-845c8977d9- deployment-5869  a622f228-3c51-4b77-8d43-4407c329d79a 9421 0 2023-05-04 11:54:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 4c3cd081-6fea-4d66-9f41-2a70e5cd7753 0xc0087585c0 0xc0087585c1}] [] [{kube-controller-manager Update v1 2023-05-04 11:54:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4c3cd081-6fea-4d66-9f41-2a70e5cd7753\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-04 11:54:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qswk8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qswk8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-216.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 11:54:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 11:54:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 11:54:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 11:54:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.216,PodIP:,StartTime:2023-05-04 11:54:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  4 11:54:06.261: INFO: Pod "test-new-deployment-845c8977d9-srqlj" is available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-srqlj test-new-deployment-845c8977d9- deployment-5869  07bf7201-9127-4858-a044-06bb858e64ae 9400 0 2023-05-04 11:54:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:a2426d6ca86b420922c63c7dbf2939b62f32cd642298f0426bc3b2833fec84dc cni.projectcalico.org/podIP:10.20.83.25/32 cni.projectcalico.org/podIPs:10.20.83.25/32] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 4c3cd081-6fea-4d66-9f41-2a70e5cd7753 0xc008758790 0xc008758791}] [] [{calico Update v1 2023-05-04 11:54:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-04 11:54:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4c3cd081-6fea-4d66-9f41-2a70e5cd7753\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-04 11:54:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.83.25\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6mgnp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6mgnp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-224.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 11:54:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 11:54:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 11:54:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 11:54:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.224,PodIP:10.20.83.25,StartTime:2023-05-04 11:54:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-04 11:54:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://06177145c2d03185a1913fe0591ad27562e4db57daddb168c6b1c6f3ce577423,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.83.25,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  4 11:54:06.261: INFO: Pod "test-new-deployment-845c8977d9-v46mm" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-v46mm test-new-deployment-845c8977d9- deployment-5869  d32a5eeb-e037-4912-873a-eadc5070ae16 9430 0 2023-05-04 11:54:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 4c3cd081-6fea-4d66-9f41-2a70e5cd7753 0xc008758980 0xc008758981}] [] [{kube-controller-manager Update v1 2023-05-04 11:54:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4c3cd081-6fea-4d66-9f41-2a70e5cd7753\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-04 11:54:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hm65l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hm65l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-232.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 11:54:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 11:54:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 11:54:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 11:54:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.232,PodIP:,StartTime:2023-05-04 11:54:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    May  4 11:54:06.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-5869" for this suite. 05/04/23 11:54:06.271
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:54:06.284
May  4 11:54:06.284: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename downward-api 05/04/23 11:54:06.286
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:54:06.308
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:54:06.316
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
STEP: Creating a pod to test downward API volume plugin 05/04/23 11:54:06.323
May  4 11:54:06.355: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cf6a2c7d-739c-4071-a893-f93d6ae5509a" in namespace "downward-api-7770" to be "Succeeded or Failed"
May  4 11:54:06.359: INFO: Pod "downwardapi-volume-cf6a2c7d-739c-4071-a893-f93d6ae5509a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.236509ms
May  4 11:54:08.366: INFO: Pod "downwardapi-volume-cf6a2c7d-739c-4071-a893-f93d6ae5509a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011348499s
May  4 11:54:10.367: INFO: Pod "downwardapi-volume-cf6a2c7d-739c-4071-a893-f93d6ae5509a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011387383s
STEP: Saw pod success 05/04/23 11:54:10.367
May  4 11:54:10.367: INFO: Pod "downwardapi-volume-cf6a2c7d-739c-4071-a893-f93d6ae5509a" satisfied condition "Succeeded or Failed"
May  4 11:54:10.371: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod downwardapi-volume-cf6a2c7d-739c-4071-a893-f93d6ae5509a container client-container: <nil>
STEP: delete the pod 05/04/23 11:54:10.381
May  4 11:54:10.395: INFO: Waiting for pod downwardapi-volume-cf6a2c7d-739c-4071-a893-f93d6ae5509a to disappear
May  4 11:54:10.401: INFO: Pod downwardapi-volume-cf6a2c7d-739c-4071-a893-f93d6ae5509a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
May  4 11:54:10.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7770" for this suite. 05/04/23 11:54:10.412
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":43,"skipped":791,"failed":0}
------------------------------
• [4.138 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:54:06.284
    May  4 11:54:06.284: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename downward-api 05/04/23 11:54:06.286
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:54:06.308
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:54:06.316
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:83
    STEP: Creating a pod to test downward API volume plugin 05/04/23 11:54:06.323
    May  4 11:54:06.355: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cf6a2c7d-739c-4071-a893-f93d6ae5509a" in namespace "downward-api-7770" to be "Succeeded or Failed"
    May  4 11:54:06.359: INFO: Pod "downwardapi-volume-cf6a2c7d-739c-4071-a893-f93d6ae5509a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.236509ms
    May  4 11:54:08.366: INFO: Pod "downwardapi-volume-cf6a2c7d-739c-4071-a893-f93d6ae5509a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011348499s
    May  4 11:54:10.367: INFO: Pod "downwardapi-volume-cf6a2c7d-739c-4071-a893-f93d6ae5509a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011387383s
    STEP: Saw pod success 05/04/23 11:54:10.367
    May  4 11:54:10.367: INFO: Pod "downwardapi-volume-cf6a2c7d-739c-4071-a893-f93d6ae5509a" satisfied condition "Succeeded or Failed"
    May  4 11:54:10.371: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod downwardapi-volume-cf6a2c7d-739c-4071-a893-f93d6ae5509a container client-container: <nil>
    STEP: delete the pod 05/04/23 11:54:10.381
    May  4 11:54:10.395: INFO: Waiting for pod downwardapi-volume-cf6a2c7d-739c-4071-a893-f93d6ae5509a to disappear
    May  4 11:54:10.401: INFO: Pod downwardapi-volume-cf6a2c7d-739c-4071-a893-f93d6ae5509a no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    May  4 11:54:10.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7770" for this suite. 05/04/23 11:54:10.412
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:54:10.423
May  4 11:54:10.423: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename pod-network-test 05/04/23 11:54:10.424
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:54:10.445
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:54:10.451
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-1103 05/04/23 11:54:10.454
STEP: creating a selector 05/04/23 11:54:10.454
STEP: Creating the service pods in kubernetes 05/04/23 11:54:10.454
May  4 11:54:10.454: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May  4 11:54:10.547: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1103" to be "running and ready"
May  4 11:54:10.558: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.280962ms
May  4 11:54:10.558: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May  4 11:54:12.564: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.016996927s
May  4 11:54:12.564: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  4 11:54:14.564: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.017725715s
May  4 11:54:14.564: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  4 11:54:16.563: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.016588981s
May  4 11:54:16.563: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  4 11:54:18.564: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.017599671s
May  4 11:54:18.564: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  4 11:54:20.564: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.017497317s
May  4 11:54:20.564: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  4 11:54:22.563: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.016707611s
May  4 11:54:22.563: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  4 11:54:24.564: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.017380328s
May  4 11:54:24.564: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  4 11:54:26.564: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.017753613s
May  4 11:54:26.564: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  4 11:54:28.565: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.018737495s
May  4 11:54:28.565: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  4 11:54:30.563: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.016731818s
May  4 11:54:30.563: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  4 11:54:32.563: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.016461868s
May  4 11:54:32.563: INFO: The phase of Pod netserver-0 is Running (Ready = true)
May  4 11:54:32.563: INFO: Pod "netserver-0" satisfied condition "running and ready"
May  4 11:54:32.569: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1103" to be "running and ready"
May  4 11:54:32.581: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 12.399376ms
May  4 11:54:32.581: INFO: The phase of Pod netserver-1 is Running (Ready = true)
May  4 11:54:32.581: INFO: Pod "netserver-1" satisfied condition "running and ready"
May  4 11:54:32.586: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-1103" to be "running and ready"
May  4 11:54:32.592: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.689621ms
May  4 11:54:32.592: INFO: The phase of Pod netserver-2 is Running (Ready = true)
May  4 11:54:32.592: INFO: Pod "netserver-2" satisfied condition "running and ready"
May  4 11:54:32.599: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-1103" to be "running and ready"
May  4 11:54:32.604: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 4.912567ms
May  4 11:54:32.604: INFO: The phase of Pod netserver-3 is Running (Ready = true)
May  4 11:54:32.604: INFO: Pod "netserver-3" satisfied condition "running and ready"
May  4 11:54:32.608: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-1103" to be "running and ready"
May  4 11:54:32.613: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 5.05307ms
May  4 11:54:32.613: INFO: The phase of Pod netserver-4 is Running (Ready = true)
May  4 11:54:32.613: INFO: Pod "netserver-4" satisfied condition "running and ready"
STEP: Creating test pods 05/04/23 11:54:32.617
May  4 11:54:32.634: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1103" to be "running"
May  4 11:54:32.640: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.451304ms
May  4 11:54:34.646: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.01195729s
May  4 11:54:34.646: INFO: Pod "test-container-pod" satisfied condition "running"
May  4 11:54:34.651: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-1103" to be "running"
May  4 11:54:34.657: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 5.569019ms
May  4 11:54:34.657: INFO: Pod "host-test-container-pod" satisfied condition "running"
May  4 11:54:34.663: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
May  4 11:54:34.663: INFO: Going to poll 10.20.11.197 on port 8083 at least 0 times, with a maximum of 55 tries before failing
May  4 11:54:34.667: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.20.11.197:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1103 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  4 11:54:34.667: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
May  4 11:54:34.668: INFO: ExecWithOptions: Clientset creation
May  4 11:54:34.668: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-1103/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.20.11.197%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May  4 11:54:34.801: INFO: Found all 1 expected endpoints: [netserver-0]
May  4 11:54:34.801: INFO: Going to poll 10.20.142.153 on port 8083 at least 0 times, with a maximum of 55 tries before failing
May  4 11:54:34.807: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.20.142.153:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1103 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  4 11:54:34.807: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
May  4 11:54:34.807: INFO: ExecWithOptions: Clientset creation
May  4 11:54:34.807: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-1103/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.20.142.153%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May  4 11:54:34.910: INFO: Found all 1 expected endpoints: [netserver-1]
May  4 11:54:34.910: INFO: Going to poll 10.20.83.27 on port 8083 at least 0 times, with a maximum of 55 tries before failing
May  4 11:54:34.919: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.20.83.27:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1103 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  4 11:54:34.919: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
May  4 11:54:34.920: INFO: ExecWithOptions: Clientset creation
May  4 11:54:34.920: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-1103/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.20.83.27%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May  4 11:54:35.004: INFO: Found all 1 expected endpoints: [netserver-2]
May  4 11:54:35.004: INFO: Going to poll 10.20.92.140 on port 8083 at least 0 times, with a maximum of 55 tries before failing
May  4 11:54:35.012: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.20.92.140:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1103 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  4 11:54:35.012: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
May  4 11:54:35.013: INFO: ExecWithOptions: Clientset creation
May  4 11:54:35.013: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-1103/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.20.92.140%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May  4 11:54:35.140: INFO: Found all 1 expected endpoints: [netserver-3]
May  4 11:54:35.140: INFO: Going to poll 10.20.1.133 on port 8083 at least 0 times, with a maximum of 55 tries before failing
May  4 11:54:35.145: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.20.1.133:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1103 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  4 11:54:35.145: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
May  4 11:54:35.146: INFO: ExecWithOptions: Clientset creation
May  4 11:54:35.146: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-1103/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.20.1.133%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May  4 11:54:35.247: INFO: Found all 1 expected endpoints: [netserver-4]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
May  4 11:54:35.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1103" for this suite. 05/04/23 11:54:35.261
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","completed":44,"skipped":795,"failed":0}
------------------------------
• [SLOW TEST] [24.849 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:54:10.423
    May  4 11:54:10.423: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename pod-network-test 05/04/23 11:54:10.424
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:54:10.445
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:54:10.451
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-1103 05/04/23 11:54:10.454
    STEP: creating a selector 05/04/23 11:54:10.454
    STEP: Creating the service pods in kubernetes 05/04/23 11:54:10.454
    May  4 11:54:10.454: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    May  4 11:54:10.547: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1103" to be "running and ready"
    May  4 11:54:10.558: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.280962ms
    May  4 11:54:10.558: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    May  4 11:54:12.564: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.016996927s
    May  4 11:54:12.564: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  4 11:54:14.564: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.017725715s
    May  4 11:54:14.564: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  4 11:54:16.563: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.016588981s
    May  4 11:54:16.563: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  4 11:54:18.564: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.017599671s
    May  4 11:54:18.564: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  4 11:54:20.564: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.017497317s
    May  4 11:54:20.564: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  4 11:54:22.563: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.016707611s
    May  4 11:54:22.563: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  4 11:54:24.564: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.017380328s
    May  4 11:54:24.564: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  4 11:54:26.564: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.017753613s
    May  4 11:54:26.564: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  4 11:54:28.565: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.018737495s
    May  4 11:54:28.565: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  4 11:54:30.563: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.016731818s
    May  4 11:54:30.563: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  4 11:54:32.563: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.016461868s
    May  4 11:54:32.563: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    May  4 11:54:32.563: INFO: Pod "netserver-0" satisfied condition "running and ready"
    May  4 11:54:32.569: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1103" to be "running and ready"
    May  4 11:54:32.581: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 12.399376ms
    May  4 11:54:32.581: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    May  4 11:54:32.581: INFO: Pod "netserver-1" satisfied condition "running and ready"
    May  4 11:54:32.586: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-1103" to be "running and ready"
    May  4 11:54:32.592: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.689621ms
    May  4 11:54:32.592: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    May  4 11:54:32.592: INFO: Pod "netserver-2" satisfied condition "running and ready"
    May  4 11:54:32.599: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-1103" to be "running and ready"
    May  4 11:54:32.604: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 4.912567ms
    May  4 11:54:32.604: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    May  4 11:54:32.604: INFO: Pod "netserver-3" satisfied condition "running and ready"
    May  4 11:54:32.608: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-1103" to be "running and ready"
    May  4 11:54:32.613: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 5.05307ms
    May  4 11:54:32.613: INFO: The phase of Pod netserver-4 is Running (Ready = true)
    May  4 11:54:32.613: INFO: Pod "netserver-4" satisfied condition "running and ready"
    STEP: Creating test pods 05/04/23 11:54:32.617
    May  4 11:54:32.634: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1103" to be "running"
    May  4 11:54:32.640: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.451304ms
    May  4 11:54:34.646: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.01195729s
    May  4 11:54:34.646: INFO: Pod "test-container-pod" satisfied condition "running"
    May  4 11:54:34.651: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-1103" to be "running"
    May  4 11:54:34.657: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 5.569019ms
    May  4 11:54:34.657: INFO: Pod "host-test-container-pod" satisfied condition "running"
    May  4 11:54:34.663: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
    May  4 11:54:34.663: INFO: Going to poll 10.20.11.197 on port 8083 at least 0 times, with a maximum of 55 tries before failing
    May  4 11:54:34.667: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.20.11.197:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1103 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  4 11:54:34.667: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    May  4 11:54:34.668: INFO: ExecWithOptions: Clientset creation
    May  4 11:54:34.668: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-1103/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.20.11.197%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May  4 11:54:34.801: INFO: Found all 1 expected endpoints: [netserver-0]
    May  4 11:54:34.801: INFO: Going to poll 10.20.142.153 on port 8083 at least 0 times, with a maximum of 55 tries before failing
    May  4 11:54:34.807: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.20.142.153:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1103 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  4 11:54:34.807: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    May  4 11:54:34.807: INFO: ExecWithOptions: Clientset creation
    May  4 11:54:34.807: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-1103/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.20.142.153%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May  4 11:54:34.910: INFO: Found all 1 expected endpoints: [netserver-1]
    May  4 11:54:34.910: INFO: Going to poll 10.20.83.27 on port 8083 at least 0 times, with a maximum of 55 tries before failing
    May  4 11:54:34.919: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.20.83.27:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1103 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  4 11:54:34.919: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    May  4 11:54:34.920: INFO: ExecWithOptions: Clientset creation
    May  4 11:54:34.920: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-1103/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.20.83.27%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May  4 11:54:35.004: INFO: Found all 1 expected endpoints: [netserver-2]
    May  4 11:54:35.004: INFO: Going to poll 10.20.92.140 on port 8083 at least 0 times, with a maximum of 55 tries before failing
    May  4 11:54:35.012: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.20.92.140:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1103 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  4 11:54:35.012: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    May  4 11:54:35.013: INFO: ExecWithOptions: Clientset creation
    May  4 11:54:35.013: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-1103/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.20.92.140%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May  4 11:54:35.140: INFO: Found all 1 expected endpoints: [netserver-3]
    May  4 11:54:35.140: INFO: Going to poll 10.20.1.133 on port 8083 at least 0 times, with a maximum of 55 tries before failing
    May  4 11:54:35.145: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.20.1.133:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1103 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  4 11:54:35.145: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    May  4 11:54:35.146: INFO: ExecWithOptions: Clientset creation
    May  4 11:54:35.146: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-1103/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.20.1.133%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May  4 11:54:35.247: INFO: Found all 1 expected endpoints: [netserver-4]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    May  4 11:54:35.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-1103" for this suite. 05/04/23 11:54:35.261
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:54:35.274
May  4 11:54:35.274: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename downward-api 05/04/23 11:54:35.275
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:54:35.294
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:54:35.301
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
STEP: Creating a pod to test downward api env vars 05/04/23 11:54:35.304
May  4 11:54:35.317: INFO: Waiting up to 5m0s for pod "downward-api-52eb3567-2b4f-42e9-94a0-a523e33f3c74" in namespace "downward-api-6675" to be "Succeeded or Failed"
May  4 11:54:35.322: INFO: Pod "downward-api-52eb3567-2b4f-42e9-94a0-a523e33f3c74": Phase="Pending", Reason="", readiness=false. Elapsed: 5.219866ms
May  4 11:54:37.327: INFO: Pod "downward-api-52eb3567-2b4f-42e9-94a0-a523e33f3c74": Phase="Running", Reason="", readiness=true. Elapsed: 2.01037988s
May  4 11:54:39.349: INFO: Pod "downward-api-52eb3567-2b4f-42e9-94a0-a523e33f3c74": Phase="Running", Reason="", readiness=false. Elapsed: 4.031582819s
May  4 11:54:41.328: INFO: Pod "downward-api-52eb3567-2b4f-42e9-94a0-a523e33f3c74": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011165272s
STEP: Saw pod success 05/04/23 11:54:41.328
May  4 11:54:41.328: INFO: Pod "downward-api-52eb3567-2b4f-42e9-94a0-a523e33f3c74" satisfied condition "Succeeded or Failed"
May  4 11:54:41.332: INFO: Trying to get logs from node ip-10-0-1-232.us-west-2.compute.internal pod downward-api-52eb3567-2b4f-42e9-94a0-a523e33f3c74 container dapi-container: <nil>
STEP: delete the pod 05/04/23 11:54:41.355
May  4 11:54:41.371: INFO: Waiting for pod downward-api-52eb3567-2b4f-42e9-94a0-a523e33f3c74 to disappear
May  4 11:54:41.375: INFO: Pod downward-api-52eb3567-2b4f-42e9-94a0-a523e33f3c74 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
May  4 11:54:41.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6675" for this suite. 05/04/23 11:54:41.383
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","completed":45,"skipped":802,"failed":0}
------------------------------
• [SLOW TEST] [6.116 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:54:35.274
    May  4 11:54:35.274: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename downward-api 05/04/23 11:54:35.275
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:54:35.294
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:54:35.301
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:89
    STEP: Creating a pod to test downward api env vars 05/04/23 11:54:35.304
    May  4 11:54:35.317: INFO: Waiting up to 5m0s for pod "downward-api-52eb3567-2b4f-42e9-94a0-a523e33f3c74" in namespace "downward-api-6675" to be "Succeeded or Failed"
    May  4 11:54:35.322: INFO: Pod "downward-api-52eb3567-2b4f-42e9-94a0-a523e33f3c74": Phase="Pending", Reason="", readiness=false. Elapsed: 5.219866ms
    May  4 11:54:37.327: INFO: Pod "downward-api-52eb3567-2b4f-42e9-94a0-a523e33f3c74": Phase="Running", Reason="", readiness=true. Elapsed: 2.01037988s
    May  4 11:54:39.349: INFO: Pod "downward-api-52eb3567-2b4f-42e9-94a0-a523e33f3c74": Phase="Running", Reason="", readiness=false. Elapsed: 4.031582819s
    May  4 11:54:41.328: INFO: Pod "downward-api-52eb3567-2b4f-42e9-94a0-a523e33f3c74": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011165272s
    STEP: Saw pod success 05/04/23 11:54:41.328
    May  4 11:54:41.328: INFO: Pod "downward-api-52eb3567-2b4f-42e9-94a0-a523e33f3c74" satisfied condition "Succeeded or Failed"
    May  4 11:54:41.332: INFO: Trying to get logs from node ip-10-0-1-232.us-west-2.compute.internal pod downward-api-52eb3567-2b4f-42e9-94a0-a523e33f3c74 container dapi-container: <nil>
    STEP: delete the pod 05/04/23 11:54:41.355
    May  4 11:54:41.371: INFO: Waiting for pod downward-api-52eb3567-2b4f-42e9-94a0-a523e33f3c74 to disappear
    May  4 11:54:41.375: INFO: Pod downward-api-52eb3567-2b4f-42e9-94a0-a523e33f3c74 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    May  4 11:54:41.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6675" for this suite. 05/04/23 11:54:41.383
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:54:41.391
May  4 11:54:41.391: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename security-context-test 05/04/23 11:54:41.392
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:54:41.418
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:54:41.421
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
May  4 11:54:41.441: INFO: Waiting up to 5m0s for pod "busybox-user-65534-ac7105fb-5cc5-4312-bb22-853e7d15f8c5" in namespace "security-context-test-5447" to be "Succeeded or Failed"
May  4 11:54:41.448: INFO: Pod "busybox-user-65534-ac7105fb-5cc5-4312-bb22-853e7d15f8c5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.92418ms
May  4 11:54:43.456: INFO: Pod "busybox-user-65534-ac7105fb-5cc5-4312-bb22-853e7d15f8c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014442918s
May  4 11:54:45.454: INFO: Pod "busybox-user-65534-ac7105fb-5cc5-4312-bb22-853e7d15f8c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0129109s
May  4 11:54:45.454: INFO: Pod "busybox-user-65534-ac7105fb-5cc5-4312-bb22-853e7d15f8c5" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
May  4 11:54:45.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5447" for this suite. 05/04/23 11:54:45.462
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","completed":46,"skipped":806,"failed":0}
------------------------------
• [4.080 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:308
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:54:41.391
    May  4 11:54:41.391: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename security-context-test 05/04/23 11:54:41.392
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:54:41.418
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:54:41.421
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:346
    May  4 11:54:41.441: INFO: Waiting up to 5m0s for pod "busybox-user-65534-ac7105fb-5cc5-4312-bb22-853e7d15f8c5" in namespace "security-context-test-5447" to be "Succeeded or Failed"
    May  4 11:54:41.448: INFO: Pod "busybox-user-65534-ac7105fb-5cc5-4312-bb22-853e7d15f8c5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.92418ms
    May  4 11:54:43.456: INFO: Pod "busybox-user-65534-ac7105fb-5cc5-4312-bb22-853e7d15f8c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014442918s
    May  4 11:54:45.454: INFO: Pod "busybox-user-65534-ac7105fb-5cc5-4312-bb22-853e7d15f8c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0129109s
    May  4 11:54:45.454: INFO: Pod "busybox-user-65534-ac7105fb-5cc5-4312-bb22-853e7d15f8c5" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    May  4 11:54:45.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-5447" for this suite. 05/04/23 11:54:45.462
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:54:45.474
May  4 11:54:45.474: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename certificates 05/04/23 11:54:45.475
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:54:45.496
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:54:45.502
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 05/04/23 11:54:46.136
STEP: getting /apis/certificates.k8s.io 05/04/23 11:54:46.139
STEP: getting /apis/certificates.k8s.io/v1 05/04/23 11:54:46.14
STEP: creating 05/04/23 11:54:46.141
STEP: getting 05/04/23 11:54:46.172
STEP: listing 05/04/23 11:54:46.18
STEP: watching 05/04/23 11:54:46.19
May  4 11:54:46.190: INFO: starting watch
STEP: patching 05/04/23 11:54:46.192
STEP: updating 05/04/23 11:54:46.202
May  4 11:54:46.213: INFO: waiting for watch events with expected annotations
May  4 11:54:46.213: INFO: saw patched and updated annotations
STEP: getting /approval 05/04/23 11:54:46.213
STEP: patching /approval 05/04/23 11:54:46.223
STEP: updating /approval 05/04/23 11:54:46.235
STEP: getting /status 05/04/23 11:54:46.254
STEP: patching /status 05/04/23 11:54:46.266
STEP: updating /status 05/04/23 11:54:46.29
STEP: deleting 05/04/23 11:54:46.305
STEP: deleting a collection 05/04/23 11:54:46.328
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  4 11:54:46.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-7779" for this suite. 05/04/23 11:54:46.364
{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","completed":47,"skipped":869,"failed":0}
------------------------------
• [0.900 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:54:45.474
    May  4 11:54:45.474: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename certificates 05/04/23 11:54:45.475
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:54:45.496
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:54:45.502
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 05/04/23 11:54:46.136
    STEP: getting /apis/certificates.k8s.io 05/04/23 11:54:46.139
    STEP: getting /apis/certificates.k8s.io/v1 05/04/23 11:54:46.14
    STEP: creating 05/04/23 11:54:46.141
    STEP: getting 05/04/23 11:54:46.172
    STEP: listing 05/04/23 11:54:46.18
    STEP: watching 05/04/23 11:54:46.19
    May  4 11:54:46.190: INFO: starting watch
    STEP: patching 05/04/23 11:54:46.192
    STEP: updating 05/04/23 11:54:46.202
    May  4 11:54:46.213: INFO: waiting for watch events with expected annotations
    May  4 11:54:46.213: INFO: saw patched and updated annotations
    STEP: getting /approval 05/04/23 11:54:46.213
    STEP: patching /approval 05/04/23 11:54:46.223
    STEP: updating /approval 05/04/23 11:54:46.235
    STEP: getting /status 05/04/23 11:54:46.254
    STEP: patching /status 05/04/23 11:54:46.266
    STEP: updating /status 05/04/23 11:54:46.29
    STEP: deleting 05/04/23 11:54:46.305
    STEP: deleting a collection 05/04/23 11:54:46.328
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  4 11:54:46.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "certificates-7779" for this suite. 05/04/23 11:54:46.364
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:54:46.376
May  4 11:54:46.376: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename emptydir-wrapper 05/04/23 11:54:46.378
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:54:46.4
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:54:46.403
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 05/04/23 11:54:46.406
STEP: Creating RC which spawns configmap-volume pods 05/04/23 11:54:46.816
May  4 11:54:46.860: INFO: Pod name wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3: Found 1 pods out of 5
May  4 11:54:51.887: INFO: Pod name wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3: Found 5 pods out of 5
STEP: Ensuring each pod is running 05/04/23 11:54:51.888
May  4 11:54:51.888: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3-459kk" in namespace "emptydir-wrapper-3594" to be "running"
May  4 11:54:51.894: INFO: Pod "wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3-459kk": Phase="Pending", Reason="", readiness=false. Elapsed: 6.512907ms
May  4 11:54:53.914: INFO: Pod "wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3-459kk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026392436s
May  4 11:54:55.905: INFO: Pod "wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3-459kk": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017110704s
May  4 11:54:57.902: INFO: Pod "wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3-459kk": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01457899s
May  4 11:54:59.906: INFO: Pod "wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3-459kk": Phase="Pending", Reason="", readiness=false. Elapsed: 8.018241825s
May  4 11:55:01.900: INFO: Pod "wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3-459kk": Phase="Running", Reason="", readiness=true. Elapsed: 10.012497749s
May  4 11:55:01.900: INFO: Pod "wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3-459kk" satisfied condition "running"
May  4 11:55:01.900: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3-66jnp" in namespace "emptydir-wrapper-3594" to be "running"
May  4 11:55:01.908: INFO: Pod "wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3-66jnp": Phase="Running", Reason="", readiness=true. Elapsed: 7.51222ms
May  4 11:55:01.908: INFO: Pod "wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3-66jnp" satisfied condition "running"
May  4 11:55:01.908: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3-f9zpd" in namespace "emptydir-wrapper-3594" to be "running"
May  4 11:55:01.915: INFO: Pod "wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3-f9zpd": Phase="Running", Reason="", readiness=true. Elapsed: 7.694173ms
May  4 11:55:01.915: INFO: Pod "wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3-f9zpd" satisfied condition "running"
May  4 11:55:01.915: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3-lj254" in namespace "emptydir-wrapper-3594" to be "running"
May  4 11:55:01.927: INFO: Pod "wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3-lj254": Phase="Running", Reason="", readiness=true. Elapsed: 11.436919ms
May  4 11:55:01.927: INFO: Pod "wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3-lj254" satisfied condition "running"
May  4 11:55:01.927: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3-x5btf" in namespace "emptydir-wrapper-3594" to be "running"
May  4 11:55:01.939: INFO: Pod "wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3-x5btf": Phase="Running", Reason="", readiness=true. Elapsed: 11.73798ms
May  4 11:55:01.939: INFO: Pod "wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3-x5btf" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3 in namespace emptydir-wrapper-3594, will wait for the garbage collector to delete the pods 05/04/23 11:55:01.939
May  4 11:55:02.010: INFO: Deleting ReplicationController wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3 took: 14.822447ms
May  4 11:55:02.111: INFO: Terminating ReplicationController wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3 pods took: 101.077784ms
STEP: Creating RC which spawns configmap-volume pods 05/04/23 11:55:05.822
May  4 11:55:05.853: INFO: Pod name wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21: Found 1 pods out of 5
May  4 11:55:10.870: INFO: Pod name wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21: Found 5 pods out of 5
STEP: Ensuring each pod is running 05/04/23 11:55:10.87
May  4 11:55:10.871: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21-j2rh7" in namespace "emptydir-wrapper-3594" to be "running"
May  4 11:55:10.876: INFO: Pod "wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21-j2rh7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.63167ms
May  4 11:55:12.882: INFO: Pod "wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21-j2rh7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011159917s
May  4 11:55:14.888: INFO: Pod "wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21-j2rh7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016731839s
May  4 11:55:16.882: INFO: Pod "wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21-j2rh7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01071301s
May  4 11:55:18.883: INFO: Pod "wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21-j2rh7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011442877s
May  4 11:55:20.885: INFO: Pod "wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21-j2rh7": Phase="Running", Reason="", readiness=true. Elapsed: 10.013830819s
May  4 11:55:20.885: INFO: Pod "wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21-j2rh7" satisfied condition "running"
May  4 11:55:20.885: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21-nxzh6" in namespace "emptydir-wrapper-3594" to be "running"
May  4 11:55:20.892: INFO: Pod "wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21-nxzh6": Phase="Running", Reason="", readiness=true. Elapsed: 6.85257ms
May  4 11:55:20.892: INFO: Pod "wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21-nxzh6" satisfied condition "running"
May  4 11:55:20.892: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21-pmlm7" in namespace "emptydir-wrapper-3594" to be "running"
May  4 11:55:20.904: INFO: Pod "wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21-pmlm7": Phase="Running", Reason="", readiness=true. Elapsed: 12.304509ms
May  4 11:55:20.904: INFO: Pod "wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21-pmlm7" satisfied condition "running"
May  4 11:55:20.904: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21-q4qn7" in namespace "emptydir-wrapper-3594" to be "running"
May  4 11:55:20.914: INFO: Pod "wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21-q4qn7": Phase="Running", Reason="", readiness=true. Elapsed: 9.604274ms
May  4 11:55:20.914: INFO: Pod "wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21-q4qn7" satisfied condition "running"
May  4 11:55:20.914: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21-rn8d9" in namespace "emptydir-wrapper-3594" to be "running"
May  4 11:55:20.920: INFO: Pod "wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21-rn8d9": Phase="Running", Reason="", readiness=true. Elapsed: 6.119156ms
May  4 11:55:20.920: INFO: Pod "wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21-rn8d9" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21 in namespace emptydir-wrapper-3594, will wait for the garbage collector to delete the pods 05/04/23 11:55:20.92
May  4 11:55:20.987: INFO: Deleting ReplicationController wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21 took: 11.435409ms
May  4 11:55:21.088: INFO: Terminating ReplicationController wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21 pods took: 100.777342ms
STEP: Creating RC which spawns configmap-volume pods 05/04/23 11:55:24.5
May  4 11:55:24.603: INFO: Pod name wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d: Found 5 pods out of 5
STEP: Ensuring each pod is running 05/04/23 11:55:24.603
May  4 11:55:24.603: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d-2rhnm" in namespace "emptydir-wrapper-3594" to be "running"
May  4 11:55:24.616: INFO: Pod "wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d-2rhnm": Phase="Pending", Reason="", readiness=false. Elapsed: 12.79066ms
May  4 11:55:26.624: INFO: Pod "wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d-2rhnm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020835563s
May  4 11:55:28.623: INFO: Pod "wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d-2rhnm": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020343474s
May  4 11:55:30.623: INFO: Pod "wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d-2rhnm": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020017799s
May  4 11:55:32.622: INFO: Pod "wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d-2rhnm": Phase="Pending", Reason="", readiness=false. Elapsed: 8.019113117s
May  4 11:55:34.642: INFO: Pod "wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d-2rhnm": Phase="Pending", Reason="", readiness=false. Elapsed: 10.039242931s
May  4 11:55:36.624: INFO: Pod "wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d-2rhnm": Phase="Pending", Reason="", readiness=false. Elapsed: 12.021425134s
May  4 11:55:38.627: INFO: Pod "wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d-2rhnm": Phase="Pending", Reason="", readiness=false. Elapsed: 14.024201943s
May  4 11:55:40.622: INFO: Pod "wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d-2rhnm": Phase="Running", Reason="", readiness=true. Elapsed: 16.019358503s
May  4 11:55:40.622: INFO: Pod "wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d-2rhnm" satisfied condition "running"
May  4 11:55:40.622: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d-mmf8b" in namespace "emptydir-wrapper-3594" to be "running"
May  4 11:55:40.628: INFO: Pod "wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d-mmf8b": Phase="Running", Reason="", readiness=true. Elapsed: 6.179814ms
May  4 11:55:40.629: INFO: Pod "wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d-mmf8b" satisfied condition "running"
May  4 11:55:40.629: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d-pplk9" in namespace "emptydir-wrapper-3594" to be "running"
May  4 11:55:40.637: INFO: Pod "wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d-pplk9": Phase="Running", Reason="", readiness=true. Elapsed: 8.737827ms
May  4 11:55:40.637: INFO: Pod "wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d-pplk9" satisfied condition "running"
May  4 11:55:40.637: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d-wgflx" in namespace "emptydir-wrapper-3594" to be "running"
May  4 11:55:40.647: INFO: Pod "wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d-wgflx": Phase="Running", Reason="", readiness=true. Elapsed: 8.479368ms
May  4 11:55:40.647: INFO: Pod "wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d-wgflx" satisfied condition "running"
May  4 11:55:40.647: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d-xj97k" in namespace "emptydir-wrapper-3594" to be "running"
May  4 11:55:40.655: INFO: Pod "wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d-xj97k": Phase="Running", Reason="", readiness=true. Elapsed: 8.737102ms
May  4 11:55:40.655: INFO: Pod "wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d-xj97k" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d in namespace emptydir-wrapper-3594, will wait for the garbage collector to delete the pods 05/04/23 11:55:40.655
May  4 11:55:40.725: INFO: Deleting ReplicationController wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d took: 12.251289ms
May  4 11:55:40.825: INFO: Terminating ReplicationController wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d pods took: 100.733165ms
STEP: Cleaning up the configMaps 05/04/23 11:55:44.127
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
May  4 11:55:44.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3594" for this suite. 05/04/23 11:55:44.623
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","completed":48,"skipped":921,"failed":0}
------------------------------
• [SLOW TEST] [58.254 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:54:46.376
    May  4 11:54:46.376: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename emptydir-wrapper 05/04/23 11:54:46.378
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:54:46.4
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:54:46.403
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 05/04/23 11:54:46.406
    STEP: Creating RC which spawns configmap-volume pods 05/04/23 11:54:46.816
    May  4 11:54:46.860: INFO: Pod name wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3: Found 1 pods out of 5
    May  4 11:54:51.887: INFO: Pod name wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3: Found 5 pods out of 5
    STEP: Ensuring each pod is running 05/04/23 11:54:51.888
    May  4 11:54:51.888: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3-459kk" in namespace "emptydir-wrapper-3594" to be "running"
    May  4 11:54:51.894: INFO: Pod "wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3-459kk": Phase="Pending", Reason="", readiness=false. Elapsed: 6.512907ms
    May  4 11:54:53.914: INFO: Pod "wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3-459kk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026392436s
    May  4 11:54:55.905: INFO: Pod "wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3-459kk": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017110704s
    May  4 11:54:57.902: INFO: Pod "wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3-459kk": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01457899s
    May  4 11:54:59.906: INFO: Pod "wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3-459kk": Phase="Pending", Reason="", readiness=false. Elapsed: 8.018241825s
    May  4 11:55:01.900: INFO: Pod "wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3-459kk": Phase="Running", Reason="", readiness=true. Elapsed: 10.012497749s
    May  4 11:55:01.900: INFO: Pod "wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3-459kk" satisfied condition "running"
    May  4 11:55:01.900: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3-66jnp" in namespace "emptydir-wrapper-3594" to be "running"
    May  4 11:55:01.908: INFO: Pod "wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3-66jnp": Phase="Running", Reason="", readiness=true. Elapsed: 7.51222ms
    May  4 11:55:01.908: INFO: Pod "wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3-66jnp" satisfied condition "running"
    May  4 11:55:01.908: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3-f9zpd" in namespace "emptydir-wrapper-3594" to be "running"
    May  4 11:55:01.915: INFO: Pod "wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3-f9zpd": Phase="Running", Reason="", readiness=true. Elapsed: 7.694173ms
    May  4 11:55:01.915: INFO: Pod "wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3-f9zpd" satisfied condition "running"
    May  4 11:55:01.915: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3-lj254" in namespace "emptydir-wrapper-3594" to be "running"
    May  4 11:55:01.927: INFO: Pod "wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3-lj254": Phase="Running", Reason="", readiness=true. Elapsed: 11.436919ms
    May  4 11:55:01.927: INFO: Pod "wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3-lj254" satisfied condition "running"
    May  4 11:55:01.927: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3-x5btf" in namespace "emptydir-wrapper-3594" to be "running"
    May  4 11:55:01.939: INFO: Pod "wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3-x5btf": Phase="Running", Reason="", readiness=true. Elapsed: 11.73798ms
    May  4 11:55:01.939: INFO: Pod "wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3-x5btf" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3 in namespace emptydir-wrapper-3594, will wait for the garbage collector to delete the pods 05/04/23 11:55:01.939
    May  4 11:55:02.010: INFO: Deleting ReplicationController wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3 took: 14.822447ms
    May  4 11:55:02.111: INFO: Terminating ReplicationController wrapped-volume-race-b4d06bab-bc7c-4857-94bd-eaf2674b97a3 pods took: 101.077784ms
    STEP: Creating RC which spawns configmap-volume pods 05/04/23 11:55:05.822
    May  4 11:55:05.853: INFO: Pod name wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21: Found 1 pods out of 5
    May  4 11:55:10.870: INFO: Pod name wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21: Found 5 pods out of 5
    STEP: Ensuring each pod is running 05/04/23 11:55:10.87
    May  4 11:55:10.871: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21-j2rh7" in namespace "emptydir-wrapper-3594" to be "running"
    May  4 11:55:10.876: INFO: Pod "wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21-j2rh7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.63167ms
    May  4 11:55:12.882: INFO: Pod "wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21-j2rh7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011159917s
    May  4 11:55:14.888: INFO: Pod "wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21-j2rh7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016731839s
    May  4 11:55:16.882: INFO: Pod "wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21-j2rh7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01071301s
    May  4 11:55:18.883: INFO: Pod "wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21-j2rh7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011442877s
    May  4 11:55:20.885: INFO: Pod "wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21-j2rh7": Phase="Running", Reason="", readiness=true. Elapsed: 10.013830819s
    May  4 11:55:20.885: INFO: Pod "wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21-j2rh7" satisfied condition "running"
    May  4 11:55:20.885: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21-nxzh6" in namespace "emptydir-wrapper-3594" to be "running"
    May  4 11:55:20.892: INFO: Pod "wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21-nxzh6": Phase="Running", Reason="", readiness=true. Elapsed: 6.85257ms
    May  4 11:55:20.892: INFO: Pod "wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21-nxzh6" satisfied condition "running"
    May  4 11:55:20.892: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21-pmlm7" in namespace "emptydir-wrapper-3594" to be "running"
    May  4 11:55:20.904: INFO: Pod "wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21-pmlm7": Phase="Running", Reason="", readiness=true. Elapsed: 12.304509ms
    May  4 11:55:20.904: INFO: Pod "wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21-pmlm7" satisfied condition "running"
    May  4 11:55:20.904: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21-q4qn7" in namespace "emptydir-wrapper-3594" to be "running"
    May  4 11:55:20.914: INFO: Pod "wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21-q4qn7": Phase="Running", Reason="", readiness=true. Elapsed: 9.604274ms
    May  4 11:55:20.914: INFO: Pod "wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21-q4qn7" satisfied condition "running"
    May  4 11:55:20.914: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21-rn8d9" in namespace "emptydir-wrapper-3594" to be "running"
    May  4 11:55:20.920: INFO: Pod "wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21-rn8d9": Phase="Running", Reason="", readiness=true. Elapsed: 6.119156ms
    May  4 11:55:20.920: INFO: Pod "wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21-rn8d9" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21 in namespace emptydir-wrapper-3594, will wait for the garbage collector to delete the pods 05/04/23 11:55:20.92
    May  4 11:55:20.987: INFO: Deleting ReplicationController wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21 took: 11.435409ms
    May  4 11:55:21.088: INFO: Terminating ReplicationController wrapped-volume-race-35f8006a-7303-42d5-9a3d-8dc140efdb21 pods took: 100.777342ms
    STEP: Creating RC which spawns configmap-volume pods 05/04/23 11:55:24.5
    May  4 11:55:24.603: INFO: Pod name wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d: Found 5 pods out of 5
    STEP: Ensuring each pod is running 05/04/23 11:55:24.603
    May  4 11:55:24.603: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d-2rhnm" in namespace "emptydir-wrapper-3594" to be "running"
    May  4 11:55:24.616: INFO: Pod "wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d-2rhnm": Phase="Pending", Reason="", readiness=false. Elapsed: 12.79066ms
    May  4 11:55:26.624: INFO: Pod "wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d-2rhnm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020835563s
    May  4 11:55:28.623: INFO: Pod "wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d-2rhnm": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020343474s
    May  4 11:55:30.623: INFO: Pod "wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d-2rhnm": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020017799s
    May  4 11:55:32.622: INFO: Pod "wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d-2rhnm": Phase="Pending", Reason="", readiness=false. Elapsed: 8.019113117s
    May  4 11:55:34.642: INFO: Pod "wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d-2rhnm": Phase="Pending", Reason="", readiness=false. Elapsed: 10.039242931s
    May  4 11:55:36.624: INFO: Pod "wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d-2rhnm": Phase="Pending", Reason="", readiness=false. Elapsed: 12.021425134s
    May  4 11:55:38.627: INFO: Pod "wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d-2rhnm": Phase="Pending", Reason="", readiness=false. Elapsed: 14.024201943s
    May  4 11:55:40.622: INFO: Pod "wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d-2rhnm": Phase="Running", Reason="", readiness=true. Elapsed: 16.019358503s
    May  4 11:55:40.622: INFO: Pod "wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d-2rhnm" satisfied condition "running"
    May  4 11:55:40.622: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d-mmf8b" in namespace "emptydir-wrapper-3594" to be "running"
    May  4 11:55:40.628: INFO: Pod "wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d-mmf8b": Phase="Running", Reason="", readiness=true. Elapsed: 6.179814ms
    May  4 11:55:40.629: INFO: Pod "wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d-mmf8b" satisfied condition "running"
    May  4 11:55:40.629: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d-pplk9" in namespace "emptydir-wrapper-3594" to be "running"
    May  4 11:55:40.637: INFO: Pod "wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d-pplk9": Phase="Running", Reason="", readiness=true. Elapsed: 8.737827ms
    May  4 11:55:40.637: INFO: Pod "wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d-pplk9" satisfied condition "running"
    May  4 11:55:40.637: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d-wgflx" in namespace "emptydir-wrapper-3594" to be "running"
    May  4 11:55:40.647: INFO: Pod "wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d-wgflx": Phase="Running", Reason="", readiness=true. Elapsed: 8.479368ms
    May  4 11:55:40.647: INFO: Pod "wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d-wgflx" satisfied condition "running"
    May  4 11:55:40.647: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d-xj97k" in namespace "emptydir-wrapper-3594" to be "running"
    May  4 11:55:40.655: INFO: Pod "wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d-xj97k": Phase="Running", Reason="", readiness=true. Elapsed: 8.737102ms
    May  4 11:55:40.655: INFO: Pod "wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d-xj97k" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d in namespace emptydir-wrapper-3594, will wait for the garbage collector to delete the pods 05/04/23 11:55:40.655
    May  4 11:55:40.725: INFO: Deleting ReplicationController wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d took: 12.251289ms
    May  4 11:55:40.825: INFO: Terminating ReplicationController wrapped-volume-race-2a87d7a7-57cb-451e-bca1-75c8e7f5fb0d pods took: 100.733165ms
    STEP: Cleaning up the configMaps 05/04/23 11:55:44.127
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    May  4 11:55:44.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-3594" for this suite. 05/04/23 11:55:44.623
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:55:44.631
May  4 11:55:44.631: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename resourcequota 05/04/23 11:55:44.632
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:55:44.659
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:55:44.663
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
STEP: Counting existing ResourceQuota 05/04/23 11:55:44.666
STEP: Creating a ResourceQuota 05/04/23 11:55:49.671
STEP: Ensuring resource quota status is calculated 05/04/23 11:55:49.678
STEP: Creating a ReplicationController 05/04/23 11:55:51.684
STEP: Ensuring resource quota status captures replication controller creation 05/04/23 11:55:51.702
STEP: Deleting a ReplicationController 05/04/23 11:55:53.708
STEP: Ensuring resource quota status released usage 05/04/23 11:55:53.718
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
May  4 11:55:55.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1593" for this suite. 05/04/23 11:55:55.73
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","completed":49,"skipped":924,"failed":0}
------------------------------
• [SLOW TEST] [11.106 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:55:44.631
    May  4 11:55:44.631: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename resourcequota 05/04/23 11:55:44.632
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:55:44.659
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:55:44.663
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:382
    STEP: Counting existing ResourceQuota 05/04/23 11:55:44.666
    STEP: Creating a ResourceQuota 05/04/23 11:55:49.671
    STEP: Ensuring resource quota status is calculated 05/04/23 11:55:49.678
    STEP: Creating a ReplicationController 05/04/23 11:55:51.684
    STEP: Ensuring resource quota status captures replication controller creation 05/04/23 11:55:51.702
    STEP: Deleting a ReplicationController 05/04/23 11:55:53.708
    STEP: Ensuring resource quota status released usage 05/04/23 11:55:53.718
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    May  4 11:55:55.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-1593" for this suite. 05/04/23 11:55:55.73
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:55:55.738
May  4 11:55:55.739: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename secrets 05/04/23 11:55:55.739
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:55:55.765
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:55:55.769
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
STEP: Creating projection with secret that has name secret-emptykey-test-889133fe-cc5b-443f-b046-9ccb9f41d90b 05/04/23 11:55:55.772
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
May  4 11:55:55.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8758" for this suite. 05/04/23 11:55:55.783
{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","completed":50,"skipped":931,"failed":0}
------------------------------
• [0.054 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:55:55.738
    May  4 11:55:55.739: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename secrets 05/04/23 11:55:55.739
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:55:55.765
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:55:55.769
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:139
    STEP: Creating projection with secret that has name secret-emptykey-test-889133fe-cc5b-443f-b046-9ccb9f41d90b 05/04/23 11:55:55.772
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    May  4 11:55:55.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8758" for this suite. 05/04/23 11:55:55.783
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:55:55.796
May  4 11:55:55.796: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename controllerrevisions 05/04/23 11:55:55.797
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:55:55.815
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:55:55.818
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-r4jfr-daemon-set" 05/04/23 11:55:55.863
STEP: Check that daemon pods launch on every node of the cluster. 05/04/23 11:55:55.873
May  4 11:55:55.882: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 11:55:55.882: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 11:55:55.882: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 11:55:55.887: INFO: Number of nodes with available pods controlled by daemonset e2e-r4jfr-daemon-set: 0
May  4 11:55:55.887: INFO: Node ip-10-0-1-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
May  4 11:55:56.897: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 11:55:56.898: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 11:55:56.898: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 11:55:56.905: INFO: Number of nodes with available pods controlled by daemonset e2e-r4jfr-daemon-set: 0
May  4 11:55:56.905: INFO: Node ip-10-0-1-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
May  4 11:55:57.909: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 11:55:57.909: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 11:55:57.909: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 11:55:57.916: INFO: Number of nodes with available pods controlled by daemonset e2e-r4jfr-daemon-set: 3
May  4 11:55:57.916: INFO: Node ip-10-0-1-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
May  4 11:55:58.895: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 11:55:58.895: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 11:55:58.895: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 11:55:58.903: INFO: Number of nodes with available pods controlled by daemonset e2e-r4jfr-daemon-set: 3
May  4 11:55:58.903: INFO: Node ip-10-0-1-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
May  4 11:55:59.896: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 11:55:59.896: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 11:55:59.896: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 11:55:59.902: INFO: Number of nodes with available pods controlled by daemonset e2e-r4jfr-daemon-set: 3
May  4 11:55:59.902: INFO: Node ip-10-0-1-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
May  4 11:56:00.898: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 11:56:00.898: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 11:56:00.898: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 11:56:00.911: INFO: Number of nodes with available pods controlled by daemonset e2e-r4jfr-daemon-set: 3
May  4 11:56:00.911: INFO: Node ip-10-0-1-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
May  4 11:56:01.896: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 11:56:01.896: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 11:56:01.896: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 11:56:01.903: INFO: Number of nodes with available pods controlled by daemonset e2e-r4jfr-daemon-set: 5
May  4 11:56:01.903: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset e2e-r4jfr-daemon-set
STEP: Confirm DaemonSet "e2e-r4jfr-daemon-set" successfully created with "daemonset-name=e2e-r4jfr-daemon-set" label 05/04/23 11:56:01.908
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-r4jfr-daemon-set" 05/04/23 11:56:01.927
May  4 11:56:01.938: INFO: Located ControllerRevision: "e2e-r4jfr-daemon-set-8bb45bd87"
STEP: Patching ControllerRevision "e2e-r4jfr-daemon-set-8bb45bd87" 05/04/23 11:56:01.953
May  4 11:56:01.968: INFO: e2e-r4jfr-daemon-set-8bb45bd87 has been patched
STEP: Create a new ControllerRevision 05/04/23 11:56:01.969
May  4 11:56:01.979: INFO: Created ControllerRevision: e2e-r4jfr-daemon-set-654bfbccdc
STEP: Confirm that there are two ControllerRevisions 05/04/23 11:56:01.979
May  4 11:56:01.979: INFO: Requesting list of ControllerRevisions to confirm quantity
May  4 11:56:01.985: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-r4jfr-daemon-set-8bb45bd87" 05/04/23 11:56:01.985
STEP: Confirm that there is only one ControllerRevision 05/04/23 11:56:01.999
May  4 11:56:01.999: INFO: Requesting list of ControllerRevisions to confirm quantity
May  4 11:56:02.007: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-r4jfr-daemon-set-654bfbccdc" 05/04/23 11:56:02.016
May  4 11:56:02.040: INFO: e2e-r4jfr-daemon-set-654bfbccdc has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 05/04/23 11:56:02.04
W0504 11:56:02.053155      21 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 05/04/23 11:56:02.053
May  4 11:56:02.053: INFO: Requesting list of ControllerRevisions to confirm quantity
May  4 11:56:03.063: INFO: Requesting list of ControllerRevisions to confirm quantity
May  4 11:56:03.081: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-r4jfr-daemon-set-654bfbccdc=updated" 05/04/23 11:56:03.081
STEP: Confirm that there is only one ControllerRevision 05/04/23 11:56:03.104
May  4 11:56:03.104: INFO: Requesting list of ControllerRevisions to confirm quantity
May  4 11:56:03.108: INFO: Found 1 ControllerRevisions
May  4 11:56:03.112: INFO: ControllerRevision "e2e-r4jfr-daemon-set-b77fb5446" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-r4jfr-daemon-set" 05/04/23 11:56:03.117
STEP: deleting DaemonSet.extensions e2e-r4jfr-daemon-set in namespace controllerrevisions-9358, will wait for the garbage collector to delete the pods 05/04/23 11:56:03.117
May  4 11:56:03.182: INFO: Deleting DaemonSet.extensions e2e-r4jfr-daemon-set took: 10.601604ms
May  4 11:56:03.283: INFO: Terminating DaemonSet.extensions e2e-r4jfr-daemon-set pods took: 101.075829ms
May  4 11:56:05.089: INFO: Number of nodes with available pods controlled by daemonset e2e-r4jfr-daemon-set: 0
May  4 11:56:05.089: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-r4jfr-daemon-set
May  4 11:56:05.093: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"11053"},"items":null}

May  4 11:56:05.098: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"11053"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:187
May  4 11:56:05.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "controllerrevisions-9358" for this suite. 05/04/23 11:56:05.138
{"msg":"PASSED [sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]","completed":51,"skipped":941,"failed":0}
------------------------------
• [SLOW TEST] [9.351 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:55:55.796
    May  4 11:55:55.796: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename controllerrevisions 05/04/23 11:55:55.797
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:55:55.815
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:55:55.818
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-r4jfr-daemon-set" 05/04/23 11:55:55.863
    STEP: Check that daemon pods launch on every node of the cluster. 05/04/23 11:55:55.873
    May  4 11:55:55.882: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 11:55:55.882: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 11:55:55.882: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 11:55:55.887: INFO: Number of nodes with available pods controlled by daemonset e2e-r4jfr-daemon-set: 0
    May  4 11:55:55.887: INFO: Node ip-10-0-1-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
    May  4 11:55:56.897: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 11:55:56.898: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 11:55:56.898: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 11:55:56.905: INFO: Number of nodes with available pods controlled by daemonset e2e-r4jfr-daemon-set: 0
    May  4 11:55:56.905: INFO: Node ip-10-0-1-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
    May  4 11:55:57.909: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 11:55:57.909: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 11:55:57.909: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 11:55:57.916: INFO: Number of nodes with available pods controlled by daemonset e2e-r4jfr-daemon-set: 3
    May  4 11:55:57.916: INFO: Node ip-10-0-1-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
    May  4 11:55:58.895: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 11:55:58.895: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 11:55:58.895: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 11:55:58.903: INFO: Number of nodes with available pods controlled by daemonset e2e-r4jfr-daemon-set: 3
    May  4 11:55:58.903: INFO: Node ip-10-0-1-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
    May  4 11:55:59.896: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 11:55:59.896: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 11:55:59.896: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 11:55:59.902: INFO: Number of nodes with available pods controlled by daemonset e2e-r4jfr-daemon-set: 3
    May  4 11:55:59.902: INFO: Node ip-10-0-1-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
    May  4 11:56:00.898: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 11:56:00.898: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 11:56:00.898: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 11:56:00.911: INFO: Number of nodes with available pods controlled by daemonset e2e-r4jfr-daemon-set: 3
    May  4 11:56:00.911: INFO: Node ip-10-0-1-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
    May  4 11:56:01.896: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 11:56:01.896: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 11:56:01.896: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 11:56:01.903: INFO: Number of nodes with available pods controlled by daemonset e2e-r4jfr-daemon-set: 5
    May  4 11:56:01.903: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset e2e-r4jfr-daemon-set
    STEP: Confirm DaemonSet "e2e-r4jfr-daemon-set" successfully created with "daemonset-name=e2e-r4jfr-daemon-set" label 05/04/23 11:56:01.908
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-r4jfr-daemon-set" 05/04/23 11:56:01.927
    May  4 11:56:01.938: INFO: Located ControllerRevision: "e2e-r4jfr-daemon-set-8bb45bd87"
    STEP: Patching ControllerRevision "e2e-r4jfr-daemon-set-8bb45bd87" 05/04/23 11:56:01.953
    May  4 11:56:01.968: INFO: e2e-r4jfr-daemon-set-8bb45bd87 has been patched
    STEP: Create a new ControllerRevision 05/04/23 11:56:01.969
    May  4 11:56:01.979: INFO: Created ControllerRevision: e2e-r4jfr-daemon-set-654bfbccdc
    STEP: Confirm that there are two ControllerRevisions 05/04/23 11:56:01.979
    May  4 11:56:01.979: INFO: Requesting list of ControllerRevisions to confirm quantity
    May  4 11:56:01.985: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-r4jfr-daemon-set-8bb45bd87" 05/04/23 11:56:01.985
    STEP: Confirm that there is only one ControllerRevision 05/04/23 11:56:01.999
    May  4 11:56:01.999: INFO: Requesting list of ControllerRevisions to confirm quantity
    May  4 11:56:02.007: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-r4jfr-daemon-set-654bfbccdc" 05/04/23 11:56:02.016
    May  4 11:56:02.040: INFO: e2e-r4jfr-daemon-set-654bfbccdc has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 05/04/23 11:56:02.04
    W0504 11:56:02.053155      21 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 05/04/23 11:56:02.053
    May  4 11:56:02.053: INFO: Requesting list of ControllerRevisions to confirm quantity
    May  4 11:56:03.063: INFO: Requesting list of ControllerRevisions to confirm quantity
    May  4 11:56:03.081: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-r4jfr-daemon-set-654bfbccdc=updated" 05/04/23 11:56:03.081
    STEP: Confirm that there is only one ControllerRevision 05/04/23 11:56:03.104
    May  4 11:56:03.104: INFO: Requesting list of ControllerRevisions to confirm quantity
    May  4 11:56:03.108: INFO: Found 1 ControllerRevisions
    May  4 11:56:03.112: INFO: ControllerRevision "e2e-r4jfr-daemon-set-b77fb5446" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-r4jfr-daemon-set" 05/04/23 11:56:03.117
    STEP: deleting DaemonSet.extensions e2e-r4jfr-daemon-set in namespace controllerrevisions-9358, will wait for the garbage collector to delete the pods 05/04/23 11:56:03.117
    May  4 11:56:03.182: INFO: Deleting DaemonSet.extensions e2e-r4jfr-daemon-set took: 10.601604ms
    May  4 11:56:03.283: INFO: Terminating DaemonSet.extensions e2e-r4jfr-daemon-set pods took: 101.075829ms
    May  4 11:56:05.089: INFO: Number of nodes with available pods controlled by daemonset e2e-r4jfr-daemon-set: 0
    May  4 11:56:05.089: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-r4jfr-daemon-set
    May  4 11:56:05.093: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"11053"},"items":null}

    May  4 11:56:05.098: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"11053"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:187
    May  4 11:56:05.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "controllerrevisions-9358" for this suite. 05/04/23 11:56:05.138
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:56:05.147
May  4 11:56:05.147: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename watch 05/04/23 11:56:05.148
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:56:05.167
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:56:05.17
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 05/04/23 11:56:05.176
STEP: creating a new configmap 05/04/23 11:56:05.178
STEP: modifying the configmap once 05/04/23 11:56:05.185
STEP: closing the watch once it receives two notifications 05/04/23 11:56:05.195
May  4 11:56:05.196: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3300  92bb39db-a92c-4c05-b19e-dfb1bd0a8a59 11058 0 2023-05-04 11:56:05 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-04 11:56:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May  4 11:56:05.196: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3300  92bb39db-a92c-4c05-b19e-dfb1bd0a8a59 11059 0 2023-05-04 11:56:05 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-04 11:56:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 05/04/23 11:56:05.196
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 05/04/23 11:56:05.206
STEP: deleting the configmap 05/04/23 11:56:05.208
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 05/04/23 11:56:05.218
May  4 11:56:05.218: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3300  92bb39db-a92c-4c05-b19e-dfb1bd0a8a59 11060 0 2023-05-04 11:56:05 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-04 11:56:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May  4 11:56:05.218: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3300  92bb39db-a92c-4c05-b19e-dfb1bd0a8a59 11061 0 2023-05-04 11:56:05 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-04 11:56:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
May  4 11:56:05.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3300" for this suite. 05/04/23 11:56:05.229
{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","completed":52,"skipped":947,"failed":0}
------------------------------
• [0.094 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:56:05.147
    May  4 11:56:05.147: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename watch 05/04/23 11:56:05.148
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:56:05.167
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:56:05.17
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 05/04/23 11:56:05.176
    STEP: creating a new configmap 05/04/23 11:56:05.178
    STEP: modifying the configmap once 05/04/23 11:56:05.185
    STEP: closing the watch once it receives two notifications 05/04/23 11:56:05.195
    May  4 11:56:05.196: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3300  92bb39db-a92c-4c05-b19e-dfb1bd0a8a59 11058 0 2023-05-04 11:56:05 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-04 11:56:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    May  4 11:56:05.196: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3300  92bb39db-a92c-4c05-b19e-dfb1bd0a8a59 11059 0 2023-05-04 11:56:05 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-04 11:56:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 05/04/23 11:56:05.196
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 05/04/23 11:56:05.206
    STEP: deleting the configmap 05/04/23 11:56:05.208
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 05/04/23 11:56:05.218
    May  4 11:56:05.218: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3300  92bb39db-a92c-4c05-b19e-dfb1bd0a8a59 11060 0 2023-05-04 11:56:05 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-04 11:56:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    May  4 11:56:05.218: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3300  92bb39db-a92c-4c05-b19e-dfb1bd0a8a59 11061 0 2023-05-04 11:56:05 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-04 11:56:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    May  4 11:56:05.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-3300" for this suite. 05/04/23 11:56:05.229
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:56:05.243
May  4 11:56:05.243: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename projected 05/04/23 11:56:05.243
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:56:05.266
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:56:05.269
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
STEP: Creating projection with secret that has name projected-secret-test-2d32a752-a07e-4a69-88bd-a33c6cba4735 05/04/23 11:56:05.272
STEP: Creating a pod to test consume secrets 05/04/23 11:56:05.277
May  4 11:56:05.288: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-99d2af36-4ae5-4317-9b7c-5048c9a0523c" in namespace "projected-4846" to be "Succeeded or Failed"
May  4 11:56:05.293: INFO: Pod "pod-projected-secrets-99d2af36-4ae5-4317-9b7c-5048c9a0523c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.892194ms
May  4 11:56:07.299: INFO: Pod "pod-projected-secrets-99d2af36-4ae5-4317-9b7c-5048c9a0523c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011168528s
May  4 11:56:09.299: INFO: Pod "pod-projected-secrets-99d2af36-4ae5-4317-9b7c-5048c9a0523c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011833122s
STEP: Saw pod success 05/04/23 11:56:09.299
May  4 11:56:09.300: INFO: Pod "pod-projected-secrets-99d2af36-4ae5-4317-9b7c-5048c9a0523c" satisfied condition "Succeeded or Failed"
May  4 11:56:09.306: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod pod-projected-secrets-99d2af36-4ae5-4317-9b7c-5048c9a0523c container projected-secret-volume-test: <nil>
STEP: delete the pod 05/04/23 11:56:09.325
May  4 11:56:09.345: INFO: Waiting for pod pod-projected-secrets-99d2af36-4ae5-4317-9b7c-5048c9a0523c to disappear
May  4 11:56:09.349: INFO: Pod pod-projected-secrets-99d2af36-4ae5-4317-9b7c-5048c9a0523c no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
May  4 11:56:09.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4846" for this suite. 05/04/23 11:56:09.357
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":53,"skipped":973,"failed":0}
------------------------------
• [4.125 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:56:05.243
    May  4 11:56:05.243: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename projected 05/04/23 11:56:05.243
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:56:05.266
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:56:05.269
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:66
    STEP: Creating projection with secret that has name projected-secret-test-2d32a752-a07e-4a69-88bd-a33c6cba4735 05/04/23 11:56:05.272
    STEP: Creating a pod to test consume secrets 05/04/23 11:56:05.277
    May  4 11:56:05.288: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-99d2af36-4ae5-4317-9b7c-5048c9a0523c" in namespace "projected-4846" to be "Succeeded or Failed"
    May  4 11:56:05.293: INFO: Pod "pod-projected-secrets-99d2af36-4ae5-4317-9b7c-5048c9a0523c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.892194ms
    May  4 11:56:07.299: INFO: Pod "pod-projected-secrets-99d2af36-4ae5-4317-9b7c-5048c9a0523c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011168528s
    May  4 11:56:09.299: INFO: Pod "pod-projected-secrets-99d2af36-4ae5-4317-9b7c-5048c9a0523c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011833122s
    STEP: Saw pod success 05/04/23 11:56:09.299
    May  4 11:56:09.300: INFO: Pod "pod-projected-secrets-99d2af36-4ae5-4317-9b7c-5048c9a0523c" satisfied condition "Succeeded or Failed"
    May  4 11:56:09.306: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod pod-projected-secrets-99d2af36-4ae5-4317-9b7c-5048c9a0523c container projected-secret-volume-test: <nil>
    STEP: delete the pod 05/04/23 11:56:09.325
    May  4 11:56:09.345: INFO: Waiting for pod pod-projected-secrets-99d2af36-4ae5-4317-9b7c-5048c9a0523c to disappear
    May  4 11:56:09.349: INFO: Pod pod-projected-secrets-99d2af36-4ae5-4317-9b7c-5048c9a0523c no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    May  4 11:56:09.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4846" for this suite. 05/04/23 11:56:09.357
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:56:09.369
May  4 11:56:09.369: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename crd-publish-openapi 05/04/23 11:56:09.37
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:56:09.393
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:56:09.396
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
STEP: set up a multi version CRD 05/04/23 11:56:09.402
May  4 11:56:09.403: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: rename a version 05/04/23 11:56:20.875
STEP: check the new version name is served 05/04/23 11:56:20.893
STEP: check the old version name is removed 05/04/23 11:56:25.44
STEP: check the other version is not changed 05/04/23 11:56:27.641
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  4 11:56:36.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7385" for this suite. 05/04/23 11:56:36.795
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","completed":54,"skipped":996,"failed":0}
------------------------------
• [SLOW TEST] [27.434 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:56:09.369
    May  4 11:56:09.369: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename crd-publish-openapi 05/04/23 11:56:09.37
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:56:09.393
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:56:09.396
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:390
    STEP: set up a multi version CRD 05/04/23 11:56:09.402
    May  4 11:56:09.403: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: rename a version 05/04/23 11:56:20.875
    STEP: check the new version name is served 05/04/23 11:56:20.893
    STEP: check the old version name is removed 05/04/23 11:56:25.44
    STEP: check the other version is not changed 05/04/23 11:56:27.641
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  4 11:56:36.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-7385" for this suite. 05/04/23 11:56:36.795
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:56:36.803
May  4 11:56:36.803: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename var-expansion 05/04/23 11:56:36.804
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:56:36.817
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:56:36.826
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
STEP: Creating a pod to test substitution in container's args 05/04/23 11:56:36.829
May  4 11:56:36.837: INFO: Waiting up to 5m0s for pod "var-expansion-95349291-61f0-4717-a4ba-9f60308dd314" in namespace "var-expansion-7838" to be "Succeeded or Failed"
May  4 11:56:36.840: INFO: Pod "var-expansion-95349291-61f0-4717-a4ba-9f60308dd314": Phase="Pending", Reason="", readiness=false. Elapsed: 2.738084ms
May  4 11:56:38.845: INFO: Pod "var-expansion-95349291-61f0-4717-a4ba-9f60308dd314": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007089438s
May  4 11:56:40.844: INFO: Pod "var-expansion-95349291-61f0-4717-a4ba-9f60308dd314": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006595338s
STEP: Saw pod success 05/04/23 11:56:40.844
May  4 11:56:40.844: INFO: Pod "var-expansion-95349291-61f0-4717-a4ba-9f60308dd314" satisfied condition "Succeeded or Failed"
May  4 11:56:40.848: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod var-expansion-95349291-61f0-4717-a4ba-9f60308dd314 container dapi-container: <nil>
STEP: delete the pod 05/04/23 11:56:40.862
May  4 11:56:40.876: INFO: Waiting for pod var-expansion-95349291-61f0-4717-a4ba-9f60308dd314 to disappear
May  4 11:56:40.879: INFO: Pod var-expansion-95349291-61f0-4717-a4ba-9f60308dd314 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
May  4 11:56:40.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7838" for this suite. 05/04/23 11:56:40.885
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","completed":55,"skipped":1000,"failed":0}
------------------------------
• [4.089 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:56:36.803
    May  4 11:56:36.803: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename var-expansion 05/04/23 11:56:36.804
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:56:36.817
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:56:36.826
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:91
    STEP: Creating a pod to test substitution in container's args 05/04/23 11:56:36.829
    May  4 11:56:36.837: INFO: Waiting up to 5m0s for pod "var-expansion-95349291-61f0-4717-a4ba-9f60308dd314" in namespace "var-expansion-7838" to be "Succeeded or Failed"
    May  4 11:56:36.840: INFO: Pod "var-expansion-95349291-61f0-4717-a4ba-9f60308dd314": Phase="Pending", Reason="", readiness=false. Elapsed: 2.738084ms
    May  4 11:56:38.845: INFO: Pod "var-expansion-95349291-61f0-4717-a4ba-9f60308dd314": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007089438s
    May  4 11:56:40.844: INFO: Pod "var-expansion-95349291-61f0-4717-a4ba-9f60308dd314": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006595338s
    STEP: Saw pod success 05/04/23 11:56:40.844
    May  4 11:56:40.844: INFO: Pod "var-expansion-95349291-61f0-4717-a4ba-9f60308dd314" satisfied condition "Succeeded or Failed"
    May  4 11:56:40.848: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod var-expansion-95349291-61f0-4717-a4ba-9f60308dd314 container dapi-container: <nil>
    STEP: delete the pod 05/04/23 11:56:40.862
    May  4 11:56:40.876: INFO: Waiting for pod var-expansion-95349291-61f0-4717-a4ba-9f60308dd314 to disappear
    May  4 11:56:40.879: INFO: Pod var-expansion-95349291-61f0-4717-a4ba-9f60308dd314 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    May  4 11:56:40.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-7838" for this suite. 05/04/23 11:56:40.885
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:56:40.894
May  4 11:56:40.894: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename kubectl 05/04/23 11:56:40.895
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:56:40.905
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:56:40.907
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1698
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 05/04/23 11:56:40.91
May  4 11:56:40.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-7038 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
May  4 11:56:41.003: INFO: stderr: ""
May  4 11:56:41.003: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 05/04/23 11:56:41.003
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1702
May  4 11:56:41.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-7038 delete pods e2e-test-httpd-pod'
May  4 11:56:43.065: INFO: stderr: ""
May  4 11:56:43.065: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  4 11:56:43.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7038" for this suite. 05/04/23 11:56:43.073
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","completed":56,"skipped":1024,"failed":0}
------------------------------
• [2.188 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1695
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1711

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:56:40.894
    May  4 11:56:40.894: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename kubectl 05/04/23 11:56:40.895
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:56:40.905
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:56:40.907
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1698
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1711
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 05/04/23 11:56:40.91
    May  4 11:56:40.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-7038 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
    May  4 11:56:41.003: INFO: stderr: ""
    May  4 11:56:41.003: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 05/04/23 11:56:41.003
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1702
    May  4 11:56:41.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-7038 delete pods e2e-test-httpd-pod'
    May  4 11:56:43.065: INFO: stderr: ""
    May  4 11:56:43.065: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  4 11:56:43.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7038" for this suite. 05/04/23 11:56:43.073
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:56:43.083
May  4 11:56:43.083: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename kubectl 05/04/23 11:56:43.084
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:56:43.098
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:56:43.103
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1732
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 05/04/23 11:56:43.107
May  4 11:56:43.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-8937 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
May  4 11:56:43.200: INFO: stderr: ""
May  4 11:56:43.200: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 05/04/23 11:56:43.2
STEP: verifying the pod e2e-test-httpd-pod was created 05/04/23 11:56:48.251
May  4 11:56:48.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-8937 get pod e2e-test-httpd-pod -o json'
May  4 11:56:48.336: INFO: stderr: ""
May  4 11:56:48.336: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"9375dd3f4ddfbc11b99e5e1da5b22d7c230f5398f3cc5a429a7721720c3b6b37\",\n            \"cni.projectcalico.org/podIP\": \"10.20.83.42/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.20.83.42/32\"\n        },\n        \"creationTimestamp\": \"2023-05-04T11:56:43Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-8937\",\n        \"resourceVersion\": \"11304\",\n        \"uid\": \"281f9522-3503-42ae-b8e2-a7b83acfc33a\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-zljkb\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-10-0-1-224.us-west-2.compute.internal\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-zljkb\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-04T11:56:43Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-04T11:56:44Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-04T11:56:44Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-04T11:56:43Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://76fa54f3739d3030620f69712160900d8330699631fb9141c53b73d27bf1ebf5\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-05-04T11:56:44Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.1.224\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.20.83.42\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.20.83.42\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-05-04T11:56:43Z\"\n    }\n}\n"
STEP: replace the image in the pod 05/04/23 11:56:48.337
May  4 11:56:48.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-8937 replace -f -'
May  4 11:56:49.377: INFO: stderr: ""
May  4 11:56:49.378: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 05/04/23 11:56:49.378
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1736
May  4 11:56:49.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-8937 delete pods e2e-test-httpd-pod'
May  4 11:56:50.745: INFO: stderr: ""
May  4 11:56:50.745: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  4 11:56:50.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8937" for this suite. 05/04/23 11:56:50.752
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","completed":57,"skipped":1042,"failed":0}
------------------------------
• [SLOW TEST] [7.675 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1729
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1745

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:56:43.083
    May  4 11:56:43.083: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename kubectl 05/04/23 11:56:43.084
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:56:43.098
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:56:43.103
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1732
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1745
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 05/04/23 11:56:43.107
    May  4 11:56:43.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-8937 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    May  4 11:56:43.200: INFO: stderr: ""
    May  4 11:56:43.200: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 05/04/23 11:56:43.2
    STEP: verifying the pod e2e-test-httpd-pod was created 05/04/23 11:56:48.251
    May  4 11:56:48.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-8937 get pod e2e-test-httpd-pod -o json'
    May  4 11:56:48.336: INFO: stderr: ""
    May  4 11:56:48.336: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"9375dd3f4ddfbc11b99e5e1da5b22d7c230f5398f3cc5a429a7721720c3b6b37\",\n            \"cni.projectcalico.org/podIP\": \"10.20.83.42/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.20.83.42/32\"\n        },\n        \"creationTimestamp\": \"2023-05-04T11:56:43Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-8937\",\n        \"resourceVersion\": \"11304\",\n        \"uid\": \"281f9522-3503-42ae-b8e2-a7b83acfc33a\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-zljkb\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-10-0-1-224.us-west-2.compute.internal\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-zljkb\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-04T11:56:43Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-04T11:56:44Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-04T11:56:44Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-04T11:56:43Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://76fa54f3739d3030620f69712160900d8330699631fb9141c53b73d27bf1ebf5\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-05-04T11:56:44Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.1.224\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.20.83.42\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.20.83.42\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-05-04T11:56:43Z\"\n    }\n}\n"
    STEP: replace the image in the pod 05/04/23 11:56:48.337
    May  4 11:56:48.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-8937 replace -f -'
    May  4 11:56:49.377: INFO: stderr: ""
    May  4 11:56:49.378: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 05/04/23 11:56:49.378
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1736
    May  4 11:56:49.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-8937 delete pods e2e-test-httpd-pod'
    May  4 11:56:50.745: INFO: stderr: ""
    May  4 11:56:50.745: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  4 11:56:50.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8937" for this suite. 05/04/23 11:56:50.752
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:56:50.759
May  4 11:56:50.759: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename endpointslicemirroring 05/04/23 11:56:50.76
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:56:50.774
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:56:50.776
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 05/04/23 11:56:50.792
May  4 11:56:50.801: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 05/04/23 11:56:52.806
May  4 11:56:52.814: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 05/04/23 11:56:54.818
May  4 11:56:54.828: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
May  4 11:56:56.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-7137" for this suite. 05/04/23 11:56:56.841
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","completed":58,"skipped":1059,"failed":0}
------------------------------
• [SLOW TEST] [6.089 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:56:50.759
    May  4 11:56:50.759: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename endpointslicemirroring 05/04/23 11:56:50.76
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:56:50.774
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:56:50.776
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 05/04/23 11:56:50.792
    May  4 11:56:50.801: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 05/04/23 11:56:52.806
    May  4 11:56:52.814: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 05/04/23 11:56:54.818
    May  4 11:56:54.828: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:187
    May  4 11:56:56.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslicemirroring-7137" for this suite. 05/04/23 11:56:56.841
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:56:56.849
May  4 11:56:56.849: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename job 05/04/23 11:56:56.85
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:56:56.869
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:56:56.874
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
STEP: Creating a job 05/04/23 11:56:56.878
STEP: Ensuring job reaches completions 05/04/23 11:56:56.885
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
May  4 11:57:08.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5704" for this suite. 05/04/23 11:57:08.904
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","completed":59,"skipped":1072,"failed":0}
------------------------------
• [SLOW TEST] [12.063 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:56:56.849
    May  4 11:56:56.849: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename job 05/04/23 11:56:56.85
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:56:56.869
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:56:56.874
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:254
    STEP: Creating a job 05/04/23 11:56:56.878
    STEP: Ensuring job reaches completions 05/04/23 11:56:56.885
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    May  4 11:57:08.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-5704" for this suite. 05/04/23 11:57:08.904
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:57:08.916
May  4 11:57:08.916: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename namespaces 05/04/23 11:57:08.918
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:57:08.934
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:57:08.937
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
STEP: Creating a test namespace 05/04/23 11:57:08.942
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:57:08.955
STEP: Creating a pod in the namespace 05/04/23 11:57:08.959
STEP: Waiting for the pod to have running status 05/04/23 11:57:08.967
May  4 11:57:08.967: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-8895" to be "running"
May  4 11:57:08.971: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.277903ms
May  4 11:57:10.976: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009267089s
May  4 11:57:12.976: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.009444036s
May  4 11:57:12.976: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 05/04/23 11:57:12.976
STEP: Waiting for the namespace to be removed. 05/04/23 11:57:12.986
STEP: Recreating the namespace 05/04/23 11:57:23.991
STEP: Verifying there are no pods in the namespace 05/04/23 11:57:24.005
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
May  4 11:57:24.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5792" for this suite. 05/04/23 11:57:24.016
STEP: Destroying namespace "nsdeletetest-8895" for this suite. 05/04/23 11:57:24.025
May  4 11:57:24.028: INFO: Namespace nsdeletetest-8895 was already deleted
STEP: Destroying namespace "nsdeletetest-5338" for this suite. 05/04/23 11:57:24.028
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","completed":60,"skipped":1089,"failed":0}
------------------------------
• [SLOW TEST] [15.119 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:57:08.916
    May  4 11:57:08.916: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename namespaces 05/04/23 11:57:08.918
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:57:08.934
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:57:08.937
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:242
    STEP: Creating a test namespace 05/04/23 11:57:08.942
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:57:08.955
    STEP: Creating a pod in the namespace 05/04/23 11:57:08.959
    STEP: Waiting for the pod to have running status 05/04/23 11:57:08.967
    May  4 11:57:08.967: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-8895" to be "running"
    May  4 11:57:08.971: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.277903ms
    May  4 11:57:10.976: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009267089s
    May  4 11:57:12.976: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.009444036s
    May  4 11:57:12.976: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 05/04/23 11:57:12.976
    STEP: Waiting for the namespace to be removed. 05/04/23 11:57:12.986
    STEP: Recreating the namespace 05/04/23 11:57:23.991
    STEP: Verifying there are no pods in the namespace 05/04/23 11:57:24.005
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    May  4 11:57:24.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-5792" for this suite. 05/04/23 11:57:24.016
    STEP: Destroying namespace "nsdeletetest-8895" for this suite. 05/04/23 11:57:24.025
    May  4 11:57:24.028: INFO: Namespace nsdeletetest-8895 was already deleted
    STEP: Destroying namespace "nsdeletetest-5338" for this suite. 05/04/23 11:57:24.028
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:57:24.044
May  4 11:57:24.044: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename daemonsets 05/04/23 11:57:24.045
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:57:24.061
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:57:24.064
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
STEP: Creating simple DaemonSet "daemon-set" 05/04/23 11:57:24.105
STEP: Check that daemon pods launch on every node of the cluster. 05/04/23 11:57:24.111
May  4 11:57:24.120: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 11:57:24.120: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 11:57:24.120: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 11:57:24.125: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  4 11:57:24.125: INFO: Node ip-10-0-1-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
May  4 11:57:25.131: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 11:57:25.131: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 11:57:25.131: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 11:57:25.136: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  4 11:57:25.136: INFO: Node ip-10-0-1-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
May  4 11:57:26.132: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 11:57:26.132: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 11:57:26.132: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 11:57:26.138: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
May  4 11:57:26.138: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
STEP: listing all DeamonSets 05/04/23 11:57:26.141
STEP: DeleteCollection of the DaemonSets 05/04/23 11:57:26.146
STEP: Verify that ReplicaSets have been deleted 05/04/23 11:57:26.154
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
May  4 11:57:26.168: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"11710"},"items":null}

May  4 11:57:26.174: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"11710"},"items":[{"metadata":{"name":"daemon-set-8pl2k","generateName":"daemon-set-","namespace":"daemonsets-9550","uid":"72b1d1e3-6100-4235-a971-11b583a4bfa0","resourceVersion":"11706","creationTimestamp":"2023-05-04T11:57:24Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"72b0f752761197ad4805884481f190d0279443d2ad88f978c1eb267b5efe2284","cni.projectcalico.org/podIP":"10.20.83.45/32","cni.projectcalico.org/podIPs":"10.20.83.45/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"138b103d-610d-4b45-be67-c63ecb230cfb","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-04T11:57:24Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-04T11:57:24Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"138b103d-610d-4b45-be67-c63ecb230cfb\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-04T11:57:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.83.45\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-frh5r","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-frh5r","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-1-224.us-west-2.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-1-224.us-west-2.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-04T11:57:24Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-04T11:57:25Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-04T11:57:25Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-04T11:57:24Z"}],"hostIP":"10.0.1.224","podIP":"10.20.83.45","podIPs":[{"ip":"10.20.83.45"}],"startTime":"2023-05-04T11:57:24Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-04T11:57:25Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://10b29e9a1356e9ee4af5f0e498f357e5e87f88e5a1b4c6af8c185e69810d8199","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-fmtx2","generateName":"daemon-set-","namespace":"daemonsets-9550","uid":"12919d6b-71ae-4eca-a611-961b4909748d","resourceVersion":"11700","creationTimestamp":"2023-05-04T11:57:24Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"b40b04a04d8f35a3acc146dbe2a6d1a2fb2c21e116a42a61a6980e4ce7dea9e4","cni.projectcalico.org/podIP":"10.20.142.160/32","cni.projectcalico.org/podIPs":"10.20.142.160/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"138b103d-610d-4b45-be67-c63ecb230cfb","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-04T11:57:24Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-04T11:57:24Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"138b103d-610d-4b45-be67-c63ecb230cfb\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-04T11:57:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.142.160\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-rv858","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-rv858","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-1-216.us-west-2.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-1-216.us-west-2.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-04T11:57:24Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-04T11:57:25Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-04T11:57:25Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-04T11:57:24Z"}],"hostIP":"10.0.1.216","podIP":"10.20.142.160","podIPs":[{"ip":"10.20.142.160"}],"startTime":"2023-05-04T11:57:24Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-04T11:57:25Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://9dd9a569a2403e2ceecf646d2c2fa0de95a9be2d6a25062e42dff0bf1423f186","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-jv49z","generateName":"daemon-set-","namespace":"daemonsets-9550","uid":"d3c745bd-4298-464d-ad41-29c0a108c23b","resourceVersion":"11698","creationTimestamp":"2023-05-04T11:57:24Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"df9a19dd5513cf05ac795e1f460c73a6900868900e8a2d174aeb7d3bdc1f5f65","cni.projectcalico.org/podIP":"10.20.1.135/32","cni.projectcalico.org/podIPs":"10.20.1.135/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"138b103d-610d-4b45-be67-c63ecb230cfb","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-04T11:57:24Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-04T11:57:24Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"138b103d-610d-4b45-be67-c63ecb230cfb\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-04T11:57:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.1.135\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-9vdfc","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-9vdfc","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-1-253.us-west-2.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-1-253.us-west-2.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-04T11:57:24Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-04T11:57:25Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-04T11:57:25Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-04T11:57:24Z"}],"hostIP":"10.0.1.253","podIP":"10.20.1.135","podIPs":[{"ip":"10.20.1.135"}],"startTime":"2023-05-04T11:57:24Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-04T11:57:25Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://77bd8512041a7053385b2fcba1961ce2e5b4ba2dd243354ed44e9eb4b5e3e797","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-kpm2d","generateName":"daemon-set-","namespace":"daemonsets-9550","uid":"bde1a76a-7aee-4032-8cee-de44b7f42012","resourceVersion":"11702","creationTimestamp":"2023-05-04T11:57:24Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"ba1d05eea35b8c60be2a48ce1b8e558c9b346bb96438a8576f29535ef1f80036","cni.projectcalico.org/podIP":"10.20.92.143/32","cni.projectcalico.org/podIPs":"10.20.92.143/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"138b103d-610d-4b45-be67-c63ecb230cfb","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-04T11:57:24Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-04T11:57:24Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"138b103d-610d-4b45-be67-c63ecb230cfb\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-04T11:57:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.92.143\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-hr5jh","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-hr5jh","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-1-232.us-west-2.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-1-232.us-west-2.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-04T11:57:24Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-04T11:57:25Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-04T11:57:25Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-04T11:57:24Z"}],"hostIP":"10.0.1.232","podIP":"10.20.92.143","podIPs":[{"ip":"10.20.92.143"}],"startTime":"2023-05-04T11:57:24Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-04T11:57:25Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://a65e8ac92c51261732dac852fba687fec8394130054a475dcbfd4a9d50b07047","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-v6n69","generateName":"daemon-set-","namespace":"daemonsets-9550","uid":"036bcd8a-b0da-4d02-a3ed-22f5fd776aa3","resourceVersion":"11708","creationTimestamp":"2023-05-04T11:57:24Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"1090f0d1ccea3cecfde22cba4b96b728e3c783f6fa4b2557da49662f1048c3ba","cni.projectcalico.org/podIP":"10.20.11.204/32","cni.projectcalico.org/podIPs":"10.20.11.204/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"138b103d-610d-4b45-be67-c63ecb230cfb","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-04T11:57:24Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-04T11:57:24Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"138b103d-610d-4b45-be67-c63ecb230cfb\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-04T11:57:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.11.204\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-d42tg","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-d42tg","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-1-189.us-west-2.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-1-189.us-west-2.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-04T11:57:24Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-04T11:57:26Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-04T11:57:26Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-04T11:57:24Z"}],"hostIP":"10.0.1.189","podIP":"10.20.11.204","podIPs":[{"ip":"10.20.11.204"}],"startTime":"2023-05-04T11:57:24Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-04T11:57:25Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://96a1ac073217e47aa19e64eac23c75824da5caac71d87ee336dab83d9124ebad","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
May  4 11:57:26.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9550" for this suite. 05/04/23 11:57:26.21
{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","completed":61,"skipped":1251,"failed":0}
------------------------------
• [2.171 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:57:24.044
    May  4 11:57:24.044: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename daemonsets 05/04/23 11:57:24.045
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:57:24.061
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:57:24.064
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:822
    STEP: Creating simple DaemonSet "daemon-set" 05/04/23 11:57:24.105
    STEP: Check that daemon pods launch on every node of the cluster. 05/04/23 11:57:24.111
    May  4 11:57:24.120: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 11:57:24.120: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 11:57:24.120: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 11:57:24.125: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  4 11:57:24.125: INFO: Node ip-10-0-1-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
    May  4 11:57:25.131: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 11:57:25.131: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 11:57:25.131: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 11:57:25.136: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  4 11:57:25.136: INFO: Node ip-10-0-1-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
    May  4 11:57:26.132: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 11:57:26.132: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 11:57:26.132: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 11:57:26.138: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    May  4 11:57:26.138: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    STEP: listing all DeamonSets 05/04/23 11:57:26.141
    STEP: DeleteCollection of the DaemonSets 05/04/23 11:57:26.146
    STEP: Verify that ReplicaSets have been deleted 05/04/23 11:57:26.154
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    May  4 11:57:26.168: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"11710"},"items":null}

    May  4 11:57:26.174: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"11710"},"items":[{"metadata":{"name":"daemon-set-8pl2k","generateName":"daemon-set-","namespace":"daemonsets-9550","uid":"72b1d1e3-6100-4235-a971-11b583a4bfa0","resourceVersion":"11706","creationTimestamp":"2023-05-04T11:57:24Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"72b0f752761197ad4805884481f190d0279443d2ad88f978c1eb267b5efe2284","cni.projectcalico.org/podIP":"10.20.83.45/32","cni.projectcalico.org/podIPs":"10.20.83.45/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"138b103d-610d-4b45-be67-c63ecb230cfb","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-04T11:57:24Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-04T11:57:24Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"138b103d-610d-4b45-be67-c63ecb230cfb\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-04T11:57:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.83.45\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-frh5r","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-frh5r","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-1-224.us-west-2.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-1-224.us-west-2.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-04T11:57:24Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-04T11:57:25Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-04T11:57:25Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-04T11:57:24Z"}],"hostIP":"10.0.1.224","podIP":"10.20.83.45","podIPs":[{"ip":"10.20.83.45"}],"startTime":"2023-05-04T11:57:24Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-04T11:57:25Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://10b29e9a1356e9ee4af5f0e498f357e5e87f88e5a1b4c6af8c185e69810d8199","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-fmtx2","generateName":"daemon-set-","namespace":"daemonsets-9550","uid":"12919d6b-71ae-4eca-a611-961b4909748d","resourceVersion":"11700","creationTimestamp":"2023-05-04T11:57:24Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"b40b04a04d8f35a3acc146dbe2a6d1a2fb2c21e116a42a61a6980e4ce7dea9e4","cni.projectcalico.org/podIP":"10.20.142.160/32","cni.projectcalico.org/podIPs":"10.20.142.160/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"138b103d-610d-4b45-be67-c63ecb230cfb","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-04T11:57:24Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-04T11:57:24Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"138b103d-610d-4b45-be67-c63ecb230cfb\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-04T11:57:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.142.160\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-rv858","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-rv858","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-1-216.us-west-2.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-1-216.us-west-2.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-04T11:57:24Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-04T11:57:25Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-04T11:57:25Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-04T11:57:24Z"}],"hostIP":"10.0.1.216","podIP":"10.20.142.160","podIPs":[{"ip":"10.20.142.160"}],"startTime":"2023-05-04T11:57:24Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-04T11:57:25Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://9dd9a569a2403e2ceecf646d2c2fa0de95a9be2d6a25062e42dff0bf1423f186","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-jv49z","generateName":"daemon-set-","namespace":"daemonsets-9550","uid":"d3c745bd-4298-464d-ad41-29c0a108c23b","resourceVersion":"11698","creationTimestamp":"2023-05-04T11:57:24Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"df9a19dd5513cf05ac795e1f460c73a6900868900e8a2d174aeb7d3bdc1f5f65","cni.projectcalico.org/podIP":"10.20.1.135/32","cni.projectcalico.org/podIPs":"10.20.1.135/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"138b103d-610d-4b45-be67-c63ecb230cfb","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-04T11:57:24Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-04T11:57:24Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"138b103d-610d-4b45-be67-c63ecb230cfb\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-04T11:57:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.1.135\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-9vdfc","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-9vdfc","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-1-253.us-west-2.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-1-253.us-west-2.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-04T11:57:24Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-04T11:57:25Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-04T11:57:25Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-04T11:57:24Z"}],"hostIP":"10.0.1.253","podIP":"10.20.1.135","podIPs":[{"ip":"10.20.1.135"}],"startTime":"2023-05-04T11:57:24Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-04T11:57:25Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://77bd8512041a7053385b2fcba1961ce2e5b4ba2dd243354ed44e9eb4b5e3e797","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-kpm2d","generateName":"daemon-set-","namespace":"daemonsets-9550","uid":"bde1a76a-7aee-4032-8cee-de44b7f42012","resourceVersion":"11702","creationTimestamp":"2023-05-04T11:57:24Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"ba1d05eea35b8c60be2a48ce1b8e558c9b346bb96438a8576f29535ef1f80036","cni.projectcalico.org/podIP":"10.20.92.143/32","cni.projectcalico.org/podIPs":"10.20.92.143/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"138b103d-610d-4b45-be67-c63ecb230cfb","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-04T11:57:24Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-04T11:57:24Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"138b103d-610d-4b45-be67-c63ecb230cfb\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-04T11:57:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.92.143\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-hr5jh","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-hr5jh","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-1-232.us-west-2.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-1-232.us-west-2.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-04T11:57:24Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-04T11:57:25Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-04T11:57:25Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-04T11:57:24Z"}],"hostIP":"10.0.1.232","podIP":"10.20.92.143","podIPs":[{"ip":"10.20.92.143"}],"startTime":"2023-05-04T11:57:24Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-04T11:57:25Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://a65e8ac92c51261732dac852fba687fec8394130054a475dcbfd4a9d50b07047","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-v6n69","generateName":"daemon-set-","namespace":"daemonsets-9550","uid":"036bcd8a-b0da-4d02-a3ed-22f5fd776aa3","resourceVersion":"11708","creationTimestamp":"2023-05-04T11:57:24Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"1090f0d1ccea3cecfde22cba4b96b728e3c783f6fa4b2557da49662f1048c3ba","cni.projectcalico.org/podIP":"10.20.11.204/32","cni.projectcalico.org/podIPs":"10.20.11.204/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"138b103d-610d-4b45-be67-c63ecb230cfb","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-04T11:57:24Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-04T11:57:24Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"138b103d-610d-4b45-be67-c63ecb230cfb\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-04T11:57:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.11.204\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-d42tg","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-d42tg","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-1-189.us-west-2.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-1-189.us-west-2.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-04T11:57:24Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-04T11:57:26Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-04T11:57:26Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-04T11:57:24Z"}],"hostIP":"10.0.1.189","podIP":"10.20.11.204","podIPs":[{"ip":"10.20.11.204"}],"startTime":"2023-05-04T11:57:24Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-04T11:57:25Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://96a1ac073217e47aa19e64eac23c75824da5caac71d87ee336dab83d9124ebad","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    May  4 11:57:26.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-9550" for this suite. 05/04/23 11:57:26.21
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:57:26.217
May  4 11:57:26.217: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename runtimeclass 05/04/23 11:57:26.218
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:57:26.233
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:57:26.236
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
May  4 11:57:26.252: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-239 to be scheduled
May  4 11:57:26.255: INFO: 1 pods are not scheduled: [runtimeclass-239/test-runtimeclass-runtimeclass-239-preconfigured-handler-thh2x(eb478ff6-076c-4e10-96ae-c513a704c28a)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
May  4 11:57:28.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-239" for this suite. 05/04/23 11:57:28.285
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","completed":62,"skipped":1268,"failed":0}
------------------------------
• [2.077 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:57:26.217
    May  4 11:57:26.217: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename runtimeclass 05/04/23 11:57:26.218
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:57:26.233
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:57:26.236
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    May  4 11:57:26.252: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-239 to be scheduled
    May  4 11:57:26.255: INFO: 1 pods are not scheduled: [runtimeclass-239/test-runtimeclass-runtimeclass-239-preconfigured-handler-thh2x(eb478ff6-076c-4e10-96ae-c513a704c28a)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    May  4 11:57:28.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-239" for this suite. 05/04/23 11:57:28.285
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:57:28.298
May  4 11:57:28.298: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename svcaccounts 05/04/23 11:57:28.299
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:57:28.314
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:57:28.317
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
May  4 11:57:28.332: INFO: created pod
May  4 11:57:28.332: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-1705" to be "Succeeded or Failed"
May  4 11:57:28.335: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.02076ms
May  4 11:57:30.340: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007557282s
May  4 11:57:32.340: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007610211s
STEP: Saw pod success 05/04/23 11:57:32.34
May  4 11:57:32.340: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
May  4 11:58:02.341: INFO: polling logs
May  4 11:58:02.358: INFO: Pod logs: 
I0504 11:57:29.757492       1 log.go:195] OK: Got token
I0504 11:57:29.757542       1 log.go:195] validating with in-cluster discovery
I0504 11:57:29.757830       1 log.go:195] OK: got issuer https://kubernetes.default.svc
I0504 11:57:29.757857       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-1705:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1683202048, NotBefore:1683201448, IssuedAt:1683201448, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1705", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"56b35f14-86f8-4e1d-916a-7224a09e3f42"}}}
I0504 11:57:29.774661       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
I0504 11:57:29.778344       1 log.go:195] OK: Validated signature on JWT
I0504 11:57:29.778469       1 log.go:195] OK: Got valid claims from token!
I0504 11:57:29.778509       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-1705:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1683202048, NotBefore:1683201448, IssuedAt:1683201448, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1705", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"56b35f14-86f8-4e1d-916a-7224a09e3f42"}}}

May  4 11:58:02.358: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
May  4 11:58:02.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1705" for this suite. 05/04/23 11:58:02.377
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","completed":63,"skipped":1293,"failed":0}
------------------------------
• [SLOW TEST] [34.091 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:57:28.298
    May  4 11:57:28.298: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename svcaccounts 05/04/23 11:57:28.299
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:57:28.314
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:57:28.317
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:528
    May  4 11:57:28.332: INFO: created pod
    May  4 11:57:28.332: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-1705" to be "Succeeded or Failed"
    May  4 11:57:28.335: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.02076ms
    May  4 11:57:30.340: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007557282s
    May  4 11:57:32.340: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007610211s
    STEP: Saw pod success 05/04/23 11:57:32.34
    May  4 11:57:32.340: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    May  4 11:58:02.341: INFO: polling logs
    May  4 11:58:02.358: INFO: Pod logs: 
    I0504 11:57:29.757492       1 log.go:195] OK: Got token
    I0504 11:57:29.757542       1 log.go:195] validating with in-cluster discovery
    I0504 11:57:29.757830       1 log.go:195] OK: got issuer https://kubernetes.default.svc
    I0504 11:57:29.757857       1 log.go:195] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-1705:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1683202048, NotBefore:1683201448, IssuedAt:1683201448, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1705", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"56b35f14-86f8-4e1d-916a-7224a09e3f42"}}}
    I0504 11:57:29.774661       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
    I0504 11:57:29.778344       1 log.go:195] OK: Validated signature on JWT
    I0504 11:57:29.778469       1 log.go:195] OK: Got valid claims from token!
    I0504 11:57:29.778509       1 log.go:195] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-1705:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1683202048, NotBefore:1683201448, IssuedAt:1683201448, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1705", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"56b35f14-86f8-4e1d-916a-7224a09e3f42"}}}

    May  4 11:58:02.358: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    May  4 11:58:02.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-1705" for this suite. 05/04/23 11:58:02.377
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:58:02.397
May  4 11:58:02.397: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename replicaset 05/04/23 11:58:02.398
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:58:02.416
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:58:02.421
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
May  4 11:58:02.423: INFO: Creating ReplicaSet my-hostname-basic-ab01bfdd-a003-4647-bdfe-641fa0cc11e0
May  4 11:58:02.433: INFO: Pod name my-hostname-basic-ab01bfdd-a003-4647-bdfe-641fa0cc11e0: Found 0 pods out of 1
May  4 11:58:07.437: INFO: Pod name my-hostname-basic-ab01bfdd-a003-4647-bdfe-641fa0cc11e0: Found 1 pods out of 1
May  4 11:58:07.437: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-ab01bfdd-a003-4647-bdfe-641fa0cc11e0" is running
May  4 11:58:07.437: INFO: Waiting up to 5m0s for pod "my-hostname-basic-ab01bfdd-a003-4647-bdfe-641fa0cc11e0-cpcgf" in namespace "replicaset-8511" to be "running"
May  4 11:58:07.442: INFO: Pod "my-hostname-basic-ab01bfdd-a003-4647-bdfe-641fa0cc11e0-cpcgf": Phase="Running", Reason="", readiness=true. Elapsed: 4.432777ms
May  4 11:58:07.442: INFO: Pod "my-hostname-basic-ab01bfdd-a003-4647-bdfe-641fa0cc11e0-cpcgf" satisfied condition "running"
May  4 11:58:07.442: INFO: Pod "my-hostname-basic-ab01bfdd-a003-4647-bdfe-641fa0cc11e0-cpcgf" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-04 11:58:02 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-04 11:58:03 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-04 11:58:03 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-04 11:58:02 +0000 UTC Reason: Message:}])
May  4 11:58:07.442: INFO: Trying to dial the pod
May  4 11:58:12.455: INFO: Controller my-hostname-basic-ab01bfdd-a003-4647-bdfe-641fa0cc11e0: Got expected result from replica 1 [my-hostname-basic-ab01bfdd-a003-4647-bdfe-641fa0cc11e0-cpcgf]: "my-hostname-basic-ab01bfdd-a003-4647-bdfe-641fa0cc11e0-cpcgf", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
May  4 11:58:12.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8511" for this suite. 05/04/23 11:58:12.461
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","completed":64,"skipped":1324,"failed":0}
------------------------------
• [SLOW TEST] [10.070 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:58:02.397
    May  4 11:58:02.397: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename replicaset 05/04/23 11:58:02.398
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:58:02.416
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:58:02.421
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    May  4 11:58:02.423: INFO: Creating ReplicaSet my-hostname-basic-ab01bfdd-a003-4647-bdfe-641fa0cc11e0
    May  4 11:58:02.433: INFO: Pod name my-hostname-basic-ab01bfdd-a003-4647-bdfe-641fa0cc11e0: Found 0 pods out of 1
    May  4 11:58:07.437: INFO: Pod name my-hostname-basic-ab01bfdd-a003-4647-bdfe-641fa0cc11e0: Found 1 pods out of 1
    May  4 11:58:07.437: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-ab01bfdd-a003-4647-bdfe-641fa0cc11e0" is running
    May  4 11:58:07.437: INFO: Waiting up to 5m0s for pod "my-hostname-basic-ab01bfdd-a003-4647-bdfe-641fa0cc11e0-cpcgf" in namespace "replicaset-8511" to be "running"
    May  4 11:58:07.442: INFO: Pod "my-hostname-basic-ab01bfdd-a003-4647-bdfe-641fa0cc11e0-cpcgf": Phase="Running", Reason="", readiness=true. Elapsed: 4.432777ms
    May  4 11:58:07.442: INFO: Pod "my-hostname-basic-ab01bfdd-a003-4647-bdfe-641fa0cc11e0-cpcgf" satisfied condition "running"
    May  4 11:58:07.442: INFO: Pod "my-hostname-basic-ab01bfdd-a003-4647-bdfe-641fa0cc11e0-cpcgf" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-04 11:58:02 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-04 11:58:03 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-04 11:58:03 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-04 11:58:02 +0000 UTC Reason: Message:}])
    May  4 11:58:07.442: INFO: Trying to dial the pod
    May  4 11:58:12.455: INFO: Controller my-hostname-basic-ab01bfdd-a003-4647-bdfe-641fa0cc11e0: Got expected result from replica 1 [my-hostname-basic-ab01bfdd-a003-4647-bdfe-641fa0cc11e0-cpcgf]: "my-hostname-basic-ab01bfdd-a003-4647-bdfe-641fa0cc11e0-cpcgf", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    May  4 11:58:12.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-8511" for this suite. 05/04/23 11:58:12.461
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:58:12.469
May  4 11:58:12.469: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename services 05/04/23 11:58:12.47
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:58:12.485
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:58:12.489
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
STEP: creating service in namespace services-1182 05/04/23 11:58:12.492
STEP: creating service affinity-nodeport-transition in namespace services-1182 05/04/23 11:58:12.492
STEP: creating replication controller affinity-nodeport-transition in namespace services-1182 05/04/23 11:58:12.507
I0504 11:58:12.518364      21 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-1182, replica count: 3
I0504 11:58:15.570071      21 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  4 11:58:15.587: INFO: Creating new exec pod
May  4 11:58:15.592: INFO: Waiting up to 5m0s for pod "execpod-affinitysk2g5" in namespace "services-1182" to be "running"
May  4 11:58:15.595: INFO: Pod "execpod-affinitysk2g5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.658224ms
May  4 11:58:17.599: INFO: Pod "execpod-affinitysk2g5": Phase="Running", Reason="", readiness=true. Elapsed: 2.007081368s
May  4 11:58:17.599: INFO: Pod "execpod-affinitysk2g5" satisfied condition "running"
May  4 11:58:18.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-1182 exec execpod-affinitysk2g5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
May  4 11:58:18.780: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
May  4 11:58:18.780: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  4 11:58:18.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-1182 exec execpod-affinitysk2g5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.21.39.39 80'
May  4 11:58:18.956: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.21.39.39 80\nConnection to 10.21.39.39 80 port [tcp/http] succeeded!\n"
May  4 11:58:18.956: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  4 11:58:18.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-1182 exec execpod-affinitysk2g5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.1.224 30037'
May  4 11:58:19.156: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.1.224 30037\nConnection to 10.0.1.224 30037 port [tcp/*] succeeded!\n"
May  4 11:58:19.156: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  4 11:58:19.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-1182 exec execpod-affinitysk2g5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.1.232 30037'
May  4 11:58:19.357: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.1.232 30037\nConnection to 10.0.1.232 30037 port [tcp/*] succeeded!\n"
May  4 11:58:19.357: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  4 11:58:19.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-1182 exec execpod-affinitysk2g5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.1.189:30037/ ; done'
May  4 11:58:19.798: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n"
May  4 11:58:19.798: INFO: stdout: "\naffinity-nodeport-transition-zpq5x\naffinity-nodeport-transition-96dj5\naffinity-nodeport-transition-96qfs\naffinity-nodeport-transition-zpq5x\naffinity-nodeport-transition-96dj5\naffinity-nodeport-transition-96qfs\naffinity-nodeport-transition-zpq5x\naffinity-nodeport-transition-96dj5\naffinity-nodeport-transition-96qfs\naffinity-nodeport-transition-zpq5x\naffinity-nodeport-transition-96dj5\naffinity-nodeport-transition-96qfs\naffinity-nodeport-transition-zpq5x\naffinity-nodeport-transition-96dj5\naffinity-nodeport-transition-96qfs\naffinity-nodeport-transition-zpq5x"
May  4 11:58:19.798: INFO: Received response from host: affinity-nodeport-transition-zpq5x
May  4 11:58:19.798: INFO: Received response from host: affinity-nodeport-transition-96dj5
May  4 11:58:19.798: INFO: Received response from host: affinity-nodeport-transition-96qfs
May  4 11:58:19.798: INFO: Received response from host: affinity-nodeport-transition-zpq5x
May  4 11:58:19.798: INFO: Received response from host: affinity-nodeport-transition-96dj5
May  4 11:58:19.798: INFO: Received response from host: affinity-nodeport-transition-96qfs
May  4 11:58:19.798: INFO: Received response from host: affinity-nodeport-transition-zpq5x
May  4 11:58:19.798: INFO: Received response from host: affinity-nodeport-transition-96dj5
May  4 11:58:19.798: INFO: Received response from host: affinity-nodeport-transition-96qfs
May  4 11:58:19.798: INFO: Received response from host: affinity-nodeport-transition-zpq5x
May  4 11:58:19.798: INFO: Received response from host: affinity-nodeport-transition-96dj5
May  4 11:58:19.798: INFO: Received response from host: affinity-nodeport-transition-96qfs
May  4 11:58:19.798: INFO: Received response from host: affinity-nodeport-transition-zpq5x
May  4 11:58:19.798: INFO: Received response from host: affinity-nodeport-transition-96dj5
May  4 11:58:19.798: INFO: Received response from host: affinity-nodeport-transition-96qfs
May  4 11:58:19.798: INFO: Received response from host: affinity-nodeport-transition-zpq5x
May  4 11:58:19.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-1182 exec execpod-affinitysk2g5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.1.189:30037/ ; done'
May  4 11:58:20.306: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n"
May  4 11:58:20.306: INFO: stdout: "\naffinity-nodeport-transition-96dj5\naffinity-nodeport-transition-96dj5\naffinity-nodeport-transition-96dj5\naffinity-nodeport-transition-96dj5\naffinity-nodeport-transition-96dj5\naffinity-nodeport-transition-96dj5\naffinity-nodeport-transition-96dj5\naffinity-nodeport-transition-96dj5\naffinity-nodeport-transition-96dj5\naffinity-nodeport-transition-96dj5\naffinity-nodeport-transition-96dj5\naffinity-nodeport-transition-96dj5\naffinity-nodeport-transition-96dj5\naffinity-nodeport-transition-96dj5\naffinity-nodeport-transition-96dj5\naffinity-nodeport-transition-96dj5"
May  4 11:58:20.306: INFO: Received response from host: affinity-nodeport-transition-96dj5
May  4 11:58:20.306: INFO: Received response from host: affinity-nodeport-transition-96dj5
May  4 11:58:20.306: INFO: Received response from host: affinity-nodeport-transition-96dj5
May  4 11:58:20.306: INFO: Received response from host: affinity-nodeport-transition-96dj5
May  4 11:58:20.307: INFO: Received response from host: affinity-nodeport-transition-96dj5
May  4 11:58:20.307: INFO: Received response from host: affinity-nodeport-transition-96dj5
May  4 11:58:20.307: INFO: Received response from host: affinity-nodeport-transition-96dj5
May  4 11:58:20.307: INFO: Received response from host: affinity-nodeport-transition-96dj5
May  4 11:58:20.307: INFO: Received response from host: affinity-nodeport-transition-96dj5
May  4 11:58:20.307: INFO: Received response from host: affinity-nodeport-transition-96dj5
May  4 11:58:20.307: INFO: Received response from host: affinity-nodeport-transition-96dj5
May  4 11:58:20.307: INFO: Received response from host: affinity-nodeport-transition-96dj5
May  4 11:58:20.307: INFO: Received response from host: affinity-nodeport-transition-96dj5
May  4 11:58:20.307: INFO: Received response from host: affinity-nodeport-transition-96dj5
May  4 11:58:20.307: INFO: Received response from host: affinity-nodeport-transition-96dj5
May  4 11:58:20.307: INFO: Received response from host: affinity-nodeport-transition-96dj5
May  4 11:58:20.307: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-1182, will wait for the garbage collector to delete the pods 05/04/23 11:58:20.32
May  4 11:58:20.381: INFO: Deleting ReplicationController affinity-nodeport-transition took: 6.795241ms
May  4 11:58:20.482: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.886528ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  4 11:58:22.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1182" for this suite. 05/04/23 11:58:22.72
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","completed":65,"skipped":1362,"failed":0}
------------------------------
• [SLOW TEST] [10.258 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:58:12.469
    May  4 11:58:12.469: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename services 05/04/23 11:58:12.47
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:58:12.485
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:58:12.489
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2237
    STEP: creating service in namespace services-1182 05/04/23 11:58:12.492
    STEP: creating service affinity-nodeport-transition in namespace services-1182 05/04/23 11:58:12.492
    STEP: creating replication controller affinity-nodeport-transition in namespace services-1182 05/04/23 11:58:12.507
    I0504 11:58:12.518364      21 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-1182, replica count: 3
    I0504 11:58:15.570071      21 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  4 11:58:15.587: INFO: Creating new exec pod
    May  4 11:58:15.592: INFO: Waiting up to 5m0s for pod "execpod-affinitysk2g5" in namespace "services-1182" to be "running"
    May  4 11:58:15.595: INFO: Pod "execpod-affinitysk2g5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.658224ms
    May  4 11:58:17.599: INFO: Pod "execpod-affinitysk2g5": Phase="Running", Reason="", readiness=true. Elapsed: 2.007081368s
    May  4 11:58:17.599: INFO: Pod "execpod-affinitysk2g5" satisfied condition "running"
    May  4 11:58:18.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-1182 exec execpod-affinitysk2g5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
    May  4 11:58:18.780: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    May  4 11:58:18.780: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  4 11:58:18.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-1182 exec execpod-affinitysk2g5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.21.39.39 80'
    May  4 11:58:18.956: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.21.39.39 80\nConnection to 10.21.39.39 80 port [tcp/http] succeeded!\n"
    May  4 11:58:18.956: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  4 11:58:18.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-1182 exec execpod-affinitysk2g5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.1.224 30037'
    May  4 11:58:19.156: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.1.224 30037\nConnection to 10.0.1.224 30037 port [tcp/*] succeeded!\n"
    May  4 11:58:19.156: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  4 11:58:19.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-1182 exec execpod-affinitysk2g5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.1.232 30037'
    May  4 11:58:19.357: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.1.232 30037\nConnection to 10.0.1.232 30037 port [tcp/*] succeeded!\n"
    May  4 11:58:19.357: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  4 11:58:19.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-1182 exec execpod-affinitysk2g5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.1.189:30037/ ; done'
    May  4 11:58:19.798: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n"
    May  4 11:58:19.798: INFO: stdout: "\naffinity-nodeport-transition-zpq5x\naffinity-nodeport-transition-96dj5\naffinity-nodeport-transition-96qfs\naffinity-nodeport-transition-zpq5x\naffinity-nodeport-transition-96dj5\naffinity-nodeport-transition-96qfs\naffinity-nodeport-transition-zpq5x\naffinity-nodeport-transition-96dj5\naffinity-nodeport-transition-96qfs\naffinity-nodeport-transition-zpq5x\naffinity-nodeport-transition-96dj5\naffinity-nodeport-transition-96qfs\naffinity-nodeport-transition-zpq5x\naffinity-nodeport-transition-96dj5\naffinity-nodeport-transition-96qfs\naffinity-nodeport-transition-zpq5x"
    May  4 11:58:19.798: INFO: Received response from host: affinity-nodeport-transition-zpq5x
    May  4 11:58:19.798: INFO: Received response from host: affinity-nodeport-transition-96dj5
    May  4 11:58:19.798: INFO: Received response from host: affinity-nodeport-transition-96qfs
    May  4 11:58:19.798: INFO: Received response from host: affinity-nodeport-transition-zpq5x
    May  4 11:58:19.798: INFO: Received response from host: affinity-nodeport-transition-96dj5
    May  4 11:58:19.798: INFO: Received response from host: affinity-nodeport-transition-96qfs
    May  4 11:58:19.798: INFO: Received response from host: affinity-nodeport-transition-zpq5x
    May  4 11:58:19.798: INFO: Received response from host: affinity-nodeport-transition-96dj5
    May  4 11:58:19.798: INFO: Received response from host: affinity-nodeport-transition-96qfs
    May  4 11:58:19.798: INFO: Received response from host: affinity-nodeport-transition-zpq5x
    May  4 11:58:19.798: INFO: Received response from host: affinity-nodeport-transition-96dj5
    May  4 11:58:19.798: INFO: Received response from host: affinity-nodeport-transition-96qfs
    May  4 11:58:19.798: INFO: Received response from host: affinity-nodeport-transition-zpq5x
    May  4 11:58:19.798: INFO: Received response from host: affinity-nodeport-transition-96dj5
    May  4 11:58:19.798: INFO: Received response from host: affinity-nodeport-transition-96qfs
    May  4 11:58:19.798: INFO: Received response from host: affinity-nodeport-transition-zpq5x
    May  4 11:58:19.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-1182 exec execpod-affinitysk2g5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.1.189:30037/ ; done'
    May  4 11:58:20.306: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:30037/\n"
    May  4 11:58:20.306: INFO: stdout: "\naffinity-nodeport-transition-96dj5\naffinity-nodeport-transition-96dj5\naffinity-nodeport-transition-96dj5\naffinity-nodeport-transition-96dj5\naffinity-nodeport-transition-96dj5\naffinity-nodeport-transition-96dj5\naffinity-nodeport-transition-96dj5\naffinity-nodeport-transition-96dj5\naffinity-nodeport-transition-96dj5\naffinity-nodeport-transition-96dj5\naffinity-nodeport-transition-96dj5\naffinity-nodeport-transition-96dj5\naffinity-nodeport-transition-96dj5\naffinity-nodeport-transition-96dj5\naffinity-nodeport-transition-96dj5\naffinity-nodeport-transition-96dj5"
    May  4 11:58:20.306: INFO: Received response from host: affinity-nodeport-transition-96dj5
    May  4 11:58:20.306: INFO: Received response from host: affinity-nodeport-transition-96dj5
    May  4 11:58:20.306: INFO: Received response from host: affinity-nodeport-transition-96dj5
    May  4 11:58:20.306: INFO: Received response from host: affinity-nodeport-transition-96dj5
    May  4 11:58:20.307: INFO: Received response from host: affinity-nodeport-transition-96dj5
    May  4 11:58:20.307: INFO: Received response from host: affinity-nodeport-transition-96dj5
    May  4 11:58:20.307: INFO: Received response from host: affinity-nodeport-transition-96dj5
    May  4 11:58:20.307: INFO: Received response from host: affinity-nodeport-transition-96dj5
    May  4 11:58:20.307: INFO: Received response from host: affinity-nodeport-transition-96dj5
    May  4 11:58:20.307: INFO: Received response from host: affinity-nodeport-transition-96dj5
    May  4 11:58:20.307: INFO: Received response from host: affinity-nodeport-transition-96dj5
    May  4 11:58:20.307: INFO: Received response from host: affinity-nodeport-transition-96dj5
    May  4 11:58:20.307: INFO: Received response from host: affinity-nodeport-transition-96dj5
    May  4 11:58:20.307: INFO: Received response from host: affinity-nodeport-transition-96dj5
    May  4 11:58:20.307: INFO: Received response from host: affinity-nodeport-transition-96dj5
    May  4 11:58:20.307: INFO: Received response from host: affinity-nodeport-transition-96dj5
    May  4 11:58:20.307: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-1182, will wait for the garbage collector to delete the pods 05/04/23 11:58:20.32
    May  4 11:58:20.381: INFO: Deleting ReplicationController affinity-nodeport-transition took: 6.795241ms
    May  4 11:58:20.482: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.886528ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  4 11:58:22.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1182" for this suite. 05/04/23 11:58:22.72
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 11:58:22.731
May  4 11:58:22.731: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename container-probe 05/04/23 11:58:22.732
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:58:22.747
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:58:22.751
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
STEP: Creating pod test-webserver-6250b215-2eaa-4cdb-9650-262a7c02e70e in namespace container-probe-6423 05/04/23 11:58:22.755
May  4 11:58:22.766: INFO: Waiting up to 5m0s for pod "test-webserver-6250b215-2eaa-4cdb-9650-262a7c02e70e" in namespace "container-probe-6423" to be "not pending"
May  4 11:58:22.770: INFO: Pod "test-webserver-6250b215-2eaa-4cdb-9650-262a7c02e70e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.278159ms
May  4 11:58:24.774: INFO: Pod "test-webserver-6250b215-2eaa-4cdb-9650-262a7c02e70e": Phase="Running", Reason="", readiness=true. Elapsed: 2.008129124s
May  4 11:58:24.774: INFO: Pod "test-webserver-6250b215-2eaa-4cdb-9650-262a7c02e70e" satisfied condition "not pending"
May  4 11:58:24.774: INFO: Started pod test-webserver-6250b215-2eaa-4cdb-9650-262a7c02e70e in namespace container-probe-6423
STEP: checking the pod's current state and verifying that restartCount is present 05/04/23 11:58:24.774
May  4 11:58:24.778: INFO: Initial restart count of pod test-webserver-6250b215-2eaa-4cdb-9650-262a7c02e70e is 0
STEP: deleting the pod 05/04/23 12:02:25.377
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
May  4 12:02:25.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6423" for this suite. 05/04/23 12:02:25.429
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":66,"skipped":1454,"failed":0}
------------------------------
• [SLOW TEST] [242.713 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 11:58:22.731
    May  4 11:58:22.731: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename container-probe 05/04/23 11:58:22.732
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 11:58:22.747
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 11:58:22.751
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:211
    STEP: Creating pod test-webserver-6250b215-2eaa-4cdb-9650-262a7c02e70e in namespace container-probe-6423 05/04/23 11:58:22.755
    May  4 11:58:22.766: INFO: Waiting up to 5m0s for pod "test-webserver-6250b215-2eaa-4cdb-9650-262a7c02e70e" in namespace "container-probe-6423" to be "not pending"
    May  4 11:58:22.770: INFO: Pod "test-webserver-6250b215-2eaa-4cdb-9650-262a7c02e70e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.278159ms
    May  4 11:58:24.774: INFO: Pod "test-webserver-6250b215-2eaa-4cdb-9650-262a7c02e70e": Phase="Running", Reason="", readiness=true. Elapsed: 2.008129124s
    May  4 11:58:24.774: INFO: Pod "test-webserver-6250b215-2eaa-4cdb-9650-262a7c02e70e" satisfied condition "not pending"
    May  4 11:58:24.774: INFO: Started pod test-webserver-6250b215-2eaa-4cdb-9650-262a7c02e70e in namespace container-probe-6423
    STEP: checking the pod's current state and verifying that restartCount is present 05/04/23 11:58:24.774
    May  4 11:58:24.778: INFO: Initial restart count of pod test-webserver-6250b215-2eaa-4cdb-9650-262a7c02e70e is 0
    STEP: deleting the pod 05/04/23 12:02:25.377
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    May  4 12:02:25.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-6423" for this suite. 05/04/23 12:02:25.429
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:02:25.446
May  4 12:02:25.446: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename sched-pred 05/04/23 12:02:25.45
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:02:25.478
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:02:25.482
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
May  4 12:02:25.486: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May  4 12:02:25.505: INFO: Waiting for terminating namespaces to be deleted...
May  4 12:02:25.509: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-189.us-west-2.compute.internal before test
May  4 12:02:25.540: INFO: calico-node-r8wl6 from kube-system started at 2023-05-04 11:14:50 +0000 UTC (1 container statuses recorded)
May  4 12:02:25.540: INFO: 	Container calico-node ready: true, restart count 0
May  4 12:02:25.540: INFO: calico-typha-85dbcd447f-6tg8g from kube-system started at 2023-05-04 11:15:25 +0000 UTC (1 container statuses recorded)
May  4 12:02:25.540: INFO: 	Container calico-typha ready: true, restart count 0
May  4 12:02:25.540: INFO: metrics-server-57bbf8548c-qkgc6 from kube-system started at 2023-05-04 11:16:29 +0000 UTC (1 container statuses recorded)
May  4 12:02:25.540: INFO: 	Container metrics-server ready: true, restart count 0
May  4 12:02:25.540: INFO: node-exporter-tsxz4 from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
May  4 12:02:25.540: INFO: 	Container node-exporter ready: true, restart count 0
May  4 12:02:25.540: INFO: prometheus-operator-85498c86bb-gdnsq from pf9-operators started at 2023-05-04 11:16:25 +0000 UTC (1 container statuses recorded)
May  4 12:02:25.540: INFO: 	Container prometheus-operator ready: true, restart count 0
May  4 12:02:25.540: INFO: sonobuoy-e2e-job-a8c15777d8b94166 from sonobuoy started at 2023-05-04 11:44:14 +0000 UTC (2 container statuses recorded)
May  4 12:02:25.540: INFO: 	Container e2e ready: true, restart count 0
May  4 12:02:25.540: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  4 12:02:25.540: INFO: sonobuoy-systemd-logs-daemon-set-d34856018c814171-f2qjv from sonobuoy started at 2023-05-04 11:44:14 +0000 UTC (2 container statuses recorded)
May  4 12:02:25.540: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  4 12:02:25.540: INFO: 	Container systemd-logs ready: true, restart count 0
May  4 12:02:25.540: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-216.us-west-2.compute.internal before test
May  4 12:02:25.571: INFO: calico-node-ll8r2 from kube-system started at 2023-05-04 11:15:13 +0000 UTC (1 container statuses recorded)
May  4 12:02:25.571: INFO: 	Container calico-node ready: true, restart count 0
May  4 12:02:25.571: INFO: calico-typha-85dbcd447f-qw96l from kube-system started at 2023-05-04 11:15:29 +0000 UTC (1 container statuses recorded)
May  4 12:02:25.571: INFO: 	Container calico-typha ready: true, restart count 0
May  4 12:02:25.571: INFO: grafana-84c9c8d6bd-rsbn8 from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (2 container statuses recorded)
May  4 12:02:25.571: INFO: 	Container grafana ready: true, restart count 0
May  4 12:02:25.571: INFO: 	Container proxy ready: true, restart count 0
May  4 12:02:25.571: INFO: node-exporter-pnw8x from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
May  4 12:02:25.571: INFO: 	Container node-exporter ready: true, restart count 0
May  4 12:02:25.571: INFO: sonobuoy from sonobuoy started at 2023-05-04 11:44:11 +0000 UTC (1 container statuses recorded)
May  4 12:02:25.571: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May  4 12:02:25.571: INFO: sonobuoy-systemd-logs-daemon-set-d34856018c814171-44cxg from sonobuoy started at 2023-05-04 11:44:14 +0000 UTC (2 container statuses recorded)
May  4 12:02:25.571: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  4 12:02:25.571: INFO: 	Container systemd-logs ready: true, restart count 0
May  4 12:02:25.572: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-224.us-west-2.compute.internal before test
May  4 12:02:25.606: INFO: calico-node-bb7nh from kube-system started at 2023-05-04 11:14:53 +0000 UTC (1 container statuses recorded)
May  4 12:02:25.606: INFO: 	Container calico-node ready: true, restart count 0
May  4 12:02:25.606: INFO: coredns-c7944df6b-gkv4x from kube-system started at 2023-05-04 11:16:34 +0000 UTC (1 container statuses recorded)
May  4 12:02:25.606: INFO: 	Container coredns ready: true, restart count 0
May  4 12:02:25.606: INFO: kube-dns-autoscaler-558cdf6846-vvkfg from kube-system started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
May  4 12:02:25.606: INFO: 	Container autoscaler ready: true, restart count 0
May  4 12:02:25.606: INFO: dashboard-metrics-scraper-7564894f4b-cpm4c from kubernetes-dashboard started at 2023-05-04 11:16:29 +0000 UTC (1 container statuses recorded)
May  4 12:02:25.606: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
May  4 12:02:25.606: INFO: kube-state-metrics-5bf549bd69-vwzfr from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
May  4 12:02:25.606: INFO: 	Container kube-state-metrics ready: true, restart count 0
May  4 12:02:25.606: INFO: node-exporter-qn2kc from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
May  4 12:02:25.606: INFO: 	Container node-exporter ready: true, restart count 0
May  4 12:02:25.606: INFO: sonobuoy-systemd-logs-daemon-set-d34856018c814171-lss27 from sonobuoy started at 2023-05-04 11:44:14 +0000 UTC (2 container statuses recorded)
May  4 12:02:25.606: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  4 12:02:25.606: INFO: 	Container systemd-logs ready: true, restart count 0
May  4 12:02:25.606: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-232.us-west-2.compute.internal before test
May  4 12:02:25.658: INFO: calico-kube-controllers-654d9ff976-ddwnz from kube-system started at 2023-05-04 11:15:10 +0000 UTC (1 container statuses recorded)
May  4 12:02:25.658: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May  4 12:02:25.658: INFO: calico-node-48gv4 from kube-system started at 2023-05-04 11:14:51 +0000 UTC (1 container statuses recorded)
May  4 12:02:25.658: INFO: 	Container calico-node ready: true, restart count 0
May  4 12:02:25.658: INFO: calico-typha-85dbcd447f-2s7df from kube-system started at 2023-05-04 11:15:10 +0000 UTC (1 container statuses recorded)
May  4 12:02:25.658: INFO: 	Container calico-typha ready: true, restart count 0
May  4 12:02:25.658: INFO: calico-typha-autoscaler-795696b9bd-qhx26 from kube-system started at 2023-05-04 11:15:10 +0000 UTC (1 container statuses recorded)
May  4 12:02:25.658: INFO: 	Container autoscaler ready: true, restart count 0
May  4 12:02:25.658: INFO: kube-state-metrics-857f6bcbbb-krrw9 from kube-system started at 2023-05-04 11:15:10 +0000 UTC (1 container statuses recorded)
May  4 12:02:25.658: INFO: 	Container kube-state-metrics ready: true, restart count 0
May  4 12:02:25.658: INFO: alertmanager-sysalert-0 from pf9-monitoring started at 2023-05-04 11:16:29 +0000 UTC (2 container statuses recorded)
May  4 12:02:25.658: INFO: 	Container alertmanager ready: true, restart count 1
May  4 12:02:25.658: INFO: 	Container config-reloader ready: true, restart count 0
May  4 12:02:25.658: INFO: node-exporter-49wcf from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
May  4 12:02:25.658: INFO: 	Container node-exporter ready: true, restart count 0
May  4 12:02:25.658: INFO: sonobuoy-systemd-logs-daemon-set-d34856018c814171-thbzc from sonobuoy started at 2023-05-04 11:44:14 +0000 UTC (2 container statuses recorded)
May  4 12:02:25.658: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  4 12:02:25.658: INFO: 	Container systemd-logs ready: true, restart count 0
May  4 12:02:25.658: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-253.us-west-2.compute.internal before test
May  4 12:02:25.690: INFO: calico-node-69h7w from kube-system started at 2023-05-04 11:14:49 +0000 UTC (1 container statuses recorded)
May  4 12:02:25.690: INFO: 	Container calico-node ready: true, restart count 0
May  4 12:02:25.690: INFO: coredns-c7944df6b-zhglb from kube-system started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
May  4 12:02:25.690: INFO: 	Container coredns ready: true, restart count 0
May  4 12:02:25.690: INFO: node-exporter-q8qnn from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
May  4 12:02:25.690: INFO: 	Container node-exporter ready: true, restart count 0
May  4 12:02:25.690: INFO: prometheus-system-0 from pf9-monitoring started at 2023-05-04 11:16:30 +0000 UTC (2 container statuses recorded)
May  4 12:02:25.690: INFO: 	Container config-reloader ready: true, restart count 0
May  4 12:02:25.690: INFO: 	Container prometheus ready: true, restart count 0
May  4 12:02:25.690: INFO: monhelper-57744bf759-6nndz from pf9-operators started at 2023-05-04 11:16:25 +0000 UTC (1 container statuses recorded)
May  4 12:02:25.690: INFO: 	Container monhelper ready: true, restart count 0
May  4 12:02:25.690: INFO: sonobuoy-systemd-logs-daemon-set-d34856018c814171-rx5lv from sonobuoy started at 2023-05-04 11:44:14 +0000 UTC (2 container statuses recorded)
May  4 12:02:25.690: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  4 12:02:25.690: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
STEP: Trying to schedule Pod with nonempty NodeSelector. 05/04/23 12:02:25.691
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.175bef11dd018b13], Reason = [FailedScheduling], Message = [0/8 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/master: true}, 8 node(s) didn't match Pod's node affinity/selector. preemption: 0/8 nodes are available: 8 Preemption is not helpful for scheduling.] 05/04/23 12:02:25.772
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
May  4 12:02:26.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3108" for this suite. 05/04/23 12:02:26.774
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","completed":67,"skipped":1483,"failed":0}
------------------------------
• [1.336 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:02:25.446
    May  4 12:02:25.446: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename sched-pred 05/04/23 12:02:25.45
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:02:25.478
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:02:25.482
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    May  4 12:02:25.486: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    May  4 12:02:25.505: INFO: Waiting for terminating namespaces to be deleted...
    May  4 12:02:25.509: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-1-189.us-west-2.compute.internal before test
    May  4 12:02:25.540: INFO: calico-node-r8wl6 from kube-system started at 2023-05-04 11:14:50 +0000 UTC (1 container statuses recorded)
    May  4 12:02:25.540: INFO: 	Container calico-node ready: true, restart count 0
    May  4 12:02:25.540: INFO: calico-typha-85dbcd447f-6tg8g from kube-system started at 2023-05-04 11:15:25 +0000 UTC (1 container statuses recorded)
    May  4 12:02:25.540: INFO: 	Container calico-typha ready: true, restart count 0
    May  4 12:02:25.540: INFO: metrics-server-57bbf8548c-qkgc6 from kube-system started at 2023-05-04 11:16:29 +0000 UTC (1 container statuses recorded)
    May  4 12:02:25.540: INFO: 	Container metrics-server ready: true, restart count 0
    May  4 12:02:25.540: INFO: node-exporter-tsxz4 from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
    May  4 12:02:25.540: INFO: 	Container node-exporter ready: true, restart count 0
    May  4 12:02:25.540: INFO: prometheus-operator-85498c86bb-gdnsq from pf9-operators started at 2023-05-04 11:16:25 +0000 UTC (1 container statuses recorded)
    May  4 12:02:25.540: INFO: 	Container prometheus-operator ready: true, restart count 0
    May  4 12:02:25.540: INFO: sonobuoy-e2e-job-a8c15777d8b94166 from sonobuoy started at 2023-05-04 11:44:14 +0000 UTC (2 container statuses recorded)
    May  4 12:02:25.540: INFO: 	Container e2e ready: true, restart count 0
    May  4 12:02:25.540: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  4 12:02:25.540: INFO: sonobuoy-systemd-logs-daemon-set-d34856018c814171-f2qjv from sonobuoy started at 2023-05-04 11:44:14 +0000 UTC (2 container statuses recorded)
    May  4 12:02:25.540: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  4 12:02:25.540: INFO: 	Container systemd-logs ready: true, restart count 0
    May  4 12:02:25.540: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-1-216.us-west-2.compute.internal before test
    May  4 12:02:25.571: INFO: calico-node-ll8r2 from kube-system started at 2023-05-04 11:15:13 +0000 UTC (1 container statuses recorded)
    May  4 12:02:25.571: INFO: 	Container calico-node ready: true, restart count 0
    May  4 12:02:25.571: INFO: calico-typha-85dbcd447f-qw96l from kube-system started at 2023-05-04 11:15:29 +0000 UTC (1 container statuses recorded)
    May  4 12:02:25.571: INFO: 	Container calico-typha ready: true, restart count 0
    May  4 12:02:25.571: INFO: grafana-84c9c8d6bd-rsbn8 from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (2 container statuses recorded)
    May  4 12:02:25.571: INFO: 	Container grafana ready: true, restart count 0
    May  4 12:02:25.571: INFO: 	Container proxy ready: true, restart count 0
    May  4 12:02:25.571: INFO: node-exporter-pnw8x from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
    May  4 12:02:25.571: INFO: 	Container node-exporter ready: true, restart count 0
    May  4 12:02:25.571: INFO: sonobuoy from sonobuoy started at 2023-05-04 11:44:11 +0000 UTC (1 container statuses recorded)
    May  4 12:02:25.571: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    May  4 12:02:25.571: INFO: sonobuoy-systemd-logs-daemon-set-d34856018c814171-44cxg from sonobuoy started at 2023-05-04 11:44:14 +0000 UTC (2 container statuses recorded)
    May  4 12:02:25.571: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  4 12:02:25.571: INFO: 	Container systemd-logs ready: true, restart count 0
    May  4 12:02:25.572: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-1-224.us-west-2.compute.internal before test
    May  4 12:02:25.606: INFO: calico-node-bb7nh from kube-system started at 2023-05-04 11:14:53 +0000 UTC (1 container statuses recorded)
    May  4 12:02:25.606: INFO: 	Container calico-node ready: true, restart count 0
    May  4 12:02:25.606: INFO: coredns-c7944df6b-gkv4x from kube-system started at 2023-05-04 11:16:34 +0000 UTC (1 container statuses recorded)
    May  4 12:02:25.606: INFO: 	Container coredns ready: true, restart count 0
    May  4 12:02:25.606: INFO: kube-dns-autoscaler-558cdf6846-vvkfg from kube-system started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
    May  4 12:02:25.606: INFO: 	Container autoscaler ready: true, restart count 0
    May  4 12:02:25.606: INFO: dashboard-metrics-scraper-7564894f4b-cpm4c from kubernetes-dashboard started at 2023-05-04 11:16:29 +0000 UTC (1 container statuses recorded)
    May  4 12:02:25.606: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    May  4 12:02:25.606: INFO: kube-state-metrics-5bf549bd69-vwzfr from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
    May  4 12:02:25.606: INFO: 	Container kube-state-metrics ready: true, restart count 0
    May  4 12:02:25.606: INFO: node-exporter-qn2kc from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
    May  4 12:02:25.606: INFO: 	Container node-exporter ready: true, restart count 0
    May  4 12:02:25.606: INFO: sonobuoy-systemd-logs-daemon-set-d34856018c814171-lss27 from sonobuoy started at 2023-05-04 11:44:14 +0000 UTC (2 container statuses recorded)
    May  4 12:02:25.606: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  4 12:02:25.606: INFO: 	Container systemd-logs ready: true, restart count 0
    May  4 12:02:25.606: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-1-232.us-west-2.compute.internal before test
    May  4 12:02:25.658: INFO: calico-kube-controllers-654d9ff976-ddwnz from kube-system started at 2023-05-04 11:15:10 +0000 UTC (1 container statuses recorded)
    May  4 12:02:25.658: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    May  4 12:02:25.658: INFO: calico-node-48gv4 from kube-system started at 2023-05-04 11:14:51 +0000 UTC (1 container statuses recorded)
    May  4 12:02:25.658: INFO: 	Container calico-node ready: true, restart count 0
    May  4 12:02:25.658: INFO: calico-typha-85dbcd447f-2s7df from kube-system started at 2023-05-04 11:15:10 +0000 UTC (1 container statuses recorded)
    May  4 12:02:25.658: INFO: 	Container calico-typha ready: true, restart count 0
    May  4 12:02:25.658: INFO: calico-typha-autoscaler-795696b9bd-qhx26 from kube-system started at 2023-05-04 11:15:10 +0000 UTC (1 container statuses recorded)
    May  4 12:02:25.658: INFO: 	Container autoscaler ready: true, restart count 0
    May  4 12:02:25.658: INFO: kube-state-metrics-857f6bcbbb-krrw9 from kube-system started at 2023-05-04 11:15:10 +0000 UTC (1 container statuses recorded)
    May  4 12:02:25.658: INFO: 	Container kube-state-metrics ready: true, restart count 0
    May  4 12:02:25.658: INFO: alertmanager-sysalert-0 from pf9-monitoring started at 2023-05-04 11:16:29 +0000 UTC (2 container statuses recorded)
    May  4 12:02:25.658: INFO: 	Container alertmanager ready: true, restart count 1
    May  4 12:02:25.658: INFO: 	Container config-reloader ready: true, restart count 0
    May  4 12:02:25.658: INFO: node-exporter-49wcf from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
    May  4 12:02:25.658: INFO: 	Container node-exporter ready: true, restart count 0
    May  4 12:02:25.658: INFO: sonobuoy-systemd-logs-daemon-set-d34856018c814171-thbzc from sonobuoy started at 2023-05-04 11:44:14 +0000 UTC (2 container statuses recorded)
    May  4 12:02:25.658: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  4 12:02:25.658: INFO: 	Container systemd-logs ready: true, restart count 0
    May  4 12:02:25.658: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-1-253.us-west-2.compute.internal before test
    May  4 12:02:25.690: INFO: calico-node-69h7w from kube-system started at 2023-05-04 11:14:49 +0000 UTC (1 container statuses recorded)
    May  4 12:02:25.690: INFO: 	Container calico-node ready: true, restart count 0
    May  4 12:02:25.690: INFO: coredns-c7944df6b-zhglb from kube-system started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
    May  4 12:02:25.690: INFO: 	Container coredns ready: true, restart count 0
    May  4 12:02:25.690: INFO: node-exporter-q8qnn from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
    May  4 12:02:25.690: INFO: 	Container node-exporter ready: true, restart count 0
    May  4 12:02:25.690: INFO: prometheus-system-0 from pf9-monitoring started at 2023-05-04 11:16:30 +0000 UTC (2 container statuses recorded)
    May  4 12:02:25.690: INFO: 	Container config-reloader ready: true, restart count 0
    May  4 12:02:25.690: INFO: 	Container prometheus ready: true, restart count 0
    May  4 12:02:25.690: INFO: monhelper-57744bf759-6nndz from pf9-operators started at 2023-05-04 11:16:25 +0000 UTC (1 container statuses recorded)
    May  4 12:02:25.690: INFO: 	Container monhelper ready: true, restart count 0
    May  4 12:02:25.690: INFO: sonobuoy-systemd-logs-daemon-set-d34856018c814171-rx5lv from sonobuoy started at 2023-05-04 11:44:14 +0000 UTC (2 container statuses recorded)
    May  4 12:02:25.690: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  4 12:02:25.690: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:438
    STEP: Trying to schedule Pod with nonempty NodeSelector. 05/04/23 12:02:25.691
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.175bef11dd018b13], Reason = [FailedScheduling], Message = [0/8 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/master: true}, 8 node(s) didn't match Pod's node affinity/selector. preemption: 0/8 nodes are available: 8 Preemption is not helpful for scheduling.] 05/04/23 12:02:25.772
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    May  4 12:02:26.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-3108" for this suite. 05/04/23 12:02:26.774
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:02:26.783
May  4 12:02:26.784: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename replicaset 05/04/23 12:02:26.785
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:02:26.801
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:02:26.806
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 05/04/23 12:02:26.808
May  4 12:02:26.818: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-9611" to be "running and ready"
May  4 12:02:26.823: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 5.095759ms
May  4 12:02:26.823: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
May  4 12:02:28.828: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.010326623s
May  4 12:02:28.828: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
May  4 12:02:28.828: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 05/04/23 12:02:28.831
STEP: Then the orphan pod is adopted 05/04/23 12:02:28.837
STEP: When the matched label of one of its pods change 05/04/23 12:02:29.848
May  4 12:02:29.852: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 05/04/23 12:02:29.864
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
May  4 12:02:30.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9611" for this suite. 05/04/23 12:02:30.896
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","completed":68,"skipped":1503,"failed":0}
------------------------------
• [4.119 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:02:26.783
    May  4 12:02:26.784: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename replicaset 05/04/23 12:02:26.785
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:02:26.801
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:02:26.806
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 05/04/23 12:02:26.808
    May  4 12:02:26.818: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-9611" to be "running and ready"
    May  4 12:02:26.823: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 5.095759ms
    May  4 12:02:26.823: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    May  4 12:02:28.828: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.010326623s
    May  4 12:02:28.828: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    May  4 12:02:28.828: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 05/04/23 12:02:28.831
    STEP: Then the orphan pod is adopted 05/04/23 12:02:28.837
    STEP: When the matched label of one of its pods change 05/04/23 12:02:29.848
    May  4 12:02:29.852: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 05/04/23 12:02:29.864
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    May  4 12:02:30.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-9611" for this suite. 05/04/23 12:02:30.896
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:02:30.904
May  4 12:02:30.904: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename security-context-test 05/04/23 12:02:30.905
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:02:30.928
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:02:30.932
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
May  4 12:02:30.956: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-30275cc9-a7df-4234-8536-98f13a2d5d8f" in namespace "security-context-test-3410" to be "Succeeded or Failed"
May  4 12:02:30.964: INFO: Pod "busybox-readonly-false-30275cc9-a7df-4234-8536-98f13a2d5d8f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.692105ms
May  4 12:02:32.969: INFO: Pod "busybox-readonly-false-30275cc9-a7df-4234-8536-98f13a2d5d8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013420136s
May  4 12:02:34.972: INFO: Pod "busybox-readonly-false-30275cc9-a7df-4234-8536-98f13a2d5d8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016570109s
May  4 12:02:34.972: INFO: Pod "busybox-readonly-false-30275cc9-a7df-4234-8536-98f13a2d5d8f" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
May  4 12:02:34.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3410" for this suite. 05/04/23 12:02:34.986
{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","completed":69,"skipped":1509,"failed":0}
------------------------------
• [4.094 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:429
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:485

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:02:30.904
    May  4 12:02:30.904: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename security-context-test 05/04/23 12:02:30.905
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:02:30.928
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:02:30.932
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:485
    May  4 12:02:30.956: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-30275cc9-a7df-4234-8536-98f13a2d5d8f" in namespace "security-context-test-3410" to be "Succeeded or Failed"
    May  4 12:02:30.964: INFO: Pod "busybox-readonly-false-30275cc9-a7df-4234-8536-98f13a2d5d8f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.692105ms
    May  4 12:02:32.969: INFO: Pod "busybox-readonly-false-30275cc9-a7df-4234-8536-98f13a2d5d8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013420136s
    May  4 12:02:34.972: INFO: Pod "busybox-readonly-false-30275cc9-a7df-4234-8536-98f13a2d5d8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016570109s
    May  4 12:02:34.972: INFO: Pod "busybox-readonly-false-30275cc9-a7df-4234-8536-98f13a2d5d8f" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    May  4 12:02:34.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-3410" for this suite. 05/04/23 12:02:34.986
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:02:35.002
May  4 12:02:35.002: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename projected 05/04/23 12:02:35.003
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:02:35.07
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:02:35.077
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
STEP: Creating projection with secret that has name projected-secret-test-map-edfd23e1-f5ac-4a54-812f-a092189f4807 05/04/23 12:02:35.08
STEP: Creating a pod to test consume secrets 05/04/23 12:02:35.087
May  4 12:02:35.101: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c81399da-f198-4863-96c1-540ad884eb5e" in namespace "projected-6791" to be "Succeeded or Failed"
May  4 12:02:35.108: INFO: Pod "pod-projected-secrets-c81399da-f198-4863-96c1-540ad884eb5e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.924424ms
May  4 12:02:37.112: INFO: Pod "pod-projected-secrets-c81399da-f198-4863-96c1-540ad884eb5e": Phase="Running", Reason="", readiness=true. Elapsed: 2.011117333s
May  4 12:02:39.113: INFO: Pod "pod-projected-secrets-c81399da-f198-4863-96c1-540ad884eb5e": Phase="Running", Reason="", readiness=false. Elapsed: 4.012459585s
May  4 12:02:41.112: INFO: Pod "pod-projected-secrets-c81399da-f198-4863-96c1-540ad884eb5e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011486376s
STEP: Saw pod success 05/04/23 12:02:41.112
May  4 12:02:41.113: INFO: Pod "pod-projected-secrets-c81399da-f198-4863-96c1-540ad884eb5e" satisfied condition "Succeeded or Failed"
May  4 12:02:41.116: INFO: Trying to get logs from node ip-10-0-1-232.us-west-2.compute.internal pod pod-projected-secrets-c81399da-f198-4863-96c1-540ad884eb5e container projected-secret-volume-test: <nil>
STEP: delete the pod 05/04/23 12:02:41.132
May  4 12:02:41.155: INFO: Waiting for pod pod-projected-secrets-c81399da-f198-4863-96c1-540ad884eb5e to disappear
May  4 12:02:41.158: INFO: Pod pod-projected-secrets-c81399da-f198-4863-96c1-540ad884eb5e no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
May  4 12:02:41.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6791" for this suite. 05/04/23 12:02:41.166
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":70,"skipped":1541,"failed":0}
------------------------------
• [SLOW TEST] [6.170 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:02:35.002
    May  4 12:02:35.002: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename projected 05/04/23 12:02:35.003
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:02:35.07
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:02:35.077
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:87
    STEP: Creating projection with secret that has name projected-secret-test-map-edfd23e1-f5ac-4a54-812f-a092189f4807 05/04/23 12:02:35.08
    STEP: Creating a pod to test consume secrets 05/04/23 12:02:35.087
    May  4 12:02:35.101: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c81399da-f198-4863-96c1-540ad884eb5e" in namespace "projected-6791" to be "Succeeded or Failed"
    May  4 12:02:35.108: INFO: Pod "pod-projected-secrets-c81399da-f198-4863-96c1-540ad884eb5e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.924424ms
    May  4 12:02:37.112: INFO: Pod "pod-projected-secrets-c81399da-f198-4863-96c1-540ad884eb5e": Phase="Running", Reason="", readiness=true. Elapsed: 2.011117333s
    May  4 12:02:39.113: INFO: Pod "pod-projected-secrets-c81399da-f198-4863-96c1-540ad884eb5e": Phase="Running", Reason="", readiness=false. Elapsed: 4.012459585s
    May  4 12:02:41.112: INFO: Pod "pod-projected-secrets-c81399da-f198-4863-96c1-540ad884eb5e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011486376s
    STEP: Saw pod success 05/04/23 12:02:41.112
    May  4 12:02:41.113: INFO: Pod "pod-projected-secrets-c81399da-f198-4863-96c1-540ad884eb5e" satisfied condition "Succeeded or Failed"
    May  4 12:02:41.116: INFO: Trying to get logs from node ip-10-0-1-232.us-west-2.compute.internal pod pod-projected-secrets-c81399da-f198-4863-96c1-540ad884eb5e container projected-secret-volume-test: <nil>
    STEP: delete the pod 05/04/23 12:02:41.132
    May  4 12:02:41.155: INFO: Waiting for pod pod-projected-secrets-c81399da-f198-4863-96c1-540ad884eb5e to disappear
    May  4 12:02:41.158: INFO: Pod pod-projected-secrets-c81399da-f198-4863-96c1-540ad884eb5e no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    May  4 12:02:41.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6791" for this suite. 05/04/23 12:02:41.166
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:02:41.173
May  4 12:02:41.173: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename dns 05/04/23 12:02:41.175
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:02:41.189
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:02:41.194
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1437.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1437.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 05/04/23 12:02:41.197
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1437.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1437.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 05/04/23 12:02:41.197
STEP: creating a pod to probe /etc/hosts 05/04/23 12:02:41.197
STEP: submitting the pod to kubernetes 05/04/23 12:02:41.198
May  4 12:02:41.208: INFO: Waiting up to 15m0s for pod "dns-test-41493dde-4e0f-4bc1-9bf4-f474c11f3405" in namespace "dns-1437" to be "running"
May  4 12:02:41.211: INFO: Pod "dns-test-41493dde-4e0f-4bc1-9bf4-f474c11f3405": Phase="Pending", Reason="", readiness=false. Elapsed: 3.439331ms
May  4 12:02:43.216: INFO: Pod "dns-test-41493dde-4e0f-4bc1-9bf4-f474c11f3405": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008208712s
May  4 12:02:45.216: INFO: Pod "dns-test-41493dde-4e0f-4bc1-9bf4-f474c11f3405": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008319886s
May  4 12:02:47.217: INFO: Pod "dns-test-41493dde-4e0f-4bc1-9bf4-f474c11f3405": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009083628s
May  4 12:02:49.217: INFO: Pod "dns-test-41493dde-4e0f-4bc1-9bf4-f474c11f3405": Phase="Running", Reason="", readiness=true. Elapsed: 8.009435212s
May  4 12:02:49.217: INFO: Pod "dns-test-41493dde-4e0f-4bc1-9bf4-f474c11f3405" satisfied condition "running"
STEP: retrieving the pod 05/04/23 12:02:49.217
STEP: looking for the results for each expected name from probers 05/04/23 12:02:49.221
May  4 12:02:49.245: INFO: DNS probes using dns-1437/dns-test-41493dde-4e0f-4bc1-9bf4-f474c11f3405 succeeded

STEP: deleting the pod 05/04/23 12:02:49.245
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
May  4 12:02:49.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1437" for this suite. 05/04/23 12:02:49.269
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","completed":71,"skipped":1542,"failed":0}
------------------------------
• [SLOW TEST] [8.102 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:02:41.173
    May  4 12:02:41.173: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename dns 05/04/23 12:02:41.175
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:02:41.189
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:02:41.194
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1437.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1437.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     05/04/23 12:02:41.197
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1437.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1437.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     05/04/23 12:02:41.197
    STEP: creating a pod to probe /etc/hosts 05/04/23 12:02:41.197
    STEP: submitting the pod to kubernetes 05/04/23 12:02:41.198
    May  4 12:02:41.208: INFO: Waiting up to 15m0s for pod "dns-test-41493dde-4e0f-4bc1-9bf4-f474c11f3405" in namespace "dns-1437" to be "running"
    May  4 12:02:41.211: INFO: Pod "dns-test-41493dde-4e0f-4bc1-9bf4-f474c11f3405": Phase="Pending", Reason="", readiness=false. Elapsed: 3.439331ms
    May  4 12:02:43.216: INFO: Pod "dns-test-41493dde-4e0f-4bc1-9bf4-f474c11f3405": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008208712s
    May  4 12:02:45.216: INFO: Pod "dns-test-41493dde-4e0f-4bc1-9bf4-f474c11f3405": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008319886s
    May  4 12:02:47.217: INFO: Pod "dns-test-41493dde-4e0f-4bc1-9bf4-f474c11f3405": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009083628s
    May  4 12:02:49.217: INFO: Pod "dns-test-41493dde-4e0f-4bc1-9bf4-f474c11f3405": Phase="Running", Reason="", readiness=true. Elapsed: 8.009435212s
    May  4 12:02:49.217: INFO: Pod "dns-test-41493dde-4e0f-4bc1-9bf4-f474c11f3405" satisfied condition "running"
    STEP: retrieving the pod 05/04/23 12:02:49.217
    STEP: looking for the results for each expected name from probers 05/04/23 12:02:49.221
    May  4 12:02:49.245: INFO: DNS probes using dns-1437/dns-test-41493dde-4e0f-4bc1-9bf4-f474c11f3405 succeeded

    STEP: deleting the pod 05/04/23 12:02:49.245
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    May  4 12:02:49.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-1437" for this suite. 05/04/23 12:02:49.269
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:02:49.276
May  4 12:02:49.276: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename gc 05/04/23 12:02:49.277
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:02:49.295
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:02:49.302
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 05/04/23 12:02:49.306
STEP: Wait for the Deployment to create new ReplicaSet 05/04/23 12:02:49.314
STEP: delete the deployment 05/04/23 12:02:49.824
STEP: wait for all rs to be garbage collected 05/04/23 12:02:49.831
STEP: expected 0 rs, got 1 rs 05/04/23 12:02:49.841
STEP: expected 0 pods, got 2 pods 05/04/23 12:02:49.851
STEP: Gathering metrics 05/04/23 12:02:50.362
W0504 12:02:50.392487      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
May  4 12:02:50.392: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
May  4 12:02:50.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4922" for this suite. 05/04/23 12:02:50.401
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","completed":72,"skipped":1545,"failed":0}
------------------------------
• [1.132 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:02:49.276
    May  4 12:02:49.276: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename gc 05/04/23 12:02:49.277
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:02:49.295
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:02:49.302
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 05/04/23 12:02:49.306
    STEP: Wait for the Deployment to create new ReplicaSet 05/04/23 12:02:49.314
    STEP: delete the deployment 05/04/23 12:02:49.824
    STEP: wait for all rs to be garbage collected 05/04/23 12:02:49.831
    STEP: expected 0 rs, got 1 rs 05/04/23 12:02:49.841
    STEP: expected 0 pods, got 2 pods 05/04/23 12:02:49.851
    STEP: Gathering metrics 05/04/23 12:02:50.362
    W0504 12:02:50.392487      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    May  4 12:02:50.392: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    May  4 12:02:50.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-4922" for this suite. 05/04/23 12:02:50.401
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:02:50.41
May  4 12:02:50.410: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename resourcequota 05/04/23 12:02:50.411
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:02:50.43
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:02:50.434
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
STEP: Counting existing ResourceQuota 05/04/23 12:02:50.439
STEP: Creating a ResourceQuota 05/04/23 12:02:55.443
STEP: Ensuring resource quota status is calculated 05/04/23 12:02:55.454
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
May  4 12:02:57.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9612" for this suite. 05/04/23 12:02:57.468
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","completed":73,"skipped":1561,"failed":0}
------------------------------
• [SLOW TEST] [7.153 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:02:50.41
    May  4 12:02:50.410: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename resourcequota 05/04/23 12:02:50.411
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:02:50.43
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:02:50.434
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:65
    STEP: Counting existing ResourceQuota 05/04/23 12:02:50.439
    STEP: Creating a ResourceQuota 05/04/23 12:02:55.443
    STEP: Ensuring resource quota status is calculated 05/04/23 12:02:55.454
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    May  4 12:02:57.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9612" for this suite. 05/04/23 12:02:57.468
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:02:57.563
May  4 12:02:57.563: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename deployment 05/04/23 12:02:57.565
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:02:57.582
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:02:57.59
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
May  4 12:02:57.605: INFO: Pod name rollover-pod: Found 0 pods out of 1
May  4 12:03:02.609: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 05/04/23 12:03:02.609
May  4 12:03:02.610: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
May  4 12:03:04.614: INFO: Creating deployment "test-rollover-deployment"
May  4 12:03:04.626: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
May  4 12:03:06.655: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
May  4 12:03:06.663: INFO: Ensure that both replica sets have 1 created replica
May  4 12:03:06.670: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
May  4 12:03:06.681: INFO: Updating deployment test-rollover-deployment
May  4 12:03:06.681: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
May  4 12:03:08.688: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
May  4 12:03:08.695: INFO: Make sure deployment "test-rollover-deployment" is complete
May  4 12:03:08.704: INFO: all replica sets need to contain the pod-template-hash label
May  4 12:03:08.704: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 12, 3, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 3, 4, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 12, 3, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 3, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  4 12:03:10.712: INFO: all replica sets need to contain the pod-template-hash label
May  4 12:03:10.712: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 12, 3, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 3, 4, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 12, 3, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 3, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  4 12:03:12.711: INFO: all replica sets need to contain the pod-template-hash label
May  4 12:03:12.711: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 12, 3, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 3, 4, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 12, 3, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 3, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  4 12:03:14.712: INFO: all replica sets need to contain the pod-template-hash label
May  4 12:03:14.712: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 12, 3, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 3, 4, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 12, 3, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 3, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  4 12:03:16.712: INFO: all replica sets need to contain the pod-template-hash label
May  4 12:03:16.712: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 12, 3, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 3, 4, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 12, 3, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 3, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  4 12:03:18.713: INFO: 
May  4 12:03:18.715: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May  4 12:03:18.725: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-5968  5917d7ec-eef9-436b-8fc2-f569cc0c4aff 13133 2 2023-05-04 12:03:04 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-04 12:03:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 12:03:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000a12a98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-04 12:03:04 +0000 UTC,LastTransitionTime:2023-05-04 12:03:04 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-05-04 12:03:18 +0000 UTC,LastTransitionTime:2023-05-04 12:03:04 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May  4 12:03:18.729: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-5968  bd5c36e9-85d0-481f-866b-7e7a8d329473 13123 2 2023-05-04 12:03:06 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 5917d7ec-eef9-436b-8fc2-f569cc0c4aff 0xc000a13457 0xc000a13458}] [] [{kube-controller-manager Update apps/v1 2023-05-04 12:03:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5917d7ec-eef9-436b-8fc2-f569cc0c4aff\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 12:03:18 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000a13508 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May  4 12:03:18.729: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
May  4 12:03:18.729: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-5968  4a152d7b-ebc4-4a93-ab21-ecdcd1bf3efb 13132 2 2023-05-04 12:02:57 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 5917d7ec-eef9-436b-8fc2-f569cc0c4aff 0xc000a13207 0xc000a13208}] [] [{e2e.test Update apps/v1 2023-05-04 12:02:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 12:03:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5917d7ec-eef9-436b-8fc2-f569cc0c4aff\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-04 12:03:18 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc000a132c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  4 12:03:18.729: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-5968  02e6b90e-63f9-4dca-80f6-94fa2e563d96 13078 2 2023-05-04 12:03:04 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 5917d7ec-eef9-436b-8fc2-f569cc0c4aff 0xc000a13337 0xc000a13338}] [] [{kube-controller-manager Update apps/v1 2023-05-04 12:03:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5917d7ec-eef9-436b-8fc2-f569cc0c4aff\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 12:03:06 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000a133e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  4 12:03:18.733: INFO: Pod "test-rollover-deployment-6d45fd857b-6p4fl" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-6p4fl test-rollover-deployment-6d45fd857b- deployment-5968  3e979654-2fb9-4e0a-b504-c4f230cdf699 13099 0 2023-05-04 12:03:06 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:eedef42d99e0d2c6405c0e5aba28cbf9958db77289bc3ba755eb81a40cc389a8 cni.projectcalico.org/podIP:10.20.83.52/32 cni.projectcalico.org/podIPs:10.20.83.52/32] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b bd5c36e9-85d0-481f-866b-7e7a8d329473 0xc000a13aa7 0xc000a13aa8}] [] [{kube-controller-manager Update v1 2023-05-04 12:03:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bd5c36e9-85d0-481f-866b-7e7a8d329473\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-04 12:03:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-04 12:03:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.83.52\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ld9hj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ld9hj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-224.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 12:03:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 12:03:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 12:03:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 12:03:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.224,PodIP:10.20.83.52,StartTime:2023-05-04 12:03:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-04 12:03:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://64066edcbc05014675d698a8e5e8551bd29c76bd264c54e16e26aa804dd1c47f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.83.52,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
May  4 12:03:18.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5968" for this suite. 05/04/23 12:03:18.739
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","completed":74,"skipped":1563,"failed":0}
------------------------------
• [SLOW TEST] [21.185 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:02:57.563
    May  4 12:02:57.563: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename deployment 05/04/23 12:02:57.565
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:02:57.582
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:02:57.59
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    May  4 12:02:57.605: INFO: Pod name rollover-pod: Found 0 pods out of 1
    May  4 12:03:02.609: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 05/04/23 12:03:02.609
    May  4 12:03:02.610: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    May  4 12:03:04.614: INFO: Creating deployment "test-rollover-deployment"
    May  4 12:03:04.626: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    May  4 12:03:06.655: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    May  4 12:03:06.663: INFO: Ensure that both replica sets have 1 created replica
    May  4 12:03:06.670: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    May  4 12:03:06.681: INFO: Updating deployment test-rollover-deployment
    May  4 12:03:06.681: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    May  4 12:03:08.688: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    May  4 12:03:08.695: INFO: Make sure deployment "test-rollover-deployment" is complete
    May  4 12:03:08.704: INFO: all replica sets need to contain the pod-template-hash label
    May  4 12:03:08.704: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 12, 3, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 3, 4, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 12, 3, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 3, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  4 12:03:10.712: INFO: all replica sets need to contain the pod-template-hash label
    May  4 12:03:10.712: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 12, 3, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 3, 4, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 12, 3, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 3, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  4 12:03:12.711: INFO: all replica sets need to contain the pod-template-hash label
    May  4 12:03:12.711: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 12, 3, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 3, 4, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 12, 3, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 3, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  4 12:03:14.712: INFO: all replica sets need to contain the pod-template-hash label
    May  4 12:03:14.712: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 12, 3, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 3, 4, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 12, 3, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 3, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  4 12:03:16.712: INFO: all replica sets need to contain the pod-template-hash label
    May  4 12:03:16.712: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 12, 3, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 3, 4, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 12, 3, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 3, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  4 12:03:18.713: INFO: 
    May  4 12:03:18.715: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May  4 12:03:18.725: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-5968  5917d7ec-eef9-436b-8fc2-f569cc0c4aff 13133 2 2023-05-04 12:03:04 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-04 12:03:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 12:03:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000a12a98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-04 12:03:04 +0000 UTC,LastTransitionTime:2023-05-04 12:03:04 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-05-04 12:03:18 +0000 UTC,LastTransitionTime:2023-05-04 12:03:04 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    May  4 12:03:18.729: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-5968  bd5c36e9-85d0-481f-866b-7e7a8d329473 13123 2 2023-05-04 12:03:06 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 5917d7ec-eef9-436b-8fc2-f569cc0c4aff 0xc000a13457 0xc000a13458}] [] [{kube-controller-manager Update apps/v1 2023-05-04 12:03:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5917d7ec-eef9-436b-8fc2-f569cc0c4aff\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 12:03:18 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000a13508 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    May  4 12:03:18.729: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    May  4 12:03:18.729: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-5968  4a152d7b-ebc4-4a93-ab21-ecdcd1bf3efb 13132 2 2023-05-04 12:02:57 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 5917d7ec-eef9-436b-8fc2-f569cc0c4aff 0xc000a13207 0xc000a13208}] [] [{e2e.test Update apps/v1 2023-05-04 12:02:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 12:03:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5917d7ec-eef9-436b-8fc2-f569cc0c4aff\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-04 12:03:18 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc000a132c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    May  4 12:03:18.729: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-5968  02e6b90e-63f9-4dca-80f6-94fa2e563d96 13078 2 2023-05-04 12:03:04 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 5917d7ec-eef9-436b-8fc2-f569cc0c4aff 0xc000a13337 0xc000a13338}] [] [{kube-controller-manager Update apps/v1 2023-05-04 12:03:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5917d7ec-eef9-436b-8fc2-f569cc0c4aff\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 12:03:06 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000a133e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    May  4 12:03:18.733: INFO: Pod "test-rollover-deployment-6d45fd857b-6p4fl" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-6p4fl test-rollover-deployment-6d45fd857b- deployment-5968  3e979654-2fb9-4e0a-b504-c4f230cdf699 13099 0 2023-05-04 12:03:06 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:eedef42d99e0d2c6405c0e5aba28cbf9958db77289bc3ba755eb81a40cc389a8 cni.projectcalico.org/podIP:10.20.83.52/32 cni.projectcalico.org/podIPs:10.20.83.52/32] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b bd5c36e9-85d0-481f-866b-7e7a8d329473 0xc000a13aa7 0xc000a13aa8}] [] [{kube-controller-manager Update v1 2023-05-04 12:03:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bd5c36e9-85d0-481f-866b-7e7a8d329473\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-04 12:03:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-04 12:03:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.83.52\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ld9hj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ld9hj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-224.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 12:03:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 12:03:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 12:03:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 12:03:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.224,PodIP:10.20.83.52,StartTime:2023-05-04 12:03:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-04 12:03:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://64066edcbc05014675d698a8e5e8551bd29c76bd264c54e16e26aa804dd1c47f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.83.52,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    May  4 12:03:18.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-5968" for this suite. 05/04/23 12:03:18.739
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:03:18.753
May  4 12:03:18.753: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename projected 05/04/23 12:03:18.754
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:03:18.769
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:03:18.772
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
STEP: Creating projection with secret that has name projected-secret-test-a32a8df2-9665-45b0-80e0-609670686fb9 05/04/23 12:03:18.776
STEP: Creating a pod to test consume secrets 05/04/23 12:03:18.782
May  4 12:03:18.795: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4fb02731-36e5-4878-b1b7-0ed3caa71b65" in namespace "projected-3734" to be "Succeeded or Failed"
May  4 12:03:18.808: INFO: Pod "pod-projected-secrets-4fb02731-36e5-4878-b1b7-0ed3caa71b65": Phase="Pending", Reason="", readiness=false. Elapsed: 12.837937ms
May  4 12:03:20.813: INFO: Pod "pod-projected-secrets-4fb02731-36e5-4878-b1b7-0ed3caa71b65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018225606s
May  4 12:03:22.812: INFO: Pod "pod-projected-secrets-4fb02731-36e5-4878-b1b7-0ed3caa71b65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016858045s
STEP: Saw pod success 05/04/23 12:03:22.812
May  4 12:03:22.812: INFO: Pod "pod-projected-secrets-4fb02731-36e5-4878-b1b7-0ed3caa71b65" satisfied condition "Succeeded or Failed"
May  4 12:03:22.816: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod pod-projected-secrets-4fb02731-36e5-4878-b1b7-0ed3caa71b65 container projected-secret-volume-test: <nil>
STEP: delete the pod 05/04/23 12:03:22.832
May  4 12:03:22.847: INFO: Waiting for pod pod-projected-secrets-4fb02731-36e5-4878-b1b7-0ed3caa71b65 to disappear
May  4 12:03:22.855: INFO: Pod pod-projected-secrets-4fb02731-36e5-4878-b1b7-0ed3caa71b65 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
May  4 12:03:22.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3734" for this suite. 05/04/23 12:03:22.865
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","completed":75,"skipped":1616,"failed":0}
------------------------------
• [4.119 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:03:18.753
    May  4 12:03:18.753: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename projected 05/04/23 12:03:18.754
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:03:18.769
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:03:18.772
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:45
    STEP: Creating projection with secret that has name projected-secret-test-a32a8df2-9665-45b0-80e0-609670686fb9 05/04/23 12:03:18.776
    STEP: Creating a pod to test consume secrets 05/04/23 12:03:18.782
    May  4 12:03:18.795: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4fb02731-36e5-4878-b1b7-0ed3caa71b65" in namespace "projected-3734" to be "Succeeded or Failed"
    May  4 12:03:18.808: INFO: Pod "pod-projected-secrets-4fb02731-36e5-4878-b1b7-0ed3caa71b65": Phase="Pending", Reason="", readiness=false. Elapsed: 12.837937ms
    May  4 12:03:20.813: INFO: Pod "pod-projected-secrets-4fb02731-36e5-4878-b1b7-0ed3caa71b65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018225606s
    May  4 12:03:22.812: INFO: Pod "pod-projected-secrets-4fb02731-36e5-4878-b1b7-0ed3caa71b65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016858045s
    STEP: Saw pod success 05/04/23 12:03:22.812
    May  4 12:03:22.812: INFO: Pod "pod-projected-secrets-4fb02731-36e5-4878-b1b7-0ed3caa71b65" satisfied condition "Succeeded or Failed"
    May  4 12:03:22.816: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod pod-projected-secrets-4fb02731-36e5-4878-b1b7-0ed3caa71b65 container projected-secret-volume-test: <nil>
    STEP: delete the pod 05/04/23 12:03:22.832
    May  4 12:03:22.847: INFO: Waiting for pod pod-projected-secrets-4fb02731-36e5-4878-b1b7-0ed3caa71b65 to disappear
    May  4 12:03:22.855: INFO: Pod pod-projected-secrets-4fb02731-36e5-4878-b1b7-0ed3caa71b65 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    May  4 12:03:22.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3734" for this suite. 05/04/23 12:03:22.865
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:03:22.881
May  4 12:03:22.881: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename kubectl 05/04/23 12:03:22.883
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:03:22.902
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:03:22.908
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
STEP: creating a replication controller 05/04/23 12:03:22.912
May  4 12:03:22.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-494 create -f -'
May  4 12:03:24.324: INFO: stderr: ""
May  4 12:03:24.325: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 05/04/23 12:03:24.325
May  4 12:03:24.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-494 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May  4 12:03:24.410: INFO: stderr: ""
May  4 12:03:24.410: INFO: stdout: "update-demo-nautilus-55x5m update-demo-nautilus-6f49z "
May  4 12:03:24.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-494 get pods update-demo-nautilus-55x5m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  4 12:03:24.480: INFO: stderr: ""
May  4 12:03:24.480: INFO: stdout: ""
May  4 12:03:24.480: INFO: update-demo-nautilus-55x5m is created but not running
May  4 12:03:29.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-494 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May  4 12:03:29.567: INFO: stderr: ""
May  4 12:03:29.567: INFO: stdout: "update-demo-nautilus-55x5m update-demo-nautilus-6f49z "
May  4 12:03:29.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-494 get pods update-demo-nautilus-55x5m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  4 12:03:29.661: INFO: stderr: ""
May  4 12:03:29.662: INFO: stdout: "true"
May  4 12:03:29.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-494 get pods update-demo-nautilus-55x5m -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May  4 12:03:29.743: INFO: stderr: ""
May  4 12:03:29.743: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
May  4 12:03:29.743: INFO: validating pod update-demo-nautilus-55x5m
May  4 12:03:29.750: INFO: got data: {
  "image": "nautilus.jpg"
}

May  4 12:03:29.750: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  4 12:03:29.750: INFO: update-demo-nautilus-55x5m is verified up and running
May  4 12:03:29.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-494 get pods update-demo-nautilus-6f49z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  4 12:03:29.857: INFO: stderr: ""
May  4 12:03:29.857: INFO: stdout: "true"
May  4 12:03:29.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-494 get pods update-demo-nautilus-6f49z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May  4 12:03:29.954: INFO: stderr: ""
May  4 12:03:29.954: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
May  4 12:03:29.954: INFO: validating pod update-demo-nautilus-6f49z
May  4 12:03:29.959: INFO: got data: {
  "image": "nautilus.jpg"
}

May  4 12:03:29.959: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  4 12:03:29.959: INFO: update-demo-nautilus-6f49z is verified up and running
STEP: using delete to clean up resources 05/04/23 12:03:29.959
May  4 12:03:29.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-494 delete --grace-period=0 --force -f -'
May  4 12:03:30.083: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  4 12:03:30.083: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May  4 12:03:30.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-494 get rc,svc -l name=update-demo --no-headers'
May  4 12:03:30.192: INFO: stderr: "No resources found in kubectl-494 namespace.\n"
May  4 12:03:30.192: INFO: stdout: ""
May  4 12:03:30.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-494 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May  4 12:03:30.278: INFO: stderr: ""
May  4 12:03:30.278: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  4 12:03:30.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-494" for this suite. 05/04/23 12:03:30.286
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","completed":76,"skipped":1682,"failed":0}
------------------------------
• [SLOW TEST] [7.412 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:337

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:03:22.881
    May  4 12:03:22.881: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename kubectl 05/04/23 12:03:22.883
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:03:22.902
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:03:22.908
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:337
    STEP: creating a replication controller 05/04/23 12:03:22.912
    May  4 12:03:22.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-494 create -f -'
    May  4 12:03:24.324: INFO: stderr: ""
    May  4 12:03:24.325: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 05/04/23 12:03:24.325
    May  4 12:03:24.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-494 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May  4 12:03:24.410: INFO: stderr: ""
    May  4 12:03:24.410: INFO: stdout: "update-demo-nautilus-55x5m update-demo-nautilus-6f49z "
    May  4 12:03:24.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-494 get pods update-demo-nautilus-55x5m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  4 12:03:24.480: INFO: stderr: ""
    May  4 12:03:24.480: INFO: stdout: ""
    May  4 12:03:24.480: INFO: update-demo-nautilus-55x5m is created but not running
    May  4 12:03:29.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-494 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May  4 12:03:29.567: INFO: stderr: ""
    May  4 12:03:29.567: INFO: stdout: "update-demo-nautilus-55x5m update-demo-nautilus-6f49z "
    May  4 12:03:29.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-494 get pods update-demo-nautilus-55x5m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  4 12:03:29.661: INFO: stderr: ""
    May  4 12:03:29.662: INFO: stdout: "true"
    May  4 12:03:29.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-494 get pods update-demo-nautilus-55x5m -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May  4 12:03:29.743: INFO: stderr: ""
    May  4 12:03:29.743: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    May  4 12:03:29.743: INFO: validating pod update-demo-nautilus-55x5m
    May  4 12:03:29.750: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May  4 12:03:29.750: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May  4 12:03:29.750: INFO: update-demo-nautilus-55x5m is verified up and running
    May  4 12:03:29.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-494 get pods update-demo-nautilus-6f49z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  4 12:03:29.857: INFO: stderr: ""
    May  4 12:03:29.857: INFO: stdout: "true"
    May  4 12:03:29.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-494 get pods update-demo-nautilus-6f49z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May  4 12:03:29.954: INFO: stderr: ""
    May  4 12:03:29.954: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    May  4 12:03:29.954: INFO: validating pod update-demo-nautilus-6f49z
    May  4 12:03:29.959: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May  4 12:03:29.959: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May  4 12:03:29.959: INFO: update-demo-nautilus-6f49z is verified up and running
    STEP: using delete to clean up resources 05/04/23 12:03:29.959
    May  4 12:03:29.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-494 delete --grace-period=0 --force -f -'
    May  4 12:03:30.083: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May  4 12:03:30.083: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    May  4 12:03:30.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-494 get rc,svc -l name=update-demo --no-headers'
    May  4 12:03:30.192: INFO: stderr: "No resources found in kubectl-494 namespace.\n"
    May  4 12:03:30.192: INFO: stdout: ""
    May  4 12:03:30.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-494 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    May  4 12:03:30.278: INFO: stderr: ""
    May  4 12:03:30.278: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  4 12:03:30.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-494" for this suite. 05/04/23 12:03:30.286
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:03:30.294
May  4 12:03:30.294: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename var-expansion 05/04/23 12:03:30.295
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:03:30.312
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:03:30.315
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
May  4 12:03:30.330: INFO: Waiting up to 2m0s for pod "var-expansion-91457bf2-bce1-4011-bea6-4cdee934f930" in namespace "var-expansion-4582" to be "container 0 failed with reason CreateContainerConfigError"
May  4 12:03:30.333: INFO: Pod "var-expansion-91457bf2-bce1-4011-bea6-4cdee934f930": Phase="Pending", Reason="", readiness=false. Elapsed: 3.506793ms
May  4 12:03:32.340: INFO: Pod "var-expansion-91457bf2-bce1-4011-bea6-4cdee934f930": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010803112s
May  4 12:03:32.340: INFO: Pod "var-expansion-91457bf2-bce1-4011-bea6-4cdee934f930" satisfied condition "container 0 failed with reason CreateContainerConfigError"
May  4 12:03:32.340: INFO: Deleting pod "var-expansion-91457bf2-bce1-4011-bea6-4cdee934f930" in namespace "var-expansion-4582"
May  4 12:03:32.351: INFO: Wait up to 5m0s for pod "var-expansion-91457bf2-bce1-4011-bea6-4cdee934f930" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
May  4 12:03:34.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4582" for this suite. 05/04/23 12:03:34.366
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","completed":77,"skipped":1683,"failed":0}
------------------------------
• [4.078 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:03:30.294
    May  4 12:03:30.294: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename var-expansion 05/04/23 12:03:30.295
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:03:30.312
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:03:30.315
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:185
    May  4 12:03:30.330: INFO: Waiting up to 2m0s for pod "var-expansion-91457bf2-bce1-4011-bea6-4cdee934f930" in namespace "var-expansion-4582" to be "container 0 failed with reason CreateContainerConfigError"
    May  4 12:03:30.333: INFO: Pod "var-expansion-91457bf2-bce1-4011-bea6-4cdee934f930": Phase="Pending", Reason="", readiness=false. Elapsed: 3.506793ms
    May  4 12:03:32.340: INFO: Pod "var-expansion-91457bf2-bce1-4011-bea6-4cdee934f930": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010803112s
    May  4 12:03:32.340: INFO: Pod "var-expansion-91457bf2-bce1-4011-bea6-4cdee934f930" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    May  4 12:03:32.340: INFO: Deleting pod "var-expansion-91457bf2-bce1-4011-bea6-4cdee934f930" in namespace "var-expansion-4582"
    May  4 12:03:32.351: INFO: Wait up to 5m0s for pod "var-expansion-91457bf2-bce1-4011-bea6-4cdee934f930" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    May  4 12:03:34.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-4582" for this suite. 05/04/23 12:03:34.366
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:03:34.373
May  4 12:03:34.374: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename conformance-tests 05/04/23 12:03:34.375
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:03:34.391
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:03:34.395
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 05/04/23 12:03:34.398
May  4 12:03:34.398: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
May  4 12:03:34.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-4248" for this suite. 05/04/23 12:03:34.415
{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","completed":78,"skipped":1685,"failed":0}
------------------------------
• [0.049 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:03:34.373
    May  4 12:03:34.374: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename conformance-tests 05/04/23 12:03:34.375
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:03:34.391
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:03:34.395
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 05/04/23 12:03:34.398
    May  4 12:03:34.398: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:187
    May  4 12:03:34.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "conformance-tests-4248" for this suite. 05/04/23 12:03:34.415
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:03:34.427
May  4 12:03:34.427: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename projected 05/04/23 12:03:34.428
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:03:34.441
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:03:34.444
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
STEP: Creating a pod to test downward API volume plugin 05/04/23 12:03:34.447
May  4 12:03:34.456: INFO: Waiting up to 5m0s for pod "downwardapi-volume-600ce5a9-84b2-4e12-9750-3a07c9745058" in namespace "projected-9097" to be "Succeeded or Failed"
May  4 12:03:34.460: INFO: Pod "downwardapi-volume-600ce5a9-84b2-4e12-9750-3a07c9745058": Phase="Pending", Reason="", readiness=false. Elapsed: 3.163946ms
May  4 12:03:36.464: INFO: Pod "downwardapi-volume-600ce5a9-84b2-4e12-9750-3a07c9745058": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00757557s
May  4 12:03:38.465: INFO: Pod "downwardapi-volume-600ce5a9-84b2-4e12-9750-3a07c9745058": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007902005s
STEP: Saw pod success 05/04/23 12:03:38.465
May  4 12:03:38.465: INFO: Pod "downwardapi-volume-600ce5a9-84b2-4e12-9750-3a07c9745058" satisfied condition "Succeeded or Failed"
May  4 12:03:38.469: INFO: Trying to get logs from node ip-10-0-1-232.us-west-2.compute.internal pod downwardapi-volume-600ce5a9-84b2-4e12-9750-3a07c9745058 container client-container: <nil>
STEP: delete the pod 05/04/23 12:03:38.475
May  4 12:03:38.497: INFO: Waiting for pod downwardapi-volume-600ce5a9-84b2-4e12-9750-3a07c9745058 to disappear
May  4 12:03:38.507: INFO: Pod downwardapi-volume-600ce5a9-84b2-4e12-9750-3a07c9745058 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
May  4 12:03:38.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9097" for this suite. 05/04/23 12:03:38.52
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","completed":79,"skipped":1691,"failed":0}
------------------------------
• [4.110 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:03:34.427
    May  4 12:03:34.427: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename projected 05/04/23 12:03:34.428
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:03:34.441
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:03:34.444
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:192
    STEP: Creating a pod to test downward API volume plugin 05/04/23 12:03:34.447
    May  4 12:03:34.456: INFO: Waiting up to 5m0s for pod "downwardapi-volume-600ce5a9-84b2-4e12-9750-3a07c9745058" in namespace "projected-9097" to be "Succeeded or Failed"
    May  4 12:03:34.460: INFO: Pod "downwardapi-volume-600ce5a9-84b2-4e12-9750-3a07c9745058": Phase="Pending", Reason="", readiness=false. Elapsed: 3.163946ms
    May  4 12:03:36.464: INFO: Pod "downwardapi-volume-600ce5a9-84b2-4e12-9750-3a07c9745058": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00757557s
    May  4 12:03:38.465: INFO: Pod "downwardapi-volume-600ce5a9-84b2-4e12-9750-3a07c9745058": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007902005s
    STEP: Saw pod success 05/04/23 12:03:38.465
    May  4 12:03:38.465: INFO: Pod "downwardapi-volume-600ce5a9-84b2-4e12-9750-3a07c9745058" satisfied condition "Succeeded or Failed"
    May  4 12:03:38.469: INFO: Trying to get logs from node ip-10-0-1-232.us-west-2.compute.internal pod downwardapi-volume-600ce5a9-84b2-4e12-9750-3a07c9745058 container client-container: <nil>
    STEP: delete the pod 05/04/23 12:03:38.475
    May  4 12:03:38.497: INFO: Waiting for pod downwardapi-volume-600ce5a9-84b2-4e12-9750-3a07c9745058 to disappear
    May  4 12:03:38.507: INFO: Pod downwardapi-volume-600ce5a9-84b2-4e12-9750-3a07c9745058 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    May  4 12:03:38.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9097" for this suite. 05/04/23 12:03:38.52
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:03:38.537
May  4 12:03:38.537: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename init-container 05/04/23 12:03:38.538
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:03:38.56
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:03:38.566
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
STEP: creating the pod 05/04/23 12:03:38.572
May  4 12:03:38.572: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
May  4 12:03:41.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2358" for this suite. 05/04/23 12:03:41.702
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","completed":80,"skipped":1692,"failed":0}
------------------------------
• [3.172 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:03:38.537
    May  4 12:03:38.537: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename init-container 05/04/23 12:03:38.538
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:03:38.56
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:03:38.566
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:254
    STEP: creating the pod 05/04/23 12:03:38.572
    May  4 12:03:38.572: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    May  4 12:03:41.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-2358" for this suite. 05/04/23 12:03:41.702
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:03:41.71
May  4 12:03:41.710: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename emptydir 05/04/23 12:03:41.711
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:03:41.722
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:03:41.725
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
STEP: Creating a pod to test emptydir 0666 on tmpfs 05/04/23 12:03:41.728
May  4 12:03:41.738: INFO: Waiting up to 5m0s for pod "pod-96ec25d3-88d6-409a-bf82-28c44eae1acd" in namespace "emptydir-6458" to be "Succeeded or Failed"
May  4 12:03:41.741: INFO: Pod "pod-96ec25d3-88d6-409a-bf82-28c44eae1acd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.031324ms
May  4 12:03:43.747: INFO: Pod "pod-96ec25d3-88d6-409a-bf82-28c44eae1acd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008792493s
May  4 12:03:45.746: INFO: Pod "pod-96ec25d3-88d6-409a-bf82-28c44eae1acd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008032487s
STEP: Saw pod success 05/04/23 12:03:45.746
May  4 12:03:45.746: INFO: Pod "pod-96ec25d3-88d6-409a-bf82-28c44eae1acd" satisfied condition "Succeeded or Failed"
May  4 12:03:45.749: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod pod-96ec25d3-88d6-409a-bf82-28c44eae1acd container test-container: <nil>
STEP: delete the pod 05/04/23 12:03:45.755
May  4 12:03:45.768: INFO: Waiting for pod pod-96ec25d3-88d6-409a-bf82-28c44eae1acd to disappear
May  4 12:03:45.775: INFO: Pod pod-96ec25d3-88d6-409a-bf82-28c44eae1acd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May  4 12:03:45.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6458" for this suite. 05/04/23 12:03:45.787
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":81,"skipped":1706,"failed":0}
------------------------------
• [4.085 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:03:41.71
    May  4 12:03:41.710: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename emptydir 05/04/23 12:03:41.711
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:03:41.722
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:03:41.725
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:106
    STEP: Creating a pod to test emptydir 0666 on tmpfs 05/04/23 12:03:41.728
    May  4 12:03:41.738: INFO: Waiting up to 5m0s for pod "pod-96ec25d3-88d6-409a-bf82-28c44eae1acd" in namespace "emptydir-6458" to be "Succeeded or Failed"
    May  4 12:03:41.741: INFO: Pod "pod-96ec25d3-88d6-409a-bf82-28c44eae1acd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.031324ms
    May  4 12:03:43.747: INFO: Pod "pod-96ec25d3-88d6-409a-bf82-28c44eae1acd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008792493s
    May  4 12:03:45.746: INFO: Pod "pod-96ec25d3-88d6-409a-bf82-28c44eae1acd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008032487s
    STEP: Saw pod success 05/04/23 12:03:45.746
    May  4 12:03:45.746: INFO: Pod "pod-96ec25d3-88d6-409a-bf82-28c44eae1acd" satisfied condition "Succeeded or Failed"
    May  4 12:03:45.749: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod pod-96ec25d3-88d6-409a-bf82-28c44eae1acd container test-container: <nil>
    STEP: delete the pod 05/04/23 12:03:45.755
    May  4 12:03:45.768: INFO: Waiting for pod pod-96ec25d3-88d6-409a-bf82-28c44eae1acd to disappear
    May  4 12:03:45.775: INFO: Pod pod-96ec25d3-88d6-409a-bf82-28c44eae1acd no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May  4 12:03:45.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6458" for this suite. 05/04/23 12:03:45.787
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:03:45.803
May  4 12:03:45.803: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename daemonsets 05/04/23 12:03:45.804
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:03:45.819
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:03:45.821
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
May  4 12:03:45.877: INFO: Create a RollingUpdate DaemonSet
May  4 12:03:45.884: INFO: Check that daemon pods launch on every node of the cluster
May  4 12:03:45.891: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:03:45.891: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:03:45.891: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:03:45.894: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  4 12:03:45.894: INFO: Node ip-10-0-1-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
May  4 12:03:46.902: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:03:46.902: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:03:46.902: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:03:46.907: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  4 12:03:46.907: INFO: Node ip-10-0-1-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
May  4 12:03:47.904: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:03:47.904: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:03:47.904: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:03:47.909: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
May  4 12:03:47.909: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
May  4 12:03:47.909: INFO: Update the DaemonSet to trigger a rollout
May  4 12:03:47.918: INFO: Updating DaemonSet daemon-set
May  4 12:03:51.950: INFO: Roll back the DaemonSet before rollout is complete
May  4 12:03:51.960: INFO: Updating DaemonSet daemon-set
May  4 12:03:51.960: INFO: Make sure DaemonSet rollback is complete
May  4 12:03:51.965: INFO: Wrong image for pod: daemon-set-tlz79. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
May  4 12:03:51.965: INFO: Pod daemon-set-tlz79 is not available
May  4 12:03:51.971: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:03:51.971: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:03:51.971: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:03:52.986: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:03:52.987: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:03:52.987: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:03:53.985: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:03:53.985: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:03:53.985: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:03:54.989: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:03:54.989: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:03:54.989: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:03:55.979: INFO: Pod daemon-set-4sqb2 is not available
May  4 12:03:55.986: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:03:55.986: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:03:55.986: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 05/04/23 12:03:55.996
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2917, will wait for the garbage collector to delete the pods 05/04/23 12:03:55.996
May  4 12:03:56.059: INFO: Deleting DaemonSet.extensions daemon-set took: 6.847887ms
May  4 12:03:56.159: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.164919ms
May  4 12:03:58.363: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  4 12:03:58.363: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May  4 12:03:58.366: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"13684"},"items":null}

May  4 12:03:58.368: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"13684"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
May  4 12:03:58.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2917" for this suite. 05/04/23 12:03:58.4
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","completed":82,"skipped":1774,"failed":0}
------------------------------
• [SLOW TEST] [12.603 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:03:45.803
    May  4 12:03:45.803: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename daemonsets 05/04/23 12:03:45.804
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:03:45.819
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:03:45.821
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:431
    May  4 12:03:45.877: INFO: Create a RollingUpdate DaemonSet
    May  4 12:03:45.884: INFO: Check that daemon pods launch on every node of the cluster
    May  4 12:03:45.891: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:03:45.891: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:03:45.891: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:03:45.894: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  4 12:03:45.894: INFO: Node ip-10-0-1-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
    May  4 12:03:46.902: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:03:46.902: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:03:46.902: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:03:46.907: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  4 12:03:46.907: INFO: Node ip-10-0-1-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
    May  4 12:03:47.904: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:03:47.904: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:03:47.904: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:03:47.909: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    May  4 12:03:47.909: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    May  4 12:03:47.909: INFO: Update the DaemonSet to trigger a rollout
    May  4 12:03:47.918: INFO: Updating DaemonSet daemon-set
    May  4 12:03:51.950: INFO: Roll back the DaemonSet before rollout is complete
    May  4 12:03:51.960: INFO: Updating DaemonSet daemon-set
    May  4 12:03:51.960: INFO: Make sure DaemonSet rollback is complete
    May  4 12:03:51.965: INFO: Wrong image for pod: daemon-set-tlz79. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
    May  4 12:03:51.965: INFO: Pod daemon-set-tlz79 is not available
    May  4 12:03:51.971: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:03:51.971: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:03:51.971: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:03:52.986: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:03:52.987: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:03:52.987: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:03:53.985: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:03:53.985: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:03:53.985: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:03:54.989: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:03:54.989: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:03:54.989: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:03:55.979: INFO: Pod daemon-set-4sqb2 is not available
    May  4 12:03:55.986: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:03:55.986: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:03:55.986: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 05/04/23 12:03:55.996
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2917, will wait for the garbage collector to delete the pods 05/04/23 12:03:55.996
    May  4 12:03:56.059: INFO: Deleting DaemonSet.extensions daemon-set took: 6.847887ms
    May  4 12:03:56.159: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.164919ms
    May  4 12:03:58.363: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  4 12:03:58.363: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    May  4 12:03:58.366: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"13684"},"items":null}

    May  4 12:03:58.368: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"13684"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    May  4 12:03:58.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-2917" for this suite. 05/04/23 12:03:58.4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:03:58.407
May  4 12:03:58.408: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename replication-controller 05/04/23 12:03:58.408
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:03:58.421
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:03:58.423
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
STEP: Given a Pod with a 'name' label pod-adoption is created 05/04/23 12:03:58.427
May  4 12:03:58.437: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-7152" to be "running and ready"
May  4 12:03:58.440: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 3.192807ms
May  4 12:03:58.440: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
May  4 12:04:00.444: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.007430962s
May  4 12:04:00.444: INFO: The phase of Pod pod-adoption is Running (Ready = true)
May  4 12:04:00.444: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 05/04/23 12:04:00.448
STEP: Then the orphan pod is adopted 05/04/23 12:04:00.454
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
May  4 12:04:01.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7152" for this suite. 05/04/23 12:04:01.468
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","completed":83,"skipped":1796,"failed":0}
------------------------------
• [3.067 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:03:58.407
    May  4 12:03:58.408: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename replication-controller 05/04/23 12:03:58.408
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:03:58.421
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:03:58.423
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:91
    STEP: Given a Pod with a 'name' label pod-adoption is created 05/04/23 12:03:58.427
    May  4 12:03:58.437: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-7152" to be "running and ready"
    May  4 12:03:58.440: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 3.192807ms
    May  4 12:03:58.440: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    May  4 12:04:00.444: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.007430962s
    May  4 12:04:00.444: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    May  4 12:04:00.444: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 05/04/23 12:04:00.448
    STEP: Then the orphan pod is adopted 05/04/23 12:04:00.454
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    May  4 12:04:01.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-7152" for this suite. 05/04/23 12:04:01.468
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:04:01.476
May  4 12:04:01.476: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename kubectl 05/04/23 12:04:01.477
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:04:01.493
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:04:01.497
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1570
STEP: creating an pod 05/04/23 12:04:01.499
May  4 12:04:01.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-4138 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
May  4 12:04:01.650: INFO: stderr: ""
May  4 12:04:01.651: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
STEP: Waiting for log generator to start. 05/04/23 12:04:01.651
May  4 12:04:01.651: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
May  4 12:04:01.651: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-4138" to be "running and ready, or succeeded"
May  4 12:04:01.657: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.203659ms
May  4 12:04:01.657: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'ip-10-0-1-224.us-west-2.compute.internal' to be 'Running' but was 'Pending'
May  4 12:04:03.664: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.012699816s
May  4 12:04:03.664: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
May  4 12:04:03.664: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 05/04/23 12:04:03.664
May  4 12:04:03.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-4138 logs logs-generator logs-generator'
May  4 12:04:03.800: INFO: stderr: ""
May  4 12:04:03.800: INFO: stdout: "I0504 12:04:02.553583       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/wh52 345\nI0504 12:04:02.753706       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/xft 235\nI0504 12:04:02.954363       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/jwl6 441\nI0504 12:04:03.153631       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/6tjx 554\nI0504 12:04:03.354072       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/87n2 583\nI0504 12:04:03.554461       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/v88c 445\nI0504 12:04:03.753725       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/6qf 543\n"
STEP: limiting log lines 05/04/23 12:04:03.8
May  4 12:04:03.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-4138 logs logs-generator logs-generator --tail=1'
May  4 12:04:03.966: INFO: stderr: ""
May  4 12:04:03.966: INFO: stdout: "I0504 12:04:03.954046       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/7lr 493\n"
May  4 12:04:03.966: INFO: got output "I0504 12:04:03.954046       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/7lr 493\n"
STEP: limiting log bytes 05/04/23 12:04:03.966
May  4 12:04:03.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-4138 logs logs-generator logs-generator --limit-bytes=1'
May  4 12:04:04.078: INFO: stderr: ""
May  4 12:04:04.078: INFO: stdout: "I"
May  4 12:04:04.078: INFO: got output "I"
STEP: exposing timestamps 05/04/23 12:04:04.078
May  4 12:04:04.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-4138 logs logs-generator logs-generator --tail=1 --timestamps'
May  4 12:04:04.202: INFO: stderr: ""
May  4 12:04:04.203: INFO: stdout: "2023-05-04T12:04:04.154490626Z I0504 12:04:04.154375       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/m4c 529\n"
May  4 12:04:04.203: INFO: got output "2023-05-04T12:04:04.154490626Z I0504 12:04:04.154375       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/m4c 529\n"
STEP: restricting to a time range 05/04/23 12:04:04.203
May  4 12:04:06.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-4138 logs logs-generator logs-generator --since=1s'
May  4 12:04:06.796: INFO: stderr: ""
May  4 12:04:06.797: INFO: stdout: "I0504 12:04:05.953677       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/qnw 312\nI0504 12:04:06.154012       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/mlwq 336\nI0504 12:04:06.354596       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/l8gh 433\nI0504 12:04:06.553932       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/rbm 475\nI0504 12:04:06.754589       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/skp 473\n"
May  4 12:04:06.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-4138 logs logs-generator logs-generator --since=24h'
May  4 12:04:06.892: INFO: stderr: ""
May  4 12:04:06.892: INFO: stdout: "I0504 12:04:02.553583       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/wh52 345\nI0504 12:04:02.753706       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/xft 235\nI0504 12:04:02.954363       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/jwl6 441\nI0504 12:04:03.153631       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/6tjx 554\nI0504 12:04:03.354072       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/87n2 583\nI0504 12:04:03.554461       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/v88c 445\nI0504 12:04:03.753725       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/6qf 543\nI0504 12:04:03.954046       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/7lr 493\nI0504 12:04:04.154375       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/m4c 529\nI0504 12:04:04.353699       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/6j4 477\nI0504 12:04:04.554042       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/jflc 577\nI0504 12:04:04.754412       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/plt8 349\nI0504 12:04:04.953693       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/8264 509\nI0504 12:04:05.154020       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/vjt 493\nI0504 12:04:05.353794       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/ml69 361\nI0504 12:04:05.554063       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/5pl 361\nI0504 12:04:05.754402       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/nq29 423\nI0504 12:04:05.953677       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/qnw 312\nI0504 12:04:06.154012       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/mlwq 336\nI0504 12:04:06.354596       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/l8gh 433\nI0504 12:04:06.553932       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/rbm 475\nI0504 12:04:06.754589       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/skp 473\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1575
May  4 12:04:06.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-4138 delete pod logs-generator'
May  4 12:04:07.785: INFO: stderr: ""
May  4 12:04:07.785: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  4 12:04:07.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4138" for this suite. 05/04/23 12:04:07.793
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","completed":84,"skipped":1802,"failed":0}
------------------------------
• [SLOW TEST] [6.325 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1567
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1590

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:04:01.476
    May  4 12:04:01.476: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename kubectl 05/04/23 12:04:01.477
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:04:01.493
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:04:01.497
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1570
    STEP: creating an pod 05/04/23 12:04:01.499
    May  4 12:04:01.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-4138 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    May  4 12:04:01.650: INFO: stderr: ""
    May  4 12:04:01.651: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1590
    STEP: Waiting for log generator to start. 05/04/23 12:04:01.651
    May  4 12:04:01.651: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    May  4 12:04:01.651: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-4138" to be "running and ready, or succeeded"
    May  4 12:04:01.657: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.203659ms
    May  4 12:04:01.657: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'ip-10-0-1-224.us-west-2.compute.internal' to be 'Running' but was 'Pending'
    May  4 12:04:03.664: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.012699816s
    May  4 12:04:03.664: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    May  4 12:04:03.664: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 05/04/23 12:04:03.664
    May  4 12:04:03.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-4138 logs logs-generator logs-generator'
    May  4 12:04:03.800: INFO: stderr: ""
    May  4 12:04:03.800: INFO: stdout: "I0504 12:04:02.553583       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/wh52 345\nI0504 12:04:02.753706       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/xft 235\nI0504 12:04:02.954363       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/jwl6 441\nI0504 12:04:03.153631       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/6tjx 554\nI0504 12:04:03.354072       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/87n2 583\nI0504 12:04:03.554461       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/v88c 445\nI0504 12:04:03.753725       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/6qf 543\n"
    STEP: limiting log lines 05/04/23 12:04:03.8
    May  4 12:04:03.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-4138 logs logs-generator logs-generator --tail=1'
    May  4 12:04:03.966: INFO: stderr: ""
    May  4 12:04:03.966: INFO: stdout: "I0504 12:04:03.954046       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/7lr 493\n"
    May  4 12:04:03.966: INFO: got output "I0504 12:04:03.954046       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/7lr 493\n"
    STEP: limiting log bytes 05/04/23 12:04:03.966
    May  4 12:04:03.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-4138 logs logs-generator logs-generator --limit-bytes=1'
    May  4 12:04:04.078: INFO: stderr: ""
    May  4 12:04:04.078: INFO: stdout: "I"
    May  4 12:04:04.078: INFO: got output "I"
    STEP: exposing timestamps 05/04/23 12:04:04.078
    May  4 12:04:04.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-4138 logs logs-generator logs-generator --tail=1 --timestamps'
    May  4 12:04:04.202: INFO: stderr: ""
    May  4 12:04:04.203: INFO: stdout: "2023-05-04T12:04:04.154490626Z I0504 12:04:04.154375       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/m4c 529\n"
    May  4 12:04:04.203: INFO: got output "2023-05-04T12:04:04.154490626Z I0504 12:04:04.154375       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/m4c 529\n"
    STEP: restricting to a time range 05/04/23 12:04:04.203
    May  4 12:04:06.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-4138 logs logs-generator logs-generator --since=1s'
    May  4 12:04:06.796: INFO: stderr: ""
    May  4 12:04:06.797: INFO: stdout: "I0504 12:04:05.953677       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/qnw 312\nI0504 12:04:06.154012       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/mlwq 336\nI0504 12:04:06.354596       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/l8gh 433\nI0504 12:04:06.553932       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/rbm 475\nI0504 12:04:06.754589       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/skp 473\n"
    May  4 12:04:06.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-4138 logs logs-generator logs-generator --since=24h'
    May  4 12:04:06.892: INFO: stderr: ""
    May  4 12:04:06.892: INFO: stdout: "I0504 12:04:02.553583       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/wh52 345\nI0504 12:04:02.753706       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/xft 235\nI0504 12:04:02.954363       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/jwl6 441\nI0504 12:04:03.153631       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/6tjx 554\nI0504 12:04:03.354072       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/87n2 583\nI0504 12:04:03.554461       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/v88c 445\nI0504 12:04:03.753725       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/6qf 543\nI0504 12:04:03.954046       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/7lr 493\nI0504 12:04:04.154375       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/m4c 529\nI0504 12:04:04.353699       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/6j4 477\nI0504 12:04:04.554042       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/jflc 577\nI0504 12:04:04.754412       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/plt8 349\nI0504 12:04:04.953693       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/8264 509\nI0504 12:04:05.154020       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/vjt 493\nI0504 12:04:05.353794       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/ml69 361\nI0504 12:04:05.554063       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/5pl 361\nI0504 12:04:05.754402       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/nq29 423\nI0504 12:04:05.953677       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/qnw 312\nI0504 12:04:06.154012       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/mlwq 336\nI0504 12:04:06.354596       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/l8gh 433\nI0504 12:04:06.553932       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/rbm 475\nI0504 12:04:06.754589       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/skp 473\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1575
    May  4 12:04:06.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-4138 delete pod logs-generator'
    May  4 12:04:07.785: INFO: stderr: ""
    May  4 12:04:07.785: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  4 12:04:07.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4138" for this suite. 05/04/23 12:04:07.793
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:04:07.802
May  4 12:04:07.802: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename deployment 05/04/23 12:04:07.803
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:04:07.818
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:04:07.822
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 05/04/23 12:04:07.841
May  4 12:04:07.841: INFO: Creating simple deployment test-deployment-s8w9j
May  4 12:04:07.856: INFO: deployment "test-deployment-s8w9j" doesn't have the required revision set
STEP: Getting /status 05/04/23 12:04:09.882
May  4 12:04:09.890: INFO: Deployment test-deployment-s8w9j has Conditions: [{Available True 2023-05-04 12:04:08 +0000 UTC 2023-05-04 12:04:08 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-05-04 12:04:08 +0000 UTC 2023-05-04 12:04:07 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-s8w9j-777898ffcc" has successfully progressed.}]
STEP: updating Deployment Status 05/04/23 12:04:09.89
May  4 12:04:09.901: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 12, 4, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 4, 8, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 12, 4, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 4, 7, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-s8w9j-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 05/04/23 12:04:09.901
May  4 12:04:09.903: INFO: Observed &Deployment event: ADDED
May  4 12:04:09.903: INFO: Observed Deployment test-deployment-s8w9j in namespace deployment-4647 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-04 12:04:07 +0000 UTC 2023-05-04 12:04:07 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-s8w9j-777898ffcc"}
May  4 12:04:09.903: INFO: Observed &Deployment event: MODIFIED
May  4 12:04:09.903: INFO: Observed Deployment test-deployment-s8w9j in namespace deployment-4647 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-04 12:04:07 +0000 UTC 2023-05-04 12:04:07 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-s8w9j-777898ffcc"}
May  4 12:04:09.903: INFO: Observed Deployment test-deployment-s8w9j in namespace deployment-4647 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-04 12:04:07 +0000 UTC 2023-05-04 12:04:07 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
May  4 12:04:09.903: INFO: Observed &Deployment event: MODIFIED
May  4 12:04:09.903: INFO: Observed Deployment test-deployment-s8w9j in namespace deployment-4647 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-04 12:04:07 +0000 UTC 2023-05-04 12:04:07 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
May  4 12:04:09.903: INFO: Observed Deployment test-deployment-s8w9j in namespace deployment-4647 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-04 12:04:07 +0000 UTC 2023-05-04 12:04:07 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-s8w9j-777898ffcc" is progressing.}
May  4 12:04:09.903: INFO: Observed &Deployment event: MODIFIED
May  4 12:04:09.903: INFO: Observed Deployment test-deployment-s8w9j in namespace deployment-4647 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-04 12:04:08 +0000 UTC 2023-05-04 12:04:08 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
May  4 12:04:09.903: INFO: Observed Deployment test-deployment-s8w9j in namespace deployment-4647 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-04 12:04:08 +0000 UTC 2023-05-04 12:04:07 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-s8w9j-777898ffcc" has successfully progressed.}
May  4 12:04:09.904: INFO: Observed &Deployment event: MODIFIED
May  4 12:04:09.904: INFO: Observed Deployment test-deployment-s8w9j in namespace deployment-4647 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-04 12:04:08 +0000 UTC 2023-05-04 12:04:08 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
May  4 12:04:09.904: INFO: Observed Deployment test-deployment-s8w9j in namespace deployment-4647 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-04 12:04:08 +0000 UTC 2023-05-04 12:04:07 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-s8w9j-777898ffcc" has successfully progressed.}
May  4 12:04:09.904: INFO: Found Deployment test-deployment-s8w9j in namespace deployment-4647 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May  4 12:04:09.904: INFO: Deployment test-deployment-s8w9j has an updated status
STEP: patching the Statefulset Status 05/04/23 12:04:09.904
May  4 12:04:09.904: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
May  4 12:04:09.917: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 05/04/23 12:04:09.917
May  4 12:04:09.919: INFO: Observed &Deployment event: ADDED
May  4 12:04:09.919: INFO: Observed deployment test-deployment-s8w9j in namespace deployment-4647 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-04 12:04:07 +0000 UTC 2023-05-04 12:04:07 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-s8w9j-777898ffcc"}
May  4 12:04:09.919: INFO: Observed &Deployment event: MODIFIED
May  4 12:04:09.919: INFO: Observed deployment test-deployment-s8w9j in namespace deployment-4647 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-04 12:04:07 +0000 UTC 2023-05-04 12:04:07 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-s8w9j-777898ffcc"}
May  4 12:04:09.919: INFO: Observed deployment test-deployment-s8w9j in namespace deployment-4647 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-04 12:04:07 +0000 UTC 2023-05-04 12:04:07 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
May  4 12:04:09.920: INFO: Observed &Deployment event: MODIFIED
May  4 12:04:09.920: INFO: Observed deployment test-deployment-s8w9j in namespace deployment-4647 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-04 12:04:07 +0000 UTC 2023-05-04 12:04:07 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
May  4 12:04:09.920: INFO: Observed deployment test-deployment-s8w9j in namespace deployment-4647 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-04 12:04:07 +0000 UTC 2023-05-04 12:04:07 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-s8w9j-777898ffcc" is progressing.}
May  4 12:04:09.920: INFO: Observed &Deployment event: MODIFIED
May  4 12:04:09.920: INFO: Observed deployment test-deployment-s8w9j in namespace deployment-4647 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-04 12:04:08 +0000 UTC 2023-05-04 12:04:08 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
May  4 12:04:09.920: INFO: Observed deployment test-deployment-s8w9j in namespace deployment-4647 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-04 12:04:08 +0000 UTC 2023-05-04 12:04:07 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-s8w9j-777898ffcc" has successfully progressed.}
May  4 12:04:09.920: INFO: Observed &Deployment event: MODIFIED
May  4 12:04:09.920: INFO: Observed deployment test-deployment-s8w9j in namespace deployment-4647 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-04 12:04:08 +0000 UTC 2023-05-04 12:04:08 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
May  4 12:04:09.920: INFO: Observed deployment test-deployment-s8w9j in namespace deployment-4647 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-04 12:04:08 +0000 UTC 2023-05-04 12:04:07 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-s8w9j-777898ffcc" has successfully progressed.}
May  4 12:04:09.920: INFO: Observed deployment test-deployment-s8w9j in namespace deployment-4647 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May  4 12:04:09.921: INFO: Observed &Deployment event: MODIFIED
May  4 12:04:09.921: INFO: Found deployment test-deployment-s8w9j in namespace deployment-4647 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
May  4 12:04:09.921: INFO: Deployment test-deployment-s8w9j has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May  4 12:04:09.930: INFO: Deployment "test-deployment-s8w9j":
&Deployment{ObjectMeta:{test-deployment-s8w9j  deployment-4647  b6eac99e-9673-40a2-97aa-6feda8d30751 13844 1 2023-05-04 12:04:07 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-05-04 12:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 12:04:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-05-04 12:04:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc008591bd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May  4 12:04:09.943: INFO: New ReplicaSet "test-deployment-s8w9j-777898ffcc" of Deployment "test-deployment-s8w9j":
&ReplicaSet{ObjectMeta:{test-deployment-s8w9j-777898ffcc  deployment-4647  efe94495-eaf3-4874-a4b9-bfdbc3791ca7 13837 1 2023-05-04 12:04:07 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-s8w9j b6eac99e-9673-40a2-97aa-6feda8d30751 0xc003125b90 0xc003125b91}] [] [{kube-controller-manager Update apps/v1 2023-05-04 12:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6eac99e-9673-40a2-97aa-6feda8d30751\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 12:04:08 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003125c38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May  4 12:04:09.955: INFO: Pod "test-deployment-s8w9j-777898ffcc-xftws" is available:
&Pod{ObjectMeta:{test-deployment-s8w9j-777898ffcc-xftws test-deployment-s8w9j-777898ffcc- deployment-4647  e35dad33-8a49-4d3a-b926-5781ffa7efb9 13836 0 2023-05-04 12:04:07 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:30e6d85292cee2e0d51fba914f492829034393af89bb3c363392a2dd4a8672db cni.projectcalico.org/podIP:10.20.83.57/32 cni.projectcalico.org/podIPs:10.20.83.57/32] [{apps/v1 ReplicaSet test-deployment-s8w9j-777898ffcc efe94495-eaf3-4874-a4b9-bfdbc3791ca7 0xc0038fa000 0xc0038fa001}] [] [{kube-controller-manager Update v1 2023-05-04 12:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"efe94495-eaf3-4874-a4b9-bfdbc3791ca7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-04 12:04:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-04 12:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.83.57\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ckmp7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ckmp7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-224.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 12:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 12:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 12:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 12:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.224,PodIP:10.20.83.57,StartTime:2023-05-04 12:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-04 12:04:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://3fdd8977634db735424b942ce66090998865ff9258277d6c28cf9e71add4f2ab,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.83.57,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
May  4 12:04:09.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4647" for this suite. 05/04/23 12:04:09.974
{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","completed":85,"skipped":1814,"failed":0}
------------------------------
• [2.179 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:04:07.802
    May  4 12:04:07.802: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename deployment 05/04/23 12:04:07.803
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:04:07.818
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:04:07.822
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 05/04/23 12:04:07.841
    May  4 12:04:07.841: INFO: Creating simple deployment test-deployment-s8w9j
    May  4 12:04:07.856: INFO: deployment "test-deployment-s8w9j" doesn't have the required revision set
    STEP: Getting /status 05/04/23 12:04:09.882
    May  4 12:04:09.890: INFO: Deployment test-deployment-s8w9j has Conditions: [{Available True 2023-05-04 12:04:08 +0000 UTC 2023-05-04 12:04:08 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-05-04 12:04:08 +0000 UTC 2023-05-04 12:04:07 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-s8w9j-777898ffcc" has successfully progressed.}]
    STEP: updating Deployment Status 05/04/23 12:04:09.89
    May  4 12:04:09.901: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 12, 4, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 4, 8, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 12, 4, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 4, 7, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-s8w9j-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 05/04/23 12:04:09.901
    May  4 12:04:09.903: INFO: Observed &Deployment event: ADDED
    May  4 12:04:09.903: INFO: Observed Deployment test-deployment-s8w9j in namespace deployment-4647 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-04 12:04:07 +0000 UTC 2023-05-04 12:04:07 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-s8w9j-777898ffcc"}
    May  4 12:04:09.903: INFO: Observed &Deployment event: MODIFIED
    May  4 12:04:09.903: INFO: Observed Deployment test-deployment-s8w9j in namespace deployment-4647 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-04 12:04:07 +0000 UTC 2023-05-04 12:04:07 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-s8w9j-777898ffcc"}
    May  4 12:04:09.903: INFO: Observed Deployment test-deployment-s8w9j in namespace deployment-4647 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-04 12:04:07 +0000 UTC 2023-05-04 12:04:07 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    May  4 12:04:09.903: INFO: Observed &Deployment event: MODIFIED
    May  4 12:04:09.903: INFO: Observed Deployment test-deployment-s8w9j in namespace deployment-4647 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-04 12:04:07 +0000 UTC 2023-05-04 12:04:07 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    May  4 12:04:09.903: INFO: Observed Deployment test-deployment-s8w9j in namespace deployment-4647 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-04 12:04:07 +0000 UTC 2023-05-04 12:04:07 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-s8w9j-777898ffcc" is progressing.}
    May  4 12:04:09.903: INFO: Observed &Deployment event: MODIFIED
    May  4 12:04:09.903: INFO: Observed Deployment test-deployment-s8w9j in namespace deployment-4647 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-04 12:04:08 +0000 UTC 2023-05-04 12:04:08 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    May  4 12:04:09.903: INFO: Observed Deployment test-deployment-s8w9j in namespace deployment-4647 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-04 12:04:08 +0000 UTC 2023-05-04 12:04:07 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-s8w9j-777898ffcc" has successfully progressed.}
    May  4 12:04:09.904: INFO: Observed &Deployment event: MODIFIED
    May  4 12:04:09.904: INFO: Observed Deployment test-deployment-s8w9j in namespace deployment-4647 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-04 12:04:08 +0000 UTC 2023-05-04 12:04:08 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    May  4 12:04:09.904: INFO: Observed Deployment test-deployment-s8w9j in namespace deployment-4647 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-04 12:04:08 +0000 UTC 2023-05-04 12:04:07 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-s8w9j-777898ffcc" has successfully progressed.}
    May  4 12:04:09.904: INFO: Found Deployment test-deployment-s8w9j in namespace deployment-4647 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    May  4 12:04:09.904: INFO: Deployment test-deployment-s8w9j has an updated status
    STEP: patching the Statefulset Status 05/04/23 12:04:09.904
    May  4 12:04:09.904: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    May  4 12:04:09.917: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 05/04/23 12:04:09.917
    May  4 12:04:09.919: INFO: Observed &Deployment event: ADDED
    May  4 12:04:09.919: INFO: Observed deployment test-deployment-s8w9j in namespace deployment-4647 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-04 12:04:07 +0000 UTC 2023-05-04 12:04:07 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-s8w9j-777898ffcc"}
    May  4 12:04:09.919: INFO: Observed &Deployment event: MODIFIED
    May  4 12:04:09.919: INFO: Observed deployment test-deployment-s8w9j in namespace deployment-4647 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-04 12:04:07 +0000 UTC 2023-05-04 12:04:07 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-s8w9j-777898ffcc"}
    May  4 12:04:09.919: INFO: Observed deployment test-deployment-s8w9j in namespace deployment-4647 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-04 12:04:07 +0000 UTC 2023-05-04 12:04:07 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    May  4 12:04:09.920: INFO: Observed &Deployment event: MODIFIED
    May  4 12:04:09.920: INFO: Observed deployment test-deployment-s8w9j in namespace deployment-4647 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-04 12:04:07 +0000 UTC 2023-05-04 12:04:07 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    May  4 12:04:09.920: INFO: Observed deployment test-deployment-s8w9j in namespace deployment-4647 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-04 12:04:07 +0000 UTC 2023-05-04 12:04:07 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-s8w9j-777898ffcc" is progressing.}
    May  4 12:04:09.920: INFO: Observed &Deployment event: MODIFIED
    May  4 12:04:09.920: INFO: Observed deployment test-deployment-s8w9j in namespace deployment-4647 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-04 12:04:08 +0000 UTC 2023-05-04 12:04:08 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    May  4 12:04:09.920: INFO: Observed deployment test-deployment-s8w9j in namespace deployment-4647 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-04 12:04:08 +0000 UTC 2023-05-04 12:04:07 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-s8w9j-777898ffcc" has successfully progressed.}
    May  4 12:04:09.920: INFO: Observed &Deployment event: MODIFIED
    May  4 12:04:09.920: INFO: Observed deployment test-deployment-s8w9j in namespace deployment-4647 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-04 12:04:08 +0000 UTC 2023-05-04 12:04:08 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    May  4 12:04:09.920: INFO: Observed deployment test-deployment-s8w9j in namespace deployment-4647 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-04 12:04:08 +0000 UTC 2023-05-04 12:04:07 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-s8w9j-777898ffcc" has successfully progressed.}
    May  4 12:04:09.920: INFO: Observed deployment test-deployment-s8w9j in namespace deployment-4647 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    May  4 12:04:09.921: INFO: Observed &Deployment event: MODIFIED
    May  4 12:04:09.921: INFO: Found deployment test-deployment-s8w9j in namespace deployment-4647 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    May  4 12:04:09.921: INFO: Deployment test-deployment-s8w9j has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May  4 12:04:09.930: INFO: Deployment "test-deployment-s8w9j":
    &Deployment{ObjectMeta:{test-deployment-s8w9j  deployment-4647  b6eac99e-9673-40a2-97aa-6feda8d30751 13844 1 2023-05-04 12:04:07 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-05-04 12:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 12:04:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-05-04 12:04:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc008591bd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    May  4 12:04:09.943: INFO: New ReplicaSet "test-deployment-s8w9j-777898ffcc" of Deployment "test-deployment-s8w9j":
    &ReplicaSet{ObjectMeta:{test-deployment-s8w9j-777898ffcc  deployment-4647  efe94495-eaf3-4874-a4b9-bfdbc3791ca7 13837 1 2023-05-04 12:04:07 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-s8w9j b6eac99e-9673-40a2-97aa-6feda8d30751 0xc003125b90 0xc003125b91}] [] [{kube-controller-manager Update apps/v1 2023-05-04 12:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6eac99e-9673-40a2-97aa-6feda8d30751\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 12:04:08 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003125c38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    May  4 12:04:09.955: INFO: Pod "test-deployment-s8w9j-777898ffcc-xftws" is available:
    &Pod{ObjectMeta:{test-deployment-s8w9j-777898ffcc-xftws test-deployment-s8w9j-777898ffcc- deployment-4647  e35dad33-8a49-4d3a-b926-5781ffa7efb9 13836 0 2023-05-04 12:04:07 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:30e6d85292cee2e0d51fba914f492829034393af89bb3c363392a2dd4a8672db cni.projectcalico.org/podIP:10.20.83.57/32 cni.projectcalico.org/podIPs:10.20.83.57/32] [{apps/v1 ReplicaSet test-deployment-s8w9j-777898ffcc efe94495-eaf3-4874-a4b9-bfdbc3791ca7 0xc0038fa000 0xc0038fa001}] [] [{kube-controller-manager Update v1 2023-05-04 12:04:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"efe94495-eaf3-4874-a4b9-bfdbc3791ca7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-04 12:04:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-04 12:04:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.83.57\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ckmp7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ckmp7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-224.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 12:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 12:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 12:04:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 12:04:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.224,PodIP:10.20.83.57,StartTime:2023-05-04 12:04:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-04 12:04:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://3fdd8977634db735424b942ce66090998865ff9258277d6c28cf9e71add4f2ab,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.83.57,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    May  4 12:04:09.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-4647" for this suite. 05/04/23 12:04:09.974
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:04:09.982
May  4 12:04:09.982: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename kubelet-test 05/04/23 12:04:09.983
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:04:09.999
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:04:10.009
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
May  4 12:04:10.023: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs54e8d9e0-b203-4765-a257-257a947bc246" in namespace "kubelet-test-1245" to be "running and ready"
May  4 12:04:10.032: INFO: Pod "busybox-readonly-fs54e8d9e0-b203-4765-a257-257a947bc246": Phase="Pending", Reason="", readiness=false. Elapsed: 8.820946ms
May  4 12:04:10.032: INFO: The phase of Pod busybox-readonly-fs54e8d9e0-b203-4765-a257-257a947bc246 is Pending, waiting for it to be Running (with Ready = true)
May  4 12:04:12.037: INFO: Pod "busybox-readonly-fs54e8d9e0-b203-4765-a257-257a947bc246": Phase="Running", Reason="", readiness=true. Elapsed: 2.014624862s
May  4 12:04:12.037: INFO: The phase of Pod busybox-readonly-fs54e8d9e0-b203-4765-a257-257a947bc246 is Running (Ready = true)
May  4 12:04:12.037: INFO: Pod "busybox-readonly-fs54e8d9e0-b203-4765-a257-257a947bc246" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
May  4 12:04:12.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1245" for this suite. 05/04/23 12:04:12.056
{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","completed":86,"skipped":1814,"failed":0}
------------------------------
• [2.080 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:04:09.982
    May  4 12:04:09.982: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename kubelet-test 05/04/23 12:04:09.983
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:04:09.999
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:04:10.009
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    May  4 12:04:10.023: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs54e8d9e0-b203-4765-a257-257a947bc246" in namespace "kubelet-test-1245" to be "running and ready"
    May  4 12:04:10.032: INFO: Pod "busybox-readonly-fs54e8d9e0-b203-4765-a257-257a947bc246": Phase="Pending", Reason="", readiness=false. Elapsed: 8.820946ms
    May  4 12:04:10.032: INFO: The phase of Pod busybox-readonly-fs54e8d9e0-b203-4765-a257-257a947bc246 is Pending, waiting for it to be Running (with Ready = true)
    May  4 12:04:12.037: INFO: Pod "busybox-readonly-fs54e8d9e0-b203-4765-a257-257a947bc246": Phase="Running", Reason="", readiness=true. Elapsed: 2.014624862s
    May  4 12:04:12.037: INFO: The phase of Pod busybox-readonly-fs54e8d9e0-b203-4765-a257-257a947bc246 is Running (Ready = true)
    May  4 12:04:12.037: INFO: Pod "busybox-readonly-fs54e8d9e0-b203-4765-a257-257a947bc246" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    May  4 12:04:12.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-1245" for this suite. 05/04/23 12:04:12.056
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:04:12.064
May  4 12:04:12.064: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename webhook 05/04/23 12:04:12.066
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:04:12.085
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:04:12.097
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/04/23 12:04:12.114
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/04/23 12:04:12.728
STEP: Deploying the webhook pod 05/04/23 12:04:12.737
STEP: Wait for the deployment to be ready 05/04/23 12:04:12.755
May  4 12:04:12.763: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/04/23 12:04:14.776
STEP: Verifying the service has paired with the endpoint 05/04/23 12:04:14.793
May  4 12:04:15.793: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 05/04/23 12:04:15.797
STEP: create a configmap that should be updated by the webhook 05/04/23 12:04:15.818
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  4 12:04:15.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9772" for this suite. 05/04/23 12:04:15.845
STEP: Destroying namespace "webhook-9772-markers" for this suite. 05/04/23 12:04:15.852
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","completed":87,"skipped":1834,"failed":0}
------------------------------
• [3.852 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:04:12.064
    May  4 12:04:12.064: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename webhook 05/04/23 12:04:12.066
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:04:12.085
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:04:12.097
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/04/23 12:04:12.114
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/04/23 12:04:12.728
    STEP: Deploying the webhook pod 05/04/23 12:04:12.737
    STEP: Wait for the deployment to be ready 05/04/23 12:04:12.755
    May  4 12:04:12.763: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/04/23 12:04:14.776
    STEP: Verifying the service has paired with the endpoint 05/04/23 12:04:14.793
    May  4 12:04:15.793: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:251
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 05/04/23 12:04:15.797
    STEP: create a configmap that should be updated by the webhook 05/04/23 12:04:15.818
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  4 12:04:15.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9772" for this suite. 05/04/23 12:04:15.845
    STEP: Destroying namespace "webhook-9772-markers" for this suite. 05/04/23 12:04:15.852
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:04:15.918
May  4 12:04:15.918: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename kubectl 05/04/23 12:04:15.92
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:04:15.945
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:04:15.951
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
STEP: create deployment with httpd image 05/04/23 12:04:15.955
May  4 12:04:15.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-2617 create -f -'
May  4 12:04:16.916: INFO: stderr: ""
May  4 12:04:16.916: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 05/04/23 12:04:16.916
May  4 12:04:16.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-2617 diff -f -'
May  4 12:04:17.273: INFO: rc: 1
May  4 12:04:17.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-2617 delete -f -'
May  4 12:04:17.368: INFO: stderr: ""
May  4 12:04:17.368: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  4 12:04:17.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2617" for this suite. 05/04/23 12:04:17.375
{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","completed":88,"skipped":1880,"failed":0}
------------------------------
• [1.465 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:923
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:929

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:04:15.918
    May  4 12:04:15.918: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename kubectl 05/04/23 12:04:15.92
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:04:15.945
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:04:15.951
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:929
    STEP: create deployment with httpd image 05/04/23 12:04:15.955
    May  4 12:04:15.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-2617 create -f -'
    May  4 12:04:16.916: INFO: stderr: ""
    May  4 12:04:16.916: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 05/04/23 12:04:16.916
    May  4 12:04:16.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-2617 diff -f -'
    May  4 12:04:17.273: INFO: rc: 1
    May  4 12:04:17.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-2617 delete -f -'
    May  4 12:04:17.368: INFO: stderr: ""
    May  4 12:04:17.368: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  4 12:04:17.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2617" for this suite. 05/04/23 12:04:17.375
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:04:17.386
May  4 12:04:17.386: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename container-probe 05/04/23 12:04:17.386
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:04:17.484
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:04:17.487
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
STEP: Creating pod liveness-5e316c57-db20-4804-b38d-694e2ff7a085 in namespace container-probe-208 05/04/23 12:04:17.49
May  4 12:04:17.501: INFO: Waiting up to 5m0s for pod "liveness-5e316c57-db20-4804-b38d-694e2ff7a085" in namespace "container-probe-208" to be "not pending"
May  4 12:04:17.504: INFO: Pod "liveness-5e316c57-db20-4804-b38d-694e2ff7a085": Phase="Pending", Reason="", readiness=false. Elapsed: 3.300574ms
May  4 12:04:19.509: INFO: Pod "liveness-5e316c57-db20-4804-b38d-694e2ff7a085": Phase="Running", Reason="", readiness=true. Elapsed: 2.007991098s
May  4 12:04:19.509: INFO: Pod "liveness-5e316c57-db20-4804-b38d-694e2ff7a085" satisfied condition "not pending"
May  4 12:04:19.509: INFO: Started pod liveness-5e316c57-db20-4804-b38d-694e2ff7a085 in namespace container-probe-208
STEP: checking the pod's current state and verifying that restartCount is present 05/04/23 12:04:19.509
May  4 12:04:19.513: INFO: Initial restart count of pod liveness-5e316c57-db20-4804-b38d-694e2ff7a085 is 0
May  4 12:04:39.575: INFO: Restart count of pod container-probe-208/liveness-5e316c57-db20-4804-b38d-694e2ff7a085 is now 1 (20.061231734s elapsed)
STEP: deleting the pod 05/04/23 12:04:39.575
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
May  4 12:04:39.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-208" for this suite. 05/04/23 12:04:39.604
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":89,"skipped":1894,"failed":0}
------------------------------
• [SLOW TEST] [22.227 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:04:17.386
    May  4 12:04:17.386: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename container-probe 05/04/23 12:04:17.386
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:04:17.484
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:04:17.487
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:165
    STEP: Creating pod liveness-5e316c57-db20-4804-b38d-694e2ff7a085 in namespace container-probe-208 05/04/23 12:04:17.49
    May  4 12:04:17.501: INFO: Waiting up to 5m0s for pod "liveness-5e316c57-db20-4804-b38d-694e2ff7a085" in namespace "container-probe-208" to be "not pending"
    May  4 12:04:17.504: INFO: Pod "liveness-5e316c57-db20-4804-b38d-694e2ff7a085": Phase="Pending", Reason="", readiness=false. Elapsed: 3.300574ms
    May  4 12:04:19.509: INFO: Pod "liveness-5e316c57-db20-4804-b38d-694e2ff7a085": Phase="Running", Reason="", readiness=true. Elapsed: 2.007991098s
    May  4 12:04:19.509: INFO: Pod "liveness-5e316c57-db20-4804-b38d-694e2ff7a085" satisfied condition "not pending"
    May  4 12:04:19.509: INFO: Started pod liveness-5e316c57-db20-4804-b38d-694e2ff7a085 in namespace container-probe-208
    STEP: checking the pod's current state and verifying that restartCount is present 05/04/23 12:04:19.509
    May  4 12:04:19.513: INFO: Initial restart count of pod liveness-5e316c57-db20-4804-b38d-694e2ff7a085 is 0
    May  4 12:04:39.575: INFO: Restart count of pod container-probe-208/liveness-5e316c57-db20-4804-b38d-694e2ff7a085 is now 1 (20.061231734s elapsed)
    STEP: deleting the pod 05/04/23 12:04:39.575
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    May  4 12:04:39.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-208" for this suite. 05/04/23 12:04:39.604
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:04:39.614
May  4 12:04:39.614: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename secrets 05/04/23 12:04:39.615
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:04:39.628
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:04:39.633
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
STEP: creating a secret 05/04/23 12:04:39.637
STEP: listing secrets in all namespaces to ensure that there are more than zero 05/04/23 12:04:39.641
STEP: patching the secret 05/04/23 12:04:39.651
STEP: deleting the secret using a LabelSelector 05/04/23 12:04:39.664
STEP: listing secrets in all namespaces, searching for label name and value in patch 05/04/23 12:04:39.673
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
May  4 12:04:39.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1571" for this suite. 05/04/23 12:04:39.683
{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","completed":90,"skipped":1903,"failed":0}
------------------------------
• [0.079 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:04:39.614
    May  4 12:04:39.614: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename secrets 05/04/23 12:04:39.615
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:04:39.628
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:04:39.633
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:153
    STEP: creating a secret 05/04/23 12:04:39.637
    STEP: listing secrets in all namespaces to ensure that there are more than zero 05/04/23 12:04:39.641
    STEP: patching the secret 05/04/23 12:04:39.651
    STEP: deleting the secret using a LabelSelector 05/04/23 12:04:39.664
    STEP: listing secrets in all namespaces, searching for label name and value in patch 05/04/23 12:04:39.673
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    May  4 12:04:39.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-1571" for this suite. 05/04/23 12:04:39.683
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:309
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:04:39.695
May  4 12:04:39.695: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename job 05/04/23 12:04:39.697
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:04:39.714
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:04:39.718
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:309
STEP: Creating a job 05/04/23 12:04:39.726
STEP: Ensuring active pods == parallelism 05/04/23 12:04:39.736
STEP: delete a job 05/04/23 12:04:41.744
STEP: deleting Job.batch foo in namespace job-7551, will wait for the garbage collector to delete the pods 05/04/23 12:04:41.744
May  4 12:04:41.812: INFO: Deleting Job.batch foo took: 11.814187ms
May  4 12:04:41.913: INFO: Terminating Job.batch foo pods took: 101.017261ms
STEP: Ensuring job was deleted 05/04/23 12:05:14.714
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
May  4 12:05:14.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7551" for this suite. 05/04/23 12:05:14.727
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","completed":91,"skipped":1956,"failed":0}
------------------------------
• [SLOW TEST] [35.047 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:04:39.695
    May  4 12:04:39.695: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename job 05/04/23 12:04:39.697
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:04:39.714
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:04:39.718
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:309
    STEP: Creating a job 05/04/23 12:04:39.726
    STEP: Ensuring active pods == parallelism 05/04/23 12:04:39.736
    STEP: delete a job 05/04/23 12:04:41.744
    STEP: deleting Job.batch foo in namespace job-7551, will wait for the garbage collector to delete the pods 05/04/23 12:04:41.744
    May  4 12:04:41.812: INFO: Deleting Job.batch foo took: 11.814187ms
    May  4 12:04:41.913: INFO: Terminating Job.batch foo pods took: 101.017261ms
    STEP: Ensuring job was deleted 05/04/23 12:05:14.714
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    May  4 12:05:14.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-7551" for this suite. 05/04/23 12:05:14.727
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:05:14.744
May  4 12:05:14.744: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename webhook 05/04/23 12:05:14.744
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:05:14.761
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:05:14.766
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/04/23 12:05:14.785
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/04/23 12:05:15.438
STEP: Deploying the webhook pod 05/04/23 12:05:15.446
STEP: Wait for the deployment to be ready 05/04/23 12:05:15.461
May  4 12:05:15.476: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/04/23 12:05:17.493
STEP: Verifying the service has paired with the endpoint 05/04/23 12:05:17.505
May  4 12:05:18.507: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
May  4 12:05:18.511: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1610-crds.webhook.example.com via the AdmissionRegistration API 05/04/23 12:05:19.024
STEP: Creating a custom resource that should be mutated by the webhook 05/04/23 12:05:19.043
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  4 12:05:21.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9920" for this suite. 05/04/23 12:05:21.666
STEP: Destroying namespace "webhook-9920-markers" for this suite. 05/04/23 12:05:21.679
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","completed":92,"skipped":1989,"failed":0}
------------------------------
• [SLOW TEST] [7.098 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:05:14.744
    May  4 12:05:14.744: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename webhook 05/04/23 12:05:14.744
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:05:14.761
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:05:14.766
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/04/23 12:05:14.785
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/04/23 12:05:15.438
    STEP: Deploying the webhook pod 05/04/23 12:05:15.446
    STEP: Wait for the deployment to be ready 05/04/23 12:05:15.461
    May  4 12:05:15.476: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/04/23 12:05:17.493
    STEP: Verifying the service has paired with the endpoint 05/04/23 12:05:17.505
    May  4 12:05:18.507: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:340
    May  4 12:05:18.511: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1610-crds.webhook.example.com via the AdmissionRegistration API 05/04/23 12:05:19.024
    STEP: Creating a custom resource that should be mutated by the webhook 05/04/23 12:05:19.043
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  4 12:05:21.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9920" for this suite. 05/04/23 12:05:21.666
    STEP: Destroying namespace "webhook-9920-markers" for this suite. 05/04/23 12:05:21.679
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:05:21.847
May  4 12:05:21.847: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename replicaset 05/04/23 12:05:21.849
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:05:21.888
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:05:21.912
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
May  4 12:05:21.947: INFO: Pod name sample-pod: Found 0 pods out of 1
May  4 12:05:26.951: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 05/04/23 12:05:26.952
STEP: Scaling up "test-rs" replicaset  05/04/23 12:05:26.952
May  4 12:05:26.975: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 05/04/23 12:05:26.975
W0504 12:05:26.999845      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
May  4 12:05:27.001: INFO: observed ReplicaSet test-rs in namespace replicaset-5996 with ReadyReplicas 1, AvailableReplicas 1
May  4 12:05:27.053: INFO: observed ReplicaSet test-rs in namespace replicaset-5996 with ReadyReplicas 1, AvailableReplicas 1
May  4 12:05:27.082: INFO: observed ReplicaSet test-rs in namespace replicaset-5996 with ReadyReplicas 1, AvailableReplicas 1
May  4 12:05:27.095: INFO: observed ReplicaSet test-rs in namespace replicaset-5996 with ReadyReplicas 1, AvailableReplicas 1
May  4 12:05:28.597: INFO: observed ReplicaSet test-rs in namespace replicaset-5996 with ReadyReplicas 2, AvailableReplicas 2
May  4 12:05:28.714: INFO: observed Replicaset test-rs in namespace replicaset-5996 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
May  4 12:05:28.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5996" for this suite. 05/04/23 12:05:28.725
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","completed":93,"skipped":2022,"failed":0}
------------------------------
• [SLOW TEST] [6.884 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:05:21.847
    May  4 12:05:21.847: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename replicaset 05/04/23 12:05:21.849
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:05:21.888
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:05:21.912
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    May  4 12:05:21.947: INFO: Pod name sample-pod: Found 0 pods out of 1
    May  4 12:05:26.951: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 05/04/23 12:05:26.952
    STEP: Scaling up "test-rs" replicaset  05/04/23 12:05:26.952
    May  4 12:05:26.975: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 05/04/23 12:05:26.975
    W0504 12:05:26.999845      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    May  4 12:05:27.001: INFO: observed ReplicaSet test-rs in namespace replicaset-5996 with ReadyReplicas 1, AvailableReplicas 1
    May  4 12:05:27.053: INFO: observed ReplicaSet test-rs in namespace replicaset-5996 with ReadyReplicas 1, AvailableReplicas 1
    May  4 12:05:27.082: INFO: observed ReplicaSet test-rs in namespace replicaset-5996 with ReadyReplicas 1, AvailableReplicas 1
    May  4 12:05:27.095: INFO: observed ReplicaSet test-rs in namespace replicaset-5996 with ReadyReplicas 1, AvailableReplicas 1
    May  4 12:05:28.597: INFO: observed ReplicaSet test-rs in namespace replicaset-5996 with ReadyReplicas 2, AvailableReplicas 2
    May  4 12:05:28.714: INFO: observed Replicaset test-rs in namespace replicaset-5996 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    May  4 12:05:28.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-5996" for this suite. 05/04/23 12:05:28.725
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:05:28.733
May  4 12:05:28.733: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename replication-controller 05/04/23 12:05:28.734
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:05:28.748
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:05:28.751
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
May  4 12:05:28.754: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 05/04/23 12:05:29.767
STEP: Checking rc "condition-test" has the desired failure condition set 05/04/23 12:05:29.773
STEP: Scaling down rc "condition-test" to satisfy pod quota 05/04/23 12:05:30.78
May  4 12:05:30.792: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 05/04/23 12:05:30.792
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
May  4 12:05:31.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8980" for this suite. 05/04/23 12:05:31.811
{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","completed":94,"skipped":2043,"failed":0}
------------------------------
• [3.085 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:05:28.733
    May  4 12:05:28.733: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename replication-controller 05/04/23 12:05:28.734
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:05:28.748
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:05:28.751
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:82
    May  4 12:05:28.754: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 05/04/23 12:05:29.767
    STEP: Checking rc "condition-test" has the desired failure condition set 05/04/23 12:05:29.773
    STEP: Scaling down rc "condition-test" to satisfy pod quota 05/04/23 12:05:30.78
    May  4 12:05:30.792: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 05/04/23 12:05:30.792
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    May  4 12:05:31.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-8980" for this suite. 05/04/23 12:05:31.811
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:05:31.818
May  4 12:05:31.818: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename services 05/04/23 12:05:31.819
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:05:31.838
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:05:31.841
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
STEP: creating service nodeport-test with type=NodePort in namespace services-2721 05/04/23 12:05:31.844
STEP: creating replication controller nodeport-test in namespace services-2721 05/04/23 12:05:31.862
I0504 12:05:31.878741      21 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-2721, replica count: 2
I0504 12:05:34.929517      21 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  4 12:05:34.929: INFO: Creating new exec pod
May  4 12:05:34.938: INFO: Waiting up to 5m0s for pod "execpodxktct" in namespace "services-2721" to be "running"
May  4 12:05:34.943: INFO: Pod "execpodxktct": Phase="Pending", Reason="", readiness=false. Elapsed: 5.092426ms
May  4 12:05:36.948: INFO: Pod "execpodxktct": Phase="Running", Reason="", readiness=true. Elapsed: 2.010773773s
May  4 12:05:36.949: INFO: Pod "execpodxktct" satisfied condition "running"
May  4 12:05:37.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-2721 exec execpodxktct -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May  4 12:05:38.169: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May  4 12:05:38.169: INFO: stdout: ""
May  4 12:05:39.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-2721 exec execpodxktct -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May  4 12:05:39.405: INFO: stderr: "+ + ncecho -v hostName -t\n -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May  4 12:05:39.405: INFO: stdout: ""
May  4 12:05:40.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-2721 exec execpodxktct -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May  4 12:05:40.400: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May  4 12:05:40.400: INFO: stdout: ""
May  4 12:05:41.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-2721 exec execpodxktct -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May  4 12:05:41.439: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May  4 12:05:41.439: INFO: stdout: "nodeport-test-m8jqq"
May  4 12:05:41.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-2721 exec execpodxktct -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.21.89.93 80'
May  4 12:05:41.673: INFO: stderr: "+ nc -v -t -w 2 10.21.89.93 80\n+ echo hostName\nConnection to 10.21.89.93 80 port [tcp/http] succeeded!\n"
May  4 12:05:41.673: INFO: stdout: "nodeport-test-pstvm"
May  4 12:05:41.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-2721 exec execpodxktct -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.1.232 30525'
May  4 12:05:41.890: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.1.232 30525\nConnection to 10.0.1.232 30525 port [tcp/*] succeeded!\n"
May  4 12:05:41.890: INFO: stdout: "nodeport-test-pstvm"
May  4 12:05:41.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-2721 exec execpodxktct -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.1.189 30525'
May  4 12:05:42.148: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.1.189 30525\nConnection to 10.0.1.189 30525 port [tcp/*] succeeded!\n"
May  4 12:05:42.148: INFO: stdout: "nodeport-test-pstvm"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  4 12:05:42.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2721" for this suite. 05/04/23 12:05:42.155
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","completed":95,"skipped":2048,"failed":0}
------------------------------
• [SLOW TEST] [10.348 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:05:31.818
    May  4 12:05:31.818: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename services 05/04/23 12:05:31.819
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:05:31.838
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:05:31.841
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1268
    STEP: creating service nodeport-test with type=NodePort in namespace services-2721 05/04/23 12:05:31.844
    STEP: creating replication controller nodeport-test in namespace services-2721 05/04/23 12:05:31.862
    I0504 12:05:31.878741      21 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-2721, replica count: 2
    I0504 12:05:34.929517      21 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  4 12:05:34.929: INFO: Creating new exec pod
    May  4 12:05:34.938: INFO: Waiting up to 5m0s for pod "execpodxktct" in namespace "services-2721" to be "running"
    May  4 12:05:34.943: INFO: Pod "execpodxktct": Phase="Pending", Reason="", readiness=false. Elapsed: 5.092426ms
    May  4 12:05:36.948: INFO: Pod "execpodxktct": Phase="Running", Reason="", readiness=true. Elapsed: 2.010773773s
    May  4 12:05:36.949: INFO: Pod "execpodxktct" satisfied condition "running"
    May  4 12:05:37.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-2721 exec execpodxktct -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    May  4 12:05:38.169: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    May  4 12:05:38.169: INFO: stdout: ""
    May  4 12:05:39.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-2721 exec execpodxktct -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    May  4 12:05:39.405: INFO: stderr: "+ + ncecho -v hostName -t\n -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    May  4 12:05:39.405: INFO: stdout: ""
    May  4 12:05:40.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-2721 exec execpodxktct -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    May  4 12:05:40.400: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    May  4 12:05:40.400: INFO: stdout: ""
    May  4 12:05:41.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-2721 exec execpodxktct -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    May  4 12:05:41.439: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    May  4 12:05:41.439: INFO: stdout: "nodeport-test-m8jqq"
    May  4 12:05:41.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-2721 exec execpodxktct -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.21.89.93 80'
    May  4 12:05:41.673: INFO: stderr: "+ nc -v -t -w 2 10.21.89.93 80\n+ echo hostName\nConnection to 10.21.89.93 80 port [tcp/http] succeeded!\n"
    May  4 12:05:41.673: INFO: stdout: "nodeport-test-pstvm"
    May  4 12:05:41.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-2721 exec execpodxktct -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.1.232 30525'
    May  4 12:05:41.890: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.1.232 30525\nConnection to 10.0.1.232 30525 port [tcp/*] succeeded!\n"
    May  4 12:05:41.890: INFO: stdout: "nodeport-test-pstvm"
    May  4 12:05:41.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-2721 exec execpodxktct -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.1.189 30525'
    May  4 12:05:42.148: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.1.189 30525\nConnection to 10.0.1.189 30525 port [tcp/*] succeeded!\n"
    May  4 12:05:42.148: INFO: stdout: "nodeport-test-pstvm"
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  4 12:05:42.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2721" for this suite. 05/04/23 12:05:42.155
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:05:42.167
May  4 12:05:42.168: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename configmap 05/04/23 12:05:42.168
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:05:42.185
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:05:42.194
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
STEP: Creating configMap with name configmap-test-upd-6066d755-ea84-4bae-8260-b5a9b11d11ad 05/04/23 12:05:42.208
STEP: Creating the pod 05/04/23 12:05:42.222
May  4 12:05:42.234: INFO: Waiting up to 5m0s for pod "pod-configmaps-6efb7951-7aca-4511-8f47-de91ca561664" in namespace "configmap-3559" to be "running"
May  4 12:05:42.239: INFO: Pod "pod-configmaps-6efb7951-7aca-4511-8f47-de91ca561664": Phase="Pending", Reason="", readiness=false. Elapsed: 4.967937ms
May  4 12:05:44.244: INFO: Pod "pod-configmaps-6efb7951-7aca-4511-8f47-de91ca561664": Phase="Running", Reason="", readiness=false. Elapsed: 2.009915707s
May  4 12:05:44.244: INFO: Pod "pod-configmaps-6efb7951-7aca-4511-8f47-de91ca561664" satisfied condition "running"
STEP: Waiting for pod with text data 05/04/23 12:05:44.244
STEP: Waiting for pod with binary data 05/04/23 12:05:44.262
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
May  4 12:05:44.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3559" for this suite. 05/04/23 12:05:44.283
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","completed":96,"skipped":2062,"failed":0}
------------------------------
• [2.125 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:05:42.167
    May  4 12:05:42.168: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename configmap 05/04/23 12:05:42.168
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:05:42.185
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:05:42.194
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:174
    STEP: Creating configMap with name configmap-test-upd-6066d755-ea84-4bae-8260-b5a9b11d11ad 05/04/23 12:05:42.208
    STEP: Creating the pod 05/04/23 12:05:42.222
    May  4 12:05:42.234: INFO: Waiting up to 5m0s for pod "pod-configmaps-6efb7951-7aca-4511-8f47-de91ca561664" in namespace "configmap-3559" to be "running"
    May  4 12:05:42.239: INFO: Pod "pod-configmaps-6efb7951-7aca-4511-8f47-de91ca561664": Phase="Pending", Reason="", readiness=false. Elapsed: 4.967937ms
    May  4 12:05:44.244: INFO: Pod "pod-configmaps-6efb7951-7aca-4511-8f47-de91ca561664": Phase="Running", Reason="", readiness=false. Elapsed: 2.009915707s
    May  4 12:05:44.244: INFO: Pod "pod-configmaps-6efb7951-7aca-4511-8f47-de91ca561664" satisfied condition "running"
    STEP: Waiting for pod with text data 05/04/23 12:05:44.244
    STEP: Waiting for pod with binary data 05/04/23 12:05:44.262
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    May  4 12:05:44.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3559" for this suite. 05/04/23 12:05:44.283
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:05:44.294
May  4 12:05:44.294: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename emptydir 05/04/23 12:05:44.295
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:05:44.311
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:05:44.315
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
STEP: Creating a pod to test emptydir 0777 on node default medium 05/04/23 12:05:44.318
May  4 12:05:44.328: INFO: Waiting up to 5m0s for pod "pod-6365a2ce-3086-43a7-a0cb-6db925d50b8c" in namespace "emptydir-7264" to be "Succeeded or Failed"
May  4 12:05:44.333: INFO: Pod "pod-6365a2ce-3086-43a7-a0cb-6db925d50b8c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.666202ms
May  4 12:05:46.338: INFO: Pod "pod-6365a2ce-3086-43a7-a0cb-6db925d50b8c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010324885s
May  4 12:05:48.336: INFO: Pod "pod-6365a2ce-3086-43a7-a0cb-6db925d50b8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008422717s
STEP: Saw pod success 05/04/23 12:05:48.336
May  4 12:05:48.337: INFO: Pod "pod-6365a2ce-3086-43a7-a0cb-6db925d50b8c" satisfied condition "Succeeded or Failed"
May  4 12:05:48.340: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-6365a2ce-3086-43a7-a0cb-6db925d50b8c container test-container: <nil>
STEP: delete the pod 05/04/23 12:05:48.357
May  4 12:05:48.376: INFO: Waiting for pod pod-6365a2ce-3086-43a7-a0cb-6db925d50b8c to disappear
May  4 12:05:48.379: INFO: Pod pod-6365a2ce-3086-43a7-a0cb-6db925d50b8c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May  4 12:05:48.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7264" for this suite. 05/04/23 12:05:48.39
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":97,"skipped":2077,"failed":0}
------------------------------
• [4.115 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:05:44.294
    May  4 12:05:44.294: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename emptydir 05/04/23 12:05:44.295
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:05:44.311
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:05:44.315
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:186
    STEP: Creating a pod to test emptydir 0777 on node default medium 05/04/23 12:05:44.318
    May  4 12:05:44.328: INFO: Waiting up to 5m0s for pod "pod-6365a2ce-3086-43a7-a0cb-6db925d50b8c" in namespace "emptydir-7264" to be "Succeeded or Failed"
    May  4 12:05:44.333: INFO: Pod "pod-6365a2ce-3086-43a7-a0cb-6db925d50b8c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.666202ms
    May  4 12:05:46.338: INFO: Pod "pod-6365a2ce-3086-43a7-a0cb-6db925d50b8c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010324885s
    May  4 12:05:48.336: INFO: Pod "pod-6365a2ce-3086-43a7-a0cb-6db925d50b8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008422717s
    STEP: Saw pod success 05/04/23 12:05:48.336
    May  4 12:05:48.337: INFO: Pod "pod-6365a2ce-3086-43a7-a0cb-6db925d50b8c" satisfied condition "Succeeded or Failed"
    May  4 12:05:48.340: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-6365a2ce-3086-43a7-a0cb-6db925d50b8c container test-container: <nil>
    STEP: delete the pod 05/04/23 12:05:48.357
    May  4 12:05:48.376: INFO: Waiting for pod pod-6365a2ce-3086-43a7-a0cb-6db925d50b8c to disappear
    May  4 12:05:48.379: INFO: Pod pod-6365a2ce-3086-43a7-a0cb-6db925d50b8c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May  4 12:05:48.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7264" for this suite. 05/04/23 12:05:48.39
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:05:48.411
May  4 12:05:48.411: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename statefulset 05/04/23 12:05:48.412
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:05:48.433
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:05:48.441
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-9658 05/04/23 12:05:48.444
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
STEP: Initializing watcher for selector baz=blah,foo=bar 05/04/23 12:05:48.449
STEP: Creating stateful set ss in namespace statefulset-9658 05/04/23 12:05:48.457
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9658 05/04/23 12:05:48.463
May  4 12:05:48.467: INFO: Found 0 stateful pods, waiting for 1
May  4 12:05:58.472: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 05/04/23 12:05:58.472
May  4 12:05:58.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=statefulset-9658 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  4 12:05:58.668: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  4 12:05:58.668: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  4 12:05:58.668: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  4 12:05:58.672: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May  4 12:06:08.678: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May  4 12:06:08.678: INFO: Waiting for statefulset status.replicas updated to 0
May  4 12:06:08.696: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999706s
May  4 12:06:09.700: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995509829s
May  4 12:06:10.706: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.990657109s
May  4 12:06:11.710: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.985466832s
May  4 12:06:12.715: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.98111504s
May  4 12:06:13.719: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.976310678s
May  4 12:06:14.727: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.972374537s
May  4 12:06:15.731: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.964795301s
May  4 12:06:16.735: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.960728789s
May  4 12:06:17.741: INFO: Verifying statefulset ss doesn't scale past 1 for another 955.463721ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9658 05/04/23 12:06:18.741
May  4 12:06:18.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=statefulset-9658 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  4 12:06:18.961: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  4 12:06:18.961: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  4 12:06:18.962: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  4 12:06:18.967: INFO: Found 1 stateful pods, waiting for 3
May  4 12:06:28.973: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May  4 12:06:28.973: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May  4 12:06:28.973: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 05/04/23 12:06:28.973
STEP: Scale down will halt with unhealthy stateful pod 05/04/23 12:06:28.973
May  4 12:06:28.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=statefulset-9658 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  4 12:06:29.178: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  4 12:06:29.178: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  4 12:06:29.178: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  4 12:06:29.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=statefulset-9658 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  4 12:06:29.361: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  4 12:06:29.361: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  4 12:06:29.361: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  4 12:06:29.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=statefulset-9658 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  4 12:06:29.575: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  4 12:06:29.575: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  4 12:06:29.575: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  4 12:06:29.575: INFO: Waiting for statefulset status.replicas updated to 0
May  4 12:06:29.578: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
May  4 12:06:39.588: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May  4 12:06:39.588: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May  4 12:06:39.588: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May  4 12:06:39.607: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999668s
May  4 12:06:40.619: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993282327s
May  4 12:06:41.624: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.981470445s
May  4 12:06:42.630: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.975333557s
May  4 12:06:43.635: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.970553169s
May  4 12:06:44.641: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.964978571s
May  4 12:06:45.645: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.95989663s
May  4 12:06:46.650: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.954913041s
May  4 12:06:47.655: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.949989729s
May  4 12:06:48.659: INFO: Verifying statefulset ss doesn't scale past 3 for another 945.59519ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9658 05/04/23 12:06:49.66
May  4 12:06:49.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=statefulset-9658 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  4 12:06:49.951: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  4 12:06:49.951: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  4 12:06:49.951: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  4 12:06:49.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=statefulset-9658 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  4 12:06:50.212: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  4 12:06:50.212: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  4 12:06:50.212: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  4 12:06:50.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=statefulset-9658 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  4 12:06:50.381: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  4 12:06:50.381: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  4 12:06:50.381: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  4 12:06:50.381: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 05/04/23 12:07:00.398
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
May  4 12:07:00.398: INFO: Deleting all statefulset in ns statefulset-9658
May  4 12:07:00.402: INFO: Scaling statefulset ss to 0
May  4 12:07:00.415: INFO: Waiting for statefulset status.replicas updated to 0
May  4 12:07:00.420: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
May  4 12:07:00.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9658" for this suite. 05/04/23 12:07:00.466
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","completed":98,"skipped":2080,"failed":0}
------------------------------
• [SLOW TEST] [72.064 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:585

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:05:48.411
    May  4 12:05:48.411: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename statefulset 05/04/23 12:05:48.412
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:05:48.433
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:05:48.441
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-9658 05/04/23 12:05:48.444
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:585
    STEP: Initializing watcher for selector baz=blah,foo=bar 05/04/23 12:05:48.449
    STEP: Creating stateful set ss in namespace statefulset-9658 05/04/23 12:05:48.457
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9658 05/04/23 12:05:48.463
    May  4 12:05:48.467: INFO: Found 0 stateful pods, waiting for 1
    May  4 12:05:58.472: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 05/04/23 12:05:58.472
    May  4 12:05:58.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=statefulset-9658 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  4 12:05:58.668: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  4 12:05:58.668: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  4 12:05:58.668: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May  4 12:05:58.672: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    May  4 12:06:08.678: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    May  4 12:06:08.678: INFO: Waiting for statefulset status.replicas updated to 0
    May  4 12:06:08.696: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999706s
    May  4 12:06:09.700: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995509829s
    May  4 12:06:10.706: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.990657109s
    May  4 12:06:11.710: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.985466832s
    May  4 12:06:12.715: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.98111504s
    May  4 12:06:13.719: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.976310678s
    May  4 12:06:14.727: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.972374537s
    May  4 12:06:15.731: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.964795301s
    May  4 12:06:16.735: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.960728789s
    May  4 12:06:17.741: INFO: Verifying statefulset ss doesn't scale past 1 for another 955.463721ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9658 05/04/23 12:06:18.741
    May  4 12:06:18.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=statefulset-9658 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  4 12:06:18.961: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May  4 12:06:18.961: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May  4 12:06:18.962: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May  4 12:06:18.967: INFO: Found 1 stateful pods, waiting for 3
    May  4 12:06:28.973: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    May  4 12:06:28.973: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    May  4 12:06:28.973: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 05/04/23 12:06:28.973
    STEP: Scale down will halt with unhealthy stateful pod 05/04/23 12:06:28.973
    May  4 12:06:28.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=statefulset-9658 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  4 12:06:29.178: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  4 12:06:29.178: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  4 12:06:29.178: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May  4 12:06:29.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=statefulset-9658 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  4 12:06:29.361: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  4 12:06:29.361: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  4 12:06:29.361: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May  4 12:06:29.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=statefulset-9658 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  4 12:06:29.575: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  4 12:06:29.575: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  4 12:06:29.575: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May  4 12:06:29.575: INFO: Waiting for statefulset status.replicas updated to 0
    May  4 12:06:29.578: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    May  4 12:06:39.588: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    May  4 12:06:39.588: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    May  4 12:06:39.588: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    May  4 12:06:39.607: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999668s
    May  4 12:06:40.619: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993282327s
    May  4 12:06:41.624: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.981470445s
    May  4 12:06:42.630: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.975333557s
    May  4 12:06:43.635: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.970553169s
    May  4 12:06:44.641: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.964978571s
    May  4 12:06:45.645: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.95989663s
    May  4 12:06:46.650: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.954913041s
    May  4 12:06:47.655: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.949989729s
    May  4 12:06:48.659: INFO: Verifying statefulset ss doesn't scale past 3 for another 945.59519ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9658 05/04/23 12:06:49.66
    May  4 12:06:49.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=statefulset-9658 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  4 12:06:49.951: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May  4 12:06:49.951: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May  4 12:06:49.951: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May  4 12:06:49.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=statefulset-9658 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  4 12:06:50.212: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May  4 12:06:50.212: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May  4 12:06:50.212: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May  4 12:06:50.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=statefulset-9658 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  4 12:06:50.381: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May  4 12:06:50.381: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May  4 12:06:50.381: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May  4 12:06:50.381: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 05/04/23 12:07:00.398
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    May  4 12:07:00.398: INFO: Deleting all statefulset in ns statefulset-9658
    May  4 12:07:00.402: INFO: Scaling statefulset ss to 0
    May  4 12:07:00.415: INFO: Waiting for statefulset status.replicas updated to 0
    May  4 12:07:00.420: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    May  4 12:07:00.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-9658" for this suite. 05/04/23 12:07:00.466
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:07:00.477
May  4 12:07:00.478: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename sched-preemption 05/04/23 12:07:00.479
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:07:00.495
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:07:00.499
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
May  4 12:07:00.525: INFO: Waiting up to 1m0s for all nodes to be ready
May  4 12:08:00.613: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:08:00.619
May  4 12:08:00.619: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename sched-preemption-path 05/04/23 12:08:00.62
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:08:00.649
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:08:00.654
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node 05/04/23 12:08:00.658
STEP: Trying to launch a pod without a label to get a node which can launch it. 05/04/23 12:08:00.658
May  4 12:08:00.670: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-4138" to be "running"
May  4 12:08:00.678: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 7.908175ms
May  4 12:08:02.684: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.014277967s
May  4 12:08:02.684: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 05/04/23 12:08:02.691
May  4 12:08:02.710: INFO: found a healthy node: ip-10-0-1-216.us-west-2.compute.internal
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
May  4 12:08:20.824: INFO: pods created so far: [1 1 1]
May  4 12:08:20.824: INFO: length of pods created so far: 3
May  4 12:08:22.846: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
May  4 12:08:29.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-4138" for this suite. 05/04/23 12:08:29.855
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
May  4 12:08:29.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-8553" for this suite. 05/04/23 12:08:29.925
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","completed":99,"skipped":2110,"failed":0}
------------------------------
• [SLOW TEST] [89.551 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:543

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:07:00.477
    May  4 12:07:00.478: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename sched-preemption 05/04/23 12:07:00.479
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:07:00.495
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:07:00.499
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    May  4 12:07:00.525: INFO: Waiting up to 1m0s for all nodes to be ready
    May  4 12:08:00.613: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:08:00.619
    May  4 12:08:00.619: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename sched-preemption-path 05/04/23 12:08:00.62
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:08:00.649
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:08:00.654
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:496
    STEP: Finding an available node 05/04/23 12:08:00.658
    STEP: Trying to launch a pod without a label to get a node which can launch it. 05/04/23 12:08:00.658
    May  4 12:08:00.670: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-4138" to be "running"
    May  4 12:08:00.678: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 7.908175ms
    May  4 12:08:02.684: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.014277967s
    May  4 12:08:02.684: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 05/04/23 12:08:02.691
    May  4 12:08:02.710: INFO: found a healthy node: ip-10-0-1-216.us-west-2.compute.internal
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:543
    May  4 12:08:20.824: INFO: pods created so far: [1 1 1]
    May  4 12:08:20.824: INFO: length of pods created so far: 3
    May  4 12:08:22.846: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:187
    May  4 12:08:29.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-4138" for this suite. 05/04/23 12:08:29.855
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:470
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    May  4 12:08:29.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-8553" for this suite. 05/04/23 12:08:29.925
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:08:30.029
May  4 12:08:30.029: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename daemonsets 05/04/23 12:08:30.03
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:08:30.05
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:08:30.06
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
May  4 12:08:30.110: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 05/04/23 12:08:30.12
May  4 12:08:30.124: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  4 12:08:30.124: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 05/04/23 12:08:30.124
May  4 12:08:30.160: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  4 12:08:30.160: INFO: Node ip-10-0-1-224.us-west-2.compute.internal is running 0 daemon pod, expected 1
May  4 12:08:31.164: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  4 12:08:31.164: INFO: Node ip-10-0-1-224.us-west-2.compute.internal is running 0 daemon pod, expected 1
May  4 12:08:32.164: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May  4 12:08:32.164: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 05/04/23 12:08:32.168
May  4 12:08:32.191: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May  4 12:08:32.191: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
May  4 12:08:33.196: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  4 12:08:33.196: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 05/04/23 12:08:33.196
May  4 12:08:33.215: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  4 12:08:33.215: INFO: Node ip-10-0-1-224.us-west-2.compute.internal is running 0 daemon pod, expected 1
May  4 12:08:34.223: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  4 12:08:34.223: INFO: Node ip-10-0-1-224.us-west-2.compute.internal is running 0 daemon pod, expected 1
May  4 12:08:35.222: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  4 12:08:35.222: INFO: Node ip-10-0-1-224.us-west-2.compute.internal is running 0 daemon pod, expected 1
May  4 12:08:36.221: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May  4 12:08:36.221: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 05/04/23 12:08:36.227
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9647, will wait for the garbage collector to delete the pods 05/04/23 12:08:36.227
May  4 12:08:36.291: INFO: Deleting DaemonSet.extensions daemon-set took: 10.369967ms
May  4 12:08:36.392: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.130235ms
May  4 12:08:38.399: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  4 12:08:38.400: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May  4 12:08:38.403: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"15654"},"items":null}

May  4 12:08:38.408: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"15654"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
May  4 12:08:38.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9647" for this suite. 05/04/23 12:08:38.462
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","completed":100,"skipped":2123,"failed":0}
------------------------------
• [SLOW TEST] [8.441 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:08:30.029
    May  4 12:08:30.029: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename daemonsets 05/04/23 12:08:30.03
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:08:30.05
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:08:30.06
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:193
    May  4 12:08:30.110: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 05/04/23 12:08:30.12
    May  4 12:08:30.124: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  4 12:08:30.124: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 05/04/23 12:08:30.124
    May  4 12:08:30.160: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  4 12:08:30.160: INFO: Node ip-10-0-1-224.us-west-2.compute.internal is running 0 daemon pod, expected 1
    May  4 12:08:31.164: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  4 12:08:31.164: INFO: Node ip-10-0-1-224.us-west-2.compute.internal is running 0 daemon pod, expected 1
    May  4 12:08:32.164: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May  4 12:08:32.164: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 05/04/23 12:08:32.168
    May  4 12:08:32.191: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May  4 12:08:32.191: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    May  4 12:08:33.196: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  4 12:08:33.196: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 05/04/23 12:08:33.196
    May  4 12:08:33.215: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  4 12:08:33.215: INFO: Node ip-10-0-1-224.us-west-2.compute.internal is running 0 daemon pod, expected 1
    May  4 12:08:34.223: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  4 12:08:34.223: INFO: Node ip-10-0-1-224.us-west-2.compute.internal is running 0 daemon pod, expected 1
    May  4 12:08:35.222: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  4 12:08:35.222: INFO: Node ip-10-0-1-224.us-west-2.compute.internal is running 0 daemon pod, expected 1
    May  4 12:08:36.221: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May  4 12:08:36.221: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 05/04/23 12:08:36.227
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9647, will wait for the garbage collector to delete the pods 05/04/23 12:08:36.227
    May  4 12:08:36.291: INFO: Deleting DaemonSet.extensions daemon-set took: 10.369967ms
    May  4 12:08:36.392: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.130235ms
    May  4 12:08:38.399: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  4 12:08:38.400: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    May  4 12:08:38.403: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"15654"},"items":null}

    May  4 12:08:38.408: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"15654"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    May  4 12:08:38.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-9647" for this suite. 05/04/23 12:08:38.462
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:08:38.474
May  4 12:08:38.474: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename services 05/04/23 12:08:38.475
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:08:38.495
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:08:38.5
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
STEP: creating service endpoint-test2 in namespace services-8434 05/04/23 12:08:38.508
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8434 to expose endpoints map[] 05/04/23 12:08:38.531
May  4 12:08:38.550: INFO: successfully validated that service endpoint-test2 in namespace services-8434 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-8434 05/04/23 12:08:38.55
May  4 12:08:38.571: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-8434" to be "running and ready"
May  4 12:08:38.588: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.704518ms
May  4 12:08:38.588: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May  4 12:08:40.595: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.023811297s
May  4 12:08:40.595: INFO: The phase of Pod pod1 is Running (Ready = true)
May  4 12:08:40.595: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8434 to expose endpoints map[pod1:[80]] 05/04/23 12:08:40.599
May  4 12:08:40.620: INFO: successfully validated that service endpoint-test2 in namespace services-8434 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 05/04/23 12:08:40.62
May  4 12:08:40.620: INFO: Creating new exec pod
May  4 12:08:40.629: INFO: Waiting up to 5m0s for pod "execpodnrr5p" in namespace "services-8434" to be "running"
May  4 12:08:40.632: INFO: Pod "execpodnrr5p": Phase="Pending", Reason="", readiness=false. Elapsed: 3.341274ms
May  4 12:08:42.636: INFO: Pod "execpodnrr5p": Phase="Running", Reason="", readiness=true. Elapsed: 2.007552021s
May  4 12:08:42.636: INFO: Pod "execpodnrr5p" satisfied condition "running"
May  4 12:08:43.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-8434 exec execpodnrr5p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
May  4 12:08:43.795: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
May  4 12:08:43.795: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  4 12:08:43.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-8434 exec execpodnrr5p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.21.169.115 80'
May  4 12:08:44.025: INFO: stderr: "+ nc -v -t -w 2 10.21.169.115 80\n+ echo hostName\nConnection to 10.21.169.115 80 port [tcp/http] succeeded!\n"
May  4 12:08:44.025: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-8434 05/04/23 12:08:44.026
May  4 12:08:44.037: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-8434" to be "running and ready"
May  4 12:08:44.042: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.661293ms
May  4 12:08:44.042: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May  4 12:08:46.047: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.010110827s
May  4 12:08:46.047: INFO: The phase of Pod pod2 is Running (Ready = true)
May  4 12:08:46.047: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8434 to expose endpoints map[pod1:[80] pod2:[80]] 05/04/23 12:08:46.051
May  4 12:08:46.070: INFO: successfully validated that service endpoint-test2 in namespace services-8434 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 05/04/23 12:08:46.07
May  4 12:08:47.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-8434 exec execpodnrr5p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
May  4 12:08:47.265: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
May  4 12:08:47.265: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  4 12:08:47.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-8434 exec execpodnrr5p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.21.169.115 80'
May  4 12:08:47.447: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.21.169.115 80\nConnection to 10.21.169.115 80 port [tcp/http] succeeded!\n"
May  4 12:08:47.447: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-8434 05/04/23 12:08:47.447
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8434 to expose endpoints map[pod2:[80]] 05/04/23 12:08:47.468
May  4 12:08:47.499: INFO: successfully validated that service endpoint-test2 in namespace services-8434 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 05/04/23 12:08:47.499
May  4 12:08:48.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-8434 exec execpodnrr5p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
May  4 12:08:48.705: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
May  4 12:08:48.705: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  4 12:08:48.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-8434 exec execpodnrr5p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.21.169.115 80'
May  4 12:08:48.964: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.21.169.115 80\nConnection to 10.21.169.115 80 port [tcp/http] succeeded!\n"
May  4 12:08:48.964: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-8434 05/04/23 12:08:48.964
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8434 to expose endpoints map[] 05/04/23 12:08:49.005
May  4 12:08:50.059: INFO: successfully validated that service endpoint-test2 in namespace services-8434 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  4 12:08:50.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8434" for this suite. 05/04/23 12:08:50.135
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","completed":101,"skipped":2159,"failed":0}
------------------------------
• [SLOW TEST] [11.680 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:08:38.474
    May  4 12:08:38.474: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename services 05/04/23 12:08:38.475
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:08:38.495
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:08:38.5
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:791
    STEP: creating service endpoint-test2 in namespace services-8434 05/04/23 12:08:38.508
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8434 to expose endpoints map[] 05/04/23 12:08:38.531
    May  4 12:08:38.550: INFO: successfully validated that service endpoint-test2 in namespace services-8434 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-8434 05/04/23 12:08:38.55
    May  4 12:08:38.571: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-8434" to be "running and ready"
    May  4 12:08:38.588: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.704518ms
    May  4 12:08:38.588: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    May  4 12:08:40.595: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.023811297s
    May  4 12:08:40.595: INFO: The phase of Pod pod1 is Running (Ready = true)
    May  4 12:08:40.595: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8434 to expose endpoints map[pod1:[80]] 05/04/23 12:08:40.599
    May  4 12:08:40.620: INFO: successfully validated that service endpoint-test2 in namespace services-8434 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 05/04/23 12:08:40.62
    May  4 12:08:40.620: INFO: Creating new exec pod
    May  4 12:08:40.629: INFO: Waiting up to 5m0s for pod "execpodnrr5p" in namespace "services-8434" to be "running"
    May  4 12:08:40.632: INFO: Pod "execpodnrr5p": Phase="Pending", Reason="", readiness=false. Elapsed: 3.341274ms
    May  4 12:08:42.636: INFO: Pod "execpodnrr5p": Phase="Running", Reason="", readiness=true. Elapsed: 2.007552021s
    May  4 12:08:42.636: INFO: Pod "execpodnrr5p" satisfied condition "running"
    May  4 12:08:43.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-8434 exec execpodnrr5p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    May  4 12:08:43.795: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    May  4 12:08:43.795: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  4 12:08:43.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-8434 exec execpodnrr5p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.21.169.115 80'
    May  4 12:08:44.025: INFO: stderr: "+ nc -v -t -w 2 10.21.169.115 80\n+ echo hostName\nConnection to 10.21.169.115 80 port [tcp/http] succeeded!\n"
    May  4 12:08:44.025: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Creating pod pod2 in namespace services-8434 05/04/23 12:08:44.026
    May  4 12:08:44.037: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-8434" to be "running and ready"
    May  4 12:08:44.042: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.661293ms
    May  4 12:08:44.042: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    May  4 12:08:46.047: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.010110827s
    May  4 12:08:46.047: INFO: The phase of Pod pod2 is Running (Ready = true)
    May  4 12:08:46.047: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8434 to expose endpoints map[pod1:[80] pod2:[80]] 05/04/23 12:08:46.051
    May  4 12:08:46.070: INFO: successfully validated that service endpoint-test2 in namespace services-8434 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 05/04/23 12:08:46.07
    May  4 12:08:47.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-8434 exec execpodnrr5p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    May  4 12:08:47.265: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    May  4 12:08:47.265: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  4 12:08:47.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-8434 exec execpodnrr5p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.21.169.115 80'
    May  4 12:08:47.447: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.21.169.115 80\nConnection to 10.21.169.115 80 port [tcp/http] succeeded!\n"
    May  4 12:08:47.447: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-8434 05/04/23 12:08:47.447
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8434 to expose endpoints map[pod2:[80]] 05/04/23 12:08:47.468
    May  4 12:08:47.499: INFO: successfully validated that service endpoint-test2 in namespace services-8434 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 05/04/23 12:08:47.499
    May  4 12:08:48.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-8434 exec execpodnrr5p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    May  4 12:08:48.705: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    May  4 12:08:48.705: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  4 12:08:48.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-8434 exec execpodnrr5p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.21.169.115 80'
    May  4 12:08:48.964: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.21.169.115 80\nConnection to 10.21.169.115 80 port [tcp/http] succeeded!\n"
    May  4 12:08:48.964: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod2 in namespace services-8434 05/04/23 12:08:48.964
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8434 to expose endpoints map[] 05/04/23 12:08:49.005
    May  4 12:08:50.059: INFO: successfully validated that service endpoint-test2 in namespace services-8434 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  4 12:08:50.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8434" for this suite. 05/04/23 12:08:50.135
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:08:50.155
May  4 12:08:50.155: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename subpath 05/04/23 12:08:50.156
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:08:50.187
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:08:50.191
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 05/04/23 12:08:50.195
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-mzl2 05/04/23 12:08:50.212
STEP: Creating a pod to test atomic-volume-subpath 05/04/23 12:08:50.212
May  4 12:08:50.227: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-mzl2" in namespace "subpath-3065" to be "Succeeded or Failed"
May  4 12:08:50.233: INFO: Pod "pod-subpath-test-projected-mzl2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.839947ms
May  4 12:08:52.239: INFO: Pod "pod-subpath-test-projected-mzl2": Phase="Running", Reason="", readiness=true. Elapsed: 2.011986811s
May  4 12:08:54.238: INFO: Pod "pod-subpath-test-projected-mzl2": Phase="Running", Reason="", readiness=true. Elapsed: 4.011501846s
May  4 12:08:56.239: INFO: Pod "pod-subpath-test-projected-mzl2": Phase="Running", Reason="", readiness=true. Elapsed: 6.012151239s
May  4 12:08:58.239: INFO: Pod "pod-subpath-test-projected-mzl2": Phase="Running", Reason="", readiness=true. Elapsed: 8.012287668s
May  4 12:09:00.238: INFO: Pod "pod-subpath-test-projected-mzl2": Phase="Running", Reason="", readiness=true. Elapsed: 10.011566478s
May  4 12:09:02.246: INFO: Pod "pod-subpath-test-projected-mzl2": Phase="Running", Reason="", readiness=true. Elapsed: 12.019175141s
May  4 12:09:04.241: INFO: Pod "pod-subpath-test-projected-mzl2": Phase="Running", Reason="", readiness=true. Elapsed: 14.013809266s
May  4 12:09:06.238: INFO: Pod "pod-subpath-test-projected-mzl2": Phase="Running", Reason="", readiness=true. Elapsed: 16.011556583s
May  4 12:09:08.241: INFO: Pod "pod-subpath-test-projected-mzl2": Phase="Running", Reason="", readiness=true. Elapsed: 18.013747627s
May  4 12:09:10.238: INFO: Pod "pod-subpath-test-projected-mzl2": Phase="Running", Reason="", readiness=true. Elapsed: 20.011475716s
May  4 12:09:12.238: INFO: Pod "pod-subpath-test-projected-mzl2": Phase="Running", Reason="", readiness=false. Elapsed: 22.011089156s
May  4 12:09:14.240: INFO: Pod "pod-subpath-test-projected-mzl2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.013646346s
STEP: Saw pod success 05/04/23 12:09:14.24
May  4 12:09:14.241: INFO: Pod "pod-subpath-test-projected-mzl2" satisfied condition "Succeeded or Failed"
May  4 12:09:14.245: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod pod-subpath-test-projected-mzl2 container test-container-subpath-projected-mzl2: <nil>
STEP: delete the pod 05/04/23 12:09:14.269
May  4 12:09:14.295: INFO: Waiting for pod pod-subpath-test-projected-mzl2 to disappear
May  4 12:09:14.300: INFO: Pod pod-subpath-test-projected-mzl2 no longer exists
STEP: Deleting pod pod-subpath-test-projected-mzl2 05/04/23 12:09:14.3
May  4 12:09:14.300: INFO: Deleting pod "pod-subpath-test-projected-mzl2" in namespace "subpath-3065"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
May  4 12:09:14.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3065" for this suite. 05/04/23 12:09:14.327
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","completed":102,"skipped":2165,"failed":0}
------------------------------
• [SLOW TEST] [24.190 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:08:50.155
    May  4 12:08:50.155: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename subpath 05/04/23 12:08:50.156
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:08:50.187
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:08:50.191
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 05/04/23 12:08:50.195
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-mzl2 05/04/23 12:08:50.212
    STEP: Creating a pod to test atomic-volume-subpath 05/04/23 12:08:50.212
    May  4 12:08:50.227: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-mzl2" in namespace "subpath-3065" to be "Succeeded or Failed"
    May  4 12:08:50.233: INFO: Pod "pod-subpath-test-projected-mzl2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.839947ms
    May  4 12:08:52.239: INFO: Pod "pod-subpath-test-projected-mzl2": Phase="Running", Reason="", readiness=true. Elapsed: 2.011986811s
    May  4 12:08:54.238: INFO: Pod "pod-subpath-test-projected-mzl2": Phase="Running", Reason="", readiness=true. Elapsed: 4.011501846s
    May  4 12:08:56.239: INFO: Pod "pod-subpath-test-projected-mzl2": Phase="Running", Reason="", readiness=true. Elapsed: 6.012151239s
    May  4 12:08:58.239: INFO: Pod "pod-subpath-test-projected-mzl2": Phase="Running", Reason="", readiness=true. Elapsed: 8.012287668s
    May  4 12:09:00.238: INFO: Pod "pod-subpath-test-projected-mzl2": Phase="Running", Reason="", readiness=true. Elapsed: 10.011566478s
    May  4 12:09:02.246: INFO: Pod "pod-subpath-test-projected-mzl2": Phase="Running", Reason="", readiness=true. Elapsed: 12.019175141s
    May  4 12:09:04.241: INFO: Pod "pod-subpath-test-projected-mzl2": Phase="Running", Reason="", readiness=true. Elapsed: 14.013809266s
    May  4 12:09:06.238: INFO: Pod "pod-subpath-test-projected-mzl2": Phase="Running", Reason="", readiness=true. Elapsed: 16.011556583s
    May  4 12:09:08.241: INFO: Pod "pod-subpath-test-projected-mzl2": Phase="Running", Reason="", readiness=true. Elapsed: 18.013747627s
    May  4 12:09:10.238: INFO: Pod "pod-subpath-test-projected-mzl2": Phase="Running", Reason="", readiness=true. Elapsed: 20.011475716s
    May  4 12:09:12.238: INFO: Pod "pod-subpath-test-projected-mzl2": Phase="Running", Reason="", readiness=false. Elapsed: 22.011089156s
    May  4 12:09:14.240: INFO: Pod "pod-subpath-test-projected-mzl2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.013646346s
    STEP: Saw pod success 05/04/23 12:09:14.24
    May  4 12:09:14.241: INFO: Pod "pod-subpath-test-projected-mzl2" satisfied condition "Succeeded or Failed"
    May  4 12:09:14.245: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod pod-subpath-test-projected-mzl2 container test-container-subpath-projected-mzl2: <nil>
    STEP: delete the pod 05/04/23 12:09:14.269
    May  4 12:09:14.295: INFO: Waiting for pod pod-subpath-test-projected-mzl2 to disappear
    May  4 12:09:14.300: INFO: Pod pod-subpath-test-projected-mzl2 no longer exists
    STEP: Deleting pod pod-subpath-test-projected-mzl2 05/04/23 12:09:14.3
    May  4 12:09:14.300: INFO: Deleting pod "pod-subpath-test-projected-mzl2" in namespace "subpath-3065"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    May  4 12:09:14.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-3065" for this suite. 05/04/23 12:09:14.327
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:09:14.347
May  4 12:09:14.347: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename webhook 05/04/23 12:09:14.348
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:09:14.367
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:09:14.37
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/04/23 12:09:14.393
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/04/23 12:09:15.023
STEP: Deploying the webhook pod 05/04/23 12:09:15.035
STEP: Wait for the deployment to be ready 05/04/23 12:09:15.056
May  4 12:09:15.074: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/04/23 12:09:17.091
STEP: Verifying the service has paired with the endpoint 05/04/23 12:09:17.107
May  4 12:09:18.110: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
STEP: Registering the webhook via the AdmissionRegistration API 05/04/23 12:09:18.116
STEP: create a pod that should be denied by the webhook 05/04/23 12:09:18.141
STEP: create a pod that causes the webhook to hang 05/04/23 12:09:18.153
STEP: create a configmap that should be denied by the webhook 05/04/23 12:09:28.161
STEP: create a configmap that should be admitted by the webhook 05/04/23 12:09:28.206
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 05/04/23 12:09:28.217
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 05/04/23 12:09:28.231
STEP: create a namespace that bypass the webhook 05/04/23 12:09:28.241
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 05/04/23 12:09:28.262
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  4 12:09:28.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2521" for this suite. 05/04/23 12:09:28.317
STEP: Destroying namespace "webhook-2521-markers" for this suite. 05/04/23 12:09:28.325
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","completed":103,"skipped":2207,"failed":0}
------------------------------
• [SLOW TEST] [14.103 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:09:14.347
    May  4 12:09:14.347: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename webhook 05/04/23 12:09:14.348
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:09:14.367
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:09:14.37
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/04/23 12:09:14.393
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/04/23 12:09:15.023
    STEP: Deploying the webhook pod 05/04/23 12:09:15.035
    STEP: Wait for the deployment to be ready 05/04/23 12:09:15.056
    May  4 12:09:15.074: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/04/23 12:09:17.091
    STEP: Verifying the service has paired with the endpoint 05/04/23 12:09:17.107
    May  4 12:09:18.110: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:196
    STEP: Registering the webhook via the AdmissionRegistration API 05/04/23 12:09:18.116
    STEP: create a pod that should be denied by the webhook 05/04/23 12:09:18.141
    STEP: create a pod that causes the webhook to hang 05/04/23 12:09:18.153
    STEP: create a configmap that should be denied by the webhook 05/04/23 12:09:28.161
    STEP: create a configmap that should be admitted by the webhook 05/04/23 12:09:28.206
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 05/04/23 12:09:28.217
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 05/04/23 12:09:28.231
    STEP: create a namespace that bypass the webhook 05/04/23 12:09:28.241
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 05/04/23 12:09:28.262
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  4 12:09:28.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2521" for this suite. 05/04/23 12:09:28.317
    STEP: Destroying namespace "webhook-2521-markers" for this suite. 05/04/23 12:09:28.325
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:09:28.452
May  4 12:09:28.458: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename services 05/04/23 12:09:28.459
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:09:28.495
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:09:28.502
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  4 12:09:28.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2485" for this suite. 05/04/23 12:09:28.526
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","completed":104,"skipped":2222,"failed":0}
------------------------------
• [0.088 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:09:28.452
    May  4 12:09:28.458: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename services 05/04/23 12:09:28.459
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:09:28.495
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:09:28.502
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:781
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  4 12:09:28.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2485" for this suite. 05/04/23 12:09:28.526
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:09:28.542
May  4 12:09:28.542: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename services 05/04/23 12:09:28.543
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:09:28.57
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:09:28.574
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
STEP: creating a collection of services 05/04/23 12:09:28.582
May  4 12:09:28.582: INFO: Creating e2e-svc-a-bklnd
May  4 12:09:28.603: INFO: Creating e2e-svc-b-jh6hj
May  4 12:09:28.622: INFO: Creating e2e-svc-c-qjkxv
STEP: deleting service collection 05/04/23 12:09:28.667
May  4 12:09:28.781: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  4 12:09:28.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5073" for this suite. 05/04/23 12:09:28.8
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","completed":105,"skipped":2242,"failed":0}
------------------------------
• [0.274 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:09:28.542
    May  4 12:09:28.542: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename services 05/04/23 12:09:28.543
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:09:28.57
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:09:28.574
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3641
    STEP: creating a collection of services 05/04/23 12:09:28.582
    May  4 12:09:28.582: INFO: Creating e2e-svc-a-bklnd
    May  4 12:09:28.603: INFO: Creating e2e-svc-b-jh6hj
    May  4 12:09:28.622: INFO: Creating e2e-svc-c-qjkxv
    STEP: deleting service collection 05/04/23 12:09:28.667
    May  4 12:09:28.781: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  4 12:09:28.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5073" for this suite. 05/04/23 12:09:28.8
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:09:28.822
May  4 12:09:28.823: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename cronjob 05/04/23 12:09:28.823
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:09:28.852
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:09:28.855
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 05/04/23 12:09:28.864
STEP: Ensuring a job is scheduled 05/04/23 12:09:28.874
STEP: Ensuring exactly one is scheduled 05/04/23 12:10:00.878
STEP: Ensuring exactly one running job exists by listing jobs explicitly 05/04/23 12:10:00.882
STEP: Ensuring the job is replaced with a new one 05/04/23 12:10:00.888
STEP: Removing cronjob 05/04/23 12:11:00.894
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
May  4 12:11:00.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6419" for this suite. 05/04/23 12:11:00.925
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","completed":106,"skipped":2284,"failed":0}
------------------------------
• [SLOW TEST] [92.123 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:09:28.822
    May  4 12:09:28.823: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename cronjob 05/04/23 12:09:28.823
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:09:28.852
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:09:28.855
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 05/04/23 12:09:28.864
    STEP: Ensuring a job is scheduled 05/04/23 12:09:28.874
    STEP: Ensuring exactly one is scheduled 05/04/23 12:10:00.878
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 05/04/23 12:10:00.882
    STEP: Ensuring the job is replaced with a new one 05/04/23 12:10:00.888
    STEP: Removing cronjob 05/04/23 12:11:00.894
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    May  4 12:11:00.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-6419" for this suite. 05/04/23 12:11:00.925
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:11:00.947
May  4 12:11:00.947: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename podtemplate 05/04/23 12:11:00.947
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:11:01.003
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:11:01.01
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 05/04/23 12:11:01.018
May  4 12:11:01.029: INFO: created test-podtemplate-1
May  4 12:11:01.039: INFO: created test-podtemplate-2
May  4 12:11:01.047: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 05/04/23 12:11:01.047
STEP: delete collection of pod templates 05/04/23 12:11:01.056
May  4 12:11:01.056: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 05/04/23 12:11:01.082
May  4 12:11:01.082: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
May  4 12:11:01.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-7544" for this suite. 05/04/23 12:11:01.102
{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","completed":107,"skipped":2307,"failed":0}
------------------------------
• [0.164 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:11:00.947
    May  4 12:11:00.947: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename podtemplate 05/04/23 12:11:00.947
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:11:01.003
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:11:01.01
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 05/04/23 12:11:01.018
    May  4 12:11:01.029: INFO: created test-podtemplate-1
    May  4 12:11:01.039: INFO: created test-podtemplate-2
    May  4 12:11:01.047: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 05/04/23 12:11:01.047
    STEP: delete collection of pod templates 05/04/23 12:11:01.056
    May  4 12:11:01.056: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 05/04/23 12:11:01.082
    May  4 12:11:01.082: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    May  4 12:11:01.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-7544" for this suite. 05/04/23 12:11:01.102
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:11:01.111
May  4 12:11:01.112: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename dns 05/04/23 12:11:01.113
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:11:01.143
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:11:01.151
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 05/04/23 12:11:01.155
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8975.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-8975.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 05/04/23 12:11:01.163
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8975.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-8975.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 05/04/23 12:11:01.164
STEP: creating a pod to probe DNS 05/04/23 12:11:01.164
STEP: submitting the pod to kubernetes 05/04/23 12:11:01.164
May  4 12:11:01.183: INFO: Waiting up to 15m0s for pod "dns-test-900cb939-1782-45fd-81bf-9906d3bb9bc3" in namespace "dns-8975" to be "running"
May  4 12:11:01.197: INFO: Pod "dns-test-900cb939-1782-45fd-81bf-9906d3bb9bc3": Phase="Pending", Reason="", readiness=false. Elapsed: 13.650052ms
May  4 12:11:03.202: INFO: Pod "dns-test-900cb939-1782-45fd-81bf-9906d3bb9bc3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018888688s
May  4 12:11:05.203: INFO: Pod "dns-test-900cb939-1782-45fd-81bf-9906d3bb9bc3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019311763s
May  4 12:11:07.202: INFO: Pod "dns-test-900cb939-1782-45fd-81bf-9906d3bb9bc3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018926963s
May  4 12:11:09.203: INFO: Pod "dns-test-900cb939-1782-45fd-81bf-9906d3bb9bc3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.01933119s
May  4 12:11:11.201: INFO: Pod "dns-test-900cb939-1782-45fd-81bf-9906d3bb9bc3": Phase="Running", Reason="", readiness=true. Elapsed: 10.018085019s
May  4 12:11:11.202: INFO: Pod "dns-test-900cb939-1782-45fd-81bf-9906d3bb9bc3" satisfied condition "running"
STEP: retrieving the pod 05/04/23 12:11:11.202
STEP: looking for the results for each expected name from probers 05/04/23 12:11:11.206
May  4 12:11:11.223: INFO: DNS probes using dns-8975/dns-test-900cb939-1782-45fd-81bf-9906d3bb9bc3 succeeded

STEP: deleting the pod 05/04/23 12:11:11.223
STEP: deleting the test headless service 05/04/23 12:11:11.263
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
May  4 12:11:11.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8975" for this suite. 05/04/23 12:11:11.293
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","completed":108,"skipped":2316,"failed":0}
------------------------------
• [SLOW TEST] [10.190 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:11:01.111
    May  4 12:11:01.112: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename dns 05/04/23 12:11:01.113
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:11:01.143
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:11:01.151
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 05/04/23 12:11:01.155
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8975.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-8975.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     05/04/23 12:11:01.163
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8975.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-8975.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     05/04/23 12:11:01.164
    STEP: creating a pod to probe DNS 05/04/23 12:11:01.164
    STEP: submitting the pod to kubernetes 05/04/23 12:11:01.164
    May  4 12:11:01.183: INFO: Waiting up to 15m0s for pod "dns-test-900cb939-1782-45fd-81bf-9906d3bb9bc3" in namespace "dns-8975" to be "running"
    May  4 12:11:01.197: INFO: Pod "dns-test-900cb939-1782-45fd-81bf-9906d3bb9bc3": Phase="Pending", Reason="", readiness=false. Elapsed: 13.650052ms
    May  4 12:11:03.202: INFO: Pod "dns-test-900cb939-1782-45fd-81bf-9906d3bb9bc3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018888688s
    May  4 12:11:05.203: INFO: Pod "dns-test-900cb939-1782-45fd-81bf-9906d3bb9bc3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019311763s
    May  4 12:11:07.202: INFO: Pod "dns-test-900cb939-1782-45fd-81bf-9906d3bb9bc3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018926963s
    May  4 12:11:09.203: INFO: Pod "dns-test-900cb939-1782-45fd-81bf-9906d3bb9bc3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.01933119s
    May  4 12:11:11.201: INFO: Pod "dns-test-900cb939-1782-45fd-81bf-9906d3bb9bc3": Phase="Running", Reason="", readiness=true. Elapsed: 10.018085019s
    May  4 12:11:11.202: INFO: Pod "dns-test-900cb939-1782-45fd-81bf-9906d3bb9bc3" satisfied condition "running"
    STEP: retrieving the pod 05/04/23 12:11:11.202
    STEP: looking for the results for each expected name from probers 05/04/23 12:11:11.206
    May  4 12:11:11.223: INFO: DNS probes using dns-8975/dns-test-900cb939-1782-45fd-81bf-9906d3bb9bc3 succeeded

    STEP: deleting the pod 05/04/23 12:11:11.223
    STEP: deleting the test headless service 05/04/23 12:11:11.263
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    May  4 12:11:11.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-8975" for this suite. 05/04/23 12:11:11.293
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:11:11.304
May  4 12:11:11.304: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename gc 05/04/23 12:11:11.305
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:11:11.325
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:11:11.329
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 05/04/23 12:11:11.342
STEP: delete the rc 05/04/23 12:11:16.375
STEP: wait for the rc to be deleted 05/04/23 12:11:16.384
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 05/04/23 12:11:21.39
STEP: Gathering metrics 05/04/23 12:11:51.424
W0504 12:11:51.445059      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
May  4 12:11:51.445: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

May  4 12:11:51.445: INFO: Deleting pod "simpletest.rc-25nmk" in namespace "gc-5497"
May  4 12:11:51.462: INFO: Deleting pod "simpletest.rc-279l9" in namespace "gc-5497"
May  4 12:11:51.496: INFO: Deleting pod "simpletest.rc-2s286" in namespace "gc-5497"
May  4 12:11:51.520: INFO: Deleting pod "simpletest.rc-4jxr8" in namespace "gc-5497"
May  4 12:11:51.533: INFO: Deleting pod "simpletest.rc-542qn" in namespace "gc-5497"
May  4 12:11:51.552: INFO: Deleting pod "simpletest.rc-546rf" in namespace "gc-5497"
May  4 12:11:51.569: INFO: Deleting pod "simpletest.rc-58dps" in namespace "gc-5497"
May  4 12:11:51.601: INFO: Deleting pod "simpletest.rc-5bznw" in namespace "gc-5497"
May  4 12:11:51.619: INFO: Deleting pod "simpletest.rc-5d9vr" in namespace "gc-5497"
May  4 12:11:51.640: INFO: Deleting pod "simpletest.rc-5t6lp" in namespace "gc-5497"
May  4 12:11:51.657: INFO: Deleting pod "simpletest.rc-627kd" in namespace "gc-5497"
May  4 12:11:51.673: INFO: Deleting pod "simpletest.rc-65l4b" in namespace "gc-5497"
May  4 12:11:51.692: INFO: Deleting pod "simpletest.rc-6fr6s" in namespace "gc-5497"
May  4 12:11:51.708: INFO: Deleting pod "simpletest.rc-6jh42" in namespace "gc-5497"
May  4 12:11:51.720: INFO: Deleting pod "simpletest.rc-6mcxq" in namespace "gc-5497"
May  4 12:11:51.734: INFO: Deleting pod "simpletest.rc-7st52" in namespace "gc-5497"
May  4 12:11:51.750: INFO: Deleting pod "simpletest.rc-8528m" in namespace "gc-5497"
May  4 12:11:51.772: INFO: Deleting pod "simpletest.rc-8bfzt" in namespace "gc-5497"
May  4 12:11:51.790: INFO: Deleting pod "simpletest.rc-9d58h" in namespace "gc-5497"
May  4 12:11:51.805: INFO: Deleting pod "simpletest.rc-9d5mg" in namespace "gc-5497"
May  4 12:11:51.825: INFO: Deleting pod "simpletest.rc-9d8qq" in namespace "gc-5497"
May  4 12:11:51.840: INFO: Deleting pod "simpletest.rc-9m6qb" in namespace "gc-5497"
May  4 12:11:51.860: INFO: Deleting pod "simpletest.rc-9mmjl" in namespace "gc-5497"
May  4 12:11:51.879: INFO: Deleting pod "simpletest.rc-9zm6w" in namespace "gc-5497"
May  4 12:11:51.899: INFO: Deleting pod "simpletest.rc-b4h4z" in namespace "gc-5497"
May  4 12:11:51.914: INFO: Deleting pod "simpletest.rc-bhmms" in namespace "gc-5497"
May  4 12:11:51.932: INFO: Deleting pod "simpletest.rc-bj8dz" in namespace "gc-5497"
May  4 12:11:51.948: INFO: Deleting pod "simpletest.rc-bk2mn" in namespace "gc-5497"
May  4 12:11:51.962: INFO: Deleting pod "simpletest.rc-blf6c" in namespace "gc-5497"
May  4 12:11:51.981: INFO: Deleting pod "simpletest.rc-bp7pt" in namespace "gc-5497"
May  4 12:11:52.000: INFO: Deleting pod "simpletest.rc-bzlgc" in namespace "gc-5497"
May  4 12:11:52.014: INFO: Deleting pod "simpletest.rc-c5d5j" in namespace "gc-5497"
May  4 12:11:52.034: INFO: Deleting pod "simpletest.rc-c5w68" in namespace "gc-5497"
May  4 12:11:52.065: INFO: Deleting pod "simpletest.rc-c7w7x" in namespace "gc-5497"
May  4 12:11:52.099: INFO: Deleting pod "simpletest.rc-d7b9j" in namespace "gc-5497"
May  4 12:11:52.118: INFO: Deleting pod "simpletest.rc-dcg65" in namespace "gc-5497"
May  4 12:11:52.137: INFO: Deleting pod "simpletest.rc-dk8fl" in namespace "gc-5497"
May  4 12:11:52.157: INFO: Deleting pod "simpletest.rc-dqrx9" in namespace "gc-5497"
May  4 12:11:52.176: INFO: Deleting pod "simpletest.rc-f4mhp" in namespace "gc-5497"
May  4 12:11:52.200: INFO: Deleting pod "simpletest.rc-fjnz8" in namespace "gc-5497"
May  4 12:11:52.216: INFO: Deleting pod "simpletest.rc-fs6sm" in namespace "gc-5497"
May  4 12:11:52.234: INFO: Deleting pod "simpletest.rc-ftbtk" in namespace "gc-5497"
May  4 12:11:52.263: INFO: Deleting pod "simpletest.rc-g66m9" in namespace "gc-5497"
May  4 12:11:52.297: INFO: Deleting pod "simpletest.rc-g7g57" in namespace "gc-5497"
May  4 12:11:52.322: INFO: Deleting pod "simpletest.rc-gfc8m" in namespace "gc-5497"
May  4 12:11:52.343: INFO: Deleting pod "simpletest.rc-gsstn" in namespace "gc-5497"
May  4 12:11:52.366: INFO: Deleting pod "simpletest.rc-gxhq2" in namespace "gc-5497"
May  4 12:11:52.395: INFO: Deleting pod "simpletest.rc-hbw66" in namespace "gc-5497"
May  4 12:11:52.433: INFO: Deleting pod "simpletest.rc-hns6k" in namespace "gc-5497"
May  4 12:11:52.456: INFO: Deleting pod "simpletest.rc-j4bzt" in namespace "gc-5497"
May  4 12:11:52.475: INFO: Deleting pod "simpletest.rc-jrrz6" in namespace "gc-5497"
May  4 12:11:52.497: INFO: Deleting pod "simpletest.rc-kdscp" in namespace "gc-5497"
May  4 12:11:52.514: INFO: Deleting pod "simpletest.rc-ktsq4" in namespace "gc-5497"
May  4 12:11:52.531: INFO: Deleting pod "simpletest.rc-l7s2x" in namespace "gc-5497"
May  4 12:11:52.579: INFO: Deleting pod "simpletest.rc-l7skx" in namespace "gc-5497"
May  4 12:11:52.593: INFO: Deleting pod "simpletest.rc-ldhg7" in namespace "gc-5497"
May  4 12:11:52.609: INFO: Deleting pod "simpletest.rc-llggv" in namespace "gc-5497"
May  4 12:11:52.621: INFO: Deleting pod "simpletest.rc-lxl8d" in namespace "gc-5497"
May  4 12:11:52.639: INFO: Deleting pod "simpletest.rc-m52l7" in namespace "gc-5497"
May  4 12:11:52.654: INFO: Deleting pod "simpletest.rc-m6nff" in namespace "gc-5497"
May  4 12:11:52.672: INFO: Deleting pod "simpletest.rc-m8nqm" in namespace "gc-5497"
May  4 12:11:52.692: INFO: Deleting pod "simpletest.rc-mpkv5" in namespace "gc-5497"
May  4 12:11:52.706: INFO: Deleting pod "simpletest.rc-msw5d" in namespace "gc-5497"
May  4 12:11:52.730: INFO: Deleting pod "simpletest.rc-mz675" in namespace "gc-5497"
May  4 12:11:52.753: INFO: Deleting pod "simpletest.rc-n8ffk" in namespace "gc-5497"
May  4 12:11:52.771: INFO: Deleting pod "simpletest.rc-nqwqv" in namespace "gc-5497"
May  4 12:11:52.787: INFO: Deleting pod "simpletest.rc-nxxgt" in namespace "gc-5497"
May  4 12:11:52.802: INFO: Deleting pod "simpletest.rc-p4fql" in namespace "gc-5497"
May  4 12:11:52.821: INFO: Deleting pod "simpletest.rc-pdkrh" in namespace "gc-5497"
May  4 12:11:52.833: INFO: Deleting pod "simpletest.rc-phhjq" in namespace "gc-5497"
May  4 12:11:52.853: INFO: Deleting pod "simpletest.rc-pphb7" in namespace "gc-5497"
May  4 12:11:52.866: INFO: Deleting pod "simpletest.rc-pxb88" in namespace "gc-5497"
May  4 12:11:52.891: INFO: Deleting pod "simpletest.rc-q7lzh" in namespace "gc-5497"
May  4 12:11:52.907: INFO: Deleting pod "simpletest.rc-q8q8x" in namespace "gc-5497"
May  4 12:11:52.920: INFO: Deleting pod "simpletest.rc-qg9p2" in namespace "gc-5497"
May  4 12:11:52.937: INFO: Deleting pod "simpletest.rc-qhtcs" in namespace "gc-5497"
May  4 12:11:52.954: INFO: Deleting pod "simpletest.rc-qrrhl" in namespace "gc-5497"
May  4 12:11:52.966: INFO: Deleting pod "simpletest.rc-r2cqh" in namespace "gc-5497"
May  4 12:11:52.991: INFO: Deleting pod "simpletest.rc-s5crn" in namespace "gc-5497"
May  4 12:11:53.007: INFO: Deleting pod "simpletest.rc-s6tdj" in namespace "gc-5497"
May  4 12:11:53.021: INFO: Deleting pod "simpletest.rc-skt2d" in namespace "gc-5497"
May  4 12:11:53.065: INFO: Deleting pod "simpletest.rc-smtrx" in namespace "gc-5497"
May  4 12:11:53.105: INFO: Deleting pod "simpletest.rc-st57w" in namespace "gc-5497"
May  4 12:11:53.157: INFO: Deleting pod "simpletest.rc-tcx2c" in namespace "gc-5497"
May  4 12:11:53.214: INFO: Deleting pod "simpletest.rc-tgxpn" in namespace "gc-5497"
May  4 12:11:53.260: INFO: Deleting pod "simpletest.rc-tjd99" in namespace "gc-5497"
May  4 12:11:53.313: INFO: Deleting pod "simpletest.rc-tvvsm" in namespace "gc-5497"
May  4 12:11:53.361: INFO: Deleting pod "simpletest.rc-txwx9" in namespace "gc-5497"
May  4 12:11:53.404: INFO: Deleting pod "simpletest.rc-vblht" in namespace "gc-5497"
May  4 12:11:53.459: INFO: Deleting pod "simpletest.rc-vg2tn" in namespace "gc-5497"
May  4 12:11:53.509: INFO: Deleting pod "simpletest.rc-vv4mq" in namespace "gc-5497"
May  4 12:11:53.564: INFO: Deleting pod "simpletest.rc-wfccr" in namespace "gc-5497"
May  4 12:11:53.609: INFO: Deleting pod "simpletest.rc-wpwcg" in namespace "gc-5497"
May  4 12:11:53.664: INFO: Deleting pod "simpletest.rc-wrfxp" in namespace "gc-5497"
May  4 12:11:53.704: INFO: Deleting pod "simpletest.rc-wt7kf" in namespace "gc-5497"
May  4 12:11:53.789: INFO: Deleting pod "simpletest.rc-wv4gb" in namespace "gc-5497"
May  4 12:11:53.808: INFO: Deleting pod "simpletest.rc-x6gnk" in namespace "gc-5497"
May  4 12:11:53.861: INFO: Deleting pod "simpletest.rc-xccsb" in namespace "gc-5497"
May  4 12:11:53.918: INFO: Deleting pod "simpletest.rc-xsx2x" in namespace "gc-5497"
May  4 12:11:53.973: INFO: Deleting pod "simpletest.rc-z9k4k" in namespace "gc-5497"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
May  4 12:11:54.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5497" for this suite. 05/04/23 12:11:54.052
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","completed":109,"skipped":2332,"failed":0}
------------------------------
• [SLOW TEST] [42.796 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:11:11.304
    May  4 12:11:11.304: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename gc 05/04/23 12:11:11.305
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:11:11.325
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:11:11.329
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 05/04/23 12:11:11.342
    STEP: delete the rc 05/04/23 12:11:16.375
    STEP: wait for the rc to be deleted 05/04/23 12:11:16.384
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 05/04/23 12:11:21.39
    STEP: Gathering metrics 05/04/23 12:11:51.424
    W0504 12:11:51.445059      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    May  4 12:11:51.445: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    May  4 12:11:51.445: INFO: Deleting pod "simpletest.rc-25nmk" in namespace "gc-5497"
    May  4 12:11:51.462: INFO: Deleting pod "simpletest.rc-279l9" in namespace "gc-5497"
    May  4 12:11:51.496: INFO: Deleting pod "simpletest.rc-2s286" in namespace "gc-5497"
    May  4 12:11:51.520: INFO: Deleting pod "simpletest.rc-4jxr8" in namespace "gc-5497"
    May  4 12:11:51.533: INFO: Deleting pod "simpletest.rc-542qn" in namespace "gc-5497"
    May  4 12:11:51.552: INFO: Deleting pod "simpletest.rc-546rf" in namespace "gc-5497"
    May  4 12:11:51.569: INFO: Deleting pod "simpletest.rc-58dps" in namespace "gc-5497"
    May  4 12:11:51.601: INFO: Deleting pod "simpletest.rc-5bznw" in namespace "gc-5497"
    May  4 12:11:51.619: INFO: Deleting pod "simpletest.rc-5d9vr" in namespace "gc-5497"
    May  4 12:11:51.640: INFO: Deleting pod "simpletest.rc-5t6lp" in namespace "gc-5497"
    May  4 12:11:51.657: INFO: Deleting pod "simpletest.rc-627kd" in namespace "gc-5497"
    May  4 12:11:51.673: INFO: Deleting pod "simpletest.rc-65l4b" in namespace "gc-5497"
    May  4 12:11:51.692: INFO: Deleting pod "simpletest.rc-6fr6s" in namespace "gc-5497"
    May  4 12:11:51.708: INFO: Deleting pod "simpletest.rc-6jh42" in namespace "gc-5497"
    May  4 12:11:51.720: INFO: Deleting pod "simpletest.rc-6mcxq" in namespace "gc-5497"
    May  4 12:11:51.734: INFO: Deleting pod "simpletest.rc-7st52" in namespace "gc-5497"
    May  4 12:11:51.750: INFO: Deleting pod "simpletest.rc-8528m" in namespace "gc-5497"
    May  4 12:11:51.772: INFO: Deleting pod "simpletest.rc-8bfzt" in namespace "gc-5497"
    May  4 12:11:51.790: INFO: Deleting pod "simpletest.rc-9d58h" in namespace "gc-5497"
    May  4 12:11:51.805: INFO: Deleting pod "simpletest.rc-9d5mg" in namespace "gc-5497"
    May  4 12:11:51.825: INFO: Deleting pod "simpletest.rc-9d8qq" in namespace "gc-5497"
    May  4 12:11:51.840: INFO: Deleting pod "simpletest.rc-9m6qb" in namespace "gc-5497"
    May  4 12:11:51.860: INFO: Deleting pod "simpletest.rc-9mmjl" in namespace "gc-5497"
    May  4 12:11:51.879: INFO: Deleting pod "simpletest.rc-9zm6w" in namespace "gc-5497"
    May  4 12:11:51.899: INFO: Deleting pod "simpletest.rc-b4h4z" in namespace "gc-5497"
    May  4 12:11:51.914: INFO: Deleting pod "simpletest.rc-bhmms" in namespace "gc-5497"
    May  4 12:11:51.932: INFO: Deleting pod "simpletest.rc-bj8dz" in namespace "gc-5497"
    May  4 12:11:51.948: INFO: Deleting pod "simpletest.rc-bk2mn" in namespace "gc-5497"
    May  4 12:11:51.962: INFO: Deleting pod "simpletest.rc-blf6c" in namespace "gc-5497"
    May  4 12:11:51.981: INFO: Deleting pod "simpletest.rc-bp7pt" in namespace "gc-5497"
    May  4 12:11:52.000: INFO: Deleting pod "simpletest.rc-bzlgc" in namespace "gc-5497"
    May  4 12:11:52.014: INFO: Deleting pod "simpletest.rc-c5d5j" in namespace "gc-5497"
    May  4 12:11:52.034: INFO: Deleting pod "simpletest.rc-c5w68" in namespace "gc-5497"
    May  4 12:11:52.065: INFO: Deleting pod "simpletest.rc-c7w7x" in namespace "gc-5497"
    May  4 12:11:52.099: INFO: Deleting pod "simpletest.rc-d7b9j" in namespace "gc-5497"
    May  4 12:11:52.118: INFO: Deleting pod "simpletest.rc-dcg65" in namespace "gc-5497"
    May  4 12:11:52.137: INFO: Deleting pod "simpletest.rc-dk8fl" in namespace "gc-5497"
    May  4 12:11:52.157: INFO: Deleting pod "simpletest.rc-dqrx9" in namespace "gc-5497"
    May  4 12:11:52.176: INFO: Deleting pod "simpletest.rc-f4mhp" in namespace "gc-5497"
    May  4 12:11:52.200: INFO: Deleting pod "simpletest.rc-fjnz8" in namespace "gc-5497"
    May  4 12:11:52.216: INFO: Deleting pod "simpletest.rc-fs6sm" in namespace "gc-5497"
    May  4 12:11:52.234: INFO: Deleting pod "simpletest.rc-ftbtk" in namespace "gc-5497"
    May  4 12:11:52.263: INFO: Deleting pod "simpletest.rc-g66m9" in namespace "gc-5497"
    May  4 12:11:52.297: INFO: Deleting pod "simpletest.rc-g7g57" in namespace "gc-5497"
    May  4 12:11:52.322: INFO: Deleting pod "simpletest.rc-gfc8m" in namespace "gc-5497"
    May  4 12:11:52.343: INFO: Deleting pod "simpletest.rc-gsstn" in namespace "gc-5497"
    May  4 12:11:52.366: INFO: Deleting pod "simpletest.rc-gxhq2" in namespace "gc-5497"
    May  4 12:11:52.395: INFO: Deleting pod "simpletest.rc-hbw66" in namespace "gc-5497"
    May  4 12:11:52.433: INFO: Deleting pod "simpletest.rc-hns6k" in namespace "gc-5497"
    May  4 12:11:52.456: INFO: Deleting pod "simpletest.rc-j4bzt" in namespace "gc-5497"
    May  4 12:11:52.475: INFO: Deleting pod "simpletest.rc-jrrz6" in namespace "gc-5497"
    May  4 12:11:52.497: INFO: Deleting pod "simpletest.rc-kdscp" in namespace "gc-5497"
    May  4 12:11:52.514: INFO: Deleting pod "simpletest.rc-ktsq4" in namespace "gc-5497"
    May  4 12:11:52.531: INFO: Deleting pod "simpletest.rc-l7s2x" in namespace "gc-5497"
    May  4 12:11:52.579: INFO: Deleting pod "simpletest.rc-l7skx" in namespace "gc-5497"
    May  4 12:11:52.593: INFO: Deleting pod "simpletest.rc-ldhg7" in namespace "gc-5497"
    May  4 12:11:52.609: INFO: Deleting pod "simpletest.rc-llggv" in namespace "gc-5497"
    May  4 12:11:52.621: INFO: Deleting pod "simpletest.rc-lxl8d" in namespace "gc-5497"
    May  4 12:11:52.639: INFO: Deleting pod "simpletest.rc-m52l7" in namespace "gc-5497"
    May  4 12:11:52.654: INFO: Deleting pod "simpletest.rc-m6nff" in namespace "gc-5497"
    May  4 12:11:52.672: INFO: Deleting pod "simpletest.rc-m8nqm" in namespace "gc-5497"
    May  4 12:11:52.692: INFO: Deleting pod "simpletest.rc-mpkv5" in namespace "gc-5497"
    May  4 12:11:52.706: INFO: Deleting pod "simpletest.rc-msw5d" in namespace "gc-5497"
    May  4 12:11:52.730: INFO: Deleting pod "simpletest.rc-mz675" in namespace "gc-5497"
    May  4 12:11:52.753: INFO: Deleting pod "simpletest.rc-n8ffk" in namespace "gc-5497"
    May  4 12:11:52.771: INFO: Deleting pod "simpletest.rc-nqwqv" in namespace "gc-5497"
    May  4 12:11:52.787: INFO: Deleting pod "simpletest.rc-nxxgt" in namespace "gc-5497"
    May  4 12:11:52.802: INFO: Deleting pod "simpletest.rc-p4fql" in namespace "gc-5497"
    May  4 12:11:52.821: INFO: Deleting pod "simpletest.rc-pdkrh" in namespace "gc-5497"
    May  4 12:11:52.833: INFO: Deleting pod "simpletest.rc-phhjq" in namespace "gc-5497"
    May  4 12:11:52.853: INFO: Deleting pod "simpletest.rc-pphb7" in namespace "gc-5497"
    May  4 12:11:52.866: INFO: Deleting pod "simpletest.rc-pxb88" in namespace "gc-5497"
    May  4 12:11:52.891: INFO: Deleting pod "simpletest.rc-q7lzh" in namespace "gc-5497"
    May  4 12:11:52.907: INFO: Deleting pod "simpletest.rc-q8q8x" in namespace "gc-5497"
    May  4 12:11:52.920: INFO: Deleting pod "simpletest.rc-qg9p2" in namespace "gc-5497"
    May  4 12:11:52.937: INFO: Deleting pod "simpletest.rc-qhtcs" in namespace "gc-5497"
    May  4 12:11:52.954: INFO: Deleting pod "simpletest.rc-qrrhl" in namespace "gc-5497"
    May  4 12:11:52.966: INFO: Deleting pod "simpletest.rc-r2cqh" in namespace "gc-5497"
    May  4 12:11:52.991: INFO: Deleting pod "simpletest.rc-s5crn" in namespace "gc-5497"
    May  4 12:11:53.007: INFO: Deleting pod "simpletest.rc-s6tdj" in namespace "gc-5497"
    May  4 12:11:53.021: INFO: Deleting pod "simpletest.rc-skt2d" in namespace "gc-5497"
    May  4 12:11:53.065: INFO: Deleting pod "simpletest.rc-smtrx" in namespace "gc-5497"
    May  4 12:11:53.105: INFO: Deleting pod "simpletest.rc-st57w" in namespace "gc-5497"
    May  4 12:11:53.157: INFO: Deleting pod "simpletest.rc-tcx2c" in namespace "gc-5497"
    May  4 12:11:53.214: INFO: Deleting pod "simpletest.rc-tgxpn" in namespace "gc-5497"
    May  4 12:11:53.260: INFO: Deleting pod "simpletest.rc-tjd99" in namespace "gc-5497"
    May  4 12:11:53.313: INFO: Deleting pod "simpletest.rc-tvvsm" in namespace "gc-5497"
    May  4 12:11:53.361: INFO: Deleting pod "simpletest.rc-txwx9" in namespace "gc-5497"
    May  4 12:11:53.404: INFO: Deleting pod "simpletest.rc-vblht" in namespace "gc-5497"
    May  4 12:11:53.459: INFO: Deleting pod "simpletest.rc-vg2tn" in namespace "gc-5497"
    May  4 12:11:53.509: INFO: Deleting pod "simpletest.rc-vv4mq" in namespace "gc-5497"
    May  4 12:11:53.564: INFO: Deleting pod "simpletest.rc-wfccr" in namespace "gc-5497"
    May  4 12:11:53.609: INFO: Deleting pod "simpletest.rc-wpwcg" in namespace "gc-5497"
    May  4 12:11:53.664: INFO: Deleting pod "simpletest.rc-wrfxp" in namespace "gc-5497"
    May  4 12:11:53.704: INFO: Deleting pod "simpletest.rc-wt7kf" in namespace "gc-5497"
    May  4 12:11:53.789: INFO: Deleting pod "simpletest.rc-wv4gb" in namespace "gc-5497"
    May  4 12:11:53.808: INFO: Deleting pod "simpletest.rc-x6gnk" in namespace "gc-5497"
    May  4 12:11:53.861: INFO: Deleting pod "simpletest.rc-xccsb" in namespace "gc-5497"
    May  4 12:11:53.918: INFO: Deleting pod "simpletest.rc-xsx2x" in namespace "gc-5497"
    May  4 12:11:53.973: INFO: Deleting pod "simpletest.rc-z9k4k" in namespace "gc-5497"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    May  4 12:11:54.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-5497" for this suite. 05/04/23 12:11:54.052
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:11:54.101
May  4 12:11:54.101: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename pods 05/04/23 12:11:54.103
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:11:54.125
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:11:54.136
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
STEP: creating the pod 05/04/23 12:11:54.14
STEP: submitting the pod to kubernetes 05/04/23 12:11:54.14
May  4 12:11:54.153: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-6f514435-fb2b-4cc4-a2a3-7bc60701b6c2" in namespace "pods-8987" to be "running and ready"
May  4 12:11:54.157: INFO: Pod "pod-update-activedeadlineseconds-6f514435-fb2b-4cc4-a2a3-7bc60701b6c2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.301909ms
May  4 12:11:54.157: INFO: The phase of Pod pod-update-activedeadlineseconds-6f514435-fb2b-4cc4-a2a3-7bc60701b6c2 is Pending, waiting for it to be Running (with Ready = true)
May  4 12:11:56.162: INFO: Pod "pod-update-activedeadlineseconds-6f514435-fb2b-4cc4-a2a3-7bc60701b6c2": Phase="Running", Reason="", readiness=true. Elapsed: 2.009164578s
May  4 12:11:56.162: INFO: The phase of Pod pod-update-activedeadlineseconds-6f514435-fb2b-4cc4-a2a3-7bc60701b6c2 is Running (Ready = true)
May  4 12:11:56.162: INFO: Pod "pod-update-activedeadlineseconds-6f514435-fb2b-4cc4-a2a3-7bc60701b6c2" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 05/04/23 12:11:56.165
STEP: updating the pod 05/04/23 12:11:56.169
May  4 12:11:56.684: INFO: Successfully updated pod "pod-update-activedeadlineseconds-6f514435-fb2b-4cc4-a2a3-7bc60701b6c2"
May  4 12:11:56.685: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-6f514435-fb2b-4cc4-a2a3-7bc60701b6c2" in namespace "pods-8987" to be "terminated with reason DeadlineExceeded"
May  4 12:11:56.691: INFO: Pod "pod-update-activedeadlineseconds-6f514435-fb2b-4cc4-a2a3-7bc60701b6c2": Phase="Running", Reason="", readiness=true. Elapsed: 6.309018ms
May  4 12:11:58.695: INFO: Pod "pod-update-activedeadlineseconds-6f514435-fb2b-4cc4-a2a3-7bc60701b6c2": Phase="Running", Reason="", readiness=true. Elapsed: 2.010482249s
May  4 12:12:00.695: INFO: Pod "pod-update-activedeadlineseconds-6f514435-fb2b-4cc4-a2a3-7bc60701b6c2": Phase="Running", Reason="", readiness=false. Elapsed: 4.010147072s
May  4 12:12:02.696: INFO: Pod "pod-update-activedeadlineseconds-6f514435-fb2b-4cc4-a2a3-7bc60701b6c2": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.01115231s
May  4 12:12:02.696: INFO: Pod "pod-update-activedeadlineseconds-6f514435-fb2b-4cc4-a2a3-7bc60701b6c2" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
May  4 12:12:02.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8987" for this suite. 05/04/23 12:12:02.702
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","completed":110,"skipped":2335,"failed":0}
------------------------------
• [SLOW TEST] [8.608 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:11:54.101
    May  4 12:11:54.101: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename pods 05/04/23 12:11:54.103
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:11:54.125
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:11:54.136
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:397
    STEP: creating the pod 05/04/23 12:11:54.14
    STEP: submitting the pod to kubernetes 05/04/23 12:11:54.14
    May  4 12:11:54.153: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-6f514435-fb2b-4cc4-a2a3-7bc60701b6c2" in namespace "pods-8987" to be "running and ready"
    May  4 12:11:54.157: INFO: Pod "pod-update-activedeadlineseconds-6f514435-fb2b-4cc4-a2a3-7bc60701b6c2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.301909ms
    May  4 12:11:54.157: INFO: The phase of Pod pod-update-activedeadlineseconds-6f514435-fb2b-4cc4-a2a3-7bc60701b6c2 is Pending, waiting for it to be Running (with Ready = true)
    May  4 12:11:56.162: INFO: Pod "pod-update-activedeadlineseconds-6f514435-fb2b-4cc4-a2a3-7bc60701b6c2": Phase="Running", Reason="", readiness=true. Elapsed: 2.009164578s
    May  4 12:11:56.162: INFO: The phase of Pod pod-update-activedeadlineseconds-6f514435-fb2b-4cc4-a2a3-7bc60701b6c2 is Running (Ready = true)
    May  4 12:11:56.162: INFO: Pod "pod-update-activedeadlineseconds-6f514435-fb2b-4cc4-a2a3-7bc60701b6c2" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 05/04/23 12:11:56.165
    STEP: updating the pod 05/04/23 12:11:56.169
    May  4 12:11:56.684: INFO: Successfully updated pod "pod-update-activedeadlineseconds-6f514435-fb2b-4cc4-a2a3-7bc60701b6c2"
    May  4 12:11:56.685: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-6f514435-fb2b-4cc4-a2a3-7bc60701b6c2" in namespace "pods-8987" to be "terminated with reason DeadlineExceeded"
    May  4 12:11:56.691: INFO: Pod "pod-update-activedeadlineseconds-6f514435-fb2b-4cc4-a2a3-7bc60701b6c2": Phase="Running", Reason="", readiness=true. Elapsed: 6.309018ms
    May  4 12:11:58.695: INFO: Pod "pod-update-activedeadlineseconds-6f514435-fb2b-4cc4-a2a3-7bc60701b6c2": Phase="Running", Reason="", readiness=true. Elapsed: 2.010482249s
    May  4 12:12:00.695: INFO: Pod "pod-update-activedeadlineseconds-6f514435-fb2b-4cc4-a2a3-7bc60701b6c2": Phase="Running", Reason="", readiness=false. Elapsed: 4.010147072s
    May  4 12:12:02.696: INFO: Pod "pod-update-activedeadlineseconds-6f514435-fb2b-4cc4-a2a3-7bc60701b6c2": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.01115231s
    May  4 12:12:02.696: INFO: Pod "pod-update-activedeadlineseconds-6f514435-fb2b-4cc4-a2a3-7bc60701b6c2" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    May  4 12:12:02.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8987" for this suite. 05/04/23 12:12:02.702
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:12:02.709
May  4 12:12:02.709: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename gc 05/04/23 12:12:02.711
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:12:02.725
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:12:02.728
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 05/04/23 12:12:02.73
STEP: delete the rc 05/04/23 12:12:07.743
STEP: wait for all pods to be garbage collected 05/04/23 12:12:07.772
STEP: Gathering metrics 05/04/23 12:12:12.779
W0504 12:12:12.824307      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
May  4 12:12:12.824: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
May  4 12:12:12.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4532" for this suite. 05/04/23 12:12:12.839
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","completed":111,"skipped":2338,"failed":0}
------------------------------
• [SLOW TEST] [10.138 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:12:02.709
    May  4 12:12:02.709: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename gc 05/04/23 12:12:02.711
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:12:02.725
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:12:02.728
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 05/04/23 12:12:02.73
    STEP: delete the rc 05/04/23 12:12:07.743
    STEP: wait for all pods to be garbage collected 05/04/23 12:12:07.772
    STEP: Gathering metrics 05/04/23 12:12:12.779
    W0504 12:12:12.824307      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    May  4 12:12:12.824: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    May  4 12:12:12.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-4532" for this suite. 05/04/23 12:12:12.839
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:12:12.853
May  4 12:12:12.854: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename daemonsets 05/04/23 12:12:12.854
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:12:12.887
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:12:12.891
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
STEP: Creating simple DaemonSet "daemon-set" 05/04/23 12:12:12.969
STEP: Check that daemon pods launch on every node of the cluster. 05/04/23 12:12:12.977
May  4 12:12:12.987: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:12:12.987: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:12:12.987: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:12:12.992: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  4 12:12:12.992: INFO: Node ip-10-0-1-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
May  4 12:12:14.002: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:12:14.002: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:12:14.002: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:12:14.008: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  4 12:12:14.008: INFO: Node ip-10-0-1-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
May  4 12:12:15.002: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:12:15.002: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:12:15.002: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:12:15.011: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May  4 12:12:15.011: INFO: Node ip-10-0-1-216.us-west-2.compute.internal is running 0 daemon pod, expected 1
May  4 12:12:16.000: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:12:16.000: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:12:16.000: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:12:16.009: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
May  4 12:12:16.009: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 05/04/23 12:12:16.013
May  4 12:12:16.034: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:12:16.034: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:12:16.034: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:12:16.039: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
May  4 12:12:16.039: INFO: Node ip-10-0-1-232.us-west-2.compute.internal is running 0 daemon pod, expected 1
May  4 12:12:17.048: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:12:17.048: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:12:17.048: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:12:17.053: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
May  4 12:12:17.053: INFO: Node ip-10-0-1-232.us-west-2.compute.internal is running 0 daemon pod, expected 1
May  4 12:12:18.050: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:12:18.050: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:12:18.050: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:12:18.055: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
May  4 12:12:18.055: INFO: Node ip-10-0-1-232.us-west-2.compute.internal is running 0 daemon pod, expected 1
May  4 12:12:19.051: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:12:19.051: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:12:19.051: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:12:19.057: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
May  4 12:12:19.057: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 05/04/23 12:12:19.061
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6303, will wait for the garbage collector to delete the pods 05/04/23 12:12:19.061
May  4 12:12:19.123: INFO: Deleting DaemonSet.extensions daemon-set took: 7.988068ms
May  4 12:12:19.224: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.058463ms
May  4 12:12:21.228: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  4 12:12:21.228: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May  4 12:12:21.232: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"19176"},"items":null}

May  4 12:12:21.235: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"19176"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
May  4 12:12:21.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6303" for this suite. 05/04/23 12:12:21.273
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","completed":112,"skipped":2388,"failed":0}
------------------------------
• [SLOW TEST] [8.429 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:12:12.853
    May  4 12:12:12.854: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename daemonsets 05/04/23 12:12:12.854
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:12:12.887
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:12:12.891
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:165
    STEP: Creating simple DaemonSet "daemon-set" 05/04/23 12:12:12.969
    STEP: Check that daemon pods launch on every node of the cluster. 05/04/23 12:12:12.977
    May  4 12:12:12.987: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:12:12.987: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:12:12.987: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:12:12.992: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  4 12:12:12.992: INFO: Node ip-10-0-1-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
    May  4 12:12:14.002: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:12:14.002: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:12:14.002: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:12:14.008: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  4 12:12:14.008: INFO: Node ip-10-0-1-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
    May  4 12:12:15.002: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:12:15.002: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:12:15.002: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:12:15.011: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May  4 12:12:15.011: INFO: Node ip-10-0-1-216.us-west-2.compute.internal is running 0 daemon pod, expected 1
    May  4 12:12:16.000: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:12:16.000: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:12:16.000: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:12:16.009: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    May  4 12:12:16.009: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 05/04/23 12:12:16.013
    May  4 12:12:16.034: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:12:16.034: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:12:16.034: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:12:16.039: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    May  4 12:12:16.039: INFO: Node ip-10-0-1-232.us-west-2.compute.internal is running 0 daemon pod, expected 1
    May  4 12:12:17.048: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:12:17.048: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:12:17.048: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:12:17.053: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    May  4 12:12:17.053: INFO: Node ip-10-0-1-232.us-west-2.compute.internal is running 0 daemon pod, expected 1
    May  4 12:12:18.050: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:12:18.050: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:12:18.050: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:12:18.055: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    May  4 12:12:18.055: INFO: Node ip-10-0-1-232.us-west-2.compute.internal is running 0 daemon pod, expected 1
    May  4 12:12:19.051: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:12:19.051: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:12:19.051: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:12:19.057: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    May  4 12:12:19.057: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 05/04/23 12:12:19.061
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6303, will wait for the garbage collector to delete the pods 05/04/23 12:12:19.061
    May  4 12:12:19.123: INFO: Deleting DaemonSet.extensions daemon-set took: 7.988068ms
    May  4 12:12:19.224: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.058463ms
    May  4 12:12:21.228: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  4 12:12:21.228: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    May  4 12:12:21.232: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"19176"},"items":null}

    May  4 12:12:21.235: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"19176"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    May  4 12:12:21.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-6303" for this suite. 05/04/23 12:12:21.273
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:12:21.284
May  4 12:12:21.284: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename svcaccounts 05/04/23 12:12:21.285
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:12:21.309
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:12:21.313
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
May  4 12:12:21.324: INFO: Got root ca configmap in namespace "svcaccounts-1966"
May  4 12:12:21.330: INFO: Deleted root ca configmap in namespace "svcaccounts-1966"
STEP: waiting for a new root ca configmap created 05/04/23 12:12:21.831
May  4 12:12:21.835: INFO: Recreated root ca configmap in namespace "svcaccounts-1966"
May  4 12:12:21.840: INFO: Updated root ca configmap in namespace "svcaccounts-1966"
STEP: waiting for the root ca configmap reconciled 05/04/23 12:12:22.341
May  4 12:12:22.347: INFO: Reconciled root ca configmap in namespace "svcaccounts-1966"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
May  4 12:12:22.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1966" for this suite. 05/04/23 12:12:22.356
{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","completed":113,"skipped":2404,"failed":0}
------------------------------
• [1.079 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:12:21.284
    May  4 12:12:21.284: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename svcaccounts 05/04/23 12:12:21.285
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:12:21.309
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:12:21.313
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:739
    May  4 12:12:21.324: INFO: Got root ca configmap in namespace "svcaccounts-1966"
    May  4 12:12:21.330: INFO: Deleted root ca configmap in namespace "svcaccounts-1966"
    STEP: waiting for a new root ca configmap created 05/04/23 12:12:21.831
    May  4 12:12:21.835: INFO: Recreated root ca configmap in namespace "svcaccounts-1966"
    May  4 12:12:21.840: INFO: Updated root ca configmap in namespace "svcaccounts-1966"
    STEP: waiting for the root ca configmap reconciled 05/04/23 12:12:22.341
    May  4 12:12:22.347: INFO: Reconciled root ca configmap in namespace "svcaccounts-1966"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    May  4 12:12:22.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-1966" for this suite. 05/04/23 12:12:22.356
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:12:22.363
May  4 12:12:22.363: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename var-expansion 05/04/23 12:12:22.364
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:12:22.385
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:12:22.391
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
STEP: Creating a pod to test env composition 05/04/23 12:12:22.401
May  4 12:12:22.412: INFO: Waiting up to 5m0s for pod "var-expansion-b7c2f1a2-c4e0-462e-aaf6-aee6ecf3e631" in namespace "var-expansion-3636" to be "Succeeded or Failed"
May  4 12:12:22.418: INFO: Pod "var-expansion-b7c2f1a2-c4e0-462e-aaf6-aee6ecf3e631": Phase="Pending", Reason="", readiness=false. Elapsed: 6.026081ms
May  4 12:12:24.424: INFO: Pod "var-expansion-b7c2f1a2-c4e0-462e-aaf6-aee6ecf3e631": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011238282s
May  4 12:12:26.426: INFO: Pod "var-expansion-b7c2f1a2-c4e0-462e-aaf6-aee6ecf3e631": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013408616s
STEP: Saw pod success 05/04/23 12:12:26.426
May  4 12:12:26.426: INFO: Pod "var-expansion-b7c2f1a2-c4e0-462e-aaf6-aee6ecf3e631" satisfied condition "Succeeded or Failed"
May  4 12:12:26.431: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod var-expansion-b7c2f1a2-c4e0-462e-aaf6-aee6ecf3e631 container dapi-container: <nil>
STEP: delete the pod 05/04/23 12:12:26.451
May  4 12:12:26.563: INFO: Waiting for pod var-expansion-b7c2f1a2-c4e0-462e-aaf6-aee6ecf3e631 to disappear
May  4 12:12:26.567: INFO: Pod var-expansion-b7c2f1a2-c4e0-462e-aaf6-aee6ecf3e631 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
May  4 12:12:26.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3636" for this suite. 05/04/23 12:12:26.576
{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","completed":114,"skipped":2412,"failed":0}
------------------------------
• [4.221 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:12:22.363
    May  4 12:12:22.363: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename var-expansion 05/04/23 12:12:22.364
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:12:22.385
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:12:22.391
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:43
    STEP: Creating a pod to test env composition 05/04/23 12:12:22.401
    May  4 12:12:22.412: INFO: Waiting up to 5m0s for pod "var-expansion-b7c2f1a2-c4e0-462e-aaf6-aee6ecf3e631" in namespace "var-expansion-3636" to be "Succeeded or Failed"
    May  4 12:12:22.418: INFO: Pod "var-expansion-b7c2f1a2-c4e0-462e-aaf6-aee6ecf3e631": Phase="Pending", Reason="", readiness=false. Elapsed: 6.026081ms
    May  4 12:12:24.424: INFO: Pod "var-expansion-b7c2f1a2-c4e0-462e-aaf6-aee6ecf3e631": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011238282s
    May  4 12:12:26.426: INFO: Pod "var-expansion-b7c2f1a2-c4e0-462e-aaf6-aee6ecf3e631": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013408616s
    STEP: Saw pod success 05/04/23 12:12:26.426
    May  4 12:12:26.426: INFO: Pod "var-expansion-b7c2f1a2-c4e0-462e-aaf6-aee6ecf3e631" satisfied condition "Succeeded or Failed"
    May  4 12:12:26.431: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod var-expansion-b7c2f1a2-c4e0-462e-aaf6-aee6ecf3e631 container dapi-container: <nil>
    STEP: delete the pod 05/04/23 12:12:26.451
    May  4 12:12:26.563: INFO: Waiting for pod var-expansion-b7c2f1a2-c4e0-462e-aaf6-aee6ecf3e631 to disappear
    May  4 12:12:26.567: INFO: Pod var-expansion-b7c2f1a2-c4e0-462e-aaf6-aee6ecf3e631 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    May  4 12:12:26.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-3636" for this suite. 05/04/23 12:12:26.576
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:12:26.586
May  4 12:12:26.586: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename disruption 05/04/23 12:12:26.587
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:12:26.607
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:12:26.61
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
STEP: Waiting for the pdb to be processed 05/04/23 12:12:26.62
STEP: Updating PodDisruptionBudget status 05/04/23 12:12:28.627
STEP: Waiting for all pods to be running 05/04/23 12:12:28.637
May  4 12:12:28.642: INFO: running pods: 0 < 1
STEP: locating a running pod 05/04/23 12:12:30.647
STEP: Waiting for the pdb to be processed 05/04/23 12:12:30.665
STEP: Patching PodDisruptionBudget status 05/04/23 12:12:30.676
STEP: Waiting for the pdb to be processed 05/04/23 12:12:30.688
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
May  4 12:12:30.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1225" for this suite. 05/04/23 12:12:30.7
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","completed":115,"skipped":2438,"failed":0}
------------------------------
• [4.124 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:12:26.586
    May  4 12:12:26.586: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename disruption 05/04/23 12:12:26.587
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:12:26.607
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:12:26.61
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:163
    STEP: Waiting for the pdb to be processed 05/04/23 12:12:26.62
    STEP: Updating PodDisruptionBudget status 05/04/23 12:12:28.627
    STEP: Waiting for all pods to be running 05/04/23 12:12:28.637
    May  4 12:12:28.642: INFO: running pods: 0 < 1
    STEP: locating a running pod 05/04/23 12:12:30.647
    STEP: Waiting for the pdb to be processed 05/04/23 12:12:30.665
    STEP: Patching PodDisruptionBudget status 05/04/23 12:12:30.676
    STEP: Waiting for the pdb to be processed 05/04/23 12:12:30.688
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    May  4 12:12:30.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-1225" for this suite. 05/04/23 12:12:30.7
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:12:30.71
May  4 12:12:30.711: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename disruption 05/04/23 12:12:30.712
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:12:30.728
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:12:30.735
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
STEP: Waiting for the pdb to be processed 05/04/23 12:12:30.744
STEP: Waiting for all pods to be running 05/04/23 12:12:32.777
May  4 12:12:32.785: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
May  4 12:12:34.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-5198" for this suite. 05/04/23 12:12:34.804
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","completed":116,"skipped":2455,"failed":0}
------------------------------
• [4.100 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:12:30.71
    May  4 12:12:30.711: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename disruption 05/04/23 12:12:30.712
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:12:30.728
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:12:30.735
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:140
    STEP: Waiting for the pdb to be processed 05/04/23 12:12:30.744
    STEP: Waiting for all pods to be running 05/04/23 12:12:32.777
    May  4 12:12:32.785: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    May  4 12:12:34.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-5198" for this suite. 05/04/23 12:12:34.804
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:12:34.811
May  4 12:12:34.811: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename downward-api 05/04/23 12:12:34.812
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:12:34.827
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:12:34.831
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
STEP: Creating a pod to test downward API volume plugin 05/04/23 12:12:34.836
May  4 12:12:34.848: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d350ff1c-2528-401d-9743-c0292a3cace7" in namespace "downward-api-4660" to be "Succeeded or Failed"
May  4 12:12:34.853: INFO: Pod "downwardapi-volume-d350ff1c-2528-401d-9743-c0292a3cace7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.064176ms
May  4 12:12:36.857: INFO: Pod "downwardapi-volume-d350ff1c-2528-401d-9743-c0292a3cace7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009704457s
May  4 12:12:38.858: INFO: Pod "downwardapi-volume-d350ff1c-2528-401d-9743-c0292a3cace7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010511425s
STEP: Saw pod success 05/04/23 12:12:38.858
May  4 12:12:38.859: INFO: Pod "downwardapi-volume-d350ff1c-2528-401d-9743-c0292a3cace7" satisfied condition "Succeeded or Failed"
May  4 12:12:38.863: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod downwardapi-volume-d350ff1c-2528-401d-9743-c0292a3cace7 container client-container: <nil>
STEP: delete the pod 05/04/23 12:12:38.87
May  4 12:12:38.886: INFO: Waiting for pod downwardapi-volume-d350ff1c-2528-401d-9743-c0292a3cace7 to disappear
May  4 12:12:38.890: INFO: Pod downwardapi-volume-d350ff1c-2528-401d-9743-c0292a3cace7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
May  4 12:12:38.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4660" for this suite. 05/04/23 12:12:38.9
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","completed":117,"skipped":2456,"failed":0}
------------------------------
• [4.099 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:12:34.811
    May  4 12:12:34.811: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename downward-api 05/04/23 12:12:34.812
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:12:34.827
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:12:34.831
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:192
    STEP: Creating a pod to test downward API volume plugin 05/04/23 12:12:34.836
    May  4 12:12:34.848: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d350ff1c-2528-401d-9743-c0292a3cace7" in namespace "downward-api-4660" to be "Succeeded or Failed"
    May  4 12:12:34.853: INFO: Pod "downwardapi-volume-d350ff1c-2528-401d-9743-c0292a3cace7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.064176ms
    May  4 12:12:36.857: INFO: Pod "downwardapi-volume-d350ff1c-2528-401d-9743-c0292a3cace7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009704457s
    May  4 12:12:38.858: INFO: Pod "downwardapi-volume-d350ff1c-2528-401d-9743-c0292a3cace7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010511425s
    STEP: Saw pod success 05/04/23 12:12:38.858
    May  4 12:12:38.859: INFO: Pod "downwardapi-volume-d350ff1c-2528-401d-9743-c0292a3cace7" satisfied condition "Succeeded or Failed"
    May  4 12:12:38.863: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod downwardapi-volume-d350ff1c-2528-401d-9743-c0292a3cace7 container client-container: <nil>
    STEP: delete the pod 05/04/23 12:12:38.87
    May  4 12:12:38.886: INFO: Waiting for pod downwardapi-volume-d350ff1c-2528-401d-9743-c0292a3cace7 to disappear
    May  4 12:12:38.890: INFO: Pod downwardapi-volume-d350ff1c-2528-401d-9743-c0292a3cace7 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    May  4 12:12:38.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-4660" for this suite. 05/04/23 12:12:38.9
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:12:38.915
May  4 12:12:38.915: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename container-lifecycle-hook 05/04/23 12:12:38.916
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:12:38.932
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:12:38.936
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 05/04/23 12:12:38.946
May  4 12:12:38.956: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9150" to be "running and ready"
May  4 12:12:38.960: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.482403ms
May  4 12:12:38.960: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May  4 12:12:40.964: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.007473229s
May  4 12:12:40.964: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
May  4 12:12:40.964: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
STEP: create the pod with lifecycle hook 05/04/23 12:12:40.967
May  4 12:12:40.972: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-9150" to be "running and ready"
May  4 12:12:40.976: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.491907ms
May  4 12:12:40.976: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
May  4 12:12:42.981: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.008759611s
May  4 12:12:42.981: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
May  4 12:12:42.981: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 05/04/23 12:12:42.985
May  4 12:12:42.997: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May  4 12:12:43.006: INFO: Pod pod-with-prestop-http-hook still exists
May  4 12:12:45.006: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May  4 12:12:45.012: INFO: Pod pod-with-prestop-http-hook still exists
May  4 12:12:47.006: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May  4 12:12:47.011: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 05/04/23 12:12:47.011
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
May  4 12:12:47.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9150" for this suite. 05/04/23 12:12:47.028
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","completed":118,"skipped":2483,"failed":0}
------------------------------
• [SLOW TEST] [8.122 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:12:38.915
    May  4 12:12:38.915: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename container-lifecycle-hook 05/04/23 12:12:38.916
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:12:38.932
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:12:38.936
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 05/04/23 12:12:38.946
    May  4 12:12:38.956: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9150" to be "running and ready"
    May  4 12:12:38.960: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.482403ms
    May  4 12:12:38.960: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    May  4 12:12:40.964: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.007473229s
    May  4 12:12:40.964: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    May  4 12:12:40.964: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:152
    STEP: create the pod with lifecycle hook 05/04/23 12:12:40.967
    May  4 12:12:40.972: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-9150" to be "running and ready"
    May  4 12:12:40.976: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.491907ms
    May  4 12:12:40.976: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    May  4 12:12:42.981: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.008759611s
    May  4 12:12:42.981: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    May  4 12:12:42.981: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 05/04/23 12:12:42.985
    May  4 12:12:42.997: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    May  4 12:12:43.006: INFO: Pod pod-with-prestop-http-hook still exists
    May  4 12:12:45.006: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    May  4 12:12:45.012: INFO: Pod pod-with-prestop-http-hook still exists
    May  4 12:12:47.006: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    May  4 12:12:47.011: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 05/04/23 12:12:47.011
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    May  4 12:12:47.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-9150" for this suite. 05/04/23 12:12:47.028
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:12:47.039
May  4 12:12:47.039: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename projected 05/04/23 12:12:47.04
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:12:47.06
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:12:47.066
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
STEP: Creating a pod to test downward API volume plugin 05/04/23 12:12:47.07
May  4 12:12:47.084: INFO: Waiting up to 5m0s for pod "downwardapi-volume-60633a44-e064-4933-b592-29e3b8fd91ab" in namespace "projected-5" to be "Succeeded or Failed"
May  4 12:12:47.088: INFO: Pod "downwardapi-volume-60633a44-e064-4933-b592-29e3b8fd91ab": Phase="Pending", Reason="", readiness=false. Elapsed: 3.876497ms
May  4 12:12:49.092: INFO: Pod "downwardapi-volume-60633a44-e064-4933-b592-29e3b8fd91ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008033566s
May  4 12:12:51.094: INFO: Pod "downwardapi-volume-60633a44-e064-4933-b592-29e3b8fd91ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009911892s
STEP: Saw pod success 05/04/23 12:12:51.094
May  4 12:12:51.094: INFO: Pod "downwardapi-volume-60633a44-e064-4933-b592-29e3b8fd91ab" satisfied condition "Succeeded or Failed"
May  4 12:12:51.098: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod downwardapi-volume-60633a44-e064-4933-b592-29e3b8fd91ab container client-container: <nil>
STEP: delete the pod 05/04/23 12:12:51.114
May  4 12:12:51.131: INFO: Waiting for pod downwardapi-volume-60633a44-e064-4933-b592-29e3b8fd91ab to disappear
May  4 12:12:51.136: INFO: Pod downwardapi-volume-60633a44-e064-4933-b592-29e3b8fd91ab no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
May  4 12:12:51.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5" for this suite. 05/04/23 12:12:51.156
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":119,"skipped":2494,"failed":0}
------------------------------
• [4.128 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:12:47.039
    May  4 12:12:47.039: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename projected 05/04/23 12:12:47.04
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:12:47.06
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:12:47.066
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:260
    STEP: Creating a pod to test downward API volume plugin 05/04/23 12:12:47.07
    May  4 12:12:47.084: INFO: Waiting up to 5m0s for pod "downwardapi-volume-60633a44-e064-4933-b592-29e3b8fd91ab" in namespace "projected-5" to be "Succeeded or Failed"
    May  4 12:12:47.088: INFO: Pod "downwardapi-volume-60633a44-e064-4933-b592-29e3b8fd91ab": Phase="Pending", Reason="", readiness=false. Elapsed: 3.876497ms
    May  4 12:12:49.092: INFO: Pod "downwardapi-volume-60633a44-e064-4933-b592-29e3b8fd91ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008033566s
    May  4 12:12:51.094: INFO: Pod "downwardapi-volume-60633a44-e064-4933-b592-29e3b8fd91ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009911892s
    STEP: Saw pod success 05/04/23 12:12:51.094
    May  4 12:12:51.094: INFO: Pod "downwardapi-volume-60633a44-e064-4933-b592-29e3b8fd91ab" satisfied condition "Succeeded or Failed"
    May  4 12:12:51.098: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod downwardapi-volume-60633a44-e064-4933-b592-29e3b8fd91ab container client-container: <nil>
    STEP: delete the pod 05/04/23 12:12:51.114
    May  4 12:12:51.131: INFO: Waiting for pod downwardapi-volume-60633a44-e064-4933-b592-29e3b8fd91ab to disappear
    May  4 12:12:51.136: INFO: Pod downwardapi-volume-60633a44-e064-4933-b592-29e3b8fd91ab no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    May  4 12:12:51.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5" for this suite. 05/04/23 12:12:51.156
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:12:51.168
May  4 12:12:51.168: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename secrets 05/04/23 12:12:51.17
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:12:51.215
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:12:51.224
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
STEP: Creating secret with name secret-test-map-48c8c477-ddb1-448f-a5dc-71f97b797a65 05/04/23 12:12:51.23
STEP: Creating a pod to test consume secrets 05/04/23 12:12:51.246
May  4 12:12:51.275: INFO: Waiting up to 5m0s for pod "pod-secrets-b9c89c88-ebd7-4537-b8b1-1d9717010ac2" in namespace "secrets-8042" to be "Succeeded or Failed"
May  4 12:12:51.292: INFO: Pod "pod-secrets-b9c89c88-ebd7-4537-b8b1-1d9717010ac2": Phase="Pending", Reason="", readiness=false. Elapsed: 16.167457ms
May  4 12:12:53.308: INFO: Pod "pod-secrets-b9c89c88-ebd7-4537-b8b1-1d9717010ac2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032125691s
May  4 12:12:55.299: INFO: Pod "pod-secrets-b9c89c88-ebd7-4537-b8b1-1d9717010ac2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023099s
STEP: Saw pod success 05/04/23 12:12:55.299
May  4 12:12:55.299: INFO: Pod "pod-secrets-b9c89c88-ebd7-4537-b8b1-1d9717010ac2" satisfied condition "Succeeded or Failed"
May  4 12:12:55.303: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-secrets-b9c89c88-ebd7-4537-b8b1-1d9717010ac2 container secret-volume-test: <nil>
STEP: delete the pod 05/04/23 12:12:55.312
May  4 12:12:55.329: INFO: Waiting for pod pod-secrets-b9c89c88-ebd7-4537-b8b1-1d9717010ac2 to disappear
May  4 12:12:55.333: INFO: Pod pod-secrets-b9c89c88-ebd7-4537-b8b1-1d9717010ac2 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
May  4 12:12:55.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8042" for this suite. 05/04/23 12:12:55.342
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":120,"skipped":2515,"failed":0}
------------------------------
• [4.188 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:12:51.168
    May  4 12:12:51.168: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename secrets 05/04/23 12:12:51.17
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:12:51.215
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:12:51.224
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:88
    STEP: Creating secret with name secret-test-map-48c8c477-ddb1-448f-a5dc-71f97b797a65 05/04/23 12:12:51.23
    STEP: Creating a pod to test consume secrets 05/04/23 12:12:51.246
    May  4 12:12:51.275: INFO: Waiting up to 5m0s for pod "pod-secrets-b9c89c88-ebd7-4537-b8b1-1d9717010ac2" in namespace "secrets-8042" to be "Succeeded or Failed"
    May  4 12:12:51.292: INFO: Pod "pod-secrets-b9c89c88-ebd7-4537-b8b1-1d9717010ac2": Phase="Pending", Reason="", readiness=false. Elapsed: 16.167457ms
    May  4 12:12:53.308: INFO: Pod "pod-secrets-b9c89c88-ebd7-4537-b8b1-1d9717010ac2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032125691s
    May  4 12:12:55.299: INFO: Pod "pod-secrets-b9c89c88-ebd7-4537-b8b1-1d9717010ac2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023099s
    STEP: Saw pod success 05/04/23 12:12:55.299
    May  4 12:12:55.299: INFO: Pod "pod-secrets-b9c89c88-ebd7-4537-b8b1-1d9717010ac2" satisfied condition "Succeeded or Failed"
    May  4 12:12:55.303: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-secrets-b9c89c88-ebd7-4537-b8b1-1d9717010ac2 container secret-volume-test: <nil>
    STEP: delete the pod 05/04/23 12:12:55.312
    May  4 12:12:55.329: INFO: Waiting for pod pod-secrets-b9c89c88-ebd7-4537-b8b1-1d9717010ac2 to disappear
    May  4 12:12:55.333: INFO: Pod pod-secrets-b9c89c88-ebd7-4537-b8b1-1d9717010ac2 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    May  4 12:12:55.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8042" for this suite. 05/04/23 12:12:55.342
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:12:55.356
May  4 12:12:55.357: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename webhook 05/04/23 12:12:55.358
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:12:55.386
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:12:55.391
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/04/23 12:12:55.412
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/04/23 12:12:56.107
STEP: Deploying the webhook pod 05/04/23 12:12:56.115
STEP: Wait for the deployment to be ready 05/04/23 12:12:56.133
May  4 12:12:56.150: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 05/04/23 12:12:58.162
STEP: Verifying the service has paired with the endpoint 05/04/23 12:12:58.174
May  4 12:12:59.174: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
STEP: Registering the webhook via the AdmissionRegistration API 05/04/23 12:12:59.178
STEP: create a pod 05/04/23 12:12:59.195
May  4 12:12:59.202: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-4011" to be "running"
May  4 12:12:59.209: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.290815ms
May  4 12:13:01.214: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011493301s
May  4 12:13:03.213: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.010922048s
May  4 12:13:03.213: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 05/04/23 12:13:03.213
May  4 12:13:03.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=webhook-4011 attach --namespace=webhook-4011 to-be-attached-pod -i -c=container1'
May  4 12:13:03.323: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  4 12:13:03.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4011" for this suite. 05/04/23 12:13:03.338
STEP: Destroying namespace "webhook-4011-markers" for this suite. 05/04/23 12:13:03.346
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","completed":121,"skipped":2515,"failed":0}
------------------------------
• [SLOW TEST] [8.077 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:12:55.356
    May  4 12:12:55.357: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename webhook 05/04/23 12:12:55.358
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:12:55.386
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:12:55.391
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/04/23 12:12:55.412
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/04/23 12:12:56.107
    STEP: Deploying the webhook pod 05/04/23 12:12:56.115
    STEP: Wait for the deployment to be ready 05/04/23 12:12:56.133
    May  4 12:12:56.150: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 05/04/23 12:12:58.162
    STEP: Verifying the service has paired with the endpoint 05/04/23 12:12:58.174
    May  4 12:12:59.174: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:208
    STEP: Registering the webhook via the AdmissionRegistration API 05/04/23 12:12:59.178
    STEP: create a pod 05/04/23 12:12:59.195
    May  4 12:12:59.202: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-4011" to be "running"
    May  4 12:12:59.209: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.290815ms
    May  4 12:13:01.214: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011493301s
    May  4 12:13:03.213: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.010922048s
    May  4 12:13:03.213: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 05/04/23 12:13:03.213
    May  4 12:13:03.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=webhook-4011 attach --namespace=webhook-4011 to-be-attached-pod -i -c=container1'
    May  4 12:13:03.323: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  4 12:13:03.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4011" for this suite. 05/04/23 12:13:03.338
    STEP: Destroying namespace "webhook-4011-markers" for this suite. 05/04/23 12:13:03.346
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:13:03.433
May  4 12:13:03.434: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename secrets 05/04/23 12:13:03.434
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:13:03.456
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:13:03.463
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
STEP: Creating secret with name s-test-opt-del-d0c523ea-4b06-4c51-950d-6b4502e7ef17 05/04/23 12:13:03.48
STEP: Creating secret with name s-test-opt-upd-d078227e-4da4-4739-a78f-9ccad6649952 05/04/23 12:13:03.489
STEP: Creating the pod 05/04/23 12:13:03.495
May  4 12:13:03.506: INFO: Waiting up to 5m0s for pod "pod-secrets-06bcd15f-98b5-4c51-9c95-b51cbb6a7ad1" in namespace "secrets-2272" to be "running and ready"
May  4 12:13:03.516: INFO: Pod "pod-secrets-06bcd15f-98b5-4c51-9c95-b51cbb6a7ad1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.05488ms
May  4 12:13:03.517: INFO: The phase of Pod pod-secrets-06bcd15f-98b5-4c51-9c95-b51cbb6a7ad1 is Pending, waiting for it to be Running (with Ready = true)
May  4 12:13:05.522: INFO: Pod "pod-secrets-06bcd15f-98b5-4c51-9c95-b51cbb6a7ad1": Phase="Running", Reason="", readiness=true. Elapsed: 2.01530129s
May  4 12:13:05.522: INFO: The phase of Pod pod-secrets-06bcd15f-98b5-4c51-9c95-b51cbb6a7ad1 is Running (Ready = true)
May  4 12:13:05.522: INFO: Pod "pod-secrets-06bcd15f-98b5-4c51-9c95-b51cbb6a7ad1" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-d0c523ea-4b06-4c51-950d-6b4502e7ef17 05/04/23 12:13:05.548
STEP: Updating secret s-test-opt-upd-d078227e-4da4-4739-a78f-9ccad6649952 05/04/23 12:13:05.554
STEP: Creating secret with name s-test-opt-create-e3d6e880-d6a9-4f5a-b2b3-774b4c8c56ca 05/04/23 12:13:05.564
STEP: waiting to observe update in volume 05/04/23 12:13:05.572
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
May  4 12:13:07.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2272" for this suite. 05/04/23 12:13:07.626
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":122,"skipped":2515,"failed":0}
------------------------------
• [4.202 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:13:03.433
    May  4 12:13:03.434: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename secrets 05/04/23 12:13:03.434
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:13:03.456
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:13:03.463
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:204
    STEP: Creating secret with name s-test-opt-del-d0c523ea-4b06-4c51-950d-6b4502e7ef17 05/04/23 12:13:03.48
    STEP: Creating secret with name s-test-opt-upd-d078227e-4da4-4739-a78f-9ccad6649952 05/04/23 12:13:03.489
    STEP: Creating the pod 05/04/23 12:13:03.495
    May  4 12:13:03.506: INFO: Waiting up to 5m0s for pod "pod-secrets-06bcd15f-98b5-4c51-9c95-b51cbb6a7ad1" in namespace "secrets-2272" to be "running and ready"
    May  4 12:13:03.516: INFO: Pod "pod-secrets-06bcd15f-98b5-4c51-9c95-b51cbb6a7ad1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.05488ms
    May  4 12:13:03.517: INFO: The phase of Pod pod-secrets-06bcd15f-98b5-4c51-9c95-b51cbb6a7ad1 is Pending, waiting for it to be Running (with Ready = true)
    May  4 12:13:05.522: INFO: Pod "pod-secrets-06bcd15f-98b5-4c51-9c95-b51cbb6a7ad1": Phase="Running", Reason="", readiness=true. Elapsed: 2.01530129s
    May  4 12:13:05.522: INFO: The phase of Pod pod-secrets-06bcd15f-98b5-4c51-9c95-b51cbb6a7ad1 is Running (Ready = true)
    May  4 12:13:05.522: INFO: Pod "pod-secrets-06bcd15f-98b5-4c51-9c95-b51cbb6a7ad1" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-d0c523ea-4b06-4c51-950d-6b4502e7ef17 05/04/23 12:13:05.548
    STEP: Updating secret s-test-opt-upd-d078227e-4da4-4739-a78f-9ccad6649952 05/04/23 12:13:05.554
    STEP: Creating secret with name s-test-opt-create-e3d6e880-d6a9-4f5a-b2b3-774b4c8c56ca 05/04/23 12:13:05.564
    STEP: waiting to observe update in volume 05/04/23 12:13:05.572
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    May  4 12:13:07.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-2272" for this suite. 05/04/23 12:13:07.626
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:13:07.636
May  4 12:13:07.636: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename configmap 05/04/23 12:13:07.637
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:13:07.663
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:13:07.67
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
May  4 12:13:07.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6699" for this suite. 05/04/23 12:13:07.748
{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","completed":123,"skipped":2516,"failed":0}
------------------------------
• [0.120 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:13:07.636
    May  4 12:13:07.636: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename configmap 05/04/23 12:13:07.637
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:13:07.663
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:13:07.67
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:503
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    May  4 12:13:07.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6699" for this suite. 05/04/23 12:13:07.748
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:13:07.758
May  4 12:13:07.758: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename subpath 05/04/23 12:13:07.759
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:13:07.775
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:13:07.779
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 05/04/23 12:13:07.791
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-ppnz 05/04/23 12:13:07.808
STEP: Creating a pod to test atomic-volume-subpath 05/04/23 12:13:07.808
May  4 12:13:07.820: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-ppnz" in namespace "subpath-4258" to be "Succeeded or Failed"
May  4 12:13:07.828: INFO: Pod "pod-subpath-test-downwardapi-ppnz": Phase="Pending", Reason="", readiness=false. Elapsed: 8.052551ms
May  4 12:13:09.834: INFO: Pod "pod-subpath-test-downwardapi-ppnz": Phase="Running", Reason="", readiness=true. Elapsed: 2.013978184s
May  4 12:13:11.837: INFO: Pod "pod-subpath-test-downwardapi-ppnz": Phase="Running", Reason="", readiness=true. Elapsed: 4.017273139s
May  4 12:13:13.833: INFO: Pod "pod-subpath-test-downwardapi-ppnz": Phase="Running", Reason="", readiness=true. Elapsed: 6.012732641s
May  4 12:13:15.834: INFO: Pod "pod-subpath-test-downwardapi-ppnz": Phase="Running", Reason="", readiness=true. Elapsed: 8.01342972s
May  4 12:13:17.833: INFO: Pod "pod-subpath-test-downwardapi-ppnz": Phase="Running", Reason="", readiness=true. Elapsed: 10.012669019s
May  4 12:13:19.834: INFO: Pod "pod-subpath-test-downwardapi-ppnz": Phase="Running", Reason="", readiness=true. Elapsed: 12.013934212s
May  4 12:13:21.833: INFO: Pod "pod-subpath-test-downwardapi-ppnz": Phase="Running", Reason="", readiness=true. Elapsed: 14.013177669s
May  4 12:13:23.840: INFO: Pod "pod-subpath-test-downwardapi-ppnz": Phase="Running", Reason="", readiness=true. Elapsed: 16.019556683s
May  4 12:13:25.834: INFO: Pod "pod-subpath-test-downwardapi-ppnz": Phase="Running", Reason="", readiness=true. Elapsed: 18.013781569s
May  4 12:13:27.835: INFO: Pod "pod-subpath-test-downwardapi-ppnz": Phase="Running", Reason="", readiness=true. Elapsed: 20.015063477s
May  4 12:13:29.833: INFO: Pod "pod-subpath-test-downwardapi-ppnz": Phase="Running", Reason="", readiness=false. Elapsed: 22.013039244s
May  4 12:13:31.833: INFO: Pod "pod-subpath-test-downwardapi-ppnz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.013105936s
STEP: Saw pod success 05/04/23 12:13:31.833
May  4 12:13:31.833: INFO: Pod "pod-subpath-test-downwardapi-ppnz" satisfied condition "Succeeded or Failed"
May  4 12:13:31.839: INFO: Trying to get logs from node ip-10-0-1-232.us-west-2.compute.internal pod pod-subpath-test-downwardapi-ppnz container test-container-subpath-downwardapi-ppnz: <nil>
STEP: delete the pod 05/04/23 12:13:31.857
May  4 12:13:31.869: INFO: Waiting for pod pod-subpath-test-downwardapi-ppnz to disappear
May  4 12:13:31.872: INFO: Pod pod-subpath-test-downwardapi-ppnz no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-ppnz 05/04/23 12:13:31.872
May  4 12:13:31.872: INFO: Deleting pod "pod-subpath-test-downwardapi-ppnz" in namespace "subpath-4258"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
May  4 12:13:31.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4258" for this suite. 05/04/23 12:13:31.883
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","completed":124,"skipped":2569,"failed":0}
------------------------------
• [SLOW TEST] [24.134 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:13:07.758
    May  4 12:13:07.758: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename subpath 05/04/23 12:13:07.759
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:13:07.775
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:13:07.779
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 05/04/23 12:13:07.791
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-ppnz 05/04/23 12:13:07.808
    STEP: Creating a pod to test atomic-volume-subpath 05/04/23 12:13:07.808
    May  4 12:13:07.820: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-ppnz" in namespace "subpath-4258" to be "Succeeded or Failed"
    May  4 12:13:07.828: INFO: Pod "pod-subpath-test-downwardapi-ppnz": Phase="Pending", Reason="", readiness=false. Elapsed: 8.052551ms
    May  4 12:13:09.834: INFO: Pod "pod-subpath-test-downwardapi-ppnz": Phase="Running", Reason="", readiness=true. Elapsed: 2.013978184s
    May  4 12:13:11.837: INFO: Pod "pod-subpath-test-downwardapi-ppnz": Phase="Running", Reason="", readiness=true. Elapsed: 4.017273139s
    May  4 12:13:13.833: INFO: Pod "pod-subpath-test-downwardapi-ppnz": Phase="Running", Reason="", readiness=true. Elapsed: 6.012732641s
    May  4 12:13:15.834: INFO: Pod "pod-subpath-test-downwardapi-ppnz": Phase="Running", Reason="", readiness=true. Elapsed: 8.01342972s
    May  4 12:13:17.833: INFO: Pod "pod-subpath-test-downwardapi-ppnz": Phase="Running", Reason="", readiness=true. Elapsed: 10.012669019s
    May  4 12:13:19.834: INFO: Pod "pod-subpath-test-downwardapi-ppnz": Phase="Running", Reason="", readiness=true. Elapsed: 12.013934212s
    May  4 12:13:21.833: INFO: Pod "pod-subpath-test-downwardapi-ppnz": Phase="Running", Reason="", readiness=true. Elapsed: 14.013177669s
    May  4 12:13:23.840: INFO: Pod "pod-subpath-test-downwardapi-ppnz": Phase="Running", Reason="", readiness=true. Elapsed: 16.019556683s
    May  4 12:13:25.834: INFO: Pod "pod-subpath-test-downwardapi-ppnz": Phase="Running", Reason="", readiness=true. Elapsed: 18.013781569s
    May  4 12:13:27.835: INFO: Pod "pod-subpath-test-downwardapi-ppnz": Phase="Running", Reason="", readiness=true. Elapsed: 20.015063477s
    May  4 12:13:29.833: INFO: Pod "pod-subpath-test-downwardapi-ppnz": Phase="Running", Reason="", readiness=false. Elapsed: 22.013039244s
    May  4 12:13:31.833: INFO: Pod "pod-subpath-test-downwardapi-ppnz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.013105936s
    STEP: Saw pod success 05/04/23 12:13:31.833
    May  4 12:13:31.833: INFO: Pod "pod-subpath-test-downwardapi-ppnz" satisfied condition "Succeeded or Failed"
    May  4 12:13:31.839: INFO: Trying to get logs from node ip-10-0-1-232.us-west-2.compute.internal pod pod-subpath-test-downwardapi-ppnz container test-container-subpath-downwardapi-ppnz: <nil>
    STEP: delete the pod 05/04/23 12:13:31.857
    May  4 12:13:31.869: INFO: Waiting for pod pod-subpath-test-downwardapi-ppnz to disappear
    May  4 12:13:31.872: INFO: Pod pod-subpath-test-downwardapi-ppnz no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-ppnz 05/04/23 12:13:31.872
    May  4 12:13:31.872: INFO: Deleting pod "pod-subpath-test-downwardapi-ppnz" in namespace "subpath-4258"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    May  4 12:13:31.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-4258" for this suite. 05/04/23 12:13:31.883
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:13:31.892
May  4 12:13:31.892: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename services 05/04/23 12:13:31.893
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:13:31.907
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:13:31.912
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6893 05/04/23 12:13:31.921
STEP: changing the ExternalName service to type=NodePort 05/04/23 12:13:31.928
STEP: creating replication controller externalname-service in namespace services-6893 05/04/23 12:13:31.976
I0504 12:13:31.990620      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6893, replica count: 2
I0504 12:13:35.041512      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  4 12:13:35.041: INFO: Creating new exec pod
May  4 12:13:35.048: INFO: Waiting up to 5m0s for pod "execpodzjbcv" in namespace "services-6893" to be "running"
May  4 12:13:35.052: INFO: Pod "execpodzjbcv": Phase="Pending", Reason="", readiness=false. Elapsed: 3.534513ms
May  4 12:13:37.057: INFO: Pod "execpodzjbcv": Phase="Running", Reason="", readiness=true. Elapsed: 2.008659237s
May  4 12:13:37.057: INFO: Pod "execpodzjbcv" satisfied condition "running"
May  4 12:13:38.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-6893 exec execpodzjbcv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May  4 12:13:38.237: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May  4 12:13:38.237: INFO: stdout: "externalname-service-kzqw8"
May  4 12:13:38.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-6893 exec execpodzjbcv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.21.205.219 80'
May  4 12:13:38.476: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.21.205.219 80\nConnection to 10.21.205.219 80 port [tcp/http] succeeded!\n"
May  4 12:13:38.476: INFO: stdout: ""
May  4 12:13:39.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-6893 exec execpodzjbcv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.21.205.219 80'
May  4 12:13:39.651: INFO: stderr: "+ nc -v -t -w 2 10.21.205.219 80\n+ echo hostName\nConnection to 10.21.205.219 80 port [tcp/http] succeeded!\n"
May  4 12:13:39.651: INFO: stdout: "externalname-service-kzqw8"
May  4 12:13:39.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-6893 exec execpodzjbcv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.1.189 30518'
May  4 12:13:39.833: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.1.189 30518\nConnection to 10.0.1.189 30518 port [tcp/*] succeeded!\n"
May  4 12:13:39.833: INFO: stdout: "externalname-service-kzqw8"
May  4 12:13:39.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-6893 exec execpodzjbcv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.1.216 30518'
May  4 12:13:40.061: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.1.216 30518\nConnection to 10.0.1.216 30518 port [tcp/*] succeeded!\n"
May  4 12:13:40.061: INFO: stdout: ""
May  4 12:13:41.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-6893 exec execpodzjbcv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.1.216 30518'
May  4 12:13:41.236: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.1.216 30518\nConnection to 10.0.1.216 30518 port [tcp/*] succeeded!\n"
May  4 12:13:41.236: INFO: stdout: ""
May  4 12:13:42.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-6893 exec execpodzjbcv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.1.216 30518'
May  4 12:13:42.240: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.1.216 30518\nConnection to 10.0.1.216 30518 port [tcp/*] succeeded!\n"
May  4 12:13:42.240: INFO: stdout: ""
May  4 12:13:43.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-6893 exec execpodzjbcv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.1.216 30518'
May  4 12:13:43.244: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.1.216 30518\nConnection to 10.0.1.216 30518 port [tcp/*] succeeded!\n"
May  4 12:13:43.244: INFO: stdout: "externalname-service-t2kpr"
May  4 12:13:43.244: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  4 12:13:43.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6893" for this suite. 05/04/23 12:13:43.339
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","completed":125,"skipped":2571,"failed":0}
------------------------------
• [SLOW TEST] [11.462 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:13:31.892
    May  4 12:13:31.892: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename services 05/04/23 12:13:31.893
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:13:31.907
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:13:31.912
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1443
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-6893 05/04/23 12:13:31.921
    STEP: changing the ExternalName service to type=NodePort 05/04/23 12:13:31.928
    STEP: creating replication controller externalname-service in namespace services-6893 05/04/23 12:13:31.976
    I0504 12:13:31.990620      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6893, replica count: 2
    I0504 12:13:35.041512      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  4 12:13:35.041: INFO: Creating new exec pod
    May  4 12:13:35.048: INFO: Waiting up to 5m0s for pod "execpodzjbcv" in namespace "services-6893" to be "running"
    May  4 12:13:35.052: INFO: Pod "execpodzjbcv": Phase="Pending", Reason="", readiness=false. Elapsed: 3.534513ms
    May  4 12:13:37.057: INFO: Pod "execpodzjbcv": Phase="Running", Reason="", readiness=true. Elapsed: 2.008659237s
    May  4 12:13:37.057: INFO: Pod "execpodzjbcv" satisfied condition "running"
    May  4 12:13:38.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-6893 exec execpodzjbcv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    May  4 12:13:38.237: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    May  4 12:13:38.237: INFO: stdout: "externalname-service-kzqw8"
    May  4 12:13:38.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-6893 exec execpodzjbcv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.21.205.219 80'
    May  4 12:13:38.476: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.21.205.219 80\nConnection to 10.21.205.219 80 port [tcp/http] succeeded!\n"
    May  4 12:13:38.476: INFO: stdout: ""
    May  4 12:13:39.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-6893 exec execpodzjbcv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.21.205.219 80'
    May  4 12:13:39.651: INFO: stderr: "+ nc -v -t -w 2 10.21.205.219 80\n+ echo hostName\nConnection to 10.21.205.219 80 port [tcp/http] succeeded!\n"
    May  4 12:13:39.651: INFO: stdout: "externalname-service-kzqw8"
    May  4 12:13:39.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-6893 exec execpodzjbcv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.1.189 30518'
    May  4 12:13:39.833: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.1.189 30518\nConnection to 10.0.1.189 30518 port [tcp/*] succeeded!\n"
    May  4 12:13:39.833: INFO: stdout: "externalname-service-kzqw8"
    May  4 12:13:39.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-6893 exec execpodzjbcv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.1.216 30518'
    May  4 12:13:40.061: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.1.216 30518\nConnection to 10.0.1.216 30518 port [tcp/*] succeeded!\n"
    May  4 12:13:40.061: INFO: stdout: ""
    May  4 12:13:41.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-6893 exec execpodzjbcv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.1.216 30518'
    May  4 12:13:41.236: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.1.216 30518\nConnection to 10.0.1.216 30518 port [tcp/*] succeeded!\n"
    May  4 12:13:41.236: INFO: stdout: ""
    May  4 12:13:42.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-6893 exec execpodzjbcv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.1.216 30518'
    May  4 12:13:42.240: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.1.216 30518\nConnection to 10.0.1.216 30518 port [tcp/*] succeeded!\n"
    May  4 12:13:42.240: INFO: stdout: ""
    May  4 12:13:43.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-6893 exec execpodzjbcv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.1.216 30518'
    May  4 12:13:43.244: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.1.216 30518\nConnection to 10.0.1.216 30518 port [tcp/*] succeeded!\n"
    May  4 12:13:43.244: INFO: stdout: "externalname-service-t2kpr"
    May  4 12:13:43.244: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  4 12:13:43.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6893" for this suite. 05/04/23 12:13:43.339
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:13:43.354
May  4 12:13:43.354: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename container-runtime 05/04/23 12:13:43.355
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:13:43.39
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:13:43.396
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
STEP: create the container 05/04/23 12:13:43.4
STEP: wait for the container to reach Succeeded 05/04/23 12:13:43.417
STEP: get the container status 05/04/23 12:13:47.441
STEP: the container should be terminated 05/04/23 12:13:47.445
STEP: the termination message should be set 05/04/23 12:13:47.445
May  4 12:13:47.445: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 05/04/23 12:13:47.445
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
May  4 12:13:47.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6353" for this suite. 05/04/23 12:13:47.473
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":126,"skipped":2582,"failed":0}
------------------------------
• [4.128 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:13:43.354
    May  4 12:13:43.354: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename container-runtime 05/04/23 12:13:43.355
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:13:43.39
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:13:43.396
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247
    STEP: create the container 05/04/23 12:13:43.4
    STEP: wait for the container to reach Succeeded 05/04/23 12:13:43.417
    STEP: get the container status 05/04/23 12:13:47.441
    STEP: the container should be terminated 05/04/23 12:13:47.445
    STEP: the termination message should be set 05/04/23 12:13:47.445
    May  4 12:13:47.445: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 05/04/23 12:13:47.445
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    May  4 12:13:47.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-6353" for this suite. 05/04/23 12:13:47.473
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:13:47.483
May  4 12:13:47.483: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename services 05/04/23 12:13:47.484
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:13:47.503
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:13:47.506
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
STEP: fetching services 05/04/23 12:13:47.509
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  4 12:13:47.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-943" for this suite. 05/04/23 12:13:47.523
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","completed":127,"skipped":2594,"failed":0}
------------------------------
• [0.047 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:13:47.483
    May  4 12:13:47.483: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename services 05/04/23 12:13:47.484
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:13:47.503
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:13:47.506
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3206
    STEP: fetching services 05/04/23 12:13:47.509
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  4 12:13:47.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-943" for this suite. 05/04/23 12:13:47.523
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:13:47.53
May  4 12:13:47.530: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename containers 05/04/23 12:13:47.533
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:13:47.548
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:13:47.551
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
STEP: Creating a pod to test override command 05/04/23 12:13:47.554
May  4 12:13:47.565: INFO: Waiting up to 5m0s for pod "client-containers-220d933f-e11b-43d1-9e1f-2d37c4039867" in namespace "containers-2379" to be "Succeeded or Failed"
May  4 12:13:47.568: INFO: Pod "client-containers-220d933f-e11b-43d1-9e1f-2d37c4039867": Phase="Pending", Reason="", readiness=false. Elapsed: 3.711699ms
May  4 12:13:49.574: INFO: Pod "client-containers-220d933f-e11b-43d1-9e1f-2d37c4039867": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009353571s
May  4 12:13:51.573: INFO: Pod "client-containers-220d933f-e11b-43d1-9e1f-2d37c4039867": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008528674s
STEP: Saw pod success 05/04/23 12:13:51.573
May  4 12:13:51.573: INFO: Pod "client-containers-220d933f-e11b-43d1-9e1f-2d37c4039867" satisfied condition "Succeeded or Failed"
May  4 12:13:51.577: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod client-containers-220d933f-e11b-43d1-9e1f-2d37c4039867 container agnhost-container: <nil>
STEP: delete the pod 05/04/23 12:13:51.583
May  4 12:13:51.599: INFO: Waiting for pod client-containers-220d933f-e11b-43d1-9e1f-2d37c4039867 to disappear
May  4 12:13:51.602: INFO: Pod client-containers-220d933f-e11b-43d1-9e1f-2d37c4039867 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
May  4 12:13:51.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2379" for this suite. 05/04/23 12:13:51.609
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","completed":128,"skipped":2611,"failed":0}
------------------------------
• [4.085 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:13:47.53
    May  4 12:13:47.530: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename containers 05/04/23 12:13:47.533
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:13:47.548
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:13:47.551
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:72
    STEP: Creating a pod to test override command 05/04/23 12:13:47.554
    May  4 12:13:47.565: INFO: Waiting up to 5m0s for pod "client-containers-220d933f-e11b-43d1-9e1f-2d37c4039867" in namespace "containers-2379" to be "Succeeded or Failed"
    May  4 12:13:47.568: INFO: Pod "client-containers-220d933f-e11b-43d1-9e1f-2d37c4039867": Phase="Pending", Reason="", readiness=false. Elapsed: 3.711699ms
    May  4 12:13:49.574: INFO: Pod "client-containers-220d933f-e11b-43d1-9e1f-2d37c4039867": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009353571s
    May  4 12:13:51.573: INFO: Pod "client-containers-220d933f-e11b-43d1-9e1f-2d37c4039867": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008528674s
    STEP: Saw pod success 05/04/23 12:13:51.573
    May  4 12:13:51.573: INFO: Pod "client-containers-220d933f-e11b-43d1-9e1f-2d37c4039867" satisfied condition "Succeeded or Failed"
    May  4 12:13:51.577: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod client-containers-220d933f-e11b-43d1-9e1f-2d37c4039867 container agnhost-container: <nil>
    STEP: delete the pod 05/04/23 12:13:51.583
    May  4 12:13:51.599: INFO: Waiting for pod client-containers-220d933f-e11b-43d1-9e1f-2d37c4039867 to disappear
    May  4 12:13:51.602: INFO: Pod client-containers-220d933f-e11b-43d1-9e1f-2d37c4039867 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    May  4 12:13:51.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-2379" for this suite. 05/04/23 12:13:51.609
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:13:51.616
May  4 12:13:51.616: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename watch 05/04/23 12:13:51.617
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:13:51.633
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:13:51.638
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 05/04/23 12:13:51.641
STEP: creating a watch on configmaps with label B 05/04/23 12:13:51.642
STEP: creating a watch on configmaps with label A or B 05/04/23 12:13:51.646
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 05/04/23 12:13:51.647
May  4 12:13:51.652: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8456  c95a5dd5-8713-4619-9a6f-c8fa80809fcd 20075 0 2023-05-04 12:13:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-04 12:13:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May  4 12:13:51.653: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8456  c95a5dd5-8713-4619-9a6f-c8fa80809fcd 20075 0 2023-05-04 12:13:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-04 12:13:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 05/04/23 12:13:51.653
May  4 12:13:51.660: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8456  c95a5dd5-8713-4619-9a6f-c8fa80809fcd 20076 0 2023-05-04 12:13:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-04 12:13:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
May  4 12:13:51.660: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8456  c95a5dd5-8713-4619-9a6f-c8fa80809fcd 20076 0 2023-05-04 12:13:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-04 12:13:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 05/04/23 12:13:51.66
May  4 12:13:51.671: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8456  c95a5dd5-8713-4619-9a6f-c8fa80809fcd 20077 0 2023-05-04 12:13:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-04 12:13:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May  4 12:13:51.671: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8456  c95a5dd5-8713-4619-9a6f-c8fa80809fcd 20077 0 2023-05-04 12:13:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-04 12:13:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 05/04/23 12:13:51.671
May  4 12:13:51.678: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8456  c95a5dd5-8713-4619-9a6f-c8fa80809fcd 20078 0 2023-05-04 12:13:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-04 12:13:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May  4 12:13:51.678: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8456  c95a5dd5-8713-4619-9a6f-c8fa80809fcd 20078 0 2023-05-04 12:13:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-04 12:13:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 05/04/23 12:13:51.678
May  4 12:13:51.682: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8456  76cd4a53-f3e8-4f57-8327-18520afbff05 20079 0 2023-05-04 12:13:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-04 12:13:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May  4 12:13:51.682: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8456  76cd4a53-f3e8-4f57-8327-18520afbff05 20079 0 2023-05-04 12:13:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-04 12:13:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 05/04/23 12:14:01.683
May  4 12:14:01.691: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8456  76cd4a53-f3e8-4f57-8327-18520afbff05 20127 0 2023-05-04 12:13:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-04 12:13:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May  4 12:14:01.691: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8456  76cd4a53-f3e8-4f57-8327-18520afbff05 20127 0 2023-05-04 12:13:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-04 12:13:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
May  4 12:14:11.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8456" for this suite. 05/04/23 12:14:11.701
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","completed":129,"skipped":2616,"failed":0}
------------------------------
• [SLOW TEST] [20.093 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:13:51.616
    May  4 12:13:51.616: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename watch 05/04/23 12:13:51.617
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:13:51.633
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:13:51.638
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 05/04/23 12:13:51.641
    STEP: creating a watch on configmaps with label B 05/04/23 12:13:51.642
    STEP: creating a watch on configmaps with label A or B 05/04/23 12:13:51.646
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 05/04/23 12:13:51.647
    May  4 12:13:51.652: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8456  c95a5dd5-8713-4619-9a6f-c8fa80809fcd 20075 0 2023-05-04 12:13:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-04 12:13:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    May  4 12:13:51.653: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8456  c95a5dd5-8713-4619-9a6f-c8fa80809fcd 20075 0 2023-05-04 12:13:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-04 12:13:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 05/04/23 12:13:51.653
    May  4 12:13:51.660: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8456  c95a5dd5-8713-4619-9a6f-c8fa80809fcd 20076 0 2023-05-04 12:13:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-04 12:13:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    May  4 12:13:51.660: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8456  c95a5dd5-8713-4619-9a6f-c8fa80809fcd 20076 0 2023-05-04 12:13:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-04 12:13:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 05/04/23 12:13:51.66
    May  4 12:13:51.671: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8456  c95a5dd5-8713-4619-9a6f-c8fa80809fcd 20077 0 2023-05-04 12:13:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-04 12:13:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    May  4 12:13:51.671: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8456  c95a5dd5-8713-4619-9a6f-c8fa80809fcd 20077 0 2023-05-04 12:13:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-04 12:13:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 05/04/23 12:13:51.671
    May  4 12:13:51.678: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8456  c95a5dd5-8713-4619-9a6f-c8fa80809fcd 20078 0 2023-05-04 12:13:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-04 12:13:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    May  4 12:13:51.678: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8456  c95a5dd5-8713-4619-9a6f-c8fa80809fcd 20078 0 2023-05-04 12:13:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-04 12:13:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 05/04/23 12:13:51.678
    May  4 12:13:51.682: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8456  76cd4a53-f3e8-4f57-8327-18520afbff05 20079 0 2023-05-04 12:13:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-04 12:13:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    May  4 12:13:51.682: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8456  76cd4a53-f3e8-4f57-8327-18520afbff05 20079 0 2023-05-04 12:13:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-04 12:13:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 05/04/23 12:14:01.683
    May  4 12:14:01.691: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8456  76cd4a53-f3e8-4f57-8327-18520afbff05 20127 0 2023-05-04 12:13:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-04 12:13:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    May  4 12:14:01.691: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8456  76cd4a53-f3e8-4f57-8327-18520afbff05 20127 0 2023-05-04 12:13:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-04 12:13:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    May  4 12:14:11.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-8456" for this suite. 05/04/23 12:14:11.701
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:14:11.709
May  4 12:14:11.709: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename crd-webhook 05/04/23 12:14:11.71
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:14:11.743
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:14:11.751
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 05/04/23 12:14:11.755
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 05/04/23 12:14:12.152
STEP: Deploying the custom resource conversion webhook pod 05/04/23 12:14:12.16
STEP: Wait for the deployment to be ready 05/04/23 12:14:12.175
May  4 12:14:12.187: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/04/23 12:14:14.211
STEP: Verifying the service has paired with the endpoint 05/04/23 12:14:14.224
May  4 12:14:15.224: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
May  4 12:14:15.228: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Creating a v1 custom resource 05/04/23 12:14:17.82
STEP: v2 custom resource should be converted 05/04/23 12:14:17.825
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  4 12:14:18.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-6103" for this suite. 05/04/23 12:14:18.35
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","completed":130,"skipped":2619,"failed":0}
------------------------------
• [SLOW TEST] [6.722 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:14:11.709
    May  4 12:14:11.709: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename crd-webhook 05/04/23 12:14:11.71
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:14:11.743
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:14:11.751
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 05/04/23 12:14:11.755
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 05/04/23 12:14:12.152
    STEP: Deploying the custom resource conversion webhook pod 05/04/23 12:14:12.16
    STEP: Wait for the deployment to be ready 05/04/23 12:14:12.175
    May  4 12:14:12.187: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/04/23 12:14:14.211
    STEP: Verifying the service has paired with the endpoint 05/04/23 12:14:14.224
    May  4 12:14:15.224: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    May  4 12:14:15.228: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Creating a v1 custom resource 05/04/23 12:14:17.82
    STEP: v2 custom resource should be converted 05/04/23 12:14:17.825
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  4 12:14:18.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-6103" for this suite. 05/04/23 12:14:18.35
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:14:18.432
May  4 12:14:18.432: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename kubectl 05/04/23 12:14:18.434
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:14:18.46
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:14:18.465
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 05/04/23 12:14:18.468
May  4 12:14:18.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-9437 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
May  4 12:14:18.625: INFO: stderr: ""
May  4 12:14:18.625: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 05/04/23 12:14:18.625
May  4 12:14:18.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-9437 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
May  4 12:14:20.284: INFO: stderr: ""
May  4 12:14:20.284: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 05/04/23 12:14:20.284
May  4 12:14:20.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-9437 delete pods e2e-test-httpd-pod'
May  4 12:14:22.540: INFO: stderr: ""
May  4 12:14:22.541: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  4 12:14:22.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9437" for this suite. 05/04/23 12:14:22.55
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","completed":131,"skipped":2633,"failed":0}
------------------------------
• [4.126 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:954
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:960

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:14:18.432
    May  4 12:14:18.432: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename kubectl 05/04/23 12:14:18.434
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:14:18.46
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:14:18.465
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:960
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 05/04/23 12:14:18.468
    May  4 12:14:18.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-9437 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    May  4 12:14:18.625: INFO: stderr: ""
    May  4 12:14:18.625: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 05/04/23 12:14:18.625
    May  4 12:14:18.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-9437 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
    May  4 12:14:20.284: INFO: stderr: ""
    May  4 12:14:20.284: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 05/04/23 12:14:20.284
    May  4 12:14:20.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-9437 delete pods e2e-test-httpd-pod'
    May  4 12:14:22.540: INFO: stderr: ""
    May  4 12:14:22.541: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  4 12:14:22.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9437" for this suite. 05/04/23 12:14:22.55
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:14:22.558
May  4 12:14:22.559: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename container-runtime 05/04/23 12:14:22.56
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:14:22.573
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:14:22.576
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
STEP: create the container 05/04/23 12:14:22.582
STEP: wait for the container to reach Failed 05/04/23 12:14:22.594
STEP: get the container status 05/04/23 12:14:26.62
STEP: the container should be terminated 05/04/23 12:14:26.624
STEP: the termination message should be set 05/04/23 12:14:26.624
May  4 12:14:26.624: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 05/04/23 12:14:26.624
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
May  4 12:14:26.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6092" for this suite. 05/04/23 12:14:26.656
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":132,"skipped":2649,"failed":0}
------------------------------
• [4.106 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:14:22.558
    May  4 12:14:22.559: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename container-runtime 05/04/23 12:14:22.56
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:14:22.573
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:14:22.576
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215
    STEP: create the container 05/04/23 12:14:22.582
    STEP: wait for the container to reach Failed 05/04/23 12:14:22.594
    STEP: get the container status 05/04/23 12:14:26.62
    STEP: the container should be terminated 05/04/23 12:14:26.624
    STEP: the termination message should be set 05/04/23 12:14:26.624
    May  4 12:14:26.624: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 05/04/23 12:14:26.624
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    May  4 12:14:26.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-6092" for this suite. 05/04/23 12:14:26.656
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:14:26.666
May  4 12:14:26.666: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename services 05/04/23 12:14:26.667
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:14:26.684
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:14:26.692
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6006 05/04/23 12:14:26.695
STEP: changing the ExternalName service to type=ClusterIP 05/04/23 12:14:26.702
STEP: creating replication controller externalname-service in namespace services-6006 05/04/23 12:14:26.728
I0504 12:14:26.737566      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6006, replica count: 2
I0504 12:14:29.788603      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  4 12:14:29.788: INFO: Creating new exec pod
May  4 12:14:29.797: INFO: Waiting up to 5m0s for pod "execpodf72f6" in namespace "services-6006" to be "running"
May  4 12:14:29.810: INFO: Pod "execpodf72f6": Phase="Pending", Reason="", readiness=false. Elapsed: 12.792614ms
May  4 12:14:31.815: INFO: Pod "execpodf72f6": Phase="Running", Reason="", readiness=true. Elapsed: 2.017631045s
May  4 12:14:31.815: INFO: Pod "execpodf72f6" satisfied condition "running"
May  4 12:14:32.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-6006 exec execpodf72f6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May  4 12:14:33.046: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May  4 12:14:33.046: INFO: stdout: "externalname-service-fmz7h"
May  4 12:14:33.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-6006 exec execpodf72f6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.21.183.47 80'
May  4 12:14:33.234: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.21.183.47 80\nConnection to 10.21.183.47 80 port [tcp/http] succeeded!\n"
May  4 12:14:33.235: INFO: stdout: "externalname-service-dgk9p"
May  4 12:14:33.235: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  4 12:14:33.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6006" for this suite. 05/04/23 12:14:33.274
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","completed":133,"skipped":2667,"failed":0}
------------------------------
• [SLOW TEST] [6.619 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:14:26.666
    May  4 12:14:26.666: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename services 05/04/23 12:14:26.667
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:14:26.684
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:14:26.692
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1404
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-6006 05/04/23 12:14:26.695
    STEP: changing the ExternalName service to type=ClusterIP 05/04/23 12:14:26.702
    STEP: creating replication controller externalname-service in namespace services-6006 05/04/23 12:14:26.728
    I0504 12:14:26.737566      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6006, replica count: 2
    I0504 12:14:29.788603      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  4 12:14:29.788: INFO: Creating new exec pod
    May  4 12:14:29.797: INFO: Waiting up to 5m0s for pod "execpodf72f6" in namespace "services-6006" to be "running"
    May  4 12:14:29.810: INFO: Pod "execpodf72f6": Phase="Pending", Reason="", readiness=false. Elapsed: 12.792614ms
    May  4 12:14:31.815: INFO: Pod "execpodf72f6": Phase="Running", Reason="", readiness=true. Elapsed: 2.017631045s
    May  4 12:14:31.815: INFO: Pod "execpodf72f6" satisfied condition "running"
    May  4 12:14:32.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-6006 exec execpodf72f6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    May  4 12:14:33.046: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    May  4 12:14:33.046: INFO: stdout: "externalname-service-fmz7h"
    May  4 12:14:33.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-6006 exec execpodf72f6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.21.183.47 80'
    May  4 12:14:33.234: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.21.183.47 80\nConnection to 10.21.183.47 80 port [tcp/http] succeeded!\n"
    May  4 12:14:33.235: INFO: stdout: "externalname-service-dgk9p"
    May  4 12:14:33.235: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  4 12:14:33.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6006" for this suite. 05/04/23 12:14:33.274
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:14:33.287
May  4 12:14:33.287: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename dns 05/04/23 12:14:33.288
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:14:33.306
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:14:33.308
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 05/04/23 12:14:33.311
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8992.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8992.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8992.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8992.svc.cluster.local;sleep 1; done
 05/04/23 12:14:33.318
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8992.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8992.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8992.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8992.svc.cluster.local;sleep 1; done
 05/04/23 12:14:33.318
STEP: creating a pod to probe DNS 05/04/23 12:14:33.318
STEP: submitting the pod to kubernetes 05/04/23 12:14:33.318
May  4 12:14:33.338: INFO: Waiting up to 15m0s for pod "dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d" in namespace "dns-8992" to be "running"
May  4 12:14:33.343: INFO: Pod "dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.150723ms
May  4 12:14:35.348: INFO: Pod "dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d": Phase="Running", Reason="", readiness=true. Elapsed: 2.010457266s
May  4 12:14:35.348: INFO: Pod "dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d" satisfied condition "running"
STEP: retrieving the pod 05/04/23 12:14:35.348
STEP: looking for the results for each expected name from probers 05/04/23 12:14:35.352
May  4 12:14:35.358: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local from pod dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d: the server could not find the requested resource (get pods dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d)
May  4 12:14:35.363: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local from pod dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d: the server could not find the requested resource (get pods dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d)
May  4 12:14:35.369: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8992.svc.cluster.local from pod dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d: the server could not find the requested resource (get pods dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d)
May  4 12:14:35.375: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8992.svc.cluster.local from pod dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d: the server could not find the requested resource (get pods dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d)
May  4 12:14:35.380: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local from pod dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d: the server could not find the requested resource (get pods dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d)
May  4 12:14:35.385: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local from pod dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d: the server could not find the requested resource (get pods dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d)
May  4 12:14:35.390: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8992.svc.cluster.local from pod dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d: the server could not find the requested resource (get pods dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d)
May  4 12:14:35.395: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8992.svc.cluster.local from pod dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d: the server could not find the requested resource (get pods dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d)
May  4 12:14:35.395: INFO: Lookups using dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8992.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8992.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local jessie_udp@dns-test-service-2.dns-8992.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8992.svc.cluster.local]

May  4 12:14:40.401: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local from pod dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d: the server could not find the requested resource (get pods dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d)
May  4 12:14:40.406: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local from pod dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d: the server could not find the requested resource (get pods dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d)
May  4 12:14:40.429: INFO: Lookups using dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local]

May  4 12:14:45.401: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local from pod dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d: the server could not find the requested resource (get pods dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d)
May  4 12:14:45.406: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local from pod dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d: the server could not find the requested resource (get pods dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d)
May  4 12:14:45.434: INFO: Lookups using dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local]

May  4 12:14:50.402: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local from pod dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d: the server could not find the requested resource (get pods dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d)
May  4 12:14:50.407: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local from pod dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d: the server could not find the requested resource (get pods dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d)
May  4 12:14:50.447: INFO: Lookups using dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local]

May  4 12:14:55.402: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local from pod dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d: the server could not find the requested resource (get pods dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d)
May  4 12:14:55.405: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local from pod dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d: the server could not find the requested resource (get pods dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d)
May  4 12:14:55.430: INFO: Lookups using dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local]

May  4 12:15:00.400: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local from pod dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d: the server could not find the requested resource (get pods dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d)
May  4 12:15:00.404: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local from pod dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d: the server could not find the requested resource (get pods dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d)
May  4 12:15:00.437: INFO: Lookups using dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local]

May  4 12:15:05.464: INFO: DNS probes using dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d succeeded

STEP: deleting the pod 05/04/23 12:15:05.464
STEP: deleting the test headless service 05/04/23 12:15:05.477
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
May  4 12:15:05.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8992" for this suite. 05/04/23 12:15:05.509
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","completed":134,"skipped":2714,"failed":0}
------------------------------
• [SLOW TEST] [32.228 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:14:33.287
    May  4 12:14:33.287: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename dns 05/04/23 12:14:33.288
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:14:33.306
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:14:33.308
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 05/04/23 12:14:33.311
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8992.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8992.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8992.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8992.svc.cluster.local;sleep 1; done
     05/04/23 12:14:33.318
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8992.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8992.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8992.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8992.svc.cluster.local;sleep 1; done
     05/04/23 12:14:33.318
    STEP: creating a pod to probe DNS 05/04/23 12:14:33.318
    STEP: submitting the pod to kubernetes 05/04/23 12:14:33.318
    May  4 12:14:33.338: INFO: Waiting up to 15m0s for pod "dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d" in namespace "dns-8992" to be "running"
    May  4 12:14:33.343: INFO: Pod "dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.150723ms
    May  4 12:14:35.348: INFO: Pod "dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d": Phase="Running", Reason="", readiness=true. Elapsed: 2.010457266s
    May  4 12:14:35.348: INFO: Pod "dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d" satisfied condition "running"
    STEP: retrieving the pod 05/04/23 12:14:35.348
    STEP: looking for the results for each expected name from probers 05/04/23 12:14:35.352
    May  4 12:14:35.358: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local from pod dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d: the server could not find the requested resource (get pods dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d)
    May  4 12:14:35.363: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local from pod dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d: the server could not find the requested resource (get pods dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d)
    May  4 12:14:35.369: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8992.svc.cluster.local from pod dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d: the server could not find the requested resource (get pods dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d)
    May  4 12:14:35.375: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8992.svc.cluster.local from pod dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d: the server could not find the requested resource (get pods dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d)
    May  4 12:14:35.380: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local from pod dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d: the server could not find the requested resource (get pods dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d)
    May  4 12:14:35.385: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local from pod dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d: the server could not find the requested resource (get pods dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d)
    May  4 12:14:35.390: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8992.svc.cluster.local from pod dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d: the server could not find the requested resource (get pods dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d)
    May  4 12:14:35.395: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8992.svc.cluster.local from pod dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d: the server could not find the requested resource (get pods dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d)
    May  4 12:14:35.395: INFO: Lookups using dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8992.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8992.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local jessie_udp@dns-test-service-2.dns-8992.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8992.svc.cluster.local]

    May  4 12:14:40.401: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local from pod dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d: the server could not find the requested resource (get pods dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d)
    May  4 12:14:40.406: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local from pod dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d: the server could not find the requested resource (get pods dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d)
    May  4 12:14:40.429: INFO: Lookups using dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local]

    May  4 12:14:45.401: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local from pod dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d: the server could not find the requested resource (get pods dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d)
    May  4 12:14:45.406: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local from pod dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d: the server could not find the requested resource (get pods dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d)
    May  4 12:14:45.434: INFO: Lookups using dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local]

    May  4 12:14:50.402: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local from pod dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d: the server could not find the requested resource (get pods dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d)
    May  4 12:14:50.407: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local from pod dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d: the server could not find the requested resource (get pods dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d)
    May  4 12:14:50.447: INFO: Lookups using dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local]

    May  4 12:14:55.402: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local from pod dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d: the server could not find the requested resource (get pods dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d)
    May  4 12:14:55.405: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local from pod dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d: the server could not find the requested resource (get pods dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d)
    May  4 12:14:55.430: INFO: Lookups using dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local]

    May  4 12:15:00.400: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local from pod dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d: the server could not find the requested resource (get pods dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d)
    May  4 12:15:00.404: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local from pod dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d: the server could not find the requested resource (get pods dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d)
    May  4 12:15:00.437: INFO: Lookups using dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8992.svc.cluster.local]

    May  4 12:15:05.464: INFO: DNS probes using dns-8992/dns-test-7be4ccd7-42f9-44a4-bb2a-dfa79873904d succeeded

    STEP: deleting the pod 05/04/23 12:15:05.464
    STEP: deleting the test headless service 05/04/23 12:15:05.477
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    May  4 12:15:05.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-8992" for this suite. 05/04/23 12:15:05.509
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:15:05.516
May  4 12:15:05.516: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename webhook 05/04/23 12:15:05.517
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:15:05.532
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:15:05.535
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/04/23 12:15:05.558
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/04/23 12:15:06.568
STEP: Deploying the webhook pod 05/04/23 12:15:06.581
STEP: Wait for the deployment to be ready 05/04/23 12:15:06.598
May  4 12:15:06.606: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/04/23 12:15:08.624
STEP: Verifying the service has paired with the endpoint 05/04/23 12:15:08.644
May  4 12:15:09.644: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
STEP: Registering the crd webhook via the AdmissionRegistration API 05/04/23 12:15:09.648
STEP: Creating a custom resource definition that should be denied by the webhook 05/04/23 12:15:09.664
May  4 12:15:09.664: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  4 12:15:09.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6034" for this suite. 05/04/23 12:15:09.69
STEP: Destroying namespace "webhook-6034-markers" for this suite. 05/04/23 12:15:09.697
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","completed":135,"skipped":2724,"failed":0}
------------------------------
• [4.253 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:15:05.516
    May  4 12:15:05.516: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename webhook 05/04/23 12:15:05.517
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:15:05.532
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:15:05.535
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/04/23 12:15:05.558
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/04/23 12:15:06.568
    STEP: Deploying the webhook pod 05/04/23 12:15:06.581
    STEP: Wait for the deployment to be ready 05/04/23 12:15:06.598
    May  4 12:15:06.606: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/04/23 12:15:08.624
    STEP: Verifying the service has paired with the endpoint 05/04/23 12:15:08.644
    May  4 12:15:09.644: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:307
    STEP: Registering the crd webhook via the AdmissionRegistration API 05/04/23 12:15:09.648
    STEP: Creating a custom resource definition that should be denied by the webhook 05/04/23 12:15:09.664
    May  4 12:15:09.664: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  4 12:15:09.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6034" for this suite. 05/04/23 12:15:09.69
    STEP: Destroying namespace "webhook-6034-markers" for this suite. 05/04/23 12:15:09.697
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:15:09.782
May  4 12:15:09.782: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename containers 05/04/23 12:15:09.788
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:15:09.808
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:15:09.812
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
May  4 12:15:09.826: INFO: Waiting up to 5m0s for pod "client-containers-8d29175c-abde-4b0f-8a1b-e1eff7ed3126" in namespace "containers-3638" to be "running"
May  4 12:15:09.832: INFO: Pod "client-containers-8d29175c-abde-4b0f-8a1b-e1eff7ed3126": Phase="Pending", Reason="", readiness=false. Elapsed: 5.814251ms
May  4 12:15:11.837: INFO: Pod "client-containers-8d29175c-abde-4b0f-8a1b-e1eff7ed3126": Phase="Running", Reason="", readiness=true. Elapsed: 2.010938343s
May  4 12:15:11.837: INFO: Pod "client-containers-8d29175c-abde-4b0f-8a1b-e1eff7ed3126" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
May  4 12:15:11.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3638" for this suite. 05/04/23 12:15:11.861
{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","completed":136,"skipped":2734,"failed":0}
------------------------------
• [2.087 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:15:09.782
    May  4 12:15:09.782: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename containers 05/04/23 12:15:09.788
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:15:09.808
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:15:09.812
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:38
    May  4 12:15:09.826: INFO: Waiting up to 5m0s for pod "client-containers-8d29175c-abde-4b0f-8a1b-e1eff7ed3126" in namespace "containers-3638" to be "running"
    May  4 12:15:09.832: INFO: Pod "client-containers-8d29175c-abde-4b0f-8a1b-e1eff7ed3126": Phase="Pending", Reason="", readiness=false. Elapsed: 5.814251ms
    May  4 12:15:11.837: INFO: Pod "client-containers-8d29175c-abde-4b0f-8a1b-e1eff7ed3126": Phase="Running", Reason="", readiness=true. Elapsed: 2.010938343s
    May  4 12:15:11.837: INFO: Pod "client-containers-8d29175c-abde-4b0f-8a1b-e1eff7ed3126" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    May  4 12:15:11.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-3638" for this suite. 05/04/23 12:15:11.861
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:15:11.868
May  4 12:15:11.868: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename downward-api 05/04/23 12:15:11.869
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:15:11.882
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:15:11.888
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
STEP: Creating a pod to test downward api env vars 05/04/23 12:15:11.891
May  4 12:15:11.900: INFO: Waiting up to 5m0s for pod "downward-api-4117d747-0c40-4c0e-8d9b-5e9152408411" in namespace "downward-api-4845" to be "Succeeded or Failed"
May  4 12:15:11.903: INFO: Pod "downward-api-4117d747-0c40-4c0e-8d9b-5e9152408411": Phase="Pending", Reason="", readiness=false. Elapsed: 3.309943ms
May  4 12:15:13.908: INFO: Pod "downward-api-4117d747-0c40-4c0e-8d9b-5e9152408411": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00779763s
May  4 12:15:15.907: INFO: Pod "downward-api-4117d747-0c40-4c0e-8d9b-5e9152408411": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007716822s
STEP: Saw pod success 05/04/23 12:15:15.907
May  4 12:15:15.908: INFO: Pod "downward-api-4117d747-0c40-4c0e-8d9b-5e9152408411" satisfied condition "Succeeded or Failed"
May  4 12:15:15.911: INFO: Trying to get logs from node ip-10-0-1-232.us-west-2.compute.internal pod downward-api-4117d747-0c40-4c0e-8d9b-5e9152408411 container dapi-container: <nil>
STEP: delete the pod 05/04/23 12:15:15.927
May  4 12:15:15.940: INFO: Waiting for pod downward-api-4117d747-0c40-4c0e-8d9b-5e9152408411 to disappear
May  4 12:15:15.943: INFO: Pod downward-api-4117d747-0c40-4c0e-8d9b-5e9152408411 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
May  4 12:15:15.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4845" for this suite. 05/04/23 12:15:15.95
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","completed":137,"skipped":2735,"failed":0}
------------------------------
• [4.091 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:15:11.868
    May  4 12:15:11.868: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename downward-api 05/04/23 12:15:11.869
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:15:11.882
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:15:11.888
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:43
    STEP: Creating a pod to test downward api env vars 05/04/23 12:15:11.891
    May  4 12:15:11.900: INFO: Waiting up to 5m0s for pod "downward-api-4117d747-0c40-4c0e-8d9b-5e9152408411" in namespace "downward-api-4845" to be "Succeeded or Failed"
    May  4 12:15:11.903: INFO: Pod "downward-api-4117d747-0c40-4c0e-8d9b-5e9152408411": Phase="Pending", Reason="", readiness=false. Elapsed: 3.309943ms
    May  4 12:15:13.908: INFO: Pod "downward-api-4117d747-0c40-4c0e-8d9b-5e9152408411": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00779763s
    May  4 12:15:15.907: INFO: Pod "downward-api-4117d747-0c40-4c0e-8d9b-5e9152408411": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007716822s
    STEP: Saw pod success 05/04/23 12:15:15.907
    May  4 12:15:15.908: INFO: Pod "downward-api-4117d747-0c40-4c0e-8d9b-5e9152408411" satisfied condition "Succeeded or Failed"
    May  4 12:15:15.911: INFO: Trying to get logs from node ip-10-0-1-232.us-west-2.compute.internal pod downward-api-4117d747-0c40-4c0e-8d9b-5e9152408411 container dapi-container: <nil>
    STEP: delete the pod 05/04/23 12:15:15.927
    May  4 12:15:15.940: INFO: Waiting for pod downward-api-4117d747-0c40-4c0e-8d9b-5e9152408411 to disappear
    May  4 12:15:15.943: INFO: Pod downward-api-4117d747-0c40-4c0e-8d9b-5e9152408411 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    May  4 12:15:15.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-4845" for this suite. 05/04/23 12:15:15.95
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:15:15.961
May  4 12:15:15.961: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename var-expansion 05/04/23 12:15:15.962
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:15:15.975
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:15:15.978
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
STEP: Creating a pod to test substitution in volume subpath 05/04/23 12:15:15.981
May  4 12:15:15.989: INFO: Waiting up to 5m0s for pod "var-expansion-60866fd5-6f96-486c-bee2-97ff9719f268" in namespace "var-expansion-9419" to be "Succeeded or Failed"
May  4 12:15:15.992: INFO: Pod "var-expansion-60866fd5-6f96-486c-bee2-97ff9719f268": Phase="Pending", Reason="", readiness=false. Elapsed: 3.576044ms
May  4 12:15:17.997: INFO: Pod "var-expansion-60866fd5-6f96-486c-bee2-97ff9719f268": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00828098s
May  4 12:15:19.998: INFO: Pod "var-expansion-60866fd5-6f96-486c-bee2-97ff9719f268": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008946386s
STEP: Saw pod success 05/04/23 12:15:19.998
May  4 12:15:19.998: INFO: Pod "var-expansion-60866fd5-6f96-486c-bee2-97ff9719f268" satisfied condition "Succeeded or Failed"
May  4 12:15:20.002: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod var-expansion-60866fd5-6f96-486c-bee2-97ff9719f268 container dapi-container: <nil>
STEP: delete the pod 05/04/23 12:15:20.009
May  4 12:15:20.031: INFO: Waiting for pod var-expansion-60866fd5-6f96-486c-bee2-97ff9719f268 to disappear
May  4 12:15:20.034: INFO: Pod var-expansion-60866fd5-6f96-486c-bee2-97ff9719f268 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
May  4 12:15:20.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9419" for this suite. 05/04/23 12:15:20.05
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","completed":138,"skipped":2762,"failed":0}
------------------------------
• [4.098 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:15:15.961
    May  4 12:15:15.961: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename var-expansion 05/04/23 12:15:15.962
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:15:15.975
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:15:15.978
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:111
    STEP: Creating a pod to test substitution in volume subpath 05/04/23 12:15:15.981
    May  4 12:15:15.989: INFO: Waiting up to 5m0s for pod "var-expansion-60866fd5-6f96-486c-bee2-97ff9719f268" in namespace "var-expansion-9419" to be "Succeeded or Failed"
    May  4 12:15:15.992: INFO: Pod "var-expansion-60866fd5-6f96-486c-bee2-97ff9719f268": Phase="Pending", Reason="", readiness=false. Elapsed: 3.576044ms
    May  4 12:15:17.997: INFO: Pod "var-expansion-60866fd5-6f96-486c-bee2-97ff9719f268": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00828098s
    May  4 12:15:19.998: INFO: Pod "var-expansion-60866fd5-6f96-486c-bee2-97ff9719f268": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008946386s
    STEP: Saw pod success 05/04/23 12:15:19.998
    May  4 12:15:19.998: INFO: Pod "var-expansion-60866fd5-6f96-486c-bee2-97ff9719f268" satisfied condition "Succeeded or Failed"
    May  4 12:15:20.002: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod var-expansion-60866fd5-6f96-486c-bee2-97ff9719f268 container dapi-container: <nil>
    STEP: delete the pod 05/04/23 12:15:20.009
    May  4 12:15:20.031: INFO: Waiting for pod var-expansion-60866fd5-6f96-486c-bee2-97ff9719f268 to disappear
    May  4 12:15:20.034: INFO: Pod var-expansion-60866fd5-6f96-486c-bee2-97ff9719f268 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    May  4 12:15:20.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-9419" for this suite. 05/04/23 12:15:20.05
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:15:20.059
May  4 12:15:20.059: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename secrets 05/04/23 12:15:20.06
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:15:20.078
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:15:20.084
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
May  4 12:15:20.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4611" for this suite. 05/04/23 12:15:20.149
{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","completed":139,"skipped":2763,"failed":0}
------------------------------
• [0.100 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:15:20.059
    May  4 12:15:20.059: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename secrets 05/04/23 12:15:20.06
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:15:20.078
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:15:20.084
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:385
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    May  4 12:15:20.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4611" for this suite. 05/04/23 12:15:20.149
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:15:20.16
May  4 12:15:20.160: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename runtimeclass 05/04/23 12:15:20.161
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:15:20.232
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:15:20.235
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
May  4 12:15:20.251: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-4638 to be scheduled
May  4 12:15:20.255: INFO: 1 pods are not scheduled: [runtimeclass-4638/test-runtimeclass-runtimeclass-4638-preconfigured-handler-xrb8n(fc58b13f-8190-4f26-a7dc-bf4d5fd1e55c)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
May  4 12:15:22.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-4638" for this suite. 05/04/23 12:15:22.277
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","completed":140,"skipped":2772,"failed":0}
------------------------------
• [2.124 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:15:20.16
    May  4 12:15:20.160: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename runtimeclass 05/04/23 12:15:20.161
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:15:20.232
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:15:20.235
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    May  4 12:15:20.251: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-4638 to be scheduled
    May  4 12:15:20.255: INFO: 1 pods are not scheduled: [runtimeclass-4638/test-runtimeclass-runtimeclass-4638-preconfigured-handler-xrb8n(fc58b13f-8190-4f26-a7dc-bf4d5fd1e55c)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    May  4 12:15:22.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-4638" for this suite. 05/04/23 12:15:22.277
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:15:22.291
May  4 12:15:22.291: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename pods 05/04/23 12:15:22.292
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:15:22.313
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:15:22.319
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
STEP: Create a pod 05/04/23 12:15:22.322
May  4 12:15:22.335: INFO: Waiting up to 5m0s for pod "pod-j5hk8" in namespace "pods-8146" to be "running"
May  4 12:15:22.347: INFO: Pod "pod-j5hk8": Phase="Pending", Reason="", readiness=false. Elapsed: 11.762791ms
May  4 12:15:24.351: INFO: Pod "pod-j5hk8": Phase="Running", Reason="", readiness=true. Elapsed: 2.015499289s
May  4 12:15:24.351: INFO: Pod "pod-j5hk8" satisfied condition "running"
STEP: patching /status 05/04/23 12:15:24.351
May  4 12:15:24.363: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
May  4 12:15:24.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8146" for this suite. 05/04/23 12:15:24.375
{"msg":"PASSED [sig-node] Pods should patch a pod status [Conformance]","completed":141,"skipped":2795,"failed":0}
------------------------------
• [2.100 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:15:22.291
    May  4 12:15:22.291: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename pods 05/04/23 12:15:22.292
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:15:22.313
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:15:22.319
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1082
    STEP: Create a pod 05/04/23 12:15:22.322
    May  4 12:15:22.335: INFO: Waiting up to 5m0s for pod "pod-j5hk8" in namespace "pods-8146" to be "running"
    May  4 12:15:22.347: INFO: Pod "pod-j5hk8": Phase="Pending", Reason="", readiness=false. Elapsed: 11.762791ms
    May  4 12:15:24.351: INFO: Pod "pod-j5hk8": Phase="Running", Reason="", readiness=true. Elapsed: 2.015499289s
    May  4 12:15:24.351: INFO: Pod "pod-j5hk8" satisfied condition "running"
    STEP: patching /status 05/04/23 12:15:24.351
    May  4 12:15:24.363: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    May  4 12:15:24.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8146" for this suite. 05/04/23 12:15:24.375
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:15:24.395
May  4 12:15:24.395: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename gc 05/04/23 12:15:24.398
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:15:24.417
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:15:24.42
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 05/04/23 12:15:24.437
STEP: create the rc2 05/04/23 12:15:24.447
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 05/04/23 12:15:29.483
STEP: delete the rc simpletest-rc-to-be-deleted 05/04/23 12:15:30.063
STEP: wait for the rc to be deleted 05/04/23 12:15:30.07
May  4 12:15:35.100: INFO: 72 pods remaining
May  4 12:15:35.100: INFO: 72 pods has nil DeletionTimestamp
May  4 12:15:35.100: INFO: 
STEP: Gathering metrics 05/04/23 12:15:40.097
W0504 12:15:40.123910      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
May  4 12:15:40.128: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

May  4 12:15:40.128: INFO: Deleting pod "simpletest-rc-to-be-deleted-25v65" in namespace "gc-4612"
May  4 12:15:40.149: INFO: Deleting pod "simpletest-rc-to-be-deleted-2z4k9" in namespace "gc-4612"
May  4 12:15:40.163: INFO: Deleting pod "simpletest-rc-to-be-deleted-47k4p" in namespace "gc-4612"
May  4 12:15:40.231: INFO: Deleting pod "simpletest-rc-to-be-deleted-4fj7d" in namespace "gc-4612"
May  4 12:15:40.248: INFO: Deleting pod "simpletest-rc-to-be-deleted-4rcsh" in namespace "gc-4612"
May  4 12:15:40.266: INFO: Deleting pod "simpletest-rc-to-be-deleted-4v568" in namespace "gc-4612"
May  4 12:15:40.292: INFO: Deleting pod "simpletest-rc-to-be-deleted-5bgll" in namespace "gc-4612"
May  4 12:15:40.305: INFO: Deleting pod "simpletest-rc-to-be-deleted-5gmlg" in namespace "gc-4612"
May  4 12:15:40.316: INFO: Deleting pod "simpletest-rc-to-be-deleted-5q5hr" in namespace "gc-4612"
May  4 12:15:40.334: INFO: Deleting pod "simpletest-rc-to-be-deleted-5v629" in namespace "gc-4612"
May  4 12:15:40.356: INFO: Deleting pod "simpletest-rc-to-be-deleted-5zfl9" in namespace "gc-4612"
May  4 12:15:40.395: INFO: Deleting pod "simpletest-rc-to-be-deleted-65rqk" in namespace "gc-4612"
May  4 12:15:40.418: INFO: Deleting pod "simpletest-rc-to-be-deleted-664fg" in namespace "gc-4612"
May  4 12:15:40.443: INFO: Deleting pod "simpletest-rc-to-be-deleted-6hjwc" in namespace "gc-4612"
May  4 12:15:40.466: INFO: Deleting pod "simpletest-rc-to-be-deleted-6plbx" in namespace "gc-4612"
May  4 12:15:40.483: INFO: Deleting pod "simpletest-rc-to-be-deleted-7mxbk" in namespace "gc-4612"
May  4 12:15:40.501: INFO: Deleting pod "simpletest-rc-to-be-deleted-879zq" in namespace "gc-4612"
May  4 12:15:40.531: INFO: Deleting pod "simpletest-rc-to-be-deleted-88jsv" in namespace "gc-4612"
May  4 12:15:40.549: INFO: Deleting pod "simpletest-rc-to-be-deleted-8x7wk" in namespace "gc-4612"
May  4 12:15:40.613: INFO: Deleting pod "simpletest-rc-to-be-deleted-8z74k" in namespace "gc-4612"
May  4 12:15:40.641: INFO: Deleting pod "simpletest-rc-to-be-deleted-9587p" in namespace "gc-4612"
May  4 12:15:40.675: INFO: Deleting pod "simpletest-rc-to-be-deleted-9rlgf" in namespace "gc-4612"
May  4 12:15:40.740: INFO: Deleting pod "simpletest-rc-to-be-deleted-9t8nw" in namespace "gc-4612"
May  4 12:15:40.784: INFO: Deleting pod "simpletest-rc-to-be-deleted-b4ctv" in namespace "gc-4612"
May  4 12:15:40.825: INFO: Deleting pod "simpletest-rc-to-be-deleted-bzfg4" in namespace "gc-4612"
May  4 12:15:40.854: INFO: Deleting pod "simpletest-rc-to-be-deleted-c2696" in namespace "gc-4612"
May  4 12:15:40.876: INFO: Deleting pod "simpletest-rc-to-be-deleted-ckndk" in namespace "gc-4612"
May  4 12:15:40.895: INFO: Deleting pod "simpletest-rc-to-be-deleted-dc8jg" in namespace "gc-4612"
May  4 12:15:40.909: INFO: Deleting pod "simpletest-rc-to-be-deleted-dl64d" in namespace "gc-4612"
May  4 12:15:40.929: INFO: Deleting pod "simpletest-rc-to-be-deleted-dr2vw" in namespace "gc-4612"
May  4 12:15:40.946: INFO: Deleting pod "simpletest-rc-to-be-deleted-dvcxp" in namespace "gc-4612"
May  4 12:15:40.957: INFO: Deleting pod "simpletest-rc-to-be-deleted-fkp92" in namespace "gc-4612"
May  4 12:15:40.971: INFO: Deleting pod "simpletest-rc-to-be-deleted-gddjs" in namespace "gc-4612"
May  4 12:15:40.987: INFO: Deleting pod "simpletest-rc-to-be-deleted-gdlj8" in namespace "gc-4612"
May  4 12:15:41.003: INFO: Deleting pod "simpletest-rc-to-be-deleted-gtbdf" in namespace "gc-4612"
May  4 12:15:41.024: INFO: Deleting pod "simpletest-rc-to-be-deleted-gvrz5" in namespace "gc-4612"
May  4 12:15:41.039: INFO: Deleting pod "simpletest-rc-to-be-deleted-gwpmx" in namespace "gc-4612"
May  4 12:15:41.053: INFO: Deleting pod "simpletest-rc-to-be-deleted-hmwqw" in namespace "gc-4612"
May  4 12:15:41.069: INFO: Deleting pod "simpletest-rc-to-be-deleted-hpfhj" in namespace "gc-4612"
May  4 12:15:41.080: INFO: Deleting pod "simpletest-rc-to-be-deleted-hsqql" in namespace "gc-4612"
May  4 12:15:41.097: INFO: Deleting pod "simpletest-rc-to-be-deleted-j8gb2" in namespace "gc-4612"
May  4 12:15:41.123: INFO: Deleting pod "simpletest-rc-to-be-deleted-jjjc8" in namespace "gc-4612"
May  4 12:15:41.138: INFO: Deleting pod "simpletest-rc-to-be-deleted-jr8ks" in namespace "gc-4612"
May  4 12:15:41.151: INFO: Deleting pod "simpletest-rc-to-be-deleted-k4wp9" in namespace "gc-4612"
May  4 12:15:41.171: INFO: Deleting pod "simpletest-rc-to-be-deleted-k6s5z" in namespace "gc-4612"
May  4 12:15:41.186: INFO: Deleting pod "simpletest-rc-to-be-deleted-khhfn" in namespace "gc-4612"
May  4 12:15:41.205: INFO: Deleting pod "simpletest-rc-to-be-deleted-khmwz" in namespace "gc-4612"
May  4 12:15:41.218: INFO: Deleting pod "simpletest-rc-to-be-deleted-kl6ld" in namespace "gc-4612"
May  4 12:15:41.230: INFO: Deleting pod "simpletest-rc-to-be-deleted-l2v2d" in namespace "gc-4612"
May  4 12:15:41.245: INFO: Deleting pod "simpletest-rc-to-be-deleted-l8tkm" in namespace "gc-4612"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
May  4 12:15:41.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4612" for this suite. 05/04/23 12:15:41.271
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","completed":142,"skipped":2809,"failed":0}
------------------------------
• [SLOW TEST] [16.886 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:15:24.395
    May  4 12:15:24.395: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename gc 05/04/23 12:15:24.398
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:15:24.417
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:15:24.42
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 05/04/23 12:15:24.437
    STEP: create the rc2 05/04/23 12:15:24.447
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 05/04/23 12:15:29.483
    STEP: delete the rc simpletest-rc-to-be-deleted 05/04/23 12:15:30.063
    STEP: wait for the rc to be deleted 05/04/23 12:15:30.07
    May  4 12:15:35.100: INFO: 72 pods remaining
    May  4 12:15:35.100: INFO: 72 pods has nil DeletionTimestamp
    May  4 12:15:35.100: INFO: 
    STEP: Gathering metrics 05/04/23 12:15:40.097
    W0504 12:15:40.123910      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    May  4 12:15:40.128: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    May  4 12:15:40.128: INFO: Deleting pod "simpletest-rc-to-be-deleted-25v65" in namespace "gc-4612"
    May  4 12:15:40.149: INFO: Deleting pod "simpletest-rc-to-be-deleted-2z4k9" in namespace "gc-4612"
    May  4 12:15:40.163: INFO: Deleting pod "simpletest-rc-to-be-deleted-47k4p" in namespace "gc-4612"
    May  4 12:15:40.231: INFO: Deleting pod "simpletest-rc-to-be-deleted-4fj7d" in namespace "gc-4612"
    May  4 12:15:40.248: INFO: Deleting pod "simpletest-rc-to-be-deleted-4rcsh" in namespace "gc-4612"
    May  4 12:15:40.266: INFO: Deleting pod "simpletest-rc-to-be-deleted-4v568" in namespace "gc-4612"
    May  4 12:15:40.292: INFO: Deleting pod "simpletest-rc-to-be-deleted-5bgll" in namespace "gc-4612"
    May  4 12:15:40.305: INFO: Deleting pod "simpletest-rc-to-be-deleted-5gmlg" in namespace "gc-4612"
    May  4 12:15:40.316: INFO: Deleting pod "simpletest-rc-to-be-deleted-5q5hr" in namespace "gc-4612"
    May  4 12:15:40.334: INFO: Deleting pod "simpletest-rc-to-be-deleted-5v629" in namespace "gc-4612"
    May  4 12:15:40.356: INFO: Deleting pod "simpletest-rc-to-be-deleted-5zfl9" in namespace "gc-4612"
    May  4 12:15:40.395: INFO: Deleting pod "simpletest-rc-to-be-deleted-65rqk" in namespace "gc-4612"
    May  4 12:15:40.418: INFO: Deleting pod "simpletest-rc-to-be-deleted-664fg" in namespace "gc-4612"
    May  4 12:15:40.443: INFO: Deleting pod "simpletest-rc-to-be-deleted-6hjwc" in namespace "gc-4612"
    May  4 12:15:40.466: INFO: Deleting pod "simpletest-rc-to-be-deleted-6plbx" in namespace "gc-4612"
    May  4 12:15:40.483: INFO: Deleting pod "simpletest-rc-to-be-deleted-7mxbk" in namespace "gc-4612"
    May  4 12:15:40.501: INFO: Deleting pod "simpletest-rc-to-be-deleted-879zq" in namespace "gc-4612"
    May  4 12:15:40.531: INFO: Deleting pod "simpletest-rc-to-be-deleted-88jsv" in namespace "gc-4612"
    May  4 12:15:40.549: INFO: Deleting pod "simpletest-rc-to-be-deleted-8x7wk" in namespace "gc-4612"
    May  4 12:15:40.613: INFO: Deleting pod "simpletest-rc-to-be-deleted-8z74k" in namespace "gc-4612"
    May  4 12:15:40.641: INFO: Deleting pod "simpletest-rc-to-be-deleted-9587p" in namespace "gc-4612"
    May  4 12:15:40.675: INFO: Deleting pod "simpletest-rc-to-be-deleted-9rlgf" in namespace "gc-4612"
    May  4 12:15:40.740: INFO: Deleting pod "simpletest-rc-to-be-deleted-9t8nw" in namespace "gc-4612"
    May  4 12:15:40.784: INFO: Deleting pod "simpletest-rc-to-be-deleted-b4ctv" in namespace "gc-4612"
    May  4 12:15:40.825: INFO: Deleting pod "simpletest-rc-to-be-deleted-bzfg4" in namespace "gc-4612"
    May  4 12:15:40.854: INFO: Deleting pod "simpletest-rc-to-be-deleted-c2696" in namespace "gc-4612"
    May  4 12:15:40.876: INFO: Deleting pod "simpletest-rc-to-be-deleted-ckndk" in namespace "gc-4612"
    May  4 12:15:40.895: INFO: Deleting pod "simpletest-rc-to-be-deleted-dc8jg" in namespace "gc-4612"
    May  4 12:15:40.909: INFO: Deleting pod "simpletest-rc-to-be-deleted-dl64d" in namespace "gc-4612"
    May  4 12:15:40.929: INFO: Deleting pod "simpletest-rc-to-be-deleted-dr2vw" in namespace "gc-4612"
    May  4 12:15:40.946: INFO: Deleting pod "simpletest-rc-to-be-deleted-dvcxp" in namespace "gc-4612"
    May  4 12:15:40.957: INFO: Deleting pod "simpletest-rc-to-be-deleted-fkp92" in namespace "gc-4612"
    May  4 12:15:40.971: INFO: Deleting pod "simpletest-rc-to-be-deleted-gddjs" in namespace "gc-4612"
    May  4 12:15:40.987: INFO: Deleting pod "simpletest-rc-to-be-deleted-gdlj8" in namespace "gc-4612"
    May  4 12:15:41.003: INFO: Deleting pod "simpletest-rc-to-be-deleted-gtbdf" in namespace "gc-4612"
    May  4 12:15:41.024: INFO: Deleting pod "simpletest-rc-to-be-deleted-gvrz5" in namespace "gc-4612"
    May  4 12:15:41.039: INFO: Deleting pod "simpletest-rc-to-be-deleted-gwpmx" in namespace "gc-4612"
    May  4 12:15:41.053: INFO: Deleting pod "simpletest-rc-to-be-deleted-hmwqw" in namespace "gc-4612"
    May  4 12:15:41.069: INFO: Deleting pod "simpletest-rc-to-be-deleted-hpfhj" in namespace "gc-4612"
    May  4 12:15:41.080: INFO: Deleting pod "simpletest-rc-to-be-deleted-hsqql" in namespace "gc-4612"
    May  4 12:15:41.097: INFO: Deleting pod "simpletest-rc-to-be-deleted-j8gb2" in namespace "gc-4612"
    May  4 12:15:41.123: INFO: Deleting pod "simpletest-rc-to-be-deleted-jjjc8" in namespace "gc-4612"
    May  4 12:15:41.138: INFO: Deleting pod "simpletest-rc-to-be-deleted-jr8ks" in namespace "gc-4612"
    May  4 12:15:41.151: INFO: Deleting pod "simpletest-rc-to-be-deleted-k4wp9" in namespace "gc-4612"
    May  4 12:15:41.171: INFO: Deleting pod "simpletest-rc-to-be-deleted-k6s5z" in namespace "gc-4612"
    May  4 12:15:41.186: INFO: Deleting pod "simpletest-rc-to-be-deleted-khhfn" in namespace "gc-4612"
    May  4 12:15:41.205: INFO: Deleting pod "simpletest-rc-to-be-deleted-khmwz" in namespace "gc-4612"
    May  4 12:15:41.218: INFO: Deleting pod "simpletest-rc-to-be-deleted-kl6ld" in namespace "gc-4612"
    May  4 12:15:41.230: INFO: Deleting pod "simpletest-rc-to-be-deleted-l2v2d" in namespace "gc-4612"
    May  4 12:15:41.245: INFO: Deleting pod "simpletest-rc-to-be-deleted-l8tkm" in namespace "gc-4612"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    May  4 12:15:41.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-4612" for this suite. 05/04/23 12:15:41.271
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:15:41.294
May  4 12:15:41.294: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename dns 05/04/23 12:15:41.297
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:15:41.313
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:15:41.318
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 05/04/23 12:15:41.321
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 05/04/23 12:15:41.321
STEP: creating a pod to probe DNS 05/04/23 12:15:41.321
STEP: submitting the pod to kubernetes 05/04/23 12:15:41.321
May  4 12:15:41.331: INFO: Waiting up to 15m0s for pod "dns-test-6e3bf275-5548-4dbe-ad2f-8393172f5049" in namespace "dns-2289" to be "running"
May  4 12:15:41.334: INFO: Pod "dns-test-6e3bf275-5548-4dbe-ad2f-8393172f5049": Phase="Pending", Reason="", readiness=false. Elapsed: 2.924775ms
May  4 12:15:43.338: INFO: Pod "dns-test-6e3bf275-5548-4dbe-ad2f-8393172f5049": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007208751s
May  4 12:15:45.341: INFO: Pod "dns-test-6e3bf275-5548-4dbe-ad2f-8393172f5049": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010336879s
May  4 12:15:47.338: INFO: Pod "dns-test-6e3bf275-5548-4dbe-ad2f-8393172f5049": Phase="Running", Reason="", readiness=true. Elapsed: 6.007403936s
May  4 12:15:47.338: INFO: Pod "dns-test-6e3bf275-5548-4dbe-ad2f-8393172f5049" satisfied condition "running"
STEP: retrieving the pod 05/04/23 12:15:47.338
STEP: looking for the results for each expected name from probers 05/04/23 12:15:47.341
May  4 12:15:47.361: INFO: DNS probes using dns-2289/dns-test-6e3bf275-5548-4dbe-ad2f-8393172f5049 succeeded

STEP: deleting the pod 05/04/23 12:15:47.361
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
May  4 12:15:47.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2289" for this suite. 05/04/23 12:15:47.387
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","completed":143,"skipped":2814,"failed":0}
------------------------------
• [SLOW TEST] [6.098 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:15:41.294
    May  4 12:15:41.294: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename dns 05/04/23 12:15:41.297
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:15:41.313
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:15:41.318
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     05/04/23 12:15:41.321
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     05/04/23 12:15:41.321
    STEP: creating a pod to probe DNS 05/04/23 12:15:41.321
    STEP: submitting the pod to kubernetes 05/04/23 12:15:41.321
    May  4 12:15:41.331: INFO: Waiting up to 15m0s for pod "dns-test-6e3bf275-5548-4dbe-ad2f-8393172f5049" in namespace "dns-2289" to be "running"
    May  4 12:15:41.334: INFO: Pod "dns-test-6e3bf275-5548-4dbe-ad2f-8393172f5049": Phase="Pending", Reason="", readiness=false. Elapsed: 2.924775ms
    May  4 12:15:43.338: INFO: Pod "dns-test-6e3bf275-5548-4dbe-ad2f-8393172f5049": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007208751s
    May  4 12:15:45.341: INFO: Pod "dns-test-6e3bf275-5548-4dbe-ad2f-8393172f5049": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010336879s
    May  4 12:15:47.338: INFO: Pod "dns-test-6e3bf275-5548-4dbe-ad2f-8393172f5049": Phase="Running", Reason="", readiness=true. Elapsed: 6.007403936s
    May  4 12:15:47.338: INFO: Pod "dns-test-6e3bf275-5548-4dbe-ad2f-8393172f5049" satisfied condition "running"
    STEP: retrieving the pod 05/04/23 12:15:47.338
    STEP: looking for the results for each expected name from probers 05/04/23 12:15:47.341
    May  4 12:15:47.361: INFO: DNS probes using dns-2289/dns-test-6e3bf275-5548-4dbe-ad2f-8393172f5049 succeeded

    STEP: deleting the pod 05/04/23 12:15:47.361
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    May  4 12:15:47.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-2289" for this suite. 05/04/23 12:15:47.387
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:15:47.393
May  4 12:15:47.393: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename subpath 05/04/23 12:15:47.394
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:15:47.415
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:15:47.418
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 05/04/23 12:15:47.423
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-m44m 05/04/23 12:15:47.435
STEP: Creating a pod to test atomic-volume-subpath 05/04/23 12:15:47.435
May  4 12:15:47.448: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-m44m" in namespace "subpath-3986" to be "Succeeded or Failed"
May  4 12:15:47.452: INFO: Pod "pod-subpath-test-configmap-m44m": Phase="Pending", Reason="", readiness=false. Elapsed: 3.868328ms
May  4 12:15:49.457: INFO: Pod "pod-subpath-test-configmap-m44m": Phase="Running", Reason="", readiness=true. Elapsed: 2.008778373s
May  4 12:15:51.461: INFO: Pod "pod-subpath-test-configmap-m44m": Phase="Running", Reason="", readiness=true. Elapsed: 4.012892062s
May  4 12:15:53.456: INFO: Pod "pod-subpath-test-configmap-m44m": Phase="Running", Reason="", readiness=true. Elapsed: 6.008177767s
May  4 12:15:55.456: INFO: Pod "pod-subpath-test-configmap-m44m": Phase="Running", Reason="", readiness=true. Elapsed: 8.007873849s
May  4 12:15:57.456: INFO: Pod "pod-subpath-test-configmap-m44m": Phase="Running", Reason="", readiness=true. Elapsed: 10.008389691s
May  4 12:15:59.457: INFO: Pod "pod-subpath-test-configmap-m44m": Phase="Running", Reason="", readiness=true. Elapsed: 12.008783613s
May  4 12:16:01.458: INFO: Pod "pod-subpath-test-configmap-m44m": Phase="Running", Reason="", readiness=true. Elapsed: 14.009729414s
May  4 12:16:03.457: INFO: Pod "pod-subpath-test-configmap-m44m": Phase="Running", Reason="", readiness=true. Elapsed: 16.009224939s
May  4 12:16:05.457: INFO: Pod "pod-subpath-test-configmap-m44m": Phase="Running", Reason="", readiness=true. Elapsed: 18.008596148s
May  4 12:16:07.457: INFO: Pod "pod-subpath-test-configmap-m44m": Phase="Running", Reason="", readiness=true. Elapsed: 20.008716149s
May  4 12:16:09.457: INFO: Pod "pod-subpath-test-configmap-m44m": Phase="Running", Reason="", readiness=false. Elapsed: 22.008761154s
May  4 12:16:11.457: INFO: Pod "pod-subpath-test-configmap-m44m": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.008584036s
STEP: Saw pod success 05/04/23 12:16:11.457
May  4 12:16:11.457: INFO: Pod "pod-subpath-test-configmap-m44m" satisfied condition "Succeeded or Failed"
May  4 12:16:11.460: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-subpath-test-configmap-m44m container test-container-subpath-configmap-m44m: <nil>
STEP: delete the pod 05/04/23 12:16:11.466
May  4 12:16:11.478: INFO: Waiting for pod pod-subpath-test-configmap-m44m to disappear
May  4 12:16:11.481: INFO: Pod pod-subpath-test-configmap-m44m no longer exists
STEP: Deleting pod pod-subpath-test-configmap-m44m 05/04/23 12:16:11.481
May  4 12:16:11.481: INFO: Deleting pod "pod-subpath-test-configmap-m44m" in namespace "subpath-3986"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
May  4 12:16:11.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3986" for this suite. 05/04/23 12:16:11.49
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","completed":144,"skipped":2817,"failed":0}
------------------------------
• [SLOW TEST] [24.102 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:15:47.393
    May  4 12:15:47.393: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename subpath 05/04/23 12:15:47.394
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:15:47.415
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:15:47.418
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 05/04/23 12:15:47.423
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-m44m 05/04/23 12:15:47.435
    STEP: Creating a pod to test atomic-volume-subpath 05/04/23 12:15:47.435
    May  4 12:15:47.448: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-m44m" in namespace "subpath-3986" to be "Succeeded or Failed"
    May  4 12:15:47.452: INFO: Pod "pod-subpath-test-configmap-m44m": Phase="Pending", Reason="", readiness=false. Elapsed: 3.868328ms
    May  4 12:15:49.457: INFO: Pod "pod-subpath-test-configmap-m44m": Phase="Running", Reason="", readiness=true. Elapsed: 2.008778373s
    May  4 12:15:51.461: INFO: Pod "pod-subpath-test-configmap-m44m": Phase="Running", Reason="", readiness=true. Elapsed: 4.012892062s
    May  4 12:15:53.456: INFO: Pod "pod-subpath-test-configmap-m44m": Phase="Running", Reason="", readiness=true. Elapsed: 6.008177767s
    May  4 12:15:55.456: INFO: Pod "pod-subpath-test-configmap-m44m": Phase="Running", Reason="", readiness=true. Elapsed: 8.007873849s
    May  4 12:15:57.456: INFO: Pod "pod-subpath-test-configmap-m44m": Phase="Running", Reason="", readiness=true. Elapsed: 10.008389691s
    May  4 12:15:59.457: INFO: Pod "pod-subpath-test-configmap-m44m": Phase="Running", Reason="", readiness=true. Elapsed: 12.008783613s
    May  4 12:16:01.458: INFO: Pod "pod-subpath-test-configmap-m44m": Phase="Running", Reason="", readiness=true. Elapsed: 14.009729414s
    May  4 12:16:03.457: INFO: Pod "pod-subpath-test-configmap-m44m": Phase="Running", Reason="", readiness=true. Elapsed: 16.009224939s
    May  4 12:16:05.457: INFO: Pod "pod-subpath-test-configmap-m44m": Phase="Running", Reason="", readiness=true. Elapsed: 18.008596148s
    May  4 12:16:07.457: INFO: Pod "pod-subpath-test-configmap-m44m": Phase="Running", Reason="", readiness=true. Elapsed: 20.008716149s
    May  4 12:16:09.457: INFO: Pod "pod-subpath-test-configmap-m44m": Phase="Running", Reason="", readiness=false. Elapsed: 22.008761154s
    May  4 12:16:11.457: INFO: Pod "pod-subpath-test-configmap-m44m": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.008584036s
    STEP: Saw pod success 05/04/23 12:16:11.457
    May  4 12:16:11.457: INFO: Pod "pod-subpath-test-configmap-m44m" satisfied condition "Succeeded or Failed"
    May  4 12:16:11.460: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-subpath-test-configmap-m44m container test-container-subpath-configmap-m44m: <nil>
    STEP: delete the pod 05/04/23 12:16:11.466
    May  4 12:16:11.478: INFO: Waiting for pod pod-subpath-test-configmap-m44m to disappear
    May  4 12:16:11.481: INFO: Pod pod-subpath-test-configmap-m44m no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-m44m 05/04/23 12:16:11.481
    May  4 12:16:11.481: INFO: Deleting pod "pod-subpath-test-configmap-m44m" in namespace "subpath-3986"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    May  4 12:16:11.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-3986" for this suite. 05/04/23 12:16:11.49
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:16:11.499
May  4 12:16:11.499: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename sched-pred 05/04/23 12:16:11.501
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:16:11.515
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:16:11.521
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
May  4 12:16:11.525: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May  4 12:16:11.539: INFO: Waiting for terminating namespaces to be deleted...
May  4 12:16:11.547: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-189.us-west-2.compute.internal before test
May  4 12:16:11.570: INFO: calico-node-r8wl6 from kube-system started at 2023-05-04 11:14:50 +0000 UTC (1 container statuses recorded)
May  4 12:16:11.570: INFO: 	Container calico-node ready: true, restart count 0
May  4 12:16:11.570: INFO: calico-typha-85dbcd447f-6tg8g from kube-system started at 2023-05-04 11:15:25 +0000 UTC (1 container statuses recorded)
May  4 12:16:11.570: INFO: 	Container calico-typha ready: true, restart count 0
May  4 12:16:11.570: INFO: metrics-server-57bbf8548c-qkgc6 from kube-system started at 2023-05-04 11:16:29 +0000 UTC (1 container statuses recorded)
May  4 12:16:11.570: INFO: 	Container metrics-server ready: true, restart count 0
May  4 12:16:11.570: INFO: node-exporter-tsxz4 from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
May  4 12:16:11.570: INFO: 	Container node-exporter ready: true, restart count 0
May  4 12:16:11.570: INFO: prometheus-operator-85498c86bb-gdnsq from pf9-operators started at 2023-05-04 11:16:25 +0000 UTC (1 container statuses recorded)
May  4 12:16:11.570: INFO: 	Container prometheus-operator ready: true, restart count 0
May  4 12:16:11.570: INFO: sonobuoy-e2e-job-a8c15777d8b94166 from sonobuoy started at 2023-05-04 11:44:14 +0000 UTC (2 container statuses recorded)
May  4 12:16:11.570: INFO: 	Container e2e ready: true, restart count 0
May  4 12:16:11.570: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  4 12:16:11.570: INFO: sonobuoy-systemd-logs-daemon-set-d34856018c814171-f2qjv from sonobuoy started at 2023-05-04 11:44:14 +0000 UTC (2 container statuses recorded)
May  4 12:16:11.570: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  4 12:16:11.570: INFO: 	Container systemd-logs ready: true, restart count 0
May  4 12:16:11.570: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-216.us-west-2.compute.internal before test
May  4 12:16:11.600: INFO: calico-node-ll8r2 from kube-system started at 2023-05-04 11:15:13 +0000 UTC (1 container statuses recorded)
May  4 12:16:11.600: INFO: 	Container calico-node ready: true, restart count 0
May  4 12:16:11.600: INFO: calico-typha-85dbcd447f-qw96l from kube-system started at 2023-05-04 11:15:29 +0000 UTC (1 container statuses recorded)
May  4 12:16:11.600: INFO: 	Container calico-typha ready: true, restart count 0
May  4 12:16:11.600: INFO: grafana-84c9c8d6bd-rsbn8 from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (2 container statuses recorded)
May  4 12:16:11.600: INFO: 	Container grafana ready: true, restart count 0
May  4 12:16:11.600: INFO: 	Container proxy ready: true, restart count 0
May  4 12:16:11.600: INFO: node-exporter-pnw8x from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
May  4 12:16:11.600: INFO: 	Container node-exporter ready: true, restart count 0
May  4 12:16:11.600: INFO: sonobuoy from sonobuoy started at 2023-05-04 11:44:11 +0000 UTC (1 container statuses recorded)
May  4 12:16:11.600: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May  4 12:16:11.600: INFO: sonobuoy-systemd-logs-daemon-set-d34856018c814171-44cxg from sonobuoy started at 2023-05-04 11:44:14 +0000 UTC (2 container statuses recorded)
May  4 12:16:11.600: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  4 12:16:11.600: INFO: 	Container systemd-logs ready: true, restart count 0
May  4 12:16:11.600: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-224.us-west-2.compute.internal before test
May  4 12:16:11.625: INFO: calico-node-bb7nh from kube-system started at 2023-05-04 11:14:53 +0000 UTC (1 container statuses recorded)
May  4 12:16:11.625: INFO: 	Container calico-node ready: true, restart count 0
May  4 12:16:11.625: INFO: coredns-c7944df6b-gkv4x from kube-system started at 2023-05-04 11:16:34 +0000 UTC (1 container statuses recorded)
May  4 12:16:11.625: INFO: 	Container coredns ready: true, restart count 0
May  4 12:16:11.625: INFO: kube-dns-autoscaler-558cdf6846-vvkfg from kube-system started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
May  4 12:16:11.625: INFO: 	Container autoscaler ready: true, restart count 0
May  4 12:16:11.625: INFO: dashboard-metrics-scraper-7564894f4b-cpm4c from kubernetes-dashboard started at 2023-05-04 11:16:29 +0000 UTC (1 container statuses recorded)
May  4 12:16:11.625: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
May  4 12:16:11.625: INFO: kube-state-metrics-5bf549bd69-vwzfr from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
May  4 12:16:11.625: INFO: 	Container kube-state-metrics ready: true, restart count 0
May  4 12:16:11.625: INFO: node-exporter-qn2kc from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
May  4 12:16:11.625: INFO: 	Container node-exporter ready: true, restart count 0
May  4 12:16:11.625: INFO: sonobuoy-systemd-logs-daemon-set-d34856018c814171-lss27 from sonobuoy started at 2023-05-04 11:44:14 +0000 UTC (2 container statuses recorded)
May  4 12:16:11.625: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  4 12:16:11.625: INFO: 	Container systemd-logs ready: true, restart count 0
May  4 12:16:11.625: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-232.us-west-2.compute.internal before test
May  4 12:16:11.647: INFO: calico-kube-controllers-654d9ff976-ddwnz from kube-system started at 2023-05-04 11:15:10 +0000 UTC (1 container statuses recorded)
May  4 12:16:11.647: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May  4 12:16:11.647: INFO: calico-node-48gv4 from kube-system started at 2023-05-04 11:14:51 +0000 UTC (1 container statuses recorded)
May  4 12:16:11.647: INFO: 	Container calico-node ready: true, restart count 0
May  4 12:16:11.647: INFO: calico-typha-85dbcd447f-2s7df from kube-system started at 2023-05-04 11:15:10 +0000 UTC (1 container statuses recorded)
May  4 12:16:11.647: INFO: 	Container calico-typha ready: true, restart count 0
May  4 12:16:11.647: INFO: calico-typha-autoscaler-795696b9bd-qhx26 from kube-system started at 2023-05-04 11:15:10 +0000 UTC (1 container statuses recorded)
May  4 12:16:11.647: INFO: 	Container autoscaler ready: true, restart count 0
May  4 12:16:11.647: INFO: kube-state-metrics-857f6bcbbb-krrw9 from kube-system started at 2023-05-04 11:15:10 +0000 UTC (1 container statuses recorded)
May  4 12:16:11.647: INFO: 	Container kube-state-metrics ready: true, restart count 0
May  4 12:16:11.647: INFO: alertmanager-sysalert-0 from pf9-monitoring started at 2023-05-04 11:16:29 +0000 UTC (2 container statuses recorded)
May  4 12:16:11.647: INFO: 	Container alertmanager ready: true, restart count 1
May  4 12:16:11.647: INFO: 	Container config-reloader ready: true, restart count 0
May  4 12:16:11.647: INFO: node-exporter-49wcf from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
May  4 12:16:11.647: INFO: 	Container node-exporter ready: true, restart count 0
May  4 12:16:11.647: INFO: sonobuoy-systemd-logs-daemon-set-d34856018c814171-thbzc from sonobuoy started at 2023-05-04 11:44:14 +0000 UTC (2 container statuses recorded)
May  4 12:16:11.647: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  4 12:16:11.647: INFO: 	Container systemd-logs ready: true, restart count 0
May  4 12:16:11.647: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-253.us-west-2.compute.internal before test
May  4 12:16:11.672: INFO: calico-node-69h7w from kube-system started at 2023-05-04 11:14:49 +0000 UTC (1 container statuses recorded)
May  4 12:16:11.672: INFO: 	Container calico-node ready: true, restart count 0
May  4 12:16:11.672: INFO: coredns-c7944df6b-zhglb from kube-system started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
May  4 12:16:11.672: INFO: 	Container coredns ready: true, restart count 0
May  4 12:16:11.672: INFO: node-exporter-q8qnn from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
May  4 12:16:11.672: INFO: 	Container node-exporter ready: true, restart count 0
May  4 12:16:11.672: INFO: prometheus-system-0 from pf9-monitoring started at 2023-05-04 11:16:30 +0000 UTC (2 container statuses recorded)
May  4 12:16:11.672: INFO: 	Container config-reloader ready: true, restart count 0
May  4 12:16:11.672: INFO: 	Container prometheus ready: true, restart count 0
May  4 12:16:11.672: INFO: monhelper-57744bf759-6nndz from pf9-operators started at 2023-05-04 11:16:25 +0000 UTC (1 container statuses recorded)
May  4 12:16:11.672: INFO: 	Container monhelper ready: true, restart count 0
May  4 12:16:11.672: INFO: sonobuoy-systemd-logs-daemon-set-d34856018c814171-rx5lv from sonobuoy started at 2023-05-04 11:44:14 +0000 UTC (2 container statuses recorded)
May  4 12:16:11.672: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  4 12:16:11.672: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it. 05/04/23 12:16:11.672
May  4 12:16:11.684: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-2775" to be "running"
May  4 12:16:11.687: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.791404ms
May  4 12:16:13.692: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.008492755s
May  4 12:16:13.692: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 05/04/23 12:16:13.695
STEP: Trying to apply a random label on the found node. 05/04/23 12:16:13.713
STEP: verifying the node has the label kubernetes.io/e2e-416fca60-7b68-43ee-bcee-c377500e5312 95 05/04/23 12:16:13.731
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 05/04/23 12:16:13.736
May  4 12:16:13.742: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-2775" to be "not pending"
May  4 12:16:13.746: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.890851ms
May  4 12:16:15.750: INFO: Pod "pod4": Phase="Running", Reason="", readiness=false. Elapsed: 2.008299978s
May  4 12:16:15.750: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.0.1.224 on the node which pod4 resides and expect not scheduled 05/04/23 12:16:15.75
May  4 12:16:15.757: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-2775" to be "not pending"
May  4 12:16:15.760: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.086303ms
May  4 12:16:17.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007614678s
May  4 12:16:19.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011111433s
May  4 12:16:21.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007390502s
May  4 12:16:23.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008899852s
May  4 12:16:25.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.009450519s
May  4 12:16:27.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.007520097s
May  4 12:16:29.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.007601993s
May  4 12:16:31.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.008303673s
May  4 12:16:33.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.00696996s
May  4 12:16:35.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.009229042s
May  4 12:16:37.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.008733548s
May  4 12:16:39.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.008447374s
May  4 12:16:41.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.007974173s
May  4 12:16:43.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.007482307s
May  4 12:16:45.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.009286658s
May  4 12:16:47.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.007106129s
May  4 12:16:49.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.008472849s
May  4 12:16:51.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.008502969s
May  4 12:16:53.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.007222409s
May  4 12:16:55.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.009299639s
May  4 12:16:57.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.008048142s
May  4 12:16:59.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.008444368s
May  4 12:17:01.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.010990159s
May  4 12:17:03.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.007522803s
May  4 12:17:05.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.007971233s
May  4 12:17:07.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.007239956s
May  4 12:17:09.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.008309635s
May  4 12:17:11.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.007743608s
May  4 12:17:13.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.007521826s
May  4 12:17:15.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.007862181s
May  4 12:17:17.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.008235058s
May  4 12:17:19.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.010585805s
May  4 12:17:21.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.010161341s
May  4 12:17:23.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.007976624s
May  4 12:17:25.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.009800568s
May  4 12:17:27.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.008208761s
May  4 12:17:29.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.007293464s
May  4 12:17:31.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.00850267s
May  4 12:17:33.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.007644115s
May  4 12:17:35.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.0081201s
May  4 12:17:37.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.00684421s
May  4 12:17:39.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.008422203s
May  4 12:17:41.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.007466447s
May  4 12:17:43.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.008414823s
May  4 12:17:45.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.008548718s
May  4 12:17:47.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.007918433s
May  4 12:17:49.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.008099091s
May  4 12:17:51.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.0079053s
May  4 12:17:53.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.00774289s
May  4 12:17:55.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.008016954s
May  4 12:17:57.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.007711222s
May  4 12:17:59.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.00810026s
May  4 12:18:01.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.008185353s
May  4 12:18:03.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.006859752s
May  4 12:18:05.770: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.01373384s
May  4 12:18:07.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.008507001s
May  4 12:18:09.769: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.012442663s
May  4 12:18:11.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.008130407s
May  4 12:18:13.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.007233063s
May  4 12:18:15.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.008044681s
May  4 12:18:17.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.007303274s
May  4 12:18:19.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.008027187s
May  4 12:18:21.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.00819041s
May  4 12:18:23.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.0075309s
May  4 12:18:25.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.00764931s
May  4 12:18:27.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.007251218s
May  4 12:18:29.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.009705907s
May  4 12:18:31.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.007062085s
May  4 12:18:33.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.00774119s
May  4 12:18:35.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.008178961s
May  4 12:18:37.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.007631175s
May  4 12:18:39.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.008001916s
May  4 12:18:41.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.007803085s
May  4 12:18:43.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.007761212s
May  4 12:18:45.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.007677527s
May  4 12:18:47.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.008060706s
May  4 12:18:49.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.007970552s
May  4 12:18:51.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.008422s
May  4 12:18:53.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.008346895s
May  4 12:18:55.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.008454906s
May  4 12:18:57.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.008074482s
May  4 12:18:59.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.007124662s
May  4 12:19:01.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.008062864s
May  4 12:19:03.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.009039201s
May  4 12:19:05.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.008763395s
May  4 12:19:07.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.007499056s
May  4 12:19:09.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.008353531s
May  4 12:19:11.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.0070979s
May  4 12:19:13.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.00766006s
May  4 12:19:15.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.00887119s
May  4 12:19:17.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.007216498s
May  4 12:19:19.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.008846062s
May  4 12:19:21.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.008185314s
May  4 12:19:23.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.008444911s
May  4 12:19:25.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.008142308s
May  4 12:19:27.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.007084971s
May  4 12:19:29.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.007339203s
May  4 12:19:31.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.008245169s
May  4 12:19:33.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.007947143s
May  4 12:19:35.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.008826831s
May  4 12:19:37.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.008698739s
May  4 12:19:39.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.008553831s
May  4 12:19:41.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.007386815s
May  4 12:19:43.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.007073218s
May  4 12:19:45.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.008994381s
May  4 12:19:47.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.008528941s
May  4 12:19:49.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.00838389s
May  4 12:19:51.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.008932697s
May  4 12:19:53.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.007671005s
May  4 12:19:55.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.008085241s
May  4 12:19:57.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.007535873s
May  4 12:19:59.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.008521569s
May  4 12:20:01.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.008013469s
May  4 12:20:03.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.007643811s
May  4 12:20:05.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.008019688s
May  4 12:20:07.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.007085641s
May  4 12:20:09.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.008433004s
May  4 12:20:11.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.008411911s
May  4 12:20:13.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.007535114s
May  4 12:20:15.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.008463086s
May  4 12:20:17.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.00840369s
May  4 12:20:19.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.007911228s
May  4 12:20:21.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.009089904s
May  4 12:20:23.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.00705128s
May  4 12:20:25.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.008326872s
May  4 12:20:27.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.00792107s
May  4 12:20:29.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.008296885s
May  4 12:20:31.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.007161541s
May  4 12:20:33.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.008414837s
May  4 12:20:35.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.007997331s
May  4 12:20:37.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.007045646s
May  4 12:20:39.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.010840199s
May  4 12:20:41.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.007293452s
May  4 12:20:43.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.007783847s
May  4 12:20:45.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.008449359s
May  4 12:20:47.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.007636687s
May  4 12:20:49.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.008581512s
May  4 12:20:51.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.009428532s
May  4 12:20:53.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.008548167s
May  4 12:20:55.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.00779084s
May  4 12:20:57.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.007943536s
May  4 12:20:59.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.006872469s
May  4 12:21:01.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.008420798s
May  4 12:21:03.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.007152904s
May  4 12:21:05.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.007982749s
May  4 12:21:07.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.007096601s
May  4 12:21:09.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.008192176s
May  4 12:21:11.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.010090452s
May  4 12:21:13.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.00782704s
May  4 12:21:15.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.007801261s
May  4 12:21:15.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.010899927s
STEP: removing the label kubernetes.io/e2e-416fca60-7b68-43ee-bcee-c377500e5312 off the node ip-10-0-1-224.us-west-2.compute.internal 05/04/23 12:21:15.768
STEP: verifying the node doesn't have the label kubernetes.io/e2e-416fca60-7b68-43ee-bcee-c377500e5312 05/04/23 12:21:15.785
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
May  4 12:21:15.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2775" for this suite. 05/04/23 12:21:15.804
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","completed":145,"skipped":2874,"failed":0}
------------------------------
• [SLOW TEST] [304.315 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:16:11.499
    May  4 12:16:11.499: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename sched-pred 05/04/23 12:16:11.501
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:16:11.515
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:16:11.521
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    May  4 12:16:11.525: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    May  4 12:16:11.539: INFO: Waiting for terminating namespaces to be deleted...
    May  4 12:16:11.547: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-1-189.us-west-2.compute.internal before test
    May  4 12:16:11.570: INFO: calico-node-r8wl6 from kube-system started at 2023-05-04 11:14:50 +0000 UTC (1 container statuses recorded)
    May  4 12:16:11.570: INFO: 	Container calico-node ready: true, restart count 0
    May  4 12:16:11.570: INFO: calico-typha-85dbcd447f-6tg8g from kube-system started at 2023-05-04 11:15:25 +0000 UTC (1 container statuses recorded)
    May  4 12:16:11.570: INFO: 	Container calico-typha ready: true, restart count 0
    May  4 12:16:11.570: INFO: metrics-server-57bbf8548c-qkgc6 from kube-system started at 2023-05-04 11:16:29 +0000 UTC (1 container statuses recorded)
    May  4 12:16:11.570: INFO: 	Container metrics-server ready: true, restart count 0
    May  4 12:16:11.570: INFO: node-exporter-tsxz4 from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
    May  4 12:16:11.570: INFO: 	Container node-exporter ready: true, restart count 0
    May  4 12:16:11.570: INFO: prometheus-operator-85498c86bb-gdnsq from pf9-operators started at 2023-05-04 11:16:25 +0000 UTC (1 container statuses recorded)
    May  4 12:16:11.570: INFO: 	Container prometheus-operator ready: true, restart count 0
    May  4 12:16:11.570: INFO: sonobuoy-e2e-job-a8c15777d8b94166 from sonobuoy started at 2023-05-04 11:44:14 +0000 UTC (2 container statuses recorded)
    May  4 12:16:11.570: INFO: 	Container e2e ready: true, restart count 0
    May  4 12:16:11.570: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  4 12:16:11.570: INFO: sonobuoy-systemd-logs-daemon-set-d34856018c814171-f2qjv from sonobuoy started at 2023-05-04 11:44:14 +0000 UTC (2 container statuses recorded)
    May  4 12:16:11.570: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  4 12:16:11.570: INFO: 	Container systemd-logs ready: true, restart count 0
    May  4 12:16:11.570: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-1-216.us-west-2.compute.internal before test
    May  4 12:16:11.600: INFO: calico-node-ll8r2 from kube-system started at 2023-05-04 11:15:13 +0000 UTC (1 container statuses recorded)
    May  4 12:16:11.600: INFO: 	Container calico-node ready: true, restart count 0
    May  4 12:16:11.600: INFO: calico-typha-85dbcd447f-qw96l from kube-system started at 2023-05-04 11:15:29 +0000 UTC (1 container statuses recorded)
    May  4 12:16:11.600: INFO: 	Container calico-typha ready: true, restart count 0
    May  4 12:16:11.600: INFO: grafana-84c9c8d6bd-rsbn8 from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (2 container statuses recorded)
    May  4 12:16:11.600: INFO: 	Container grafana ready: true, restart count 0
    May  4 12:16:11.600: INFO: 	Container proxy ready: true, restart count 0
    May  4 12:16:11.600: INFO: node-exporter-pnw8x from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
    May  4 12:16:11.600: INFO: 	Container node-exporter ready: true, restart count 0
    May  4 12:16:11.600: INFO: sonobuoy from sonobuoy started at 2023-05-04 11:44:11 +0000 UTC (1 container statuses recorded)
    May  4 12:16:11.600: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    May  4 12:16:11.600: INFO: sonobuoy-systemd-logs-daemon-set-d34856018c814171-44cxg from sonobuoy started at 2023-05-04 11:44:14 +0000 UTC (2 container statuses recorded)
    May  4 12:16:11.600: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  4 12:16:11.600: INFO: 	Container systemd-logs ready: true, restart count 0
    May  4 12:16:11.600: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-1-224.us-west-2.compute.internal before test
    May  4 12:16:11.625: INFO: calico-node-bb7nh from kube-system started at 2023-05-04 11:14:53 +0000 UTC (1 container statuses recorded)
    May  4 12:16:11.625: INFO: 	Container calico-node ready: true, restart count 0
    May  4 12:16:11.625: INFO: coredns-c7944df6b-gkv4x from kube-system started at 2023-05-04 11:16:34 +0000 UTC (1 container statuses recorded)
    May  4 12:16:11.625: INFO: 	Container coredns ready: true, restart count 0
    May  4 12:16:11.625: INFO: kube-dns-autoscaler-558cdf6846-vvkfg from kube-system started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
    May  4 12:16:11.625: INFO: 	Container autoscaler ready: true, restart count 0
    May  4 12:16:11.625: INFO: dashboard-metrics-scraper-7564894f4b-cpm4c from kubernetes-dashboard started at 2023-05-04 11:16:29 +0000 UTC (1 container statuses recorded)
    May  4 12:16:11.625: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    May  4 12:16:11.625: INFO: kube-state-metrics-5bf549bd69-vwzfr from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
    May  4 12:16:11.625: INFO: 	Container kube-state-metrics ready: true, restart count 0
    May  4 12:16:11.625: INFO: node-exporter-qn2kc from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
    May  4 12:16:11.625: INFO: 	Container node-exporter ready: true, restart count 0
    May  4 12:16:11.625: INFO: sonobuoy-systemd-logs-daemon-set-d34856018c814171-lss27 from sonobuoy started at 2023-05-04 11:44:14 +0000 UTC (2 container statuses recorded)
    May  4 12:16:11.625: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  4 12:16:11.625: INFO: 	Container systemd-logs ready: true, restart count 0
    May  4 12:16:11.625: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-1-232.us-west-2.compute.internal before test
    May  4 12:16:11.647: INFO: calico-kube-controllers-654d9ff976-ddwnz from kube-system started at 2023-05-04 11:15:10 +0000 UTC (1 container statuses recorded)
    May  4 12:16:11.647: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    May  4 12:16:11.647: INFO: calico-node-48gv4 from kube-system started at 2023-05-04 11:14:51 +0000 UTC (1 container statuses recorded)
    May  4 12:16:11.647: INFO: 	Container calico-node ready: true, restart count 0
    May  4 12:16:11.647: INFO: calico-typha-85dbcd447f-2s7df from kube-system started at 2023-05-04 11:15:10 +0000 UTC (1 container statuses recorded)
    May  4 12:16:11.647: INFO: 	Container calico-typha ready: true, restart count 0
    May  4 12:16:11.647: INFO: calico-typha-autoscaler-795696b9bd-qhx26 from kube-system started at 2023-05-04 11:15:10 +0000 UTC (1 container statuses recorded)
    May  4 12:16:11.647: INFO: 	Container autoscaler ready: true, restart count 0
    May  4 12:16:11.647: INFO: kube-state-metrics-857f6bcbbb-krrw9 from kube-system started at 2023-05-04 11:15:10 +0000 UTC (1 container statuses recorded)
    May  4 12:16:11.647: INFO: 	Container kube-state-metrics ready: true, restart count 0
    May  4 12:16:11.647: INFO: alertmanager-sysalert-0 from pf9-monitoring started at 2023-05-04 11:16:29 +0000 UTC (2 container statuses recorded)
    May  4 12:16:11.647: INFO: 	Container alertmanager ready: true, restart count 1
    May  4 12:16:11.647: INFO: 	Container config-reloader ready: true, restart count 0
    May  4 12:16:11.647: INFO: node-exporter-49wcf from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
    May  4 12:16:11.647: INFO: 	Container node-exporter ready: true, restart count 0
    May  4 12:16:11.647: INFO: sonobuoy-systemd-logs-daemon-set-d34856018c814171-thbzc from sonobuoy started at 2023-05-04 11:44:14 +0000 UTC (2 container statuses recorded)
    May  4 12:16:11.647: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  4 12:16:11.647: INFO: 	Container systemd-logs ready: true, restart count 0
    May  4 12:16:11.647: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-1-253.us-west-2.compute.internal before test
    May  4 12:16:11.672: INFO: calico-node-69h7w from kube-system started at 2023-05-04 11:14:49 +0000 UTC (1 container statuses recorded)
    May  4 12:16:11.672: INFO: 	Container calico-node ready: true, restart count 0
    May  4 12:16:11.672: INFO: coredns-c7944df6b-zhglb from kube-system started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
    May  4 12:16:11.672: INFO: 	Container coredns ready: true, restart count 0
    May  4 12:16:11.672: INFO: node-exporter-q8qnn from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
    May  4 12:16:11.672: INFO: 	Container node-exporter ready: true, restart count 0
    May  4 12:16:11.672: INFO: prometheus-system-0 from pf9-monitoring started at 2023-05-04 11:16:30 +0000 UTC (2 container statuses recorded)
    May  4 12:16:11.672: INFO: 	Container config-reloader ready: true, restart count 0
    May  4 12:16:11.672: INFO: 	Container prometheus ready: true, restart count 0
    May  4 12:16:11.672: INFO: monhelper-57744bf759-6nndz from pf9-operators started at 2023-05-04 11:16:25 +0000 UTC (1 container statuses recorded)
    May  4 12:16:11.672: INFO: 	Container monhelper ready: true, restart count 0
    May  4 12:16:11.672: INFO: sonobuoy-systemd-logs-daemon-set-d34856018c814171-rx5lv from sonobuoy started at 2023-05-04 11:44:14 +0000 UTC (2 container statuses recorded)
    May  4 12:16:11.672: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  4 12:16:11.672: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:699
    STEP: Trying to launch a pod without a label to get a node which can launch it. 05/04/23 12:16:11.672
    May  4 12:16:11.684: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-2775" to be "running"
    May  4 12:16:11.687: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.791404ms
    May  4 12:16:13.692: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.008492755s
    May  4 12:16:13.692: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 05/04/23 12:16:13.695
    STEP: Trying to apply a random label on the found node. 05/04/23 12:16:13.713
    STEP: verifying the node has the label kubernetes.io/e2e-416fca60-7b68-43ee-bcee-c377500e5312 95 05/04/23 12:16:13.731
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 05/04/23 12:16:13.736
    May  4 12:16:13.742: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-2775" to be "not pending"
    May  4 12:16:13.746: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.890851ms
    May  4 12:16:15.750: INFO: Pod "pod4": Phase="Running", Reason="", readiness=false. Elapsed: 2.008299978s
    May  4 12:16:15.750: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.0.1.224 on the node which pod4 resides and expect not scheduled 05/04/23 12:16:15.75
    May  4 12:16:15.757: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-2775" to be "not pending"
    May  4 12:16:15.760: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.086303ms
    May  4 12:16:17.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007614678s
    May  4 12:16:19.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011111433s
    May  4 12:16:21.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007390502s
    May  4 12:16:23.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008899852s
    May  4 12:16:25.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.009450519s
    May  4 12:16:27.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.007520097s
    May  4 12:16:29.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.007601993s
    May  4 12:16:31.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.008303673s
    May  4 12:16:33.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.00696996s
    May  4 12:16:35.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.009229042s
    May  4 12:16:37.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.008733548s
    May  4 12:16:39.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.008447374s
    May  4 12:16:41.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.007974173s
    May  4 12:16:43.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.007482307s
    May  4 12:16:45.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.009286658s
    May  4 12:16:47.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.007106129s
    May  4 12:16:49.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.008472849s
    May  4 12:16:51.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.008502969s
    May  4 12:16:53.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.007222409s
    May  4 12:16:55.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.009299639s
    May  4 12:16:57.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.008048142s
    May  4 12:16:59.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.008444368s
    May  4 12:17:01.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.010990159s
    May  4 12:17:03.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.007522803s
    May  4 12:17:05.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.007971233s
    May  4 12:17:07.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.007239956s
    May  4 12:17:09.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.008309635s
    May  4 12:17:11.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.007743608s
    May  4 12:17:13.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.007521826s
    May  4 12:17:15.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.007862181s
    May  4 12:17:17.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.008235058s
    May  4 12:17:19.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.010585805s
    May  4 12:17:21.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.010161341s
    May  4 12:17:23.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.007976624s
    May  4 12:17:25.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.009800568s
    May  4 12:17:27.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.008208761s
    May  4 12:17:29.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.007293464s
    May  4 12:17:31.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.00850267s
    May  4 12:17:33.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.007644115s
    May  4 12:17:35.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.0081201s
    May  4 12:17:37.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.00684421s
    May  4 12:17:39.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.008422203s
    May  4 12:17:41.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.007466447s
    May  4 12:17:43.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.008414823s
    May  4 12:17:45.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.008548718s
    May  4 12:17:47.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.007918433s
    May  4 12:17:49.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.008099091s
    May  4 12:17:51.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.0079053s
    May  4 12:17:53.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.00774289s
    May  4 12:17:55.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.008016954s
    May  4 12:17:57.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.007711222s
    May  4 12:17:59.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.00810026s
    May  4 12:18:01.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.008185353s
    May  4 12:18:03.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.006859752s
    May  4 12:18:05.770: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.01373384s
    May  4 12:18:07.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.008507001s
    May  4 12:18:09.769: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.012442663s
    May  4 12:18:11.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.008130407s
    May  4 12:18:13.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.007233063s
    May  4 12:18:15.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.008044681s
    May  4 12:18:17.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.007303274s
    May  4 12:18:19.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.008027187s
    May  4 12:18:21.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.00819041s
    May  4 12:18:23.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.0075309s
    May  4 12:18:25.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.00764931s
    May  4 12:18:27.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.007251218s
    May  4 12:18:29.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.009705907s
    May  4 12:18:31.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.007062085s
    May  4 12:18:33.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.00774119s
    May  4 12:18:35.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.008178961s
    May  4 12:18:37.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.007631175s
    May  4 12:18:39.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.008001916s
    May  4 12:18:41.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.007803085s
    May  4 12:18:43.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.007761212s
    May  4 12:18:45.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.007677527s
    May  4 12:18:47.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.008060706s
    May  4 12:18:49.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.007970552s
    May  4 12:18:51.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.008422s
    May  4 12:18:53.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.008346895s
    May  4 12:18:55.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.008454906s
    May  4 12:18:57.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.008074482s
    May  4 12:18:59.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.007124662s
    May  4 12:19:01.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.008062864s
    May  4 12:19:03.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.009039201s
    May  4 12:19:05.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.008763395s
    May  4 12:19:07.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.007499056s
    May  4 12:19:09.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.008353531s
    May  4 12:19:11.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.0070979s
    May  4 12:19:13.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.00766006s
    May  4 12:19:15.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.00887119s
    May  4 12:19:17.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.007216498s
    May  4 12:19:19.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.008846062s
    May  4 12:19:21.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.008185314s
    May  4 12:19:23.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.008444911s
    May  4 12:19:25.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.008142308s
    May  4 12:19:27.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.007084971s
    May  4 12:19:29.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.007339203s
    May  4 12:19:31.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.008245169s
    May  4 12:19:33.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.007947143s
    May  4 12:19:35.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.008826831s
    May  4 12:19:37.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.008698739s
    May  4 12:19:39.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.008553831s
    May  4 12:19:41.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.007386815s
    May  4 12:19:43.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.007073218s
    May  4 12:19:45.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.008994381s
    May  4 12:19:47.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.008528941s
    May  4 12:19:49.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.00838389s
    May  4 12:19:51.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.008932697s
    May  4 12:19:53.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.007671005s
    May  4 12:19:55.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.008085241s
    May  4 12:19:57.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.007535873s
    May  4 12:19:59.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.008521569s
    May  4 12:20:01.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.008013469s
    May  4 12:20:03.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.007643811s
    May  4 12:20:05.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.008019688s
    May  4 12:20:07.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.007085641s
    May  4 12:20:09.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.008433004s
    May  4 12:20:11.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.008411911s
    May  4 12:20:13.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.007535114s
    May  4 12:20:15.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.008463086s
    May  4 12:20:17.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.00840369s
    May  4 12:20:19.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.007911228s
    May  4 12:20:21.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.009089904s
    May  4 12:20:23.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.00705128s
    May  4 12:20:25.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.008326872s
    May  4 12:20:27.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.00792107s
    May  4 12:20:29.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.008296885s
    May  4 12:20:31.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.007161541s
    May  4 12:20:33.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.008414837s
    May  4 12:20:35.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.007997331s
    May  4 12:20:37.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.007045646s
    May  4 12:20:39.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.010840199s
    May  4 12:20:41.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.007293452s
    May  4 12:20:43.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.007783847s
    May  4 12:20:45.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.008449359s
    May  4 12:20:47.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.007636687s
    May  4 12:20:49.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.008581512s
    May  4 12:20:51.766: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.009428532s
    May  4 12:20:53.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.008548167s
    May  4 12:20:55.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.00779084s
    May  4 12:20:57.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.007943536s
    May  4 12:20:59.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.006872469s
    May  4 12:21:01.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.008420798s
    May  4 12:21:03.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.007152904s
    May  4 12:21:05.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.007982749s
    May  4 12:21:07.764: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.007096601s
    May  4 12:21:09.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.008192176s
    May  4 12:21:11.767: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.010090452s
    May  4 12:21:13.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.00782704s
    May  4 12:21:15.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.007801261s
    May  4 12:21:15.768: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.010899927s
    STEP: removing the label kubernetes.io/e2e-416fca60-7b68-43ee-bcee-c377500e5312 off the node ip-10-0-1-224.us-west-2.compute.internal 05/04/23 12:21:15.768
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-416fca60-7b68-43ee-bcee-c377500e5312 05/04/23 12:21:15.785
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    May  4 12:21:15.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-2775" for this suite. 05/04/23 12:21:15.804
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:21:15.814
May  4 12:21:15.814: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename podtemplate 05/04/23 12:21:15.815
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:21:15.833
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:21:15.84
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
May  4 12:21:15.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-1254" for this suite. 05/04/23 12:21:15.898
{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","completed":146,"skipped":2878,"failed":0}
------------------------------
• [0.103 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:21:15.814
    May  4 12:21:15.814: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename podtemplate 05/04/23 12:21:15.815
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:21:15.833
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:21:15.84
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    May  4 12:21:15.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-1254" for this suite. 05/04/23 12:21:15.898
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:21:15.918
May  4 12:21:15.918: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename services 05/04/23 12:21:15.922
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:21:15.946
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:21:15.95
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
STEP: creating service in namespace services-1810 05/04/23 12:21:15.957
May  4 12:21:15.973: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-1810" to be "running and ready"
May  4 12:21:15.978: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 5.788005ms
May  4 12:21:15.979: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
May  4 12:21:17.982: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.009738919s
May  4 12:21:17.982: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
May  4 12:21:17.982: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
May  4 12:21:17.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-1810 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
May  4 12:21:18.130: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
May  4 12:21:18.130: INFO: stdout: "ipvs"
May  4 12:21:18.130: INFO: proxyMode: ipvs
May  4 12:21:18.143: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May  4 12:21:18.148: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-1810 05/04/23 12:21:18.148
STEP: creating replication controller affinity-clusterip-timeout in namespace services-1810 05/04/23 12:21:18.163
I0504 12:21:18.177543      21 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-1810, replica count: 3
I0504 12:21:21.229353      21 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  4 12:21:21.239: INFO: Creating new exec pod
May  4 12:21:21.248: INFO: Waiting up to 5m0s for pod "execpod-affinitywxnkp" in namespace "services-1810" to be "running"
May  4 12:21:21.251: INFO: Pod "execpod-affinitywxnkp": Phase="Pending", Reason="", readiness=false. Elapsed: 3.040127ms
May  4 12:21:23.256: INFO: Pod "execpod-affinitywxnkp": Phase="Running", Reason="", readiness=true. Elapsed: 2.007947638s
May  4 12:21:23.256: INFO: Pod "execpod-affinitywxnkp" satisfied condition "running"
May  4 12:21:24.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-1810 exec execpod-affinitywxnkp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
May  4 12:21:24.447: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
May  4 12:21:24.447: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  4 12:21:24.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-1810 exec execpod-affinitywxnkp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.21.101.161 80'
May  4 12:21:24.645: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.21.101.161 80\nConnection to 10.21.101.161 80 port [tcp/http] succeeded!\n"
May  4 12:21:24.645: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  4 12:21:24.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-1810 exec execpod-affinitywxnkp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.21.101.161:80/ ; done'
May  4 12:21:24.907: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.101.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.101.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.101.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.101.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.101.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.101.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.101.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.101.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.101.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.101.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.101.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.101.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.101.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.101.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.101.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.101.161:80/\n"
May  4 12:21:24.907: INFO: stdout: "\naffinity-clusterip-timeout-j6kg5\naffinity-clusterip-timeout-j6kg5\naffinity-clusterip-timeout-j6kg5\naffinity-clusterip-timeout-j6kg5\naffinity-clusterip-timeout-j6kg5\naffinity-clusterip-timeout-j6kg5\naffinity-clusterip-timeout-j6kg5\naffinity-clusterip-timeout-j6kg5\naffinity-clusterip-timeout-j6kg5\naffinity-clusterip-timeout-j6kg5\naffinity-clusterip-timeout-j6kg5\naffinity-clusterip-timeout-j6kg5\naffinity-clusterip-timeout-j6kg5\naffinity-clusterip-timeout-j6kg5\naffinity-clusterip-timeout-j6kg5\naffinity-clusterip-timeout-j6kg5"
May  4 12:21:24.907: INFO: Received response from host: affinity-clusterip-timeout-j6kg5
May  4 12:21:24.907: INFO: Received response from host: affinity-clusterip-timeout-j6kg5
May  4 12:21:24.907: INFO: Received response from host: affinity-clusterip-timeout-j6kg5
May  4 12:21:24.907: INFO: Received response from host: affinity-clusterip-timeout-j6kg5
May  4 12:21:24.907: INFO: Received response from host: affinity-clusterip-timeout-j6kg5
May  4 12:21:24.907: INFO: Received response from host: affinity-clusterip-timeout-j6kg5
May  4 12:21:24.907: INFO: Received response from host: affinity-clusterip-timeout-j6kg5
May  4 12:21:24.907: INFO: Received response from host: affinity-clusterip-timeout-j6kg5
May  4 12:21:24.907: INFO: Received response from host: affinity-clusterip-timeout-j6kg5
May  4 12:21:24.907: INFO: Received response from host: affinity-clusterip-timeout-j6kg5
May  4 12:21:24.907: INFO: Received response from host: affinity-clusterip-timeout-j6kg5
May  4 12:21:24.907: INFO: Received response from host: affinity-clusterip-timeout-j6kg5
May  4 12:21:24.907: INFO: Received response from host: affinity-clusterip-timeout-j6kg5
May  4 12:21:24.907: INFO: Received response from host: affinity-clusterip-timeout-j6kg5
May  4 12:21:24.907: INFO: Received response from host: affinity-clusterip-timeout-j6kg5
May  4 12:21:24.907: INFO: Received response from host: affinity-clusterip-timeout-j6kg5
May  4 12:21:24.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-1810 exec execpod-affinitywxnkp -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.21.101.161:80/'
May  4 12:21:25.071: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.21.101.161:80/\n"
May  4 12:21:25.071: INFO: stdout: "affinity-clusterip-timeout-j6kg5"
May  4 12:23:35.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-1810 exec execpod-affinitywxnkp -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.21.101.161:80/'
May  4 12:23:35.254: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.21.101.161:80/\n"
May  4 12:23:35.254: INFO: stdout: "affinity-clusterip-timeout-4kvmk"
May  4 12:23:35.254: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-1810, will wait for the garbage collector to delete the pods 05/04/23 12:23:35.274
May  4 12:23:35.337: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 9.838937ms
May  4 12:23:35.539: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 201.112982ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  4 12:23:38.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1810" for this suite. 05/04/23 12:23:38.076
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","completed":147,"skipped":2885,"failed":0}
------------------------------
• [SLOW TEST] [142.166 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:21:15.918
    May  4 12:21:15.918: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename services 05/04/23 12:21:15.922
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:21:15.946
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:21:15.95
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2173
    STEP: creating service in namespace services-1810 05/04/23 12:21:15.957
    May  4 12:21:15.973: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-1810" to be "running and ready"
    May  4 12:21:15.978: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 5.788005ms
    May  4 12:21:15.979: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    May  4 12:21:17.982: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.009738919s
    May  4 12:21:17.982: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    May  4 12:21:17.982: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    May  4 12:21:17.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-1810 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    May  4 12:21:18.130: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    May  4 12:21:18.130: INFO: stdout: "ipvs"
    May  4 12:21:18.130: INFO: proxyMode: ipvs
    May  4 12:21:18.143: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    May  4 12:21:18.148: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-clusterip-timeout in namespace services-1810 05/04/23 12:21:18.148
    STEP: creating replication controller affinity-clusterip-timeout in namespace services-1810 05/04/23 12:21:18.163
    I0504 12:21:18.177543      21 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-1810, replica count: 3
    I0504 12:21:21.229353      21 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  4 12:21:21.239: INFO: Creating new exec pod
    May  4 12:21:21.248: INFO: Waiting up to 5m0s for pod "execpod-affinitywxnkp" in namespace "services-1810" to be "running"
    May  4 12:21:21.251: INFO: Pod "execpod-affinitywxnkp": Phase="Pending", Reason="", readiness=false. Elapsed: 3.040127ms
    May  4 12:21:23.256: INFO: Pod "execpod-affinitywxnkp": Phase="Running", Reason="", readiness=true. Elapsed: 2.007947638s
    May  4 12:21:23.256: INFO: Pod "execpod-affinitywxnkp" satisfied condition "running"
    May  4 12:21:24.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-1810 exec execpod-affinitywxnkp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
    May  4 12:21:24.447: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
    May  4 12:21:24.447: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  4 12:21:24.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-1810 exec execpod-affinitywxnkp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.21.101.161 80'
    May  4 12:21:24.645: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.21.101.161 80\nConnection to 10.21.101.161 80 port [tcp/http] succeeded!\n"
    May  4 12:21:24.645: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  4 12:21:24.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-1810 exec execpod-affinitywxnkp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.21.101.161:80/ ; done'
    May  4 12:21:24.907: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.101.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.101.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.101.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.101.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.101.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.101.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.101.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.101.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.101.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.101.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.101.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.101.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.101.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.101.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.101.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.101.161:80/\n"
    May  4 12:21:24.907: INFO: stdout: "\naffinity-clusterip-timeout-j6kg5\naffinity-clusterip-timeout-j6kg5\naffinity-clusterip-timeout-j6kg5\naffinity-clusterip-timeout-j6kg5\naffinity-clusterip-timeout-j6kg5\naffinity-clusterip-timeout-j6kg5\naffinity-clusterip-timeout-j6kg5\naffinity-clusterip-timeout-j6kg5\naffinity-clusterip-timeout-j6kg5\naffinity-clusterip-timeout-j6kg5\naffinity-clusterip-timeout-j6kg5\naffinity-clusterip-timeout-j6kg5\naffinity-clusterip-timeout-j6kg5\naffinity-clusterip-timeout-j6kg5\naffinity-clusterip-timeout-j6kg5\naffinity-clusterip-timeout-j6kg5"
    May  4 12:21:24.907: INFO: Received response from host: affinity-clusterip-timeout-j6kg5
    May  4 12:21:24.907: INFO: Received response from host: affinity-clusterip-timeout-j6kg5
    May  4 12:21:24.907: INFO: Received response from host: affinity-clusterip-timeout-j6kg5
    May  4 12:21:24.907: INFO: Received response from host: affinity-clusterip-timeout-j6kg5
    May  4 12:21:24.907: INFO: Received response from host: affinity-clusterip-timeout-j6kg5
    May  4 12:21:24.907: INFO: Received response from host: affinity-clusterip-timeout-j6kg5
    May  4 12:21:24.907: INFO: Received response from host: affinity-clusterip-timeout-j6kg5
    May  4 12:21:24.907: INFO: Received response from host: affinity-clusterip-timeout-j6kg5
    May  4 12:21:24.907: INFO: Received response from host: affinity-clusterip-timeout-j6kg5
    May  4 12:21:24.907: INFO: Received response from host: affinity-clusterip-timeout-j6kg5
    May  4 12:21:24.907: INFO: Received response from host: affinity-clusterip-timeout-j6kg5
    May  4 12:21:24.907: INFO: Received response from host: affinity-clusterip-timeout-j6kg5
    May  4 12:21:24.907: INFO: Received response from host: affinity-clusterip-timeout-j6kg5
    May  4 12:21:24.907: INFO: Received response from host: affinity-clusterip-timeout-j6kg5
    May  4 12:21:24.907: INFO: Received response from host: affinity-clusterip-timeout-j6kg5
    May  4 12:21:24.907: INFO: Received response from host: affinity-clusterip-timeout-j6kg5
    May  4 12:21:24.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-1810 exec execpod-affinitywxnkp -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.21.101.161:80/'
    May  4 12:21:25.071: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.21.101.161:80/\n"
    May  4 12:21:25.071: INFO: stdout: "affinity-clusterip-timeout-j6kg5"
    May  4 12:23:35.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-1810 exec execpod-affinitywxnkp -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.21.101.161:80/'
    May  4 12:23:35.254: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.21.101.161:80/\n"
    May  4 12:23:35.254: INFO: stdout: "affinity-clusterip-timeout-4kvmk"
    May  4 12:23:35.254: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-1810, will wait for the garbage collector to delete the pods 05/04/23 12:23:35.274
    May  4 12:23:35.337: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 9.838937ms
    May  4 12:23:35.539: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 201.112982ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  4 12:23:38.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1810" for this suite. 05/04/23 12:23:38.076
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:23:38.095
May  4 12:23:38.095: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename crd-publish-openapi 05/04/23 12:23:38.099
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:23:38.121
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:23:38.131
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
May  4 12:23:38.135: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 05/04/23 12:23:45.011
May  4 12:23:45.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-24 --namespace=crd-publish-openapi-24 create -f -'
May  4 12:23:46.082: INFO: stderr: ""
May  4 12:23:46.082: INFO: stdout: "e2e-test-crd-publish-openapi-5029-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May  4 12:23:46.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-24 --namespace=crd-publish-openapi-24 delete e2e-test-crd-publish-openapi-5029-crds test-cr'
May  4 12:23:46.212: INFO: stderr: ""
May  4 12:23:46.212: INFO: stdout: "e2e-test-crd-publish-openapi-5029-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
May  4 12:23:46.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-24 --namespace=crd-publish-openapi-24 apply -f -'
May  4 12:23:47.225: INFO: stderr: ""
May  4 12:23:47.225: INFO: stdout: "e2e-test-crd-publish-openapi-5029-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May  4 12:23:47.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-24 --namespace=crd-publish-openapi-24 delete e2e-test-crd-publish-openapi-5029-crds test-cr'
May  4 12:23:47.303: INFO: stderr: ""
May  4 12:23:47.303: INFO: stdout: "e2e-test-crd-publish-openapi-5029-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 05/04/23 12:23:47.303
May  4 12:23:47.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-24 explain e2e-test-crd-publish-openapi-5029-crds'
May  4 12:23:48.755: INFO: stderr: ""
May  4 12:23:48.755: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5029-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  4 12:23:55.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-24" for this suite. 05/04/23 12:23:55.284
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","completed":148,"skipped":2915,"failed":0}
------------------------------
• [SLOW TEST] [17.199 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:23:38.095
    May  4 12:23:38.095: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename crd-publish-openapi 05/04/23 12:23:38.099
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:23:38.121
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:23:38.131
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:235
    May  4 12:23:38.135: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 05/04/23 12:23:45.011
    May  4 12:23:45.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-24 --namespace=crd-publish-openapi-24 create -f -'
    May  4 12:23:46.082: INFO: stderr: ""
    May  4 12:23:46.082: INFO: stdout: "e2e-test-crd-publish-openapi-5029-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    May  4 12:23:46.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-24 --namespace=crd-publish-openapi-24 delete e2e-test-crd-publish-openapi-5029-crds test-cr'
    May  4 12:23:46.212: INFO: stderr: ""
    May  4 12:23:46.212: INFO: stdout: "e2e-test-crd-publish-openapi-5029-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    May  4 12:23:46.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-24 --namespace=crd-publish-openapi-24 apply -f -'
    May  4 12:23:47.225: INFO: stderr: ""
    May  4 12:23:47.225: INFO: stdout: "e2e-test-crd-publish-openapi-5029-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    May  4 12:23:47.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-24 --namespace=crd-publish-openapi-24 delete e2e-test-crd-publish-openapi-5029-crds test-cr'
    May  4 12:23:47.303: INFO: stderr: ""
    May  4 12:23:47.303: INFO: stdout: "e2e-test-crd-publish-openapi-5029-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 05/04/23 12:23:47.303
    May  4 12:23:47.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-24 explain e2e-test-crd-publish-openapi-5029-crds'
    May  4 12:23:48.755: INFO: stderr: ""
    May  4 12:23:48.755: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5029-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  4 12:23:55.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-24" for this suite. 05/04/23 12:23:55.284
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:23:55.295
May  4 12:23:55.295: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename disruption 05/04/23 12:23:55.297
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:23:55.322
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:23:55.331
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
STEP: creating the pdb 05/04/23 12:23:55.342
STEP: Waiting for the pdb to be processed 05/04/23 12:23:55.351
STEP: updating the pdb 05/04/23 12:23:55.357
STEP: Waiting for the pdb to be processed 05/04/23 12:23:55.37
STEP: patching the pdb 05/04/23 12:23:57.383
STEP: Waiting for the pdb to be processed 05/04/23 12:23:57.397
STEP: Waiting for the pdb to be deleted 05/04/23 12:23:57.417
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
May  4 12:23:57.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-6889" for this suite. 05/04/23 12:23:57.435
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","completed":149,"skipped":2927,"failed":0}
------------------------------
• [2.151 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:23:55.295
    May  4 12:23:55.295: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename disruption 05/04/23 12:23:55.297
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:23:55.322
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:23:55.331
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:107
    STEP: creating the pdb 05/04/23 12:23:55.342
    STEP: Waiting for the pdb to be processed 05/04/23 12:23:55.351
    STEP: updating the pdb 05/04/23 12:23:55.357
    STEP: Waiting for the pdb to be processed 05/04/23 12:23:55.37
    STEP: patching the pdb 05/04/23 12:23:57.383
    STEP: Waiting for the pdb to be processed 05/04/23 12:23:57.397
    STEP: Waiting for the pdb to be deleted 05/04/23 12:23:57.417
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    May  4 12:23:57.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-6889" for this suite. 05/04/23 12:23:57.435
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:23:57.446
May  4 12:23:57.446: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename crd-publish-openapi 05/04/23 12:23:57.447
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:23:57.474
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:23:57.481
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
May  4 12:23:57.490: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 05/04/23 12:24:04.298
May  4 12:24:04.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-8428 --namespace=crd-publish-openapi-8428 create -f -'
May  4 12:24:05.323: INFO: stderr: ""
May  4 12:24:05.323: INFO: stdout: "e2e-test-crd-publish-openapi-9926-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May  4 12:24:05.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-8428 --namespace=crd-publish-openapi-8428 delete e2e-test-crd-publish-openapi-9926-crds test-foo'
May  4 12:24:05.451: INFO: stderr: ""
May  4 12:24:05.451: INFO: stdout: "e2e-test-crd-publish-openapi-9926-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
May  4 12:24:05.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-8428 --namespace=crd-publish-openapi-8428 apply -f -'
May  4 12:24:06.500: INFO: stderr: ""
May  4 12:24:06.500: INFO: stdout: "e2e-test-crd-publish-openapi-9926-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May  4 12:24:06.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-8428 --namespace=crd-publish-openapi-8428 delete e2e-test-crd-publish-openapi-9926-crds test-foo'
May  4 12:24:06.639: INFO: stderr: ""
May  4 12:24:06.639: INFO: stdout: "e2e-test-crd-publish-openapi-9926-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 05/04/23 12:24:06.639
May  4 12:24:06.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-8428 --namespace=crd-publish-openapi-8428 create -f -'
May  4 12:24:07.754: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 05/04/23 12:24:07.754
May  4 12:24:07.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-8428 --namespace=crd-publish-openapi-8428 create -f -'
May  4 12:24:08.090: INFO: rc: 1
May  4 12:24:08.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-8428 --namespace=crd-publish-openapi-8428 apply -f -'
May  4 12:24:08.463: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 05/04/23 12:24:08.463
May  4 12:24:08.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-8428 --namespace=crd-publish-openapi-8428 create -f -'
May  4 12:24:08.875: INFO: rc: 1
May  4 12:24:08.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-8428 --namespace=crd-publish-openapi-8428 apply -f -'
May  4 12:24:09.220: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 05/04/23 12:24:09.22
May  4 12:24:09.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-8428 explain e2e-test-crd-publish-openapi-9926-crds'
May  4 12:24:09.502: INFO: stderr: ""
May  4 12:24:09.502: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9926-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 05/04/23 12:24:09.502
May  4 12:24:09.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-8428 explain e2e-test-crd-publish-openapi-9926-crds.metadata'
May  4 12:24:09.769: INFO: stderr: ""
May  4 12:24:09.769: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9926-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
May  4 12:24:09.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-8428 explain e2e-test-crd-publish-openapi-9926-crds.spec'
May  4 12:24:10.059: INFO: stderr: ""
May  4 12:24:10.059: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9926-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
May  4 12:24:10.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-8428 explain e2e-test-crd-publish-openapi-9926-crds.spec.bars'
May  4 12:24:10.339: INFO: stderr: ""
May  4 12:24:10.339: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9926-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 05/04/23 12:24:10.339
May  4 12:24:10.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-8428 explain e2e-test-crd-publish-openapi-9926-crds.spec.bars2'
May  4 12:24:10.609: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  4 12:24:16.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8428" for this suite. 05/04/23 12:24:16.576
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","completed":150,"skipped":2945,"failed":0}
------------------------------
• [SLOW TEST] [19.183 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:23:57.446
    May  4 12:23:57.446: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename crd-publish-openapi 05/04/23 12:23:57.447
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:23:57.474
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:23:57.481
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:68
    May  4 12:23:57.490: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 05/04/23 12:24:04.298
    May  4 12:24:04.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-8428 --namespace=crd-publish-openapi-8428 create -f -'
    May  4 12:24:05.323: INFO: stderr: ""
    May  4 12:24:05.323: INFO: stdout: "e2e-test-crd-publish-openapi-9926-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    May  4 12:24:05.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-8428 --namespace=crd-publish-openapi-8428 delete e2e-test-crd-publish-openapi-9926-crds test-foo'
    May  4 12:24:05.451: INFO: stderr: ""
    May  4 12:24:05.451: INFO: stdout: "e2e-test-crd-publish-openapi-9926-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    May  4 12:24:05.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-8428 --namespace=crd-publish-openapi-8428 apply -f -'
    May  4 12:24:06.500: INFO: stderr: ""
    May  4 12:24:06.500: INFO: stdout: "e2e-test-crd-publish-openapi-9926-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    May  4 12:24:06.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-8428 --namespace=crd-publish-openapi-8428 delete e2e-test-crd-publish-openapi-9926-crds test-foo'
    May  4 12:24:06.639: INFO: stderr: ""
    May  4 12:24:06.639: INFO: stdout: "e2e-test-crd-publish-openapi-9926-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 05/04/23 12:24:06.639
    May  4 12:24:06.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-8428 --namespace=crd-publish-openapi-8428 create -f -'
    May  4 12:24:07.754: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 05/04/23 12:24:07.754
    May  4 12:24:07.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-8428 --namespace=crd-publish-openapi-8428 create -f -'
    May  4 12:24:08.090: INFO: rc: 1
    May  4 12:24:08.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-8428 --namespace=crd-publish-openapi-8428 apply -f -'
    May  4 12:24:08.463: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 05/04/23 12:24:08.463
    May  4 12:24:08.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-8428 --namespace=crd-publish-openapi-8428 create -f -'
    May  4 12:24:08.875: INFO: rc: 1
    May  4 12:24:08.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-8428 --namespace=crd-publish-openapi-8428 apply -f -'
    May  4 12:24:09.220: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 05/04/23 12:24:09.22
    May  4 12:24:09.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-8428 explain e2e-test-crd-publish-openapi-9926-crds'
    May  4 12:24:09.502: INFO: stderr: ""
    May  4 12:24:09.502: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9926-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 05/04/23 12:24:09.502
    May  4 12:24:09.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-8428 explain e2e-test-crd-publish-openapi-9926-crds.metadata'
    May  4 12:24:09.769: INFO: stderr: ""
    May  4 12:24:09.769: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9926-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    May  4 12:24:09.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-8428 explain e2e-test-crd-publish-openapi-9926-crds.spec'
    May  4 12:24:10.059: INFO: stderr: ""
    May  4 12:24:10.059: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9926-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    May  4 12:24:10.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-8428 explain e2e-test-crd-publish-openapi-9926-crds.spec.bars'
    May  4 12:24:10.339: INFO: stderr: ""
    May  4 12:24:10.339: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9926-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 05/04/23 12:24:10.339
    May  4 12:24:10.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-8428 explain e2e-test-crd-publish-openapi-9926-crds.spec.bars2'
    May  4 12:24:10.609: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  4 12:24:16.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-8428" for this suite. 05/04/23 12:24:16.576
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:24:16.63
May  4 12:24:16.630: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename sysctl 05/04/23 12:24:16.631
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:24:16.651
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:24:16.655
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 05/04/23 12:24:16.658
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
May  4 12:24:16.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-7818" for this suite. 05/04/23 12:24:16.681
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":151,"skipped":2951,"failed":0}
------------------------------
• [0.060 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:24:16.63
    May  4 12:24:16.630: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename sysctl 05/04/23 12:24:16.631
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:24:16.651
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:24:16.655
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 05/04/23 12:24:16.658
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    May  4 12:24:16.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-7818" for this suite. 05/04/23 12:24:16.681
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:24:16.691
May  4 12:24:16.691: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename security-context-test 05/04/23 12:24:16.692
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:24:16.711
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:24:16.721
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
May  4 12:24:16.736: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-9760becb-a71e-4cd0-8a09-0d12e6f8fbc9" in namespace "security-context-test-8069" to be "Succeeded or Failed"
May  4 12:24:16.743: INFO: Pod "busybox-privileged-false-9760becb-a71e-4cd0-8a09-0d12e6f8fbc9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.046087ms
May  4 12:24:18.749: INFO: Pod "busybox-privileged-false-9760becb-a71e-4cd0-8a09-0d12e6f8fbc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012746806s
May  4 12:24:20.750: INFO: Pod "busybox-privileged-false-9760becb-a71e-4cd0-8a09-0d12e6f8fbc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014129616s
May  4 12:24:20.750: INFO: Pod "busybox-privileged-false-9760becb-a71e-4cd0-8a09-0d12e6f8fbc9" satisfied condition "Succeeded or Failed"
May  4 12:24:20.765: INFO: Got logs for pod "busybox-privileged-false-9760becb-a71e-4cd0-8a09-0d12e6f8fbc9": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
May  4 12:24:20.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8069" for this suite. 05/04/23 12:24:20.776
{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","completed":152,"skipped":2959,"failed":0}
------------------------------
• [4.106 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:490
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:527

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:24:16.691
    May  4 12:24:16.691: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename security-context-test 05/04/23 12:24:16.692
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:24:16.711
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:24:16.721
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:527
    May  4 12:24:16.736: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-9760becb-a71e-4cd0-8a09-0d12e6f8fbc9" in namespace "security-context-test-8069" to be "Succeeded or Failed"
    May  4 12:24:16.743: INFO: Pod "busybox-privileged-false-9760becb-a71e-4cd0-8a09-0d12e6f8fbc9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.046087ms
    May  4 12:24:18.749: INFO: Pod "busybox-privileged-false-9760becb-a71e-4cd0-8a09-0d12e6f8fbc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012746806s
    May  4 12:24:20.750: INFO: Pod "busybox-privileged-false-9760becb-a71e-4cd0-8a09-0d12e6f8fbc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014129616s
    May  4 12:24:20.750: INFO: Pod "busybox-privileged-false-9760becb-a71e-4cd0-8a09-0d12e6f8fbc9" satisfied condition "Succeeded or Failed"
    May  4 12:24:20.765: INFO: Got logs for pod "busybox-privileged-false-9760becb-a71e-4cd0-8a09-0d12e6f8fbc9": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    May  4 12:24:20.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-8069" for this suite. 05/04/23 12:24:20.776
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:24:20.801
May  4 12:24:20.802: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename security-context 05/04/23 12:24:20.803
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:24:20.825
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:24:20.829
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 05/04/23 12:24:20.833
May  4 12:24:20.850: INFO: Waiting up to 5m0s for pod "security-context-5eddddc0-6bd1-4c2b-83c3-1f041a510ba9" in namespace "security-context-1191" to be "Succeeded or Failed"
May  4 12:24:20.856: INFO: Pod "security-context-5eddddc0-6bd1-4c2b-83c3-1f041a510ba9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.726688ms
May  4 12:24:22.860: INFO: Pod "security-context-5eddddc0-6bd1-4c2b-83c3-1f041a510ba9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010349014s
May  4 12:24:24.862: INFO: Pod "security-context-5eddddc0-6bd1-4c2b-83c3-1f041a510ba9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011850285s
STEP: Saw pod success 05/04/23 12:24:24.862
May  4 12:24:24.862: INFO: Pod "security-context-5eddddc0-6bd1-4c2b-83c3-1f041a510ba9" satisfied condition "Succeeded or Failed"
May  4 12:24:24.867: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod security-context-5eddddc0-6bd1-4c2b-83c3-1f041a510ba9 container test-container: <nil>
STEP: delete the pod 05/04/23 12:24:24.876
May  4 12:24:24.896: INFO: Waiting for pod security-context-5eddddc0-6bd1-4c2b-83c3-1f041a510ba9 to disappear
May  4 12:24:24.902: INFO: Pod security-context-5eddddc0-6bd1-4c2b-83c3-1f041a510ba9 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
May  4 12:24:24.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-1191" for this suite. 05/04/23 12:24:24.945
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":153,"skipped":3001,"failed":0}
------------------------------
• [4.163 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:24:20.801
    May  4 12:24:20.802: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename security-context 05/04/23 12:24:20.803
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:24:20.825
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:24:20.829
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:97
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 05/04/23 12:24:20.833
    May  4 12:24:20.850: INFO: Waiting up to 5m0s for pod "security-context-5eddddc0-6bd1-4c2b-83c3-1f041a510ba9" in namespace "security-context-1191" to be "Succeeded or Failed"
    May  4 12:24:20.856: INFO: Pod "security-context-5eddddc0-6bd1-4c2b-83c3-1f041a510ba9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.726688ms
    May  4 12:24:22.860: INFO: Pod "security-context-5eddddc0-6bd1-4c2b-83c3-1f041a510ba9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010349014s
    May  4 12:24:24.862: INFO: Pod "security-context-5eddddc0-6bd1-4c2b-83c3-1f041a510ba9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011850285s
    STEP: Saw pod success 05/04/23 12:24:24.862
    May  4 12:24:24.862: INFO: Pod "security-context-5eddddc0-6bd1-4c2b-83c3-1f041a510ba9" satisfied condition "Succeeded or Failed"
    May  4 12:24:24.867: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod security-context-5eddddc0-6bd1-4c2b-83c3-1f041a510ba9 container test-container: <nil>
    STEP: delete the pod 05/04/23 12:24:24.876
    May  4 12:24:24.896: INFO: Waiting for pod security-context-5eddddc0-6bd1-4c2b-83c3-1f041a510ba9 to disappear
    May  4 12:24:24.902: INFO: Pod security-context-5eddddc0-6bd1-4c2b-83c3-1f041a510ba9 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    May  4 12:24:24.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-1191" for this suite. 05/04/23 12:24:24.945
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:24:24.965
May  4 12:24:24.965: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename downward-api 05/04/23 12:24:24.966
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:24:25.006
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:24:25.01
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
STEP: Creating a pod to test downward API volume plugin 05/04/23 12:24:25.015
May  4 12:24:25.037: INFO: Waiting up to 5m0s for pod "downwardapi-volume-64d88d19-6470-4875-8737-16da5fafa582" in namespace "downward-api-5809" to be "Succeeded or Failed"
May  4 12:24:25.050: INFO: Pod "downwardapi-volume-64d88d19-6470-4875-8737-16da5fafa582": Phase="Pending", Reason="", readiness=false. Elapsed: 13.014397ms
May  4 12:24:27.056: INFO: Pod "downwardapi-volume-64d88d19-6470-4875-8737-16da5fafa582": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018969679s
May  4 12:24:29.059: INFO: Pod "downwardapi-volume-64d88d19-6470-4875-8737-16da5fafa582": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021625582s
STEP: Saw pod success 05/04/23 12:24:29.059
May  4 12:24:29.059: INFO: Pod "downwardapi-volume-64d88d19-6470-4875-8737-16da5fafa582" satisfied condition "Succeeded or Failed"
May  4 12:24:29.068: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod downwardapi-volume-64d88d19-6470-4875-8737-16da5fafa582 container client-container: <nil>
STEP: delete the pod 05/04/23 12:24:29.084
May  4 12:24:29.130: INFO: Waiting for pod downwardapi-volume-64d88d19-6470-4875-8737-16da5fafa582 to disappear
May  4 12:24:29.137: INFO: Pod downwardapi-volume-64d88d19-6470-4875-8737-16da5fafa582 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
May  4 12:24:29.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5809" for this suite. 05/04/23 12:24:29.155
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":154,"skipped":3006,"failed":0}
------------------------------
• [4.214 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:24:24.965
    May  4 12:24:24.965: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename downward-api 05/04/23 12:24:24.966
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:24:25.006
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:24:25.01
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:248
    STEP: Creating a pod to test downward API volume plugin 05/04/23 12:24:25.015
    May  4 12:24:25.037: INFO: Waiting up to 5m0s for pod "downwardapi-volume-64d88d19-6470-4875-8737-16da5fafa582" in namespace "downward-api-5809" to be "Succeeded or Failed"
    May  4 12:24:25.050: INFO: Pod "downwardapi-volume-64d88d19-6470-4875-8737-16da5fafa582": Phase="Pending", Reason="", readiness=false. Elapsed: 13.014397ms
    May  4 12:24:27.056: INFO: Pod "downwardapi-volume-64d88d19-6470-4875-8737-16da5fafa582": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018969679s
    May  4 12:24:29.059: INFO: Pod "downwardapi-volume-64d88d19-6470-4875-8737-16da5fafa582": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021625582s
    STEP: Saw pod success 05/04/23 12:24:29.059
    May  4 12:24:29.059: INFO: Pod "downwardapi-volume-64d88d19-6470-4875-8737-16da5fafa582" satisfied condition "Succeeded or Failed"
    May  4 12:24:29.068: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod downwardapi-volume-64d88d19-6470-4875-8737-16da5fafa582 container client-container: <nil>
    STEP: delete the pod 05/04/23 12:24:29.084
    May  4 12:24:29.130: INFO: Waiting for pod downwardapi-volume-64d88d19-6470-4875-8737-16da5fafa582 to disappear
    May  4 12:24:29.137: INFO: Pod downwardapi-volume-64d88d19-6470-4875-8737-16da5fafa582 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    May  4 12:24:29.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5809" for this suite. 05/04/23 12:24:29.155
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:24:29.179
May  4 12:24:29.179: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename services 05/04/23 12:24:29.18
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:24:29.251
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:24:29.271
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
STEP: creating service in namespace services-2252 05/04/23 12:24:29.281
STEP: creating service affinity-clusterip in namespace services-2252 05/04/23 12:24:29.282
STEP: creating replication controller affinity-clusterip in namespace services-2252 05/04/23 12:24:29.308
I0504 12:24:29.325215      21 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-2252, replica count: 3
I0504 12:24:32.379110      21 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  4 12:24:32.389: INFO: Creating new exec pod
May  4 12:24:32.399: INFO: Waiting up to 5m0s for pod "execpod-affinityhp9bs" in namespace "services-2252" to be "running"
May  4 12:24:32.407: INFO: Pod "execpod-affinityhp9bs": Phase="Pending", Reason="", readiness=false. Elapsed: 7.38726ms
May  4 12:24:34.414: INFO: Pod "execpod-affinityhp9bs": Phase="Running", Reason="", readiness=true. Elapsed: 2.014789839s
May  4 12:24:34.414: INFO: Pod "execpod-affinityhp9bs" satisfied condition "running"
May  4 12:24:35.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-2252 exec execpod-affinityhp9bs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
May  4 12:24:35.612: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
May  4 12:24:35.612: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  4 12:24:35.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-2252 exec execpod-affinityhp9bs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.21.157.10 80'
May  4 12:24:35.755: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.21.157.10 80\nConnection to 10.21.157.10 80 port [tcp/http] succeeded!\n"
May  4 12:24:35.755: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  4 12:24:35.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-2252 exec execpod-affinityhp9bs -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.21.157.10:80/ ; done'
May  4 12:24:35.994: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.157.10:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.157.10:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.157.10:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.157.10:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.157.10:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.157.10:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.157.10:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.157.10:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.157.10:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.157.10:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.157.10:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.157.10:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.157.10:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.157.10:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.157.10:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.157.10:80/\n"
May  4 12:24:35.994: INFO: stdout: "\naffinity-clusterip-nhv48\naffinity-clusterip-nhv48\naffinity-clusterip-nhv48\naffinity-clusterip-nhv48\naffinity-clusterip-nhv48\naffinity-clusterip-nhv48\naffinity-clusterip-nhv48\naffinity-clusterip-nhv48\naffinity-clusterip-nhv48\naffinity-clusterip-nhv48\naffinity-clusterip-nhv48\naffinity-clusterip-nhv48\naffinity-clusterip-nhv48\naffinity-clusterip-nhv48\naffinity-clusterip-nhv48\naffinity-clusterip-nhv48"
May  4 12:24:35.994: INFO: Received response from host: affinity-clusterip-nhv48
May  4 12:24:35.994: INFO: Received response from host: affinity-clusterip-nhv48
May  4 12:24:35.994: INFO: Received response from host: affinity-clusterip-nhv48
May  4 12:24:35.994: INFO: Received response from host: affinity-clusterip-nhv48
May  4 12:24:35.994: INFO: Received response from host: affinity-clusterip-nhv48
May  4 12:24:35.994: INFO: Received response from host: affinity-clusterip-nhv48
May  4 12:24:35.994: INFO: Received response from host: affinity-clusterip-nhv48
May  4 12:24:35.994: INFO: Received response from host: affinity-clusterip-nhv48
May  4 12:24:35.994: INFO: Received response from host: affinity-clusterip-nhv48
May  4 12:24:35.994: INFO: Received response from host: affinity-clusterip-nhv48
May  4 12:24:35.994: INFO: Received response from host: affinity-clusterip-nhv48
May  4 12:24:35.994: INFO: Received response from host: affinity-clusterip-nhv48
May  4 12:24:35.994: INFO: Received response from host: affinity-clusterip-nhv48
May  4 12:24:35.994: INFO: Received response from host: affinity-clusterip-nhv48
May  4 12:24:35.994: INFO: Received response from host: affinity-clusterip-nhv48
May  4 12:24:35.994: INFO: Received response from host: affinity-clusterip-nhv48
May  4 12:24:35.994: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-2252, will wait for the garbage collector to delete the pods 05/04/23 12:24:36.022
May  4 12:24:36.092: INFO: Deleting ReplicationController affinity-clusterip took: 11.43664ms
May  4 12:24:36.193: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.467071ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  4 12:24:38.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2252" for this suite. 05/04/23 12:24:38.553
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","completed":155,"skipped":3020,"failed":0}
------------------------------
• [SLOW TEST] [9.394 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:24:29.179
    May  4 12:24:29.179: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename services 05/04/23 12:24:29.18
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:24:29.251
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:24:29.271
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2157
    STEP: creating service in namespace services-2252 05/04/23 12:24:29.281
    STEP: creating service affinity-clusterip in namespace services-2252 05/04/23 12:24:29.282
    STEP: creating replication controller affinity-clusterip in namespace services-2252 05/04/23 12:24:29.308
    I0504 12:24:29.325215      21 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-2252, replica count: 3
    I0504 12:24:32.379110      21 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  4 12:24:32.389: INFO: Creating new exec pod
    May  4 12:24:32.399: INFO: Waiting up to 5m0s for pod "execpod-affinityhp9bs" in namespace "services-2252" to be "running"
    May  4 12:24:32.407: INFO: Pod "execpod-affinityhp9bs": Phase="Pending", Reason="", readiness=false. Elapsed: 7.38726ms
    May  4 12:24:34.414: INFO: Pod "execpod-affinityhp9bs": Phase="Running", Reason="", readiness=true. Elapsed: 2.014789839s
    May  4 12:24:34.414: INFO: Pod "execpod-affinityhp9bs" satisfied condition "running"
    May  4 12:24:35.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-2252 exec execpod-affinityhp9bs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
    May  4 12:24:35.612: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    May  4 12:24:35.612: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  4 12:24:35.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-2252 exec execpod-affinityhp9bs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.21.157.10 80'
    May  4 12:24:35.755: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.21.157.10 80\nConnection to 10.21.157.10 80 port [tcp/http] succeeded!\n"
    May  4 12:24:35.755: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  4 12:24:35.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-2252 exec execpod-affinityhp9bs -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.21.157.10:80/ ; done'
    May  4 12:24:35.994: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.157.10:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.157.10:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.157.10:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.157.10:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.157.10:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.157.10:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.157.10:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.157.10:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.157.10:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.157.10:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.157.10:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.157.10:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.157.10:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.157.10:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.157.10:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.157.10:80/\n"
    May  4 12:24:35.994: INFO: stdout: "\naffinity-clusterip-nhv48\naffinity-clusterip-nhv48\naffinity-clusterip-nhv48\naffinity-clusterip-nhv48\naffinity-clusterip-nhv48\naffinity-clusterip-nhv48\naffinity-clusterip-nhv48\naffinity-clusterip-nhv48\naffinity-clusterip-nhv48\naffinity-clusterip-nhv48\naffinity-clusterip-nhv48\naffinity-clusterip-nhv48\naffinity-clusterip-nhv48\naffinity-clusterip-nhv48\naffinity-clusterip-nhv48\naffinity-clusterip-nhv48"
    May  4 12:24:35.994: INFO: Received response from host: affinity-clusterip-nhv48
    May  4 12:24:35.994: INFO: Received response from host: affinity-clusterip-nhv48
    May  4 12:24:35.994: INFO: Received response from host: affinity-clusterip-nhv48
    May  4 12:24:35.994: INFO: Received response from host: affinity-clusterip-nhv48
    May  4 12:24:35.994: INFO: Received response from host: affinity-clusterip-nhv48
    May  4 12:24:35.994: INFO: Received response from host: affinity-clusterip-nhv48
    May  4 12:24:35.994: INFO: Received response from host: affinity-clusterip-nhv48
    May  4 12:24:35.994: INFO: Received response from host: affinity-clusterip-nhv48
    May  4 12:24:35.994: INFO: Received response from host: affinity-clusterip-nhv48
    May  4 12:24:35.994: INFO: Received response from host: affinity-clusterip-nhv48
    May  4 12:24:35.994: INFO: Received response from host: affinity-clusterip-nhv48
    May  4 12:24:35.994: INFO: Received response from host: affinity-clusterip-nhv48
    May  4 12:24:35.994: INFO: Received response from host: affinity-clusterip-nhv48
    May  4 12:24:35.994: INFO: Received response from host: affinity-clusterip-nhv48
    May  4 12:24:35.994: INFO: Received response from host: affinity-clusterip-nhv48
    May  4 12:24:35.994: INFO: Received response from host: affinity-clusterip-nhv48
    May  4 12:24:35.994: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-2252, will wait for the garbage collector to delete the pods 05/04/23 12:24:36.022
    May  4 12:24:36.092: INFO: Deleting ReplicationController affinity-clusterip took: 11.43664ms
    May  4 12:24:36.193: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.467071ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  4 12:24:38.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2252" for this suite. 05/04/23 12:24:38.553
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:24:38.574
May  4 12:24:38.575: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename resourcequota 05/04/23 12:24:38.575
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:24:38.605
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:24:38.611
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
STEP: Creating a ResourceQuota with terminating scope 05/04/23 12:24:38.617
STEP: Ensuring ResourceQuota status is calculated 05/04/23 12:24:38.627
STEP: Creating a ResourceQuota with not terminating scope 05/04/23 12:24:40.632
STEP: Ensuring ResourceQuota status is calculated 05/04/23 12:24:40.639
STEP: Creating a long running pod 05/04/23 12:24:42.644
STEP: Ensuring resource quota with not terminating scope captures the pod usage 05/04/23 12:24:42.668
STEP: Ensuring resource quota with terminating scope ignored the pod usage 05/04/23 12:24:44.673
STEP: Deleting the pod 05/04/23 12:24:46.679
STEP: Ensuring resource quota status released the pod usage 05/04/23 12:24:46.698
STEP: Creating a terminating pod 05/04/23 12:24:48.704
STEP: Ensuring resource quota with terminating scope captures the pod usage 05/04/23 12:24:48.722
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 05/04/23 12:24:50.728
STEP: Deleting the pod 05/04/23 12:24:52.736
STEP: Ensuring resource quota status released the pod usage 05/04/23 12:24:52.755
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
May  4 12:24:54.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8564" for this suite. 05/04/23 12:24:54.776
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","completed":156,"skipped":3039,"failed":0}
------------------------------
• [SLOW TEST] [16.216 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:24:38.574
    May  4 12:24:38.575: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename resourcequota 05/04/23 12:24:38.575
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:24:38.605
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:24:38.611
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:680
    STEP: Creating a ResourceQuota with terminating scope 05/04/23 12:24:38.617
    STEP: Ensuring ResourceQuota status is calculated 05/04/23 12:24:38.627
    STEP: Creating a ResourceQuota with not terminating scope 05/04/23 12:24:40.632
    STEP: Ensuring ResourceQuota status is calculated 05/04/23 12:24:40.639
    STEP: Creating a long running pod 05/04/23 12:24:42.644
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 05/04/23 12:24:42.668
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 05/04/23 12:24:44.673
    STEP: Deleting the pod 05/04/23 12:24:46.679
    STEP: Ensuring resource quota status released the pod usage 05/04/23 12:24:46.698
    STEP: Creating a terminating pod 05/04/23 12:24:48.704
    STEP: Ensuring resource quota with terminating scope captures the pod usage 05/04/23 12:24:48.722
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 05/04/23 12:24:50.728
    STEP: Deleting the pod 05/04/23 12:24:52.736
    STEP: Ensuring resource quota status released the pod usage 05/04/23 12:24:52.755
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    May  4 12:24:54.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-8564" for this suite. 05/04/23 12:24:54.776
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:24:54.791
May  4 12:24:54.791: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename projected 05/04/23 12:24:54.792
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:24:54.817
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:24:54.824
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
STEP: Creating configMap with name projected-configmap-test-volume-map-8800a0b5-3886-4f8d-92c7-55aae5d90624 05/04/23 12:24:54.828
STEP: Creating a pod to test consume configMaps 05/04/23 12:24:54.839
May  4 12:24:54.852: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-889113fc-7c78-476c-9243-5f643010d1b6" in namespace "projected-5862" to be "Succeeded or Failed"
May  4 12:24:54.861: INFO: Pod "pod-projected-configmaps-889113fc-7c78-476c-9243-5f643010d1b6": Phase="Pending", Reason="", readiness=false. Elapsed: 9.502344ms
May  4 12:24:56.867: INFO: Pod "pod-projected-configmaps-889113fc-7c78-476c-9243-5f643010d1b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015156935s
May  4 12:24:58.868: INFO: Pod "pod-projected-configmaps-889113fc-7c78-476c-9243-5f643010d1b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015549994s
STEP: Saw pod success 05/04/23 12:24:58.868
May  4 12:24:58.868: INFO: Pod "pod-projected-configmaps-889113fc-7c78-476c-9243-5f643010d1b6" satisfied condition "Succeeded or Failed"
May  4 12:24:58.872: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-projected-configmaps-889113fc-7c78-476c-9243-5f643010d1b6 container agnhost-container: <nil>
STEP: delete the pod 05/04/23 12:24:58.888
May  4 12:24:58.908: INFO: Waiting for pod pod-projected-configmaps-889113fc-7c78-476c-9243-5f643010d1b6 to disappear
May  4 12:24:58.911: INFO: Pod pod-projected-configmaps-889113fc-7c78-476c-9243-5f643010d1b6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
May  4 12:24:58.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5862" for this suite. 05/04/23 12:24:58.924
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":157,"skipped":3048,"failed":0}
------------------------------
• [4.144 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:24:54.791
    May  4 12:24:54.791: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename projected 05/04/23 12:24:54.792
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:24:54.817
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:24:54.824
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:88
    STEP: Creating configMap with name projected-configmap-test-volume-map-8800a0b5-3886-4f8d-92c7-55aae5d90624 05/04/23 12:24:54.828
    STEP: Creating a pod to test consume configMaps 05/04/23 12:24:54.839
    May  4 12:24:54.852: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-889113fc-7c78-476c-9243-5f643010d1b6" in namespace "projected-5862" to be "Succeeded or Failed"
    May  4 12:24:54.861: INFO: Pod "pod-projected-configmaps-889113fc-7c78-476c-9243-5f643010d1b6": Phase="Pending", Reason="", readiness=false. Elapsed: 9.502344ms
    May  4 12:24:56.867: INFO: Pod "pod-projected-configmaps-889113fc-7c78-476c-9243-5f643010d1b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015156935s
    May  4 12:24:58.868: INFO: Pod "pod-projected-configmaps-889113fc-7c78-476c-9243-5f643010d1b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015549994s
    STEP: Saw pod success 05/04/23 12:24:58.868
    May  4 12:24:58.868: INFO: Pod "pod-projected-configmaps-889113fc-7c78-476c-9243-5f643010d1b6" satisfied condition "Succeeded or Failed"
    May  4 12:24:58.872: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-projected-configmaps-889113fc-7c78-476c-9243-5f643010d1b6 container agnhost-container: <nil>
    STEP: delete the pod 05/04/23 12:24:58.888
    May  4 12:24:58.908: INFO: Waiting for pod pod-projected-configmaps-889113fc-7c78-476c-9243-5f643010d1b6 to disappear
    May  4 12:24:58.911: INFO: Pod pod-projected-configmaps-889113fc-7c78-476c-9243-5f643010d1b6 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    May  4 12:24:58.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5862" for this suite. 05/04/23 12:24:58.924
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:24:58.938
May  4 12:24:58.938: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename custom-resource-definition 05/04/23 12:24:58.939
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:24:58.969
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:24:58.973
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 05/04/23 12:24:58.975
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 05/04/23 12:24:58.979
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 05/04/23 12:24:58.979
STEP: fetching the /apis/apiextensions.k8s.io discovery document 05/04/23 12:24:58.979
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 05/04/23 12:24:58.983
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 05/04/23 12:24:58.983
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 05/04/23 12:24:58.985
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  4 12:24:58.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-626" for this suite. 05/04/23 12:24:58.994
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","completed":158,"skipped":3099,"failed":0}
------------------------------
• [0.070 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:24:58.938
    May  4 12:24:58.938: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename custom-resource-definition 05/04/23 12:24:58.939
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:24:58.969
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:24:58.973
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 05/04/23 12:24:58.975
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 05/04/23 12:24:58.979
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 05/04/23 12:24:58.979
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 05/04/23 12:24:58.979
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 05/04/23 12:24:58.983
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 05/04/23 12:24:58.983
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 05/04/23 12:24:58.985
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  4 12:24:58.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-626" for this suite. 05/04/23 12:24:58.994
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:24:59.01
May  4 12:24:59.010: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename svcaccounts 05/04/23 12:24:59.011
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:24:59.034
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:24:59.038
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
STEP: Creating a pod to test service account token:  05/04/23 12:24:59.042
May  4 12:24:59.058: INFO: Waiting up to 5m0s for pod "test-pod-6cd01d32-0acf-4772-aa2f-df2b3b7a4fd4" in namespace "svcaccounts-4506" to be "Succeeded or Failed"
May  4 12:24:59.064: INFO: Pod "test-pod-6cd01d32-0acf-4772-aa2f-df2b3b7a4fd4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.174395ms
May  4 12:25:01.072: INFO: Pod "test-pod-6cd01d32-0acf-4772-aa2f-df2b3b7a4fd4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013921009s
May  4 12:25:03.069: INFO: Pod "test-pod-6cd01d32-0acf-4772-aa2f-df2b3b7a4fd4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011600448s
STEP: Saw pod success 05/04/23 12:25:03.069
May  4 12:25:03.069: INFO: Pod "test-pod-6cd01d32-0acf-4772-aa2f-df2b3b7a4fd4" satisfied condition "Succeeded or Failed"
May  4 12:25:03.073: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod test-pod-6cd01d32-0acf-4772-aa2f-df2b3b7a4fd4 container agnhost-container: <nil>
STEP: delete the pod 05/04/23 12:25:03.081
May  4 12:25:03.098: INFO: Waiting for pod test-pod-6cd01d32-0acf-4772-aa2f-df2b3b7a4fd4 to disappear
May  4 12:25:03.104: INFO: Pod test-pod-6cd01d32-0acf-4772-aa2f-df2b3b7a4fd4 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
May  4 12:25:03.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4506" for this suite. 05/04/23 12:25:03.117
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","completed":159,"skipped":3139,"failed":0}
------------------------------
• [4.115 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:24:59.01
    May  4 12:24:59.010: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename svcaccounts 05/04/23 12:24:59.011
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:24:59.034
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:24:59.038
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:272
    STEP: Creating a pod to test service account token:  05/04/23 12:24:59.042
    May  4 12:24:59.058: INFO: Waiting up to 5m0s for pod "test-pod-6cd01d32-0acf-4772-aa2f-df2b3b7a4fd4" in namespace "svcaccounts-4506" to be "Succeeded or Failed"
    May  4 12:24:59.064: INFO: Pod "test-pod-6cd01d32-0acf-4772-aa2f-df2b3b7a4fd4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.174395ms
    May  4 12:25:01.072: INFO: Pod "test-pod-6cd01d32-0acf-4772-aa2f-df2b3b7a4fd4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013921009s
    May  4 12:25:03.069: INFO: Pod "test-pod-6cd01d32-0acf-4772-aa2f-df2b3b7a4fd4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011600448s
    STEP: Saw pod success 05/04/23 12:25:03.069
    May  4 12:25:03.069: INFO: Pod "test-pod-6cd01d32-0acf-4772-aa2f-df2b3b7a4fd4" satisfied condition "Succeeded or Failed"
    May  4 12:25:03.073: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod test-pod-6cd01d32-0acf-4772-aa2f-df2b3b7a4fd4 container agnhost-container: <nil>
    STEP: delete the pod 05/04/23 12:25:03.081
    May  4 12:25:03.098: INFO: Waiting for pod test-pod-6cd01d32-0acf-4772-aa2f-df2b3b7a4fd4 to disappear
    May  4 12:25:03.104: INFO: Pod test-pod-6cd01d32-0acf-4772-aa2f-df2b3b7a4fd4 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    May  4 12:25:03.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-4506" for this suite. 05/04/23 12:25:03.117
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:25:03.126
May  4 12:25:03.126: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename webhook 05/04/23 12:25:03.127
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:25:03.146
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:25:03.151
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/04/23 12:25:03.186
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/04/23 12:25:03.392
STEP: Deploying the webhook pod 05/04/23 12:25:03.41
STEP: Wait for the deployment to be ready 05/04/23 12:25:03.429
May  4 12:25:03.463: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/04/23 12:25:05.477
STEP: Verifying the service has paired with the endpoint 05/04/23 12:25:05.501
May  4 12:25:06.502: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
STEP: fetching the /apis discovery document 05/04/23 12:25:06.507
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 05/04/23 12:25:06.508
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 05/04/23 12:25:06.508
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 05/04/23 12:25:06.508
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 05/04/23 12:25:06.51
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 05/04/23 12:25:06.51
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 05/04/23 12:25:06.51
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  4 12:25:06.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3225" for this suite. 05/04/23 12:25:06.519
STEP: Destroying namespace "webhook-3225-markers" for this suite. 05/04/23 12:25:06.528
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","completed":160,"skipped":3144,"failed":0}
------------------------------
• [3.500 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:25:03.126
    May  4 12:25:03.126: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename webhook 05/04/23 12:25:03.127
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:25:03.146
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:25:03.151
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/04/23 12:25:03.186
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/04/23 12:25:03.392
    STEP: Deploying the webhook pod 05/04/23 12:25:03.41
    STEP: Wait for the deployment to be ready 05/04/23 12:25:03.429
    May  4 12:25:03.463: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/04/23 12:25:05.477
    STEP: Verifying the service has paired with the endpoint 05/04/23 12:25:05.501
    May  4 12:25:06.502: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:116
    STEP: fetching the /apis discovery document 05/04/23 12:25:06.507
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 05/04/23 12:25:06.508
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 05/04/23 12:25:06.508
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 05/04/23 12:25:06.508
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 05/04/23 12:25:06.51
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 05/04/23 12:25:06.51
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 05/04/23 12:25:06.51
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  4 12:25:06.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3225" for this suite. 05/04/23 12:25:06.519
    STEP: Destroying namespace "webhook-3225-markers" for this suite. 05/04/23 12:25:06.528
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:25:06.627
May  4 12:25:06.627: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename sched-preemption 05/04/23 12:25:06.628
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:25:06.654
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:25:06.663
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
May  4 12:25:06.700: INFO: Waiting up to 1m0s for all nodes to be ready
May  4 12:26:06.799: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:26:06.805
May  4 12:26:06.805: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename sched-preemption-path 05/04/23 12:26:06.806
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:26:06.84
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:26:06.846
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
May  4 12:26:06.872: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
May  4 12:26:06.877: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
May  4 12:26:06.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-6032" for this suite. 05/04/23 12:26:06.941
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
May  4 12:26:06.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6005" for this suite. 05/04/23 12:26:07.004
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","completed":161,"skipped":3170,"failed":0}
------------------------------
• [SLOW TEST] [60.564 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:733

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:25:06.627
    May  4 12:25:06.627: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename sched-preemption 05/04/23 12:25:06.628
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:25:06.654
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:25:06.663
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    May  4 12:25:06.700: INFO: Waiting up to 1m0s for all nodes to be ready
    May  4 12:26:06.799: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:26:06.805
    May  4 12:26:06.805: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename sched-preemption-path 05/04/23 12:26:06.806
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:26:06.84
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:26:06.846
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:690
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:733
    May  4 12:26:06.872: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    May  4 12:26:06.877: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/framework.go:187
    May  4 12:26:06.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-6032" for this suite. 05/04/23 12:26:06.941
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:706
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    May  4 12:26:06.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-6005" for this suite. 05/04/23 12:26:07.004
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:26:07.191
May  4 12:26:07.191: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename container-probe 05/04/23 12:26:07.192
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:26:07.228
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:26:07.232
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
May  4 12:26:07.265: INFO: Waiting up to 5m0s for pod "test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e" in namespace "container-probe-9191" to be "running and ready"
May  4 12:26:07.271: INFO: Pod "test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.200645ms
May  4 12:26:07.271: INFO: The phase of Pod test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e is Pending, waiting for it to be Running (with Ready = true)
May  4 12:26:09.277: INFO: Pod "test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e": Phase="Running", Reason="", readiness=false. Elapsed: 2.01246207s
May  4 12:26:09.277: INFO: The phase of Pod test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e is Running (Ready = false)
May  4 12:26:11.276: INFO: Pod "test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e": Phase="Running", Reason="", readiness=false. Elapsed: 4.011760491s
May  4 12:26:11.276: INFO: The phase of Pod test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e is Running (Ready = false)
May  4 12:26:13.276: INFO: Pod "test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e": Phase="Running", Reason="", readiness=false. Elapsed: 6.011607809s
May  4 12:26:13.276: INFO: The phase of Pod test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e is Running (Ready = false)
May  4 12:26:15.280: INFO: Pod "test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e": Phase="Running", Reason="", readiness=false. Elapsed: 8.015357145s
May  4 12:26:15.280: INFO: The phase of Pod test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e is Running (Ready = false)
May  4 12:26:17.279: INFO: Pod "test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e": Phase="Running", Reason="", readiness=false. Elapsed: 10.013864948s
May  4 12:26:17.279: INFO: The phase of Pod test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e is Running (Ready = false)
May  4 12:26:19.276: INFO: Pod "test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e": Phase="Running", Reason="", readiness=false. Elapsed: 12.011708081s
May  4 12:26:19.276: INFO: The phase of Pod test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e is Running (Ready = false)
May  4 12:26:21.276: INFO: Pod "test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e": Phase="Running", Reason="", readiness=false. Elapsed: 14.011791164s
May  4 12:26:21.276: INFO: The phase of Pod test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e is Running (Ready = false)
May  4 12:26:23.277: INFO: Pod "test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e": Phase="Running", Reason="", readiness=false. Elapsed: 16.011923178s
May  4 12:26:23.277: INFO: The phase of Pod test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e is Running (Ready = false)
May  4 12:26:25.277: INFO: Pod "test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e": Phase="Running", Reason="", readiness=false. Elapsed: 18.01247859s
May  4 12:26:25.277: INFO: The phase of Pod test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e is Running (Ready = false)
May  4 12:26:27.277: INFO: Pod "test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e": Phase="Running", Reason="", readiness=false. Elapsed: 20.012012281s
May  4 12:26:27.277: INFO: The phase of Pod test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e is Running (Ready = false)
May  4 12:26:29.277: INFO: Pod "test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e": Phase="Running", Reason="", readiness=true. Elapsed: 22.012682562s
May  4 12:26:29.277: INFO: The phase of Pod test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e is Running (Ready = true)
May  4 12:26:29.277: INFO: Pod "test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e" satisfied condition "running and ready"
May  4 12:26:29.284: INFO: Container started at 2023-05-04 12:26:08 +0000 UTC, pod became ready at 2023-05-04 12:26:27 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
May  4 12:26:29.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9191" for this suite. 05/04/23 12:26:29.296
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","completed":162,"skipped":3171,"failed":0}
------------------------------
• [SLOW TEST] [22.120 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:26:07.191
    May  4 12:26:07.191: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename container-probe 05/04/23 12:26:07.192
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:26:07.228
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:26:07.232
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:68
    May  4 12:26:07.265: INFO: Waiting up to 5m0s for pod "test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e" in namespace "container-probe-9191" to be "running and ready"
    May  4 12:26:07.271: INFO: Pod "test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.200645ms
    May  4 12:26:07.271: INFO: The phase of Pod test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e is Pending, waiting for it to be Running (with Ready = true)
    May  4 12:26:09.277: INFO: Pod "test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e": Phase="Running", Reason="", readiness=false. Elapsed: 2.01246207s
    May  4 12:26:09.277: INFO: The phase of Pod test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e is Running (Ready = false)
    May  4 12:26:11.276: INFO: Pod "test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e": Phase="Running", Reason="", readiness=false. Elapsed: 4.011760491s
    May  4 12:26:11.276: INFO: The phase of Pod test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e is Running (Ready = false)
    May  4 12:26:13.276: INFO: Pod "test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e": Phase="Running", Reason="", readiness=false. Elapsed: 6.011607809s
    May  4 12:26:13.276: INFO: The phase of Pod test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e is Running (Ready = false)
    May  4 12:26:15.280: INFO: Pod "test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e": Phase="Running", Reason="", readiness=false. Elapsed: 8.015357145s
    May  4 12:26:15.280: INFO: The phase of Pod test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e is Running (Ready = false)
    May  4 12:26:17.279: INFO: Pod "test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e": Phase="Running", Reason="", readiness=false. Elapsed: 10.013864948s
    May  4 12:26:17.279: INFO: The phase of Pod test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e is Running (Ready = false)
    May  4 12:26:19.276: INFO: Pod "test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e": Phase="Running", Reason="", readiness=false. Elapsed: 12.011708081s
    May  4 12:26:19.276: INFO: The phase of Pod test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e is Running (Ready = false)
    May  4 12:26:21.276: INFO: Pod "test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e": Phase="Running", Reason="", readiness=false. Elapsed: 14.011791164s
    May  4 12:26:21.276: INFO: The phase of Pod test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e is Running (Ready = false)
    May  4 12:26:23.277: INFO: Pod "test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e": Phase="Running", Reason="", readiness=false. Elapsed: 16.011923178s
    May  4 12:26:23.277: INFO: The phase of Pod test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e is Running (Ready = false)
    May  4 12:26:25.277: INFO: Pod "test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e": Phase="Running", Reason="", readiness=false. Elapsed: 18.01247859s
    May  4 12:26:25.277: INFO: The phase of Pod test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e is Running (Ready = false)
    May  4 12:26:27.277: INFO: Pod "test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e": Phase="Running", Reason="", readiness=false. Elapsed: 20.012012281s
    May  4 12:26:27.277: INFO: The phase of Pod test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e is Running (Ready = false)
    May  4 12:26:29.277: INFO: Pod "test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e": Phase="Running", Reason="", readiness=true. Elapsed: 22.012682562s
    May  4 12:26:29.277: INFO: The phase of Pod test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e is Running (Ready = true)
    May  4 12:26:29.277: INFO: Pod "test-webserver-b3a43e89-3daf-4bdd-b75d-5b7ab780908e" satisfied condition "running and ready"
    May  4 12:26:29.284: INFO: Container started at 2023-05-04 12:26:08 +0000 UTC, pod became ready at 2023-05-04 12:26:27 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    May  4 12:26:29.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-9191" for this suite. 05/04/23 12:26:29.296
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:26:29.313
May  4 12:26:29.313: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename kubectl 05/04/23 12:26:29.314
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:26:29.337
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:26:29.344
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
STEP: creating all guestbook components 05/04/23 12:26:29.347
May  4 12:26:29.347: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

May  4 12:26:29.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-7666 create -f -'
May  4 12:26:30.468: INFO: stderr: ""
May  4 12:26:30.468: INFO: stdout: "service/agnhost-replica created\n"
May  4 12:26:30.468: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

May  4 12:26:30.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-7666 create -f -'
May  4 12:26:31.420: INFO: stderr: ""
May  4 12:26:31.420: INFO: stdout: "service/agnhost-primary created\n"
May  4 12:26:31.420: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

May  4 12:26:31.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-7666 create -f -'
May  4 12:26:32.662: INFO: stderr: ""
May  4 12:26:32.662: INFO: stdout: "service/frontend created\n"
May  4 12:26:32.662: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

May  4 12:26:32.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-7666 create -f -'
May  4 12:26:32.958: INFO: stderr: ""
May  4 12:26:32.958: INFO: stdout: "deployment.apps/frontend created\n"
May  4 12:26:32.958: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May  4 12:26:32.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-7666 create -f -'
May  4 12:26:33.231: INFO: stderr: ""
May  4 12:26:33.231: INFO: stdout: "deployment.apps/agnhost-primary created\n"
May  4 12:26:33.231: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May  4 12:26:33.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-7666 create -f -'
May  4 12:26:33.503: INFO: stderr: ""
May  4 12:26:33.503: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 05/04/23 12:26:33.503
May  4 12:26:33.503: INFO: Waiting for all frontend pods to be Running.
May  4 12:26:38.554: INFO: Waiting for frontend to serve content.
May  4 12:26:38.566: INFO: Trying to add a new entry to the guestbook.
May  4 12:26:38.582: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 05/04/23 12:26:38.594
May  4 12:26:38.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-7666 delete --grace-period=0 --force -f -'
May  4 12:26:38.773: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  4 12:26:38.773: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 05/04/23 12:26:38.773
May  4 12:26:38.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-7666 delete --grace-period=0 --force -f -'
May  4 12:26:38.944: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  4 12:26:38.944: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 05/04/23 12:26:38.944
May  4 12:26:38.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-7666 delete --grace-period=0 --force -f -'
May  4 12:26:39.075: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  4 12:26:39.075: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 05/04/23 12:26:39.075
May  4 12:26:39.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-7666 delete --grace-period=0 --force -f -'
May  4 12:26:39.187: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  4 12:26:39.187: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 05/04/23 12:26:39.187
May  4 12:26:39.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-7666 delete --grace-period=0 --force -f -'
May  4 12:26:39.313: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  4 12:26:39.313: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 05/04/23 12:26:39.313
May  4 12:26:39.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-7666 delete --grace-period=0 --force -f -'
May  4 12:26:39.424: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  4 12:26:39.424: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  4 12:26:39.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7666" for this suite. 05/04/23 12:26:39.435
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","completed":163,"skipped":3204,"failed":0}
------------------------------
• [SLOW TEST] [10.145 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:367
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:26:29.313
    May  4 12:26:29.313: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename kubectl 05/04/23 12:26:29.314
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:26:29.337
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:26:29.344
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:392
    STEP: creating all guestbook components 05/04/23 12:26:29.347
    May  4 12:26:29.347: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    May  4 12:26:29.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-7666 create -f -'
    May  4 12:26:30.468: INFO: stderr: ""
    May  4 12:26:30.468: INFO: stdout: "service/agnhost-replica created\n"
    May  4 12:26:30.468: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    May  4 12:26:30.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-7666 create -f -'
    May  4 12:26:31.420: INFO: stderr: ""
    May  4 12:26:31.420: INFO: stdout: "service/agnhost-primary created\n"
    May  4 12:26:31.420: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    May  4 12:26:31.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-7666 create -f -'
    May  4 12:26:32.662: INFO: stderr: ""
    May  4 12:26:32.662: INFO: stdout: "service/frontend created\n"
    May  4 12:26:32.662: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    May  4 12:26:32.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-7666 create -f -'
    May  4 12:26:32.958: INFO: stderr: ""
    May  4 12:26:32.958: INFO: stdout: "deployment.apps/frontend created\n"
    May  4 12:26:32.958: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    May  4 12:26:32.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-7666 create -f -'
    May  4 12:26:33.231: INFO: stderr: ""
    May  4 12:26:33.231: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    May  4 12:26:33.231: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    May  4 12:26:33.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-7666 create -f -'
    May  4 12:26:33.503: INFO: stderr: ""
    May  4 12:26:33.503: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 05/04/23 12:26:33.503
    May  4 12:26:33.503: INFO: Waiting for all frontend pods to be Running.
    May  4 12:26:38.554: INFO: Waiting for frontend to serve content.
    May  4 12:26:38.566: INFO: Trying to add a new entry to the guestbook.
    May  4 12:26:38.582: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 05/04/23 12:26:38.594
    May  4 12:26:38.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-7666 delete --grace-period=0 --force -f -'
    May  4 12:26:38.773: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May  4 12:26:38.773: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 05/04/23 12:26:38.773
    May  4 12:26:38.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-7666 delete --grace-period=0 --force -f -'
    May  4 12:26:38.944: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May  4 12:26:38.944: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 05/04/23 12:26:38.944
    May  4 12:26:38.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-7666 delete --grace-period=0 --force -f -'
    May  4 12:26:39.075: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May  4 12:26:39.075: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 05/04/23 12:26:39.075
    May  4 12:26:39.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-7666 delete --grace-period=0 --force -f -'
    May  4 12:26:39.187: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May  4 12:26:39.187: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 05/04/23 12:26:39.187
    May  4 12:26:39.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-7666 delete --grace-period=0 --force -f -'
    May  4 12:26:39.313: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May  4 12:26:39.313: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 05/04/23 12:26:39.313
    May  4 12:26:39.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-7666 delete --grace-period=0 --force -f -'
    May  4 12:26:39.424: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May  4 12:26:39.424: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  4 12:26:39.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7666" for this suite. 05/04/23 12:26:39.435
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:26:39.461
May  4 12:26:39.461: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename replicaset 05/04/23 12:26:39.463
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:26:39.5
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:26:39.508
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 05/04/23 12:26:39.529
STEP: Verify that the required pods have come up. 05/04/23 12:26:39.538
May  4 12:26:39.544: INFO: Pod name sample-pod: Found 0 pods out of 1
May  4 12:26:44.554: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 05/04/23 12:26:44.554
STEP: Getting /status 05/04/23 12:26:44.554
May  4 12:26:44.561: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 05/04/23 12:26:44.561
May  4 12:26:44.576: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 05/04/23 12:26:44.576
May  4 12:26:44.577: INFO: Observed &ReplicaSet event: ADDED
May  4 12:26:44.578: INFO: Observed &ReplicaSet event: MODIFIED
May  4 12:26:44.578: INFO: Observed &ReplicaSet event: MODIFIED
May  4 12:26:44.578: INFO: Observed &ReplicaSet event: MODIFIED
May  4 12:26:44.578: INFO: Found replicaset test-rs in namespace replicaset-7193 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May  4 12:26:44.578: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 05/04/23 12:26:44.578
May  4 12:26:44.578: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
May  4 12:26:44.592: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 05/04/23 12:26:44.592
May  4 12:26:44.594: INFO: Observed &ReplicaSet event: ADDED
May  4 12:26:44.594: INFO: Observed &ReplicaSet event: MODIFIED
May  4 12:26:44.594: INFO: Observed &ReplicaSet event: MODIFIED
May  4 12:26:44.595: INFO: Observed &ReplicaSet event: MODIFIED
May  4 12:26:44.595: INFO: Observed replicaset test-rs in namespace replicaset-7193 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May  4 12:26:44.595: INFO: Observed &ReplicaSet event: MODIFIED
May  4 12:26:44.595: INFO: Found replicaset test-rs in namespace replicaset-7193 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
May  4 12:26:44.595: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
May  4 12:26:44.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7193" for this suite. 05/04/23 12:26:44.604
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","completed":164,"skipped":3257,"failed":0}
------------------------------
• [SLOW TEST] [5.156 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:26:39.461
    May  4 12:26:39.461: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename replicaset 05/04/23 12:26:39.463
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:26:39.5
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:26:39.508
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 05/04/23 12:26:39.529
    STEP: Verify that the required pods have come up. 05/04/23 12:26:39.538
    May  4 12:26:39.544: INFO: Pod name sample-pod: Found 0 pods out of 1
    May  4 12:26:44.554: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 05/04/23 12:26:44.554
    STEP: Getting /status 05/04/23 12:26:44.554
    May  4 12:26:44.561: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 05/04/23 12:26:44.561
    May  4 12:26:44.576: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 05/04/23 12:26:44.576
    May  4 12:26:44.577: INFO: Observed &ReplicaSet event: ADDED
    May  4 12:26:44.578: INFO: Observed &ReplicaSet event: MODIFIED
    May  4 12:26:44.578: INFO: Observed &ReplicaSet event: MODIFIED
    May  4 12:26:44.578: INFO: Observed &ReplicaSet event: MODIFIED
    May  4 12:26:44.578: INFO: Found replicaset test-rs in namespace replicaset-7193 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    May  4 12:26:44.578: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 05/04/23 12:26:44.578
    May  4 12:26:44.578: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    May  4 12:26:44.592: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 05/04/23 12:26:44.592
    May  4 12:26:44.594: INFO: Observed &ReplicaSet event: ADDED
    May  4 12:26:44.594: INFO: Observed &ReplicaSet event: MODIFIED
    May  4 12:26:44.594: INFO: Observed &ReplicaSet event: MODIFIED
    May  4 12:26:44.595: INFO: Observed &ReplicaSet event: MODIFIED
    May  4 12:26:44.595: INFO: Observed replicaset test-rs in namespace replicaset-7193 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    May  4 12:26:44.595: INFO: Observed &ReplicaSet event: MODIFIED
    May  4 12:26:44.595: INFO: Found replicaset test-rs in namespace replicaset-7193 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    May  4 12:26:44.595: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    May  4 12:26:44.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-7193" for this suite. 05/04/23 12:26:44.604
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:26:44.618
May  4 12:26:44.619: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename pods 05/04/23 12:26:44.62
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:26:44.656
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:26:44.661
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
May  4 12:26:44.664: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: creating the pod 05/04/23 12:26:44.664
STEP: submitting the pod to kubernetes 05/04/23 12:26:44.664
May  4 12:26:44.683: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-887056d9-9a2d-4833-94e1-9cbf024bf2f2" in namespace "pods-7148" to be "running and ready"
May  4 12:26:44.692: INFO: Pod "pod-exec-websocket-887056d9-9a2d-4833-94e1-9cbf024bf2f2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.165482ms
May  4 12:26:44.692: INFO: The phase of Pod pod-exec-websocket-887056d9-9a2d-4833-94e1-9cbf024bf2f2 is Pending, waiting for it to be Running (with Ready = true)
May  4 12:26:46.698: INFO: Pod "pod-exec-websocket-887056d9-9a2d-4833-94e1-9cbf024bf2f2": Phase="Running", Reason="", readiness=true. Elapsed: 2.014712902s
May  4 12:26:46.698: INFO: The phase of Pod pod-exec-websocket-887056d9-9a2d-4833-94e1-9cbf024bf2f2 is Running (Ready = true)
May  4 12:26:46.698: INFO: Pod "pod-exec-websocket-887056d9-9a2d-4833-94e1-9cbf024bf2f2" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
May  4 12:26:46.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7148" for this suite. 05/04/23 12:26:46.81
{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","completed":165,"skipped":3262,"failed":0}
------------------------------
• [2.210 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:26:44.618
    May  4 12:26:44.619: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename pods 05/04/23 12:26:44.62
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:26:44.656
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:26:44.661
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:535
    May  4 12:26:44.664: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: creating the pod 05/04/23 12:26:44.664
    STEP: submitting the pod to kubernetes 05/04/23 12:26:44.664
    May  4 12:26:44.683: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-887056d9-9a2d-4833-94e1-9cbf024bf2f2" in namespace "pods-7148" to be "running and ready"
    May  4 12:26:44.692: INFO: Pod "pod-exec-websocket-887056d9-9a2d-4833-94e1-9cbf024bf2f2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.165482ms
    May  4 12:26:44.692: INFO: The phase of Pod pod-exec-websocket-887056d9-9a2d-4833-94e1-9cbf024bf2f2 is Pending, waiting for it to be Running (with Ready = true)
    May  4 12:26:46.698: INFO: Pod "pod-exec-websocket-887056d9-9a2d-4833-94e1-9cbf024bf2f2": Phase="Running", Reason="", readiness=true. Elapsed: 2.014712902s
    May  4 12:26:46.698: INFO: The phase of Pod pod-exec-websocket-887056d9-9a2d-4833-94e1-9cbf024bf2f2 is Running (Ready = true)
    May  4 12:26:46.698: INFO: Pod "pod-exec-websocket-887056d9-9a2d-4833-94e1-9cbf024bf2f2" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    May  4 12:26:46.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-7148" for this suite. 05/04/23 12:26:46.81
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:26:46.83
May  4 12:26:46.831: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename emptydir 05/04/23 12:26:46.834
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:26:46.859
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:26:46.864
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
STEP: Creating a pod to test emptydir volume type on tmpfs 05/04/23 12:26:46.871
May  4 12:26:46.896: INFO: Waiting up to 5m0s for pod "pod-faaaa85c-6b68-4c8e-8042-bde3db7e326c" in namespace "emptydir-5527" to be "Succeeded or Failed"
May  4 12:26:46.904: INFO: Pod "pod-faaaa85c-6b68-4c8e-8042-bde3db7e326c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.664483ms
May  4 12:26:48.910: INFO: Pod "pod-faaaa85c-6b68-4c8e-8042-bde3db7e326c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01378643s
May  4 12:26:50.909: INFO: Pod "pod-faaaa85c-6b68-4c8e-8042-bde3db7e326c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013048476s
STEP: Saw pod success 05/04/23 12:26:50.909
May  4 12:26:50.909: INFO: Pod "pod-faaaa85c-6b68-4c8e-8042-bde3db7e326c" satisfied condition "Succeeded or Failed"
May  4 12:26:50.913: INFO: Trying to get logs from node ip-10-0-1-232.us-west-2.compute.internal pod pod-faaaa85c-6b68-4c8e-8042-bde3db7e326c container test-container: <nil>
STEP: delete the pod 05/04/23 12:26:50.93
May  4 12:26:50.949: INFO: Waiting for pod pod-faaaa85c-6b68-4c8e-8042-bde3db7e326c to disappear
May  4 12:26:50.953: INFO: Pod pod-faaaa85c-6b68-4c8e-8042-bde3db7e326c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May  4 12:26:50.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5527" for this suite. 05/04/23 12:26:50.964
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":166,"skipped":3299,"failed":0}
------------------------------
• [4.145 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:26:46.83
    May  4 12:26:46.831: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename emptydir 05/04/23 12:26:46.834
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:26:46.859
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:26:46.864
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:86
    STEP: Creating a pod to test emptydir volume type on tmpfs 05/04/23 12:26:46.871
    May  4 12:26:46.896: INFO: Waiting up to 5m0s for pod "pod-faaaa85c-6b68-4c8e-8042-bde3db7e326c" in namespace "emptydir-5527" to be "Succeeded or Failed"
    May  4 12:26:46.904: INFO: Pod "pod-faaaa85c-6b68-4c8e-8042-bde3db7e326c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.664483ms
    May  4 12:26:48.910: INFO: Pod "pod-faaaa85c-6b68-4c8e-8042-bde3db7e326c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01378643s
    May  4 12:26:50.909: INFO: Pod "pod-faaaa85c-6b68-4c8e-8042-bde3db7e326c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013048476s
    STEP: Saw pod success 05/04/23 12:26:50.909
    May  4 12:26:50.909: INFO: Pod "pod-faaaa85c-6b68-4c8e-8042-bde3db7e326c" satisfied condition "Succeeded or Failed"
    May  4 12:26:50.913: INFO: Trying to get logs from node ip-10-0-1-232.us-west-2.compute.internal pod pod-faaaa85c-6b68-4c8e-8042-bde3db7e326c container test-container: <nil>
    STEP: delete the pod 05/04/23 12:26:50.93
    May  4 12:26:50.949: INFO: Waiting for pod pod-faaaa85c-6b68-4c8e-8042-bde3db7e326c to disappear
    May  4 12:26:50.953: INFO: Pod pod-faaaa85c-6b68-4c8e-8042-bde3db7e326c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May  4 12:26:50.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5527" for this suite. 05/04/23 12:26:50.964
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:26:50.976
May  4 12:26:50.976: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename secrets 05/04/23 12:26:50.977
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:26:51.008
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:26:51.011
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
STEP: creating secret secrets-1878/secret-test-0a470047-ff46-4af9-a0fa-8a45eb2da730 05/04/23 12:26:51.013
STEP: Creating a pod to test consume secrets 05/04/23 12:26:51.022
May  4 12:26:51.035: INFO: Waiting up to 5m0s for pod "pod-configmaps-57d82736-e798-4563-91b7-5044aae21ccc" in namespace "secrets-1878" to be "Succeeded or Failed"
May  4 12:26:51.039: INFO: Pod "pod-configmaps-57d82736-e798-4563-91b7-5044aae21ccc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.789048ms
May  4 12:26:53.046: INFO: Pod "pod-configmaps-57d82736-e798-4563-91b7-5044aae21ccc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010966407s
May  4 12:26:55.045: INFO: Pod "pod-configmaps-57d82736-e798-4563-91b7-5044aae21ccc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010157537s
STEP: Saw pod success 05/04/23 12:26:55.045
May  4 12:26:55.045: INFO: Pod "pod-configmaps-57d82736-e798-4563-91b7-5044aae21ccc" satisfied condition "Succeeded or Failed"
May  4 12:26:55.052: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod pod-configmaps-57d82736-e798-4563-91b7-5044aae21ccc container env-test: <nil>
STEP: delete the pod 05/04/23 12:26:55.069
May  4 12:26:55.083: INFO: Waiting for pod pod-configmaps-57d82736-e798-4563-91b7-5044aae21ccc to disappear
May  4 12:26:55.086: INFO: Pod pod-configmaps-57d82736-e798-4563-91b7-5044aae21ccc no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
May  4 12:26:55.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1878" for this suite. 05/04/23 12:26:55.094
{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","completed":167,"skipped":3309,"failed":0}
------------------------------
• [4.125 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:26:50.976
    May  4 12:26:50.976: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename secrets 05/04/23 12:26:50.977
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:26:51.008
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:26:51.011
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:94
    STEP: creating secret secrets-1878/secret-test-0a470047-ff46-4af9-a0fa-8a45eb2da730 05/04/23 12:26:51.013
    STEP: Creating a pod to test consume secrets 05/04/23 12:26:51.022
    May  4 12:26:51.035: INFO: Waiting up to 5m0s for pod "pod-configmaps-57d82736-e798-4563-91b7-5044aae21ccc" in namespace "secrets-1878" to be "Succeeded or Failed"
    May  4 12:26:51.039: INFO: Pod "pod-configmaps-57d82736-e798-4563-91b7-5044aae21ccc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.789048ms
    May  4 12:26:53.046: INFO: Pod "pod-configmaps-57d82736-e798-4563-91b7-5044aae21ccc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010966407s
    May  4 12:26:55.045: INFO: Pod "pod-configmaps-57d82736-e798-4563-91b7-5044aae21ccc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010157537s
    STEP: Saw pod success 05/04/23 12:26:55.045
    May  4 12:26:55.045: INFO: Pod "pod-configmaps-57d82736-e798-4563-91b7-5044aae21ccc" satisfied condition "Succeeded or Failed"
    May  4 12:26:55.052: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod pod-configmaps-57d82736-e798-4563-91b7-5044aae21ccc container env-test: <nil>
    STEP: delete the pod 05/04/23 12:26:55.069
    May  4 12:26:55.083: INFO: Waiting for pod pod-configmaps-57d82736-e798-4563-91b7-5044aae21ccc to disappear
    May  4 12:26:55.086: INFO: Pod pod-configmaps-57d82736-e798-4563-91b7-5044aae21ccc no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    May  4 12:26:55.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-1878" for this suite. 05/04/23 12:26:55.094
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:26:55.102
May  4 12:26:55.105: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename events 05/04/23 12:26:55.106
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:26:55.127
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:26:55.134
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 05/04/23 12:26:55.137
STEP: get a list of Events with a label in the current namespace 05/04/23 12:26:55.156
STEP: delete a list of events 05/04/23 12:26:55.16
May  4 12:26:55.160: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 05/04/23 12:26:55.186
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
May  4 12:26:55.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4910" for this suite. 05/04/23 12:26:55.198
{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","completed":168,"skipped":3326,"failed":0}
------------------------------
• [0.105 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:26:55.102
    May  4 12:26:55.105: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename events 05/04/23 12:26:55.106
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:26:55.127
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:26:55.134
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 05/04/23 12:26:55.137
    STEP: get a list of Events with a label in the current namespace 05/04/23 12:26:55.156
    STEP: delete a list of events 05/04/23 12:26:55.16
    May  4 12:26:55.160: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 05/04/23 12:26:55.186
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    May  4 12:26:55.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-4910" for this suite. 05/04/23 12:26:55.198
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:26:55.21
May  4 12:26:55.213: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename resourcequota 05/04/23 12:26:55.214
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:26:55.231
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:26:55.234
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
STEP: Creating a ResourceQuota with best effort scope 05/04/23 12:26:55.237
STEP: Ensuring ResourceQuota status is calculated 05/04/23 12:26:55.244
STEP: Creating a ResourceQuota with not best effort scope 05/04/23 12:26:57.25
STEP: Ensuring ResourceQuota status is calculated 05/04/23 12:26:57.26
STEP: Creating a best-effort pod 05/04/23 12:26:59.265
STEP: Ensuring resource quota with best effort scope captures the pod usage 05/04/23 12:26:59.285
STEP: Ensuring resource quota with not best effort ignored the pod usage 05/04/23 12:27:01.294
STEP: Deleting the pod 05/04/23 12:27:03.299
STEP: Ensuring resource quota status released the pod usage 05/04/23 12:27:03.314
STEP: Creating a not best-effort pod 05/04/23 12:27:05.319
STEP: Ensuring resource quota with not best effort scope captures the pod usage 05/04/23 12:27:05.333
STEP: Ensuring resource quota with best effort scope ignored the pod usage 05/04/23 12:27:07.338
STEP: Deleting the pod 05/04/23 12:27:09.344
STEP: Ensuring resource quota status released the pod usage 05/04/23 12:27:09.37
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
May  4 12:27:11.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8718" for this suite. 05/04/23 12:27:11.384
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","completed":169,"skipped":3379,"failed":0}
------------------------------
• [SLOW TEST] [16.184 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:26:55.21
    May  4 12:26:55.213: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename resourcequota 05/04/23 12:26:55.214
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:26:55.231
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:26:55.234
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:793
    STEP: Creating a ResourceQuota with best effort scope 05/04/23 12:26:55.237
    STEP: Ensuring ResourceQuota status is calculated 05/04/23 12:26:55.244
    STEP: Creating a ResourceQuota with not best effort scope 05/04/23 12:26:57.25
    STEP: Ensuring ResourceQuota status is calculated 05/04/23 12:26:57.26
    STEP: Creating a best-effort pod 05/04/23 12:26:59.265
    STEP: Ensuring resource quota with best effort scope captures the pod usage 05/04/23 12:26:59.285
    STEP: Ensuring resource quota with not best effort ignored the pod usage 05/04/23 12:27:01.294
    STEP: Deleting the pod 05/04/23 12:27:03.299
    STEP: Ensuring resource quota status released the pod usage 05/04/23 12:27:03.314
    STEP: Creating a not best-effort pod 05/04/23 12:27:05.319
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 05/04/23 12:27:05.333
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 05/04/23 12:27:07.338
    STEP: Deleting the pod 05/04/23 12:27:09.344
    STEP: Ensuring resource quota status released the pod usage 05/04/23 12:27:09.37
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    May  4 12:27:11.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-8718" for this suite. 05/04/23 12:27:11.384
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:27:11.395
May  4 12:27:11.395: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename cronjob 05/04/23 12:27:11.396
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:27:11.414
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:27:11.418
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 05/04/23 12:27:11.421
STEP: Ensuring a job is scheduled 05/04/23 12:27:11.432
STEP: Ensuring exactly one is scheduled 05/04/23 12:28:01.439
STEP: Ensuring exactly one running job exists by listing jobs explicitly 05/04/23 12:28:01.444
STEP: Ensuring no more jobs are scheduled 05/04/23 12:28:01.449
STEP: Removing cronjob 05/04/23 12:33:01.458
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
May  4 12:33:01.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-2121" for this suite. 05/04/23 12:33:01.481
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","completed":170,"skipped":3394,"failed":0}
------------------------------
• [SLOW TEST] [350.097 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:27:11.395
    May  4 12:27:11.395: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename cronjob 05/04/23 12:27:11.396
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:27:11.414
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:27:11.418
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 05/04/23 12:27:11.421
    STEP: Ensuring a job is scheduled 05/04/23 12:27:11.432
    STEP: Ensuring exactly one is scheduled 05/04/23 12:28:01.439
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 05/04/23 12:28:01.444
    STEP: Ensuring no more jobs are scheduled 05/04/23 12:28:01.449
    STEP: Removing cronjob 05/04/23 12:33:01.458
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    May  4 12:33:01.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-2121" for this suite. 05/04/23 12:33:01.481
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:33:01.493
May  4 12:33:01.493: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename gc 05/04/23 12:33:01.495
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:33:01.522
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:33:01.527
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
May  4 12:33:01.573: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"9eb926c6-6a93-4a3f-bdeb-dd3672e9329f", Controller:(*bool)(0xc00527c97e), BlockOwnerDeletion:(*bool)(0xc00527c97f)}}
May  4 12:33:01.581: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"863a8b3c-3c42-4733-8bf0-d7deff2f2025", Controller:(*bool)(0xc00568aa8e), BlockOwnerDeletion:(*bool)(0xc00568aa8f)}}
May  4 12:33:01.592: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"55af14b0-70c3-4522-9739-fa191c8e2e3d", Controller:(*bool)(0xc00568ac9e), BlockOwnerDeletion:(*bool)(0xc00568ac9f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
May  4 12:33:06.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2933" for this suite. 05/04/23 12:33:06.638
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","completed":171,"skipped":3398,"failed":0}
------------------------------
• [SLOW TEST] [5.155 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:33:01.493
    May  4 12:33:01.493: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename gc 05/04/23 12:33:01.495
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:33:01.522
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:33:01.527
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    May  4 12:33:01.573: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"9eb926c6-6a93-4a3f-bdeb-dd3672e9329f", Controller:(*bool)(0xc00527c97e), BlockOwnerDeletion:(*bool)(0xc00527c97f)}}
    May  4 12:33:01.581: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"863a8b3c-3c42-4733-8bf0-d7deff2f2025", Controller:(*bool)(0xc00568aa8e), BlockOwnerDeletion:(*bool)(0xc00568aa8f)}}
    May  4 12:33:01.592: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"55af14b0-70c3-4522-9739-fa191c8e2e3d", Controller:(*bool)(0xc00568ac9e), BlockOwnerDeletion:(*bool)(0xc00568ac9f)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    May  4 12:33:06.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-2933" for this suite. 05/04/23 12:33:06.638
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:33:06.647
May  4 12:33:06.648: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename statefulset 05/04/23 12:33:06.649
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:33:06.667
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:33:06.673
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-4377 05/04/23 12:33:06.675
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
STEP: Creating a new StatefulSet 05/04/23 12:33:06.681
May  4 12:33:06.707: INFO: Found 0 stateful pods, waiting for 3
May  4 12:33:16.717: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May  4 12:33:16.717: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May  4 12:33:16.717: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 05/04/23 12:33:16.732
May  4 12:33:16.755: INFO: Updating stateful set ss2
STEP: Creating a new revision 05/04/23 12:33:16.755
STEP: Not applying an update when the partition is greater than the number of replicas 05/04/23 12:33:26.781
STEP: Performing a canary update 05/04/23 12:33:26.781
May  4 12:33:26.805: INFO: Updating stateful set ss2
May  4 12:33:26.819: INFO: Waiting for Pod statefulset-4377/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
STEP: Restoring Pods to the correct revision when they are deleted 05/04/23 12:33:36.833
May  4 12:33:36.966: INFO: Found 2 stateful pods, waiting for 3
May  4 12:33:46.979: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May  4 12:33:46.979: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May  4 12:33:46.979: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 05/04/23 12:33:46.989
May  4 12:33:47.017: INFO: Updating stateful set ss2
May  4 12:33:47.032: INFO: Waiting for Pod statefulset-4377/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
May  4 12:33:57.076: INFO: Updating stateful set ss2
May  4 12:33:57.091: INFO: Waiting for StatefulSet statefulset-4377/ss2 to complete update
May  4 12:33:57.091: INFO: Waiting for Pod statefulset-4377/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
May  4 12:34:07.110: INFO: Deleting all statefulset in ns statefulset-4377
May  4 12:34:07.121: INFO: Scaling statefulset ss2 to 0
May  4 12:34:17.157: INFO: Waiting for statefulset status.replicas updated to 0
May  4 12:34:17.162: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
May  4 12:34:17.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4377" for this suite. 05/04/23 12:34:17.187
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","completed":172,"skipped":3399,"failed":0}
------------------------------
• [SLOW TEST] [70.550 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:315

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:33:06.647
    May  4 12:33:06.648: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename statefulset 05/04/23 12:33:06.649
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:33:06.667
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:33:06.673
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-4377 05/04/23 12:33:06.675
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:315
    STEP: Creating a new StatefulSet 05/04/23 12:33:06.681
    May  4 12:33:06.707: INFO: Found 0 stateful pods, waiting for 3
    May  4 12:33:16.717: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    May  4 12:33:16.717: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    May  4 12:33:16.717: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 05/04/23 12:33:16.732
    May  4 12:33:16.755: INFO: Updating stateful set ss2
    STEP: Creating a new revision 05/04/23 12:33:16.755
    STEP: Not applying an update when the partition is greater than the number of replicas 05/04/23 12:33:26.781
    STEP: Performing a canary update 05/04/23 12:33:26.781
    May  4 12:33:26.805: INFO: Updating stateful set ss2
    May  4 12:33:26.819: INFO: Waiting for Pod statefulset-4377/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    STEP: Restoring Pods to the correct revision when they are deleted 05/04/23 12:33:36.833
    May  4 12:33:36.966: INFO: Found 2 stateful pods, waiting for 3
    May  4 12:33:46.979: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    May  4 12:33:46.979: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    May  4 12:33:46.979: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 05/04/23 12:33:46.989
    May  4 12:33:47.017: INFO: Updating stateful set ss2
    May  4 12:33:47.032: INFO: Waiting for Pod statefulset-4377/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    May  4 12:33:57.076: INFO: Updating stateful set ss2
    May  4 12:33:57.091: INFO: Waiting for StatefulSet statefulset-4377/ss2 to complete update
    May  4 12:33:57.091: INFO: Waiting for Pod statefulset-4377/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    May  4 12:34:07.110: INFO: Deleting all statefulset in ns statefulset-4377
    May  4 12:34:07.121: INFO: Scaling statefulset ss2 to 0
    May  4 12:34:17.157: INFO: Waiting for statefulset status.replicas updated to 0
    May  4 12:34:17.162: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    May  4 12:34:17.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-4377" for this suite. 05/04/23 12:34:17.187
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:34:17.199
May  4 12:34:17.199: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename ephemeral-containers-test 05/04/23 12:34:17.2
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:34:17.222
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:34:17.225
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 05/04/23 12:34:17.228
May  4 12:34:17.240: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-590" to be "running and ready"
May  4 12:34:17.246: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.5714ms
May  4 12:34:17.246: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
May  4 12:34:19.251: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010364289s
May  4 12:34:19.251: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
May  4 12:34:19.251: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 05/04/23 12:34:19.255
May  4 12:34:19.275: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-590" to be "container debugger running"
May  4 12:34:19.280: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.355079ms
May  4 12:34:21.287: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011840363s
May  4 12:34:23.285: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.009957733s
May  4 12:34:23.285: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 05/04/23 12:34:23.285
May  4 12:34:23.286: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-590 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  4 12:34:23.286: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
May  4 12:34:23.286: INFO: ExecWithOptions: Clientset creation
May  4 12:34:23.286: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/ephemeral-containers-test-590/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
May  4 12:34:23.372: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:187
May  4 12:34:23.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ephemeral-containers-test-590" for this suite. 05/04/23 12:34:23.405
{"msg":"PASSED [sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]","completed":173,"skipped":3423,"failed":0}
------------------------------
• [SLOW TEST] [6.214 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:34:17.199
    May  4 12:34:17.199: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename ephemeral-containers-test 05/04/23 12:34:17.2
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:34:17.222
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:34:17.225
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 05/04/23 12:34:17.228
    May  4 12:34:17.240: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-590" to be "running and ready"
    May  4 12:34:17.246: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.5714ms
    May  4 12:34:17.246: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    May  4 12:34:19.251: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010364289s
    May  4 12:34:19.251: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    May  4 12:34:19.251: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 05/04/23 12:34:19.255
    May  4 12:34:19.275: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-590" to be "container debugger running"
    May  4 12:34:19.280: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.355079ms
    May  4 12:34:21.287: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011840363s
    May  4 12:34:23.285: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.009957733s
    May  4 12:34:23.285: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 05/04/23 12:34:23.285
    May  4 12:34:23.286: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-590 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  4 12:34:23.286: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    May  4 12:34:23.286: INFO: ExecWithOptions: Clientset creation
    May  4 12:34:23.286: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/ephemeral-containers-test-590/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    May  4 12:34:23.372: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:187
    May  4 12:34:23.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ephemeral-containers-test-590" for this suite. 05/04/23 12:34:23.405
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:34:23.415
May  4 12:34:23.415: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename statefulset 05/04/23 12:34:23.416
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:34:23.434
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:34:23.44
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-2237 05/04/23 12:34:23.442
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
STEP: Creating statefulset ss in namespace statefulset-2237 05/04/23 12:34:23.448
May  4 12:34:23.464: INFO: Found 0 stateful pods, waiting for 1
May  4 12:34:33.470: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 05/04/23 12:34:33.482
STEP: updating a scale subresource 05/04/23 12:34:33.486
STEP: verifying the statefulset Spec.Replicas was modified 05/04/23 12:34:33.5
STEP: Patch a scale subresource 05/04/23 12:34:33.517
STEP: verifying the statefulset Spec.Replicas was modified 05/04/23 12:34:33.534
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
May  4 12:34:33.540: INFO: Deleting all statefulset in ns statefulset-2237
May  4 12:34:33.546: INFO: Scaling statefulset ss to 0
May  4 12:34:43.569: INFO: Waiting for statefulset status.replicas updated to 0
May  4 12:34:43.575: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
May  4 12:34:43.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2237" for this suite. 05/04/23 12:34:43.607
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","completed":174,"skipped":3444,"failed":0}
------------------------------
• [SLOW TEST] [20.204 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:846

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:34:23.415
    May  4 12:34:23.415: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename statefulset 05/04/23 12:34:23.416
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:34:23.434
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:34:23.44
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-2237 05/04/23 12:34:23.442
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:846
    STEP: Creating statefulset ss in namespace statefulset-2237 05/04/23 12:34:23.448
    May  4 12:34:23.464: INFO: Found 0 stateful pods, waiting for 1
    May  4 12:34:33.470: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 05/04/23 12:34:33.482
    STEP: updating a scale subresource 05/04/23 12:34:33.486
    STEP: verifying the statefulset Spec.Replicas was modified 05/04/23 12:34:33.5
    STEP: Patch a scale subresource 05/04/23 12:34:33.517
    STEP: verifying the statefulset Spec.Replicas was modified 05/04/23 12:34:33.534
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    May  4 12:34:33.540: INFO: Deleting all statefulset in ns statefulset-2237
    May  4 12:34:33.546: INFO: Scaling statefulset ss to 0
    May  4 12:34:43.569: INFO: Waiting for statefulset status.replicas updated to 0
    May  4 12:34:43.575: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    May  4 12:34:43.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-2237" for this suite. 05/04/23 12:34:43.607
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:34:43.62
May  4 12:34:43.620: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename webhook 05/04/23 12:34:43.623
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:34:43.646
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:34:43.649
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/04/23 12:34:43.667
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/04/23 12:34:44.166
STEP: Deploying the webhook pod 05/04/23 12:34:44.174
STEP: Wait for the deployment to be ready 05/04/23 12:34:44.187
May  4 12:34:44.199: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/04/23 12:34:46.212
STEP: Verifying the service has paired with the endpoint 05/04/23 12:34:46.225
May  4 12:34:47.226: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
May  4 12:34:47.230: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3273-crds.webhook.example.com via the AdmissionRegistration API 05/04/23 12:34:47.743
STEP: Creating a custom resource that should be mutated by the webhook 05/04/23 12:34:47.763
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  4 12:34:50.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7842" for this suite. 05/04/23 12:34:50.345
STEP: Destroying namespace "webhook-7842-markers" for this suite. 05/04/23 12:34:50.356
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","completed":175,"skipped":3460,"failed":0}
------------------------------
• [SLOW TEST] [6.852 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:34:43.62
    May  4 12:34:43.620: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename webhook 05/04/23 12:34:43.623
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:34:43.646
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:34:43.649
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/04/23 12:34:43.667
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/04/23 12:34:44.166
    STEP: Deploying the webhook pod 05/04/23 12:34:44.174
    STEP: Wait for the deployment to be ready 05/04/23 12:34:44.187
    May  4 12:34:44.199: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/04/23 12:34:46.212
    STEP: Verifying the service has paired with the endpoint 05/04/23 12:34:46.225
    May  4 12:34:47.226: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:290
    May  4 12:34:47.230: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3273-crds.webhook.example.com via the AdmissionRegistration API 05/04/23 12:34:47.743
    STEP: Creating a custom resource that should be mutated by the webhook 05/04/23 12:34:47.763
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  4 12:34:50.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7842" for this suite. 05/04/23 12:34:50.345
    STEP: Destroying namespace "webhook-7842-markers" for this suite. 05/04/23 12:34:50.356
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:34:50.482
May  4 12:34:50.482: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename projected 05/04/23 12:34:50.486
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:34:50.523
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:34:50.529
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
STEP: Creating configMap with name cm-test-opt-del-f3b8abcf-3598-4166-9344-0580704b4196 05/04/23 12:34:50.545
STEP: Creating configMap with name cm-test-opt-upd-45187fcb-0bf5-4c00-b9fc-d02bf22114d5 05/04/23 12:34:50.555
STEP: Creating the pod 05/04/23 12:34:50.564
May  4 12:34:50.577: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9a5b432d-7ac0-453a-8d83-a242adb6682b" in namespace "projected-7512" to be "running and ready"
May  4 12:34:50.582: INFO: Pod "pod-projected-configmaps-9a5b432d-7ac0-453a-8d83-a242adb6682b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.08449ms
May  4 12:34:50.582: INFO: The phase of Pod pod-projected-configmaps-9a5b432d-7ac0-453a-8d83-a242adb6682b is Pending, waiting for it to be Running (with Ready = true)
May  4 12:34:52.589: INFO: Pod "pod-projected-configmaps-9a5b432d-7ac0-453a-8d83-a242adb6682b": Phase="Running", Reason="", readiness=true. Elapsed: 2.012254335s
May  4 12:34:52.589: INFO: The phase of Pod pod-projected-configmaps-9a5b432d-7ac0-453a-8d83-a242adb6682b is Running (Ready = true)
May  4 12:34:52.589: INFO: Pod "pod-projected-configmaps-9a5b432d-7ac0-453a-8d83-a242adb6682b" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-f3b8abcf-3598-4166-9344-0580704b4196 05/04/23 12:34:52.64
STEP: Updating configmap cm-test-opt-upd-45187fcb-0bf5-4c00-b9fc-d02bf22114d5 05/04/23 12:34:52.648
STEP: Creating configMap with name cm-test-opt-create-4955c267-8601-41d3-b3e6-18f33c6710a0 05/04/23 12:34:52.655
STEP: waiting to observe update in volume 05/04/23 12:34:52.66
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
May  4 12:34:56.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7512" for this suite. 05/04/23 12:34:56.718
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":176,"skipped":3504,"failed":0}
------------------------------
• [SLOW TEST] [6.244 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:34:50.482
    May  4 12:34:50.482: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename projected 05/04/23 12:34:50.486
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:34:50.523
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:34:50.529
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:173
    STEP: Creating configMap with name cm-test-opt-del-f3b8abcf-3598-4166-9344-0580704b4196 05/04/23 12:34:50.545
    STEP: Creating configMap with name cm-test-opt-upd-45187fcb-0bf5-4c00-b9fc-d02bf22114d5 05/04/23 12:34:50.555
    STEP: Creating the pod 05/04/23 12:34:50.564
    May  4 12:34:50.577: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9a5b432d-7ac0-453a-8d83-a242adb6682b" in namespace "projected-7512" to be "running and ready"
    May  4 12:34:50.582: INFO: Pod "pod-projected-configmaps-9a5b432d-7ac0-453a-8d83-a242adb6682b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.08449ms
    May  4 12:34:50.582: INFO: The phase of Pod pod-projected-configmaps-9a5b432d-7ac0-453a-8d83-a242adb6682b is Pending, waiting for it to be Running (with Ready = true)
    May  4 12:34:52.589: INFO: Pod "pod-projected-configmaps-9a5b432d-7ac0-453a-8d83-a242adb6682b": Phase="Running", Reason="", readiness=true. Elapsed: 2.012254335s
    May  4 12:34:52.589: INFO: The phase of Pod pod-projected-configmaps-9a5b432d-7ac0-453a-8d83-a242adb6682b is Running (Ready = true)
    May  4 12:34:52.589: INFO: Pod "pod-projected-configmaps-9a5b432d-7ac0-453a-8d83-a242adb6682b" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-f3b8abcf-3598-4166-9344-0580704b4196 05/04/23 12:34:52.64
    STEP: Updating configmap cm-test-opt-upd-45187fcb-0bf5-4c00-b9fc-d02bf22114d5 05/04/23 12:34:52.648
    STEP: Creating configMap with name cm-test-opt-create-4955c267-8601-41d3-b3e6-18f33c6710a0 05/04/23 12:34:52.655
    STEP: waiting to observe update in volume 05/04/23 12:34:52.66
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    May  4 12:34:56.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7512" for this suite. 05/04/23 12:34:56.718
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:34:56.727
May  4 12:34:56.727: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename container-probe 05/04/23 12:34:56.728
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:34:56.744
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:34:56.749
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
STEP: Creating pod busybox-b9340c3c-044d-4465-bb19-dcb06b6f84a2 in namespace container-probe-9888 05/04/23 12:34:56.751
May  4 12:34:56.763: INFO: Waiting up to 5m0s for pod "busybox-b9340c3c-044d-4465-bb19-dcb06b6f84a2" in namespace "container-probe-9888" to be "not pending"
May  4 12:34:56.766: INFO: Pod "busybox-b9340c3c-044d-4465-bb19-dcb06b6f84a2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.338928ms
May  4 12:34:58.772: INFO: Pod "busybox-b9340c3c-044d-4465-bb19-dcb06b6f84a2": Phase="Running", Reason="", readiness=true. Elapsed: 2.009907926s
May  4 12:34:58.773: INFO: Pod "busybox-b9340c3c-044d-4465-bb19-dcb06b6f84a2" satisfied condition "not pending"
May  4 12:34:58.773: INFO: Started pod busybox-b9340c3c-044d-4465-bb19-dcb06b6f84a2 in namespace container-probe-9888
STEP: checking the pod's current state and verifying that restartCount is present 05/04/23 12:34:58.773
May  4 12:34:58.778: INFO: Initial restart count of pod busybox-b9340c3c-044d-4465-bb19-dcb06b6f84a2 is 0
STEP: deleting the pod 05/04/23 12:38:59.527
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
May  4 12:38:59.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9888" for this suite. 05/04/23 12:38:59.56
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":177,"skipped":3512,"failed":0}
------------------------------
• [SLOW TEST] [242.842 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:34:56.727
    May  4 12:34:56.727: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename container-probe 05/04/23 12:34:56.728
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:34:56.744
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:34:56.749
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:148
    STEP: Creating pod busybox-b9340c3c-044d-4465-bb19-dcb06b6f84a2 in namespace container-probe-9888 05/04/23 12:34:56.751
    May  4 12:34:56.763: INFO: Waiting up to 5m0s for pod "busybox-b9340c3c-044d-4465-bb19-dcb06b6f84a2" in namespace "container-probe-9888" to be "not pending"
    May  4 12:34:56.766: INFO: Pod "busybox-b9340c3c-044d-4465-bb19-dcb06b6f84a2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.338928ms
    May  4 12:34:58.772: INFO: Pod "busybox-b9340c3c-044d-4465-bb19-dcb06b6f84a2": Phase="Running", Reason="", readiness=true. Elapsed: 2.009907926s
    May  4 12:34:58.773: INFO: Pod "busybox-b9340c3c-044d-4465-bb19-dcb06b6f84a2" satisfied condition "not pending"
    May  4 12:34:58.773: INFO: Started pod busybox-b9340c3c-044d-4465-bb19-dcb06b6f84a2 in namespace container-probe-9888
    STEP: checking the pod's current state and verifying that restartCount is present 05/04/23 12:34:58.773
    May  4 12:34:58.778: INFO: Initial restart count of pod busybox-b9340c3c-044d-4465-bb19-dcb06b6f84a2 is 0
    STEP: deleting the pod 05/04/23 12:38:59.527
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    May  4 12:38:59.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-9888" for this suite. 05/04/23 12:38:59.56
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:38:59.57
May  4 12:38:59.570: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename resourcequota 05/04/23 12:38:59.571
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:38:59.589
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:38:59.593
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
STEP: Creating a ResourceQuota 05/04/23 12:38:59.595
STEP: Getting a ResourceQuota 05/04/23 12:38:59.603
STEP: Updating a ResourceQuota 05/04/23 12:38:59.608
STEP: Verifying a ResourceQuota was modified 05/04/23 12:38:59.618
STEP: Deleting a ResourceQuota 05/04/23 12:38:59.625
STEP: Verifying the deleted ResourceQuota 05/04/23 12:38:59.632
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
May  4 12:38:59.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1188" for this suite. 05/04/23 12:38:59.643
{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","completed":178,"skipped":3530,"failed":0}
------------------------------
• [0.082 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:38:59.57
    May  4 12:38:59.570: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename resourcequota 05/04/23 12:38:59.571
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:38:59.589
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:38:59.593
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:874
    STEP: Creating a ResourceQuota 05/04/23 12:38:59.595
    STEP: Getting a ResourceQuota 05/04/23 12:38:59.603
    STEP: Updating a ResourceQuota 05/04/23 12:38:59.608
    STEP: Verifying a ResourceQuota was modified 05/04/23 12:38:59.618
    STEP: Deleting a ResourceQuota 05/04/23 12:38:59.625
    STEP: Verifying the deleted ResourceQuota 05/04/23 12:38:59.632
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    May  4 12:38:59.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-1188" for this suite. 05/04/23 12:38:59.643
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:38:59.654
May  4 12:38:59.655: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename downward-api 05/04/23 12:38:59.655
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:38:59.677
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:38:59.685
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
STEP: Creating a pod to test downward api env vars 05/04/23 12:38:59.688
May  4 12:38:59.701: INFO: Waiting up to 5m0s for pod "downward-api-87e6497f-b60a-4698-a0b3-00a68001cd01" in namespace "downward-api-6371" to be "Succeeded or Failed"
May  4 12:38:59.710: INFO: Pod "downward-api-87e6497f-b60a-4698-a0b3-00a68001cd01": Phase="Pending", Reason="", readiness=false. Elapsed: 9.159464ms
May  4 12:39:01.716: INFO: Pod "downward-api-87e6497f-b60a-4698-a0b3-00a68001cd01": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015580499s
May  4 12:39:03.716: INFO: Pod "downward-api-87e6497f-b60a-4698-a0b3-00a68001cd01": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015370779s
STEP: Saw pod success 05/04/23 12:39:03.716
May  4 12:39:03.716: INFO: Pod "downward-api-87e6497f-b60a-4698-a0b3-00a68001cd01" satisfied condition "Succeeded or Failed"
May  4 12:39:03.723: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod downward-api-87e6497f-b60a-4698-a0b3-00a68001cd01 container dapi-container: <nil>
STEP: delete the pod 05/04/23 12:39:03.753
May  4 12:39:03.779: INFO: Waiting for pod downward-api-87e6497f-b60a-4698-a0b3-00a68001cd01 to disappear
May  4 12:39:03.790: INFO: Pod downward-api-87e6497f-b60a-4698-a0b3-00a68001cd01 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
May  4 12:39:03.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6371" for this suite. 05/04/23 12:39:03.8
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","completed":179,"skipped":3546,"failed":0}
------------------------------
• [4.158 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:38:59.654
    May  4 12:38:59.655: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename downward-api 05/04/23 12:38:59.655
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:38:59.677
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:38:59.685
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:266
    STEP: Creating a pod to test downward api env vars 05/04/23 12:38:59.688
    May  4 12:38:59.701: INFO: Waiting up to 5m0s for pod "downward-api-87e6497f-b60a-4698-a0b3-00a68001cd01" in namespace "downward-api-6371" to be "Succeeded or Failed"
    May  4 12:38:59.710: INFO: Pod "downward-api-87e6497f-b60a-4698-a0b3-00a68001cd01": Phase="Pending", Reason="", readiness=false. Elapsed: 9.159464ms
    May  4 12:39:01.716: INFO: Pod "downward-api-87e6497f-b60a-4698-a0b3-00a68001cd01": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015580499s
    May  4 12:39:03.716: INFO: Pod "downward-api-87e6497f-b60a-4698-a0b3-00a68001cd01": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015370779s
    STEP: Saw pod success 05/04/23 12:39:03.716
    May  4 12:39:03.716: INFO: Pod "downward-api-87e6497f-b60a-4698-a0b3-00a68001cd01" satisfied condition "Succeeded or Failed"
    May  4 12:39:03.723: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod downward-api-87e6497f-b60a-4698-a0b3-00a68001cd01 container dapi-container: <nil>
    STEP: delete the pod 05/04/23 12:39:03.753
    May  4 12:39:03.779: INFO: Waiting for pod downward-api-87e6497f-b60a-4698-a0b3-00a68001cd01 to disappear
    May  4 12:39:03.790: INFO: Pod downward-api-87e6497f-b60a-4698-a0b3-00a68001cd01 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    May  4 12:39:03.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6371" for this suite. 05/04/23 12:39:03.8
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:39:03.813
May  4 12:39:03.813: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename pods 05/04/23 12:39:03.814
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:39:03.842
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:39:03.848
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
May  4 12:39:03.852: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: creating the pod 05/04/23 12:39:03.852
STEP: submitting the pod to kubernetes 05/04/23 12:39:03.853
May  4 12:39:03.873: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-3132f022-aa9d-4486-aadd-ec50289d1d64" in namespace "pods-7338" to be "running and ready"
May  4 12:39:03.879: INFO: Pod "pod-logs-websocket-3132f022-aa9d-4486-aadd-ec50289d1d64": Phase="Pending", Reason="", readiness=false. Elapsed: 6.285037ms
May  4 12:39:03.879: INFO: The phase of Pod pod-logs-websocket-3132f022-aa9d-4486-aadd-ec50289d1d64 is Pending, waiting for it to be Running (with Ready = true)
May  4 12:39:05.886: INFO: Pod "pod-logs-websocket-3132f022-aa9d-4486-aadd-ec50289d1d64": Phase="Running", Reason="", readiness=true. Elapsed: 2.012878347s
May  4 12:39:05.886: INFO: The phase of Pod pod-logs-websocket-3132f022-aa9d-4486-aadd-ec50289d1d64 is Running (Ready = true)
May  4 12:39:05.886: INFO: Pod "pod-logs-websocket-3132f022-aa9d-4486-aadd-ec50289d1d64" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
May  4 12:39:05.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7338" for this suite. 05/04/23 12:39:05.918
{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","completed":180,"skipped":3551,"failed":0}
------------------------------
• [2.115 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:39:03.813
    May  4 12:39:03.813: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename pods 05/04/23 12:39:03.814
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:39:03.842
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:39:03.848
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:617
    May  4 12:39:03.852: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: creating the pod 05/04/23 12:39:03.852
    STEP: submitting the pod to kubernetes 05/04/23 12:39:03.853
    May  4 12:39:03.873: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-3132f022-aa9d-4486-aadd-ec50289d1d64" in namespace "pods-7338" to be "running and ready"
    May  4 12:39:03.879: INFO: Pod "pod-logs-websocket-3132f022-aa9d-4486-aadd-ec50289d1d64": Phase="Pending", Reason="", readiness=false. Elapsed: 6.285037ms
    May  4 12:39:03.879: INFO: The phase of Pod pod-logs-websocket-3132f022-aa9d-4486-aadd-ec50289d1d64 is Pending, waiting for it to be Running (with Ready = true)
    May  4 12:39:05.886: INFO: Pod "pod-logs-websocket-3132f022-aa9d-4486-aadd-ec50289d1d64": Phase="Running", Reason="", readiness=true. Elapsed: 2.012878347s
    May  4 12:39:05.886: INFO: The phase of Pod pod-logs-websocket-3132f022-aa9d-4486-aadd-ec50289d1d64 is Running (Ready = true)
    May  4 12:39:05.886: INFO: Pod "pod-logs-websocket-3132f022-aa9d-4486-aadd-ec50289d1d64" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    May  4 12:39:05.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-7338" for this suite. 05/04/23 12:39:05.918
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:39:05.928
May  4 12:39:05.928: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename configmap 05/04/23 12:39:05.929
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:39:05.956
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:39:05.959
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
STEP: Creating configMap that has name configmap-test-emptyKey-15813fc8-2527-4227-853e-19342fcec797 05/04/23 12:39:05.962
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
May  4 12:39:05.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4516" for this suite. 05/04/23 12:39:05.976
{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","completed":181,"skipped":3556,"failed":0}
------------------------------
• [0.057 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:39:05.928
    May  4 12:39:05.928: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename configmap 05/04/23 12:39:05.929
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:39:05.956
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:39:05.959
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:137
    STEP: Creating configMap that has name configmap-test-emptyKey-15813fc8-2527-4227-853e-19342fcec797 05/04/23 12:39:05.962
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    May  4 12:39:05.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4516" for this suite. 05/04/23 12:39:05.976
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:39:05.986
May  4 12:39:05.986: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename container-probe 05/04/23 12:39:05.987
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:39:06.007
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:39:06.01
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
May  4 12:40:06.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1240" for this suite. 05/04/23 12:40:06.042
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","completed":182,"skipped":3561,"failed":0}
------------------------------
• [SLOW TEST] [60.066 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:39:05.986
    May  4 12:39:05.986: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename container-probe 05/04/23 12:39:05.987
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:39:06.007
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:39:06.01
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:104
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    May  4 12:40:06.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-1240" for this suite. 05/04/23 12:40:06.042
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:40:06.054
May  4 12:40:06.055: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename webhook 05/04/23 12:40:06.056
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:40:06.077
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:40:06.081
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/04/23 12:40:06.109
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/04/23 12:40:06.604
STEP: Deploying the webhook pod 05/04/23 12:40:06.614
STEP: Wait for the deployment to be ready 05/04/23 12:40:06.632
May  4 12:40:06.644: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/04/23 12:40:08.661
STEP: Verifying the service has paired with the endpoint 05/04/23 12:40:08.676
May  4 12:40:09.677: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 05/04/23 12:40:09.681
STEP: create a pod that should be updated by the webhook 05/04/23 12:40:09.696
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  4 12:40:09.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2422" for this suite. 05/04/23 12:40:09.735
STEP: Destroying namespace "webhook-2422-markers" for this suite. 05/04/23 12:40:09.745
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","completed":183,"skipped":3587,"failed":0}
------------------------------
• [3.760 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:40:06.054
    May  4 12:40:06.055: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename webhook 05/04/23 12:40:06.056
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:40:06.077
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:40:06.081
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/04/23 12:40:06.109
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/04/23 12:40:06.604
    STEP: Deploying the webhook pod 05/04/23 12:40:06.614
    STEP: Wait for the deployment to be ready 05/04/23 12:40:06.632
    May  4 12:40:06.644: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/04/23 12:40:08.661
    STEP: Verifying the service has paired with the endpoint 05/04/23 12:40:08.676
    May  4 12:40:09.677: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:263
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 05/04/23 12:40:09.681
    STEP: create a pod that should be updated by the webhook 05/04/23 12:40:09.696
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  4 12:40:09.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2422" for this suite. 05/04/23 12:40:09.735
    STEP: Destroying namespace "webhook-2422-markers" for this suite. 05/04/23 12:40:09.745
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:40:09.815
May  4 12:40:09.815: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename proxy 05/04/23 12:40:09.817
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:40:09.862
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:40:09.869
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
May  4 12:40:09.875: INFO: Creating pod...
May  4 12:40:09.886: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-1108" to be "running"
May  4 12:40:09.894: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 8.040621ms
May  4 12:40:11.901: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.014962066s
May  4 12:40:11.901: INFO: Pod "agnhost" satisfied condition "running"
May  4 12:40:11.901: INFO: Creating service...
May  4 12:40:11.917: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-1108/pods/agnhost/proxy/some/path/with/DELETE
May  4 12:40:11.936: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
May  4 12:40:11.936: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-1108/pods/agnhost/proxy/some/path/with/GET
May  4 12:40:11.944: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
May  4 12:40:11.945: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-1108/pods/agnhost/proxy/some/path/with/HEAD
May  4 12:40:11.951: INFO: http.Client request:HEAD | StatusCode:200
May  4 12:40:11.951: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-1108/pods/agnhost/proxy/some/path/with/OPTIONS
May  4 12:40:11.956: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
May  4 12:40:11.956: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-1108/pods/agnhost/proxy/some/path/with/PATCH
May  4 12:40:11.961: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
May  4 12:40:11.961: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-1108/pods/agnhost/proxy/some/path/with/POST
May  4 12:40:11.967: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
May  4 12:40:11.967: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-1108/pods/agnhost/proxy/some/path/with/PUT
May  4 12:40:11.972: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
May  4 12:40:11.972: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-1108/services/test-service/proxy/some/path/with/DELETE
May  4 12:40:11.982: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
May  4 12:40:11.982: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-1108/services/test-service/proxy/some/path/with/GET
May  4 12:40:11.998: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
May  4 12:40:11.998: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-1108/services/test-service/proxy/some/path/with/HEAD
May  4 12:40:12.007: INFO: http.Client request:HEAD | StatusCode:200
May  4 12:40:12.007: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-1108/services/test-service/proxy/some/path/with/OPTIONS
May  4 12:40:12.016: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
May  4 12:40:12.016: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-1108/services/test-service/proxy/some/path/with/PATCH
May  4 12:40:12.023: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
May  4 12:40:12.023: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-1108/services/test-service/proxy/some/path/with/POST
May  4 12:40:12.037: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
May  4 12:40:12.037: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-1108/services/test-service/proxy/some/path/with/PUT
May  4 12:40:12.051: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:187
May  4 12:40:12.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1108" for this suite. 05/04/23 12:40:12.064
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","completed":184,"skipped":3587,"failed":0}
------------------------------
• [2.265 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:40:09.815
    May  4 12:40:09.815: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename proxy 05/04/23 12:40:09.817
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:40:09.862
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:40:09.869
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    May  4 12:40:09.875: INFO: Creating pod...
    May  4 12:40:09.886: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-1108" to be "running"
    May  4 12:40:09.894: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 8.040621ms
    May  4 12:40:11.901: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.014962066s
    May  4 12:40:11.901: INFO: Pod "agnhost" satisfied condition "running"
    May  4 12:40:11.901: INFO: Creating service...
    May  4 12:40:11.917: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-1108/pods/agnhost/proxy/some/path/with/DELETE
    May  4 12:40:11.936: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    May  4 12:40:11.936: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-1108/pods/agnhost/proxy/some/path/with/GET
    May  4 12:40:11.944: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    May  4 12:40:11.945: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-1108/pods/agnhost/proxy/some/path/with/HEAD
    May  4 12:40:11.951: INFO: http.Client request:HEAD | StatusCode:200
    May  4 12:40:11.951: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-1108/pods/agnhost/proxy/some/path/with/OPTIONS
    May  4 12:40:11.956: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    May  4 12:40:11.956: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-1108/pods/agnhost/proxy/some/path/with/PATCH
    May  4 12:40:11.961: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    May  4 12:40:11.961: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-1108/pods/agnhost/proxy/some/path/with/POST
    May  4 12:40:11.967: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    May  4 12:40:11.967: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-1108/pods/agnhost/proxy/some/path/with/PUT
    May  4 12:40:11.972: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    May  4 12:40:11.972: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-1108/services/test-service/proxy/some/path/with/DELETE
    May  4 12:40:11.982: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    May  4 12:40:11.982: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-1108/services/test-service/proxy/some/path/with/GET
    May  4 12:40:11.998: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    May  4 12:40:11.998: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-1108/services/test-service/proxy/some/path/with/HEAD
    May  4 12:40:12.007: INFO: http.Client request:HEAD | StatusCode:200
    May  4 12:40:12.007: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-1108/services/test-service/proxy/some/path/with/OPTIONS
    May  4 12:40:12.016: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    May  4 12:40:12.016: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-1108/services/test-service/proxy/some/path/with/PATCH
    May  4 12:40:12.023: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    May  4 12:40:12.023: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-1108/services/test-service/proxy/some/path/with/POST
    May  4 12:40:12.037: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    May  4 12:40:12.037: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-1108/services/test-service/proxy/some/path/with/PUT
    May  4 12:40:12.051: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    May  4 12:40:12.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-1108" for this suite. 05/04/23 12:40:12.064
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:40:12.081
May  4 12:40:12.081: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename downward-api 05/04/23 12:40:12.082
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:40:12.1
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:40:12.104
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
STEP: Creating a pod to test downward API volume plugin 05/04/23 12:40:12.108
May  4 12:40:12.132: INFO: Waiting up to 5m0s for pod "downwardapi-volume-205e4bde-02bc-489f-88b4-c68d8900153a" in namespace "downward-api-8145" to be "Succeeded or Failed"
May  4 12:40:12.139: INFO: Pod "downwardapi-volume-205e4bde-02bc-489f-88b4-c68d8900153a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.208681ms
May  4 12:40:14.146: INFO: Pod "downwardapi-volume-205e4bde-02bc-489f-88b4-c68d8900153a": Phase="Running", Reason="", readiness=false. Elapsed: 2.014590604s
May  4 12:40:16.145: INFO: Pod "downwardapi-volume-205e4bde-02bc-489f-88b4-c68d8900153a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012894449s
STEP: Saw pod success 05/04/23 12:40:16.145
May  4 12:40:16.145: INFO: Pod "downwardapi-volume-205e4bde-02bc-489f-88b4-c68d8900153a" satisfied condition "Succeeded or Failed"
May  4 12:40:16.150: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod downwardapi-volume-205e4bde-02bc-489f-88b4-c68d8900153a container client-container: <nil>
STEP: delete the pod 05/04/23 12:40:16.166
May  4 12:40:16.180: INFO: Waiting for pod downwardapi-volume-205e4bde-02bc-489f-88b4-c68d8900153a to disappear
May  4 12:40:16.184: INFO: Pod downwardapi-volume-205e4bde-02bc-489f-88b4-c68d8900153a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
May  4 12:40:16.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8145" for this suite. 05/04/23 12:40:16.193
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":185,"skipped":3593,"failed":0}
------------------------------
• [4.121 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:40:12.081
    May  4 12:40:12.081: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename downward-api 05/04/23 12:40:12.082
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:40:12.1
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:40:12.104
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:260
    STEP: Creating a pod to test downward API volume plugin 05/04/23 12:40:12.108
    May  4 12:40:12.132: INFO: Waiting up to 5m0s for pod "downwardapi-volume-205e4bde-02bc-489f-88b4-c68d8900153a" in namespace "downward-api-8145" to be "Succeeded or Failed"
    May  4 12:40:12.139: INFO: Pod "downwardapi-volume-205e4bde-02bc-489f-88b4-c68d8900153a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.208681ms
    May  4 12:40:14.146: INFO: Pod "downwardapi-volume-205e4bde-02bc-489f-88b4-c68d8900153a": Phase="Running", Reason="", readiness=false. Elapsed: 2.014590604s
    May  4 12:40:16.145: INFO: Pod "downwardapi-volume-205e4bde-02bc-489f-88b4-c68d8900153a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012894449s
    STEP: Saw pod success 05/04/23 12:40:16.145
    May  4 12:40:16.145: INFO: Pod "downwardapi-volume-205e4bde-02bc-489f-88b4-c68d8900153a" satisfied condition "Succeeded or Failed"
    May  4 12:40:16.150: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod downwardapi-volume-205e4bde-02bc-489f-88b4-c68d8900153a container client-container: <nil>
    STEP: delete the pod 05/04/23 12:40:16.166
    May  4 12:40:16.180: INFO: Waiting for pod downwardapi-volume-205e4bde-02bc-489f-88b4-c68d8900153a to disappear
    May  4 12:40:16.184: INFO: Pod downwardapi-volume-205e4bde-02bc-489f-88b4-c68d8900153a no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    May  4 12:40:16.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8145" for this suite. 05/04/23 12:40:16.193
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:40:16.204
May  4 12:40:16.205: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename sysctl 05/04/23 12:40:16.206
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:40:16.226
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:40:16.228
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 05/04/23 12:40:16.231
STEP: Watching for error events or started pod 05/04/23 12:40:16.242
STEP: Waiting for pod completion 05/04/23 12:40:18.247
May  4 12:40:18.247: INFO: Waiting up to 3m0s for pod "sysctl-0b028983-d552-4b9b-a1bb-3cb2d2439c03" in namespace "sysctl-9694" to be "completed"
May  4 12:40:18.252: INFO: Pod "sysctl-0b028983-d552-4b9b-a1bb-3cb2d2439c03": Phase="Running", Reason="", readiness=false. Elapsed: 4.193238ms
May  4 12:40:20.261: INFO: Pod "sysctl-0b028983-d552-4b9b-a1bb-3cb2d2439c03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013885721s
May  4 12:40:20.261: INFO: Pod "sysctl-0b028983-d552-4b9b-a1bb-3cb2d2439c03" satisfied condition "completed"
STEP: Checking that the pod succeeded 05/04/23 12:40:20.265
STEP: Getting logs from the pod 05/04/23 12:40:20.266
STEP: Checking that the sysctl is actually updated 05/04/23 12:40:20.273
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
May  4 12:40:20.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-9694" for this suite. 05/04/23 12:40:20.283
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":186,"skipped":3624,"failed":0}
------------------------------
• [4.091 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:40:16.204
    May  4 12:40:16.205: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename sysctl 05/04/23 12:40:16.206
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:40:16.226
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:40:16.228
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 05/04/23 12:40:16.231
    STEP: Watching for error events or started pod 05/04/23 12:40:16.242
    STEP: Waiting for pod completion 05/04/23 12:40:18.247
    May  4 12:40:18.247: INFO: Waiting up to 3m0s for pod "sysctl-0b028983-d552-4b9b-a1bb-3cb2d2439c03" in namespace "sysctl-9694" to be "completed"
    May  4 12:40:18.252: INFO: Pod "sysctl-0b028983-d552-4b9b-a1bb-3cb2d2439c03": Phase="Running", Reason="", readiness=false. Elapsed: 4.193238ms
    May  4 12:40:20.261: INFO: Pod "sysctl-0b028983-d552-4b9b-a1bb-3cb2d2439c03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013885721s
    May  4 12:40:20.261: INFO: Pod "sysctl-0b028983-d552-4b9b-a1bb-3cb2d2439c03" satisfied condition "completed"
    STEP: Checking that the pod succeeded 05/04/23 12:40:20.265
    STEP: Getting logs from the pod 05/04/23 12:40:20.266
    STEP: Checking that the sysctl is actually updated 05/04/23 12:40:20.273
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    May  4 12:40:20.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-9694" for this suite. 05/04/23 12:40:20.283
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:40:20.296
May  4 12:40:20.296: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename dns 05/04/23 12:40:20.297
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:40:20.317
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:40:20.323
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 05/04/23 12:40:20.328
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7374.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7374.svc.cluster.local; sleep 1; done
 05/04/23 12:40:20.334
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7374.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7374.svc.cluster.local; sleep 1; done
 05/04/23 12:40:20.334
STEP: creating a pod to probe DNS 05/04/23 12:40:20.334
STEP: submitting the pod to kubernetes 05/04/23 12:40:20.334
May  4 12:40:20.350: INFO: Waiting up to 15m0s for pod "dns-test-2e1b17a8-77d0-4c5a-86cd-db4d232c92a2" in namespace "dns-7374" to be "running"
May  4 12:40:20.358: INFO: Pod "dns-test-2e1b17a8-77d0-4c5a-86cd-db4d232c92a2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.931493ms
May  4 12:40:22.367: INFO: Pod "dns-test-2e1b17a8-77d0-4c5a-86cd-db4d232c92a2": Phase="Running", Reason="", readiness=true. Elapsed: 2.017259043s
May  4 12:40:22.367: INFO: Pod "dns-test-2e1b17a8-77d0-4c5a-86cd-db4d232c92a2" satisfied condition "running"
STEP: retrieving the pod 05/04/23 12:40:22.367
STEP: looking for the results for each expected name from probers 05/04/23 12:40:22.372
May  4 12:40:22.383: INFO: DNS probes using dns-test-2e1b17a8-77d0-4c5a-86cd-db4d232c92a2 succeeded

STEP: deleting the pod 05/04/23 12:40:22.384
STEP: changing the externalName to bar.example.com 05/04/23 12:40:22.407
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7374.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7374.svc.cluster.local; sleep 1; done
 05/04/23 12:40:22.416
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7374.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7374.svc.cluster.local; sleep 1; done
 05/04/23 12:40:22.416
STEP: creating a second pod to probe DNS 05/04/23 12:40:22.416
STEP: submitting the pod to kubernetes 05/04/23 12:40:22.416
May  4 12:40:22.428: INFO: Waiting up to 15m0s for pod "dns-test-4ca10299-8266-4133-b5ac-b0207ecff422" in namespace "dns-7374" to be "running"
May  4 12:40:22.438: INFO: Pod "dns-test-4ca10299-8266-4133-b5ac-b0207ecff422": Phase="Pending", Reason="", readiness=false. Elapsed: 10.274332ms
May  4 12:40:24.445: INFO: Pod "dns-test-4ca10299-8266-4133-b5ac-b0207ecff422": Phase="Running", Reason="", readiness=true. Elapsed: 2.017717487s
May  4 12:40:24.446: INFO: Pod "dns-test-4ca10299-8266-4133-b5ac-b0207ecff422" satisfied condition "running"
STEP: retrieving the pod 05/04/23 12:40:24.446
STEP: looking for the results for each expected name from probers 05/04/23 12:40:24.449
May  4 12:40:24.454: INFO: File wheezy_udp@dns-test-service-3.dns-7374.svc.cluster.local from pod  dns-7374/dns-test-4ca10299-8266-4133-b5ac-b0207ecff422 contains 'foo.example.com.
' instead of 'bar.example.com.'
May  4 12:40:24.459: INFO: File jessie_udp@dns-test-service-3.dns-7374.svc.cluster.local from pod  dns-7374/dns-test-4ca10299-8266-4133-b5ac-b0207ecff422 contains 'foo.example.com.
' instead of 'bar.example.com.'
May  4 12:40:24.459: INFO: Lookups using dns-7374/dns-test-4ca10299-8266-4133-b5ac-b0207ecff422 failed for: [wheezy_udp@dns-test-service-3.dns-7374.svc.cluster.local jessie_udp@dns-test-service-3.dns-7374.svc.cluster.local]

May  4 12:40:29.466: INFO: File wheezy_udp@dns-test-service-3.dns-7374.svc.cluster.local from pod  dns-7374/dns-test-4ca10299-8266-4133-b5ac-b0207ecff422 contains 'foo.example.com.
' instead of 'bar.example.com.'
May  4 12:40:29.473: INFO: File jessie_udp@dns-test-service-3.dns-7374.svc.cluster.local from pod  dns-7374/dns-test-4ca10299-8266-4133-b5ac-b0207ecff422 contains 'foo.example.com.
' instead of 'bar.example.com.'
May  4 12:40:29.473: INFO: Lookups using dns-7374/dns-test-4ca10299-8266-4133-b5ac-b0207ecff422 failed for: [wheezy_udp@dns-test-service-3.dns-7374.svc.cluster.local jessie_udp@dns-test-service-3.dns-7374.svc.cluster.local]

May  4 12:40:34.467: INFO: File wheezy_udp@dns-test-service-3.dns-7374.svc.cluster.local from pod  dns-7374/dns-test-4ca10299-8266-4133-b5ac-b0207ecff422 contains 'foo.example.com.
' instead of 'bar.example.com.'
May  4 12:40:34.473: INFO: File jessie_udp@dns-test-service-3.dns-7374.svc.cluster.local from pod  dns-7374/dns-test-4ca10299-8266-4133-b5ac-b0207ecff422 contains 'foo.example.com.
' instead of 'bar.example.com.'
May  4 12:40:34.473: INFO: Lookups using dns-7374/dns-test-4ca10299-8266-4133-b5ac-b0207ecff422 failed for: [wheezy_udp@dns-test-service-3.dns-7374.svc.cluster.local jessie_udp@dns-test-service-3.dns-7374.svc.cluster.local]

May  4 12:40:39.466: INFO: File wheezy_udp@dns-test-service-3.dns-7374.svc.cluster.local from pod  dns-7374/dns-test-4ca10299-8266-4133-b5ac-b0207ecff422 contains 'foo.example.com.
' instead of 'bar.example.com.'
May  4 12:40:39.472: INFO: File jessie_udp@dns-test-service-3.dns-7374.svc.cluster.local from pod  dns-7374/dns-test-4ca10299-8266-4133-b5ac-b0207ecff422 contains 'foo.example.com.
' instead of 'bar.example.com.'
May  4 12:40:39.472: INFO: Lookups using dns-7374/dns-test-4ca10299-8266-4133-b5ac-b0207ecff422 failed for: [wheezy_udp@dns-test-service-3.dns-7374.svc.cluster.local jessie_udp@dns-test-service-3.dns-7374.svc.cluster.local]

May  4 12:40:44.465: INFO: File wheezy_udp@dns-test-service-3.dns-7374.svc.cluster.local from pod  dns-7374/dns-test-4ca10299-8266-4133-b5ac-b0207ecff422 contains 'foo.example.com.
' instead of 'bar.example.com.'
May  4 12:40:44.471: INFO: File jessie_udp@dns-test-service-3.dns-7374.svc.cluster.local from pod  dns-7374/dns-test-4ca10299-8266-4133-b5ac-b0207ecff422 contains 'foo.example.com.
' instead of 'bar.example.com.'
May  4 12:40:44.471: INFO: Lookups using dns-7374/dns-test-4ca10299-8266-4133-b5ac-b0207ecff422 failed for: [wheezy_udp@dns-test-service-3.dns-7374.svc.cluster.local jessie_udp@dns-test-service-3.dns-7374.svc.cluster.local]

May  4 12:40:49.466: INFO: File wheezy_udp@dns-test-service-3.dns-7374.svc.cluster.local from pod  dns-7374/dns-test-4ca10299-8266-4133-b5ac-b0207ecff422 contains 'foo.example.com.
' instead of 'bar.example.com.'
May  4 12:40:49.474: INFO: File jessie_udp@dns-test-service-3.dns-7374.svc.cluster.local from pod  dns-7374/dns-test-4ca10299-8266-4133-b5ac-b0207ecff422 contains 'foo.example.com.
' instead of 'bar.example.com.'
May  4 12:40:49.474: INFO: Lookups using dns-7374/dns-test-4ca10299-8266-4133-b5ac-b0207ecff422 failed for: [wheezy_udp@dns-test-service-3.dns-7374.svc.cluster.local jessie_udp@dns-test-service-3.dns-7374.svc.cluster.local]

May  4 12:40:54.471: INFO: DNS probes using dns-test-4ca10299-8266-4133-b5ac-b0207ecff422 succeeded

STEP: deleting the pod 05/04/23 12:40:54.471
STEP: changing the service to type=ClusterIP 05/04/23 12:40:54.489
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7374.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-7374.svc.cluster.local; sleep 1; done
 05/04/23 12:40:54.51
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7374.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-7374.svc.cluster.local; sleep 1; done
 05/04/23 12:40:54.51
STEP: creating a third pod to probe DNS 05/04/23 12:40:54.51
STEP: submitting the pod to kubernetes 05/04/23 12:40:54.515
May  4 12:40:54.532: INFO: Waiting up to 15m0s for pod "dns-test-99023c7d-8648-4305-b7e3-c8095fea97d5" in namespace "dns-7374" to be "running"
May  4 12:40:54.539: INFO: Pod "dns-test-99023c7d-8648-4305-b7e3-c8095fea97d5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.359876ms
May  4 12:40:56.544: INFO: Pod "dns-test-99023c7d-8648-4305-b7e3-c8095fea97d5": Phase="Running", Reason="", readiness=true. Elapsed: 2.011776606s
May  4 12:40:56.544: INFO: Pod "dns-test-99023c7d-8648-4305-b7e3-c8095fea97d5" satisfied condition "running"
STEP: retrieving the pod 05/04/23 12:40:56.544
STEP: looking for the results for each expected name from probers 05/04/23 12:40:56.548
May  4 12:40:56.553: INFO: File wheezy_udp@dns-test-service-3.dns-7374.svc.cluster.local from pod  dns-7374/dns-test-99023c7d-8648-4305-b7e3-c8095fea97d5 contains '' instead of '10.21.249.120'
May  4 12:40:56.557: INFO: Lookups using dns-7374/dns-test-99023c7d-8648-4305-b7e3-c8095fea97d5 failed for: [wheezy_udp@dns-test-service-3.dns-7374.svc.cluster.local]

May  4 12:41:01.571: INFO: DNS probes using dns-test-99023c7d-8648-4305-b7e3-c8095fea97d5 succeeded

STEP: deleting the pod 05/04/23 12:41:01.571
STEP: deleting the test externalName service 05/04/23 12:41:01.589
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
May  4 12:41:01.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7374" for this suite. 05/04/23 12:41:01.63
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","completed":187,"skipped":3627,"failed":0}
------------------------------
• [SLOW TEST] [41.356 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:40:20.296
    May  4 12:40:20.296: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename dns 05/04/23 12:40:20.297
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:40:20.317
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:40:20.323
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 05/04/23 12:40:20.328
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7374.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7374.svc.cluster.local; sleep 1; done
     05/04/23 12:40:20.334
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7374.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7374.svc.cluster.local; sleep 1; done
     05/04/23 12:40:20.334
    STEP: creating a pod to probe DNS 05/04/23 12:40:20.334
    STEP: submitting the pod to kubernetes 05/04/23 12:40:20.334
    May  4 12:40:20.350: INFO: Waiting up to 15m0s for pod "dns-test-2e1b17a8-77d0-4c5a-86cd-db4d232c92a2" in namespace "dns-7374" to be "running"
    May  4 12:40:20.358: INFO: Pod "dns-test-2e1b17a8-77d0-4c5a-86cd-db4d232c92a2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.931493ms
    May  4 12:40:22.367: INFO: Pod "dns-test-2e1b17a8-77d0-4c5a-86cd-db4d232c92a2": Phase="Running", Reason="", readiness=true. Elapsed: 2.017259043s
    May  4 12:40:22.367: INFO: Pod "dns-test-2e1b17a8-77d0-4c5a-86cd-db4d232c92a2" satisfied condition "running"
    STEP: retrieving the pod 05/04/23 12:40:22.367
    STEP: looking for the results for each expected name from probers 05/04/23 12:40:22.372
    May  4 12:40:22.383: INFO: DNS probes using dns-test-2e1b17a8-77d0-4c5a-86cd-db4d232c92a2 succeeded

    STEP: deleting the pod 05/04/23 12:40:22.384
    STEP: changing the externalName to bar.example.com 05/04/23 12:40:22.407
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7374.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7374.svc.cluster.local; sleep 1; done
     05/04/23 12:40:22.416
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7374.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7374.svc.cluster.local; sleep 1; done
     05/04/23 12:40:22.416
    STEP: creating a second pod to probe DNS 05/04/23 12:40:22.416
    STEP: submitting the pod to kubernetes 05/04/23 12:40:22.416
    May  4 12:40:22.428: INFO: Waiting up to 15m0s for pod "dns-test-4ca10299-8266-4133-b5ac-b0207ecff422" in namespace "dns-7374" to be "running"
    May  4 12:40:22.438: INFO: Pod "dns-test-4ca10299-8266-4133-b5ac-b0207ecff422": Phase="Pending", Reason="", readiness=false. Elapsed: 10.274332ms
    May  4 12:40:24.445: INFO: Pod "dns-test-4ca10299-8266-4133-b5ac-b0207ecff422": Phase="Running", Reason="", readiness=true. Elapsed: 2.017717487s
    May  4 12:40:24.446: INFO: Pod "dns-test-4ca10299-8266-4133-b5ac-b0207ecff422" satisfied condition "running"
    STEP: retrieving the pod 05/04/23 12:40:24.446
    STEP: looking for the results for each expected name from probers 05/04/23 12:40:24.449
    May  4 12:40:24.454: INFO: File wheezy_udp@dns-test-service-3.dns-7374.svc.cluster.local from pod  dns-7374/dns-test-4ca10299-8266-4133-b5ac-b0207ecff422 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May  4 12:40:24.459: INFO: File jessie_udp@dns-test-service-3.dns-7374.svc.cluster.local from pod  dns-7374/dns-test-4ca10299-8266-4133-b5ac-b0207ecff422 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May  4 12:40:24.459: INFO: Lookups using dns-7374/dns-test-4ca10299-8266-4133-b5ac-b0207ecff422 failed for: [wheezy_udp@dns-test-service-3.dns-7374.svc.cluster.local jessie_udp@dns-test-service-3.dns-7374.svc.cluster.local]

    May  4 12:40:29.466: INFO: File wheezy_udp@dns-test-service-3.dns-7374.svc.cluster.local from pod  dns-7374/dns-test-4ca10299-8266-4133-b5ac-b0207ecff422 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May  4 12:40:29.473: INFO: File jessie_udp@dns-test-service-3.dns-7374.svc.cluster.local from pod  dns-7374/dns-test-4ca10299-8266-4133-b5ac-b0207ecff422 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May  4 12:40:29.473: INFO: Lookups using dns-7374/dns-test-4ca10299-8266-4133-b5ac-b0207ecff422 failed for: [wheezy_udp@dns-test-service-3.dns-7374.svc.cluster.local jessie_udp@dns-test-service-3.dns-7374.svc.cluster.local]

    May  4 12:40:34.467: INFO: File wheezy_udp@dns-test-service-3.dns-7374.svc.cluster.local from pod  dns-7374/dns-test-4ca10299-8266-4133-b5ac-b0207ecff422 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May  4 12:40:34.473: INFO: File jessie_udp@dns-test-service-3.dns-7374.svc.cluster.local from pod  dns-7374/dns-test-4ca10299-8266-4133-b5ac-b0207ecff422 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May  4 12:40:34.473: INFO: Lookups using dns-7374/dns-test-4ca10299-8266-4133-b5ac-b0207ecff422 failed for: [wheezy_udp@dns-test-service-3.dns-7374.svc.cluster.local jessie_udp@dns-test-service-3.dns-7374.svc.cluster.local]

    May  4 12:40:39.466: INFO: File wheezy_udp@dns-test-service-3.dns-7374.svc.cluster.local from pod  dns-7374/dns-test-4ca10299-8266-4133-b5ac-b0207ecff422 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May  4 12:40:39.472: INFO: File jessie_udp@dns-test-service-3.dns-7374.svc.cluster.local from pod  dns-7374/dns-test-4ca10299-8266-4133-b5ac-b0207ecff422 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May  4 12:40:39.472: INFO: Lookups using dns-7374/dns-test-4ca10299-8266-4133-b5ac-b0207ecff422 failed for: [wheezy_udp@dns-test-service-3.dns-7374.svc.cluster.local jessie_udp@dns-test-service-3.dns-7374.svc.cluster.local]

    May  4 12:40:44.465: INFO: File wheezy_udp@dns-test-service-3.dns-7374.svc.cluster.local from pod  dns-7374/dns-test-4ca10299-8266-4133-b5ac-b0207ecff422 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May  4 12:40:44.471: INFO: File jessie_udp@dns-test-service-3.dns-7374.svc.cluster.local from pod  dns-7374/dns-test-4ca10299-8266-4133-b5ac-b0207ecff422 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May  4 12:40:44.471: INFO: Lookups using dns-7374/dns-test-4ca10299-8266-4133-b5ac-b0207ecff422 failed for: [wheezy_udp@dns-test-service-3.dns-7374.svc.cluster.local jessie_udp@dns-test-service-3.dns-7374.svc.cluster.local]

    May  4 12:40:49.466: INFO: File wheezy_udp@dns-test-service-3.dns-7374.svc.cluster.local from pod  dns-7374/dns-test-4ca10299-8266-4133-b5ac-b0207ecff422 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May  4 12:40:49.474: INFO: File jessie_udp@dns-test-service-3.dns-7374.svc.cluster.local from pod  dns-7374/dns-test-4ca10299-8266-4133-b5ac-b0207ecff422 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May  4 12:40:49.474: INFO: Lookups using dns-7374/dns-test-4ca10299-8266-4133-b5ac-b0207ecff422 failed for: [wheezy_udp@dns-test-service-3.dns-7374.svc.cluster.local jessie_udp@dns-test-service-3.dns-7374.svc.cluster.local]

    May  4 12:40:54.471: INFO: DNS probes using dns-test-4ca10299-8266-4133-b5ac-b0207ecff422 succeeded

    STEP: deleting the pod 05/04/23 12:40:54.471
    STEP: changing the service to type=ClusterIP 05/04/23 12:40:54.489
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7374.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-7374.svc.cluster.local; sleep 1; done
     05/04/23 12:40:54.51
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7374.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-7374.svc.cluster.local; sleep 1; done
     05/04/23 12:40:54.51
    STEP: creating a third pod to probe DNS 05/04/23 12:40:54.51
    STEP: submitting the pod to kubernetes 05/04/23 12:40:54.515
    May  4 12:40:54.532: INFO: Waiting up to 15m0s for pod "dns-test-99023c7d-8648-4305-b7e3-c8095fea97d5" in namespace "dns-7374" to be "running"
    May  4 12:40:54.539: INFO: Pod "dns-test-99023c7d-8648-4305-b7e3-c8095fea97d5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.359876ms
    May  4 12:40:56.544: INFO: Pod "dns-test-99023c7d-8648-4305-b7e3-c8095fea97d5": Phase="Running", Reason="", readiness=true. Elapsed: 2.011776606s
    May  4 12:40:56.544: INFO: Pod "dns-test-99023c7d-8648-4305-b7e3-c8095fea97d5" satisfied condition "running"
    STEP: retrieving the pod 05/04/23 12:40:56.544
    STEP: looking for the results for each expected name from probers 05/04/23 12:40:56.548
    May  4 12:40:56.553: INFO: File wheezy_udp@dns-test-service-3.dns-7374.svc.cluster.local from pod  dns-7374/dns-test-99023c7d-8648-4305-b7e3-c8095fea97d5 contains '' instead of '10.21.249.120'
    May  4 12:40:56.557: INFO: Lookups using dns-7374/dns-test-99023c7d-8648-4305-b7e3-c8095fea97d5 failed for: [wheezy_udp@dns-test-service-3.dns-7374.svc.cluster.local]

    May  4 12:41:01.571: INFO: DNS probes using dns-test-99023c7d-8648-4305-b7e3-c8095fea97d5 succeeded

    STEP: deleting the pod 05/04/23 12:41:01.571
    STEP: deleting the test externalName service 05/04/23 12:41:01.589
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    May  4 12:41:01.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-7374" for this suite. 05/04/23 12:41:01.63
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:41:01.654
May  4 12:41:01.654: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename projected 05/04/23 12:41:01.656
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:41:01.677
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:41:01.681
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
STEP: Creating configMap with name projected-configmap-test-volume-map-932b9998-8aac-4a2b-bcd2-eadb27c2b703 05/04/23 12:41:01.685
STEP: Creating a pod to test consume configMaps 05/04/23 12:41:01.692
May  4 12:41:01.706: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3610239f-eb95-4697-838a-abe2b54dd828" in namespace "projected-2220" to be "Succeeded or Failed"
May  4 12:41:01.714: INFO: Pod "pod-projected-configmaps-3610239f-eb95-4697-838a-abe2b54dd828": Phase="Pending", Reason="", readiness=false. Elapsed: 7.970228ms
May  4 12:41:03.719: INFO: Pod "pod-projected-configmaps-3610239f-eb95-4697-838a-abe2b54dd828": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013803329s
May  4 12:41:05.721: INFO: Pod "pod-projected-configmaps-3610239f-eb95-4697-838a-abe2b54dd828": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015338244s
STEP: Saw pod success 05/04/23 12:41:05.721
May  4 12:41:05.721: INFO: Pod "pod-projected-configmaps-3610239f-eb95-4697-838a-abe2b54dd828" satisfied condition "Succeeded or Failed"
May  4 12:41:05.725: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-projected-configmaps-3610239f-eb95-4697-838a-abe2b54dd828 container agnhost-container: <nil>
STEP: delete the pod 05/04/23 12:41:05.733
May  4 12:41:05.752: INFO: Waiting for pod pod-projected-configmaps-3610239f-eb95-4697-838a-abe2b54dd828 to disappear
May  4 12:41:05.756: INFO: Pod pod-projected-configmaps-3610239f-eb95-4697-838a-abe2b54dd828 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
May  4 12:41:05.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2220" for this suite. 05/04/23 12:41:05.764
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":188,"skipped":3634,"failed":0}
------------------------------
• [4.119 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:41:01.654
    May  4 12:41:01.654: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename projected 05/04/23 12:41:01.656
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:41:01.677
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:41:01.681
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:98
    STEP: Creating configMap with name projected-configmap-test-volume-map-932b9998-8aac-4a2b-bcd2-eadb27c2b703 05/04/23 12:41:01.685
    STEP: Creating a pod to test consume configMaps 05/04/23 12:41:01.692
    May  4 12:41:01.706: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3610239f-eb95-4697-838a-abe2b54dd828" in namespace "projected-2220" to be "Succeeded or Failed"
    May  4 12:41:01.714: INFO: Pod "pod-projected-configmaps-3610239f-eb95-4697-838a-abe2b54dd828": Phase="Pending", Reason="", readiness=false. Elapsed: 7.970228ms
    May  4 12:41:03.719: INFO: Pod "pod-projected-configmaps-3610239f-eb95-4697-838a-abe2b54dd828": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013803329s
    May  4 12:41:05.721: INFO: Pod "pod-projected-configmaps-3610239f-eb95-4697-838a-abe2b54dd828": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015338244s
    STEP: Saw pod success 05/04/23 12:41:05.721
    May  4 12:41:05.721: INFO: Pod "pod-projected-configmaps-3610239f-eb95-4697-838a-abe2b54dd828" satisfied condition "Succeeded or Failed"
    May  4 12:41:05.725: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-projected-configmaps-3610239f-eb95-4697-838a-abe2b54dd828 container agnhost-container: <nil>
    STEP: delete the pod 05/04/23 12:41:05.733
    May  4 12:41:05.752: INFO: Waiting for pod pod-projected-configmaps-3610239f-eb95-4697-838a-abe2b54dd828 to disappear
    May  4 12:41:05.756: INFO: Pod pod-projected-configmaps-3610239f-eb95-4697-838a-abe2b54dd828 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    May  4 12:41:05.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2220" for this suite. 05/04/23 12:41:05.764
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:41:05.774
May  4 12:41:05.774: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename downward-api 05/04/23 12:41:05.775
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:41:05.793
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:41:05.798
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
STEP: Creating a pod to test downward API volume plugin 05/04/23 12:41:05.801
May  4 12:41:05.813: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c6d3fb48-0d21-4eab-996a-1adddb76b059" in namespace "downward-api-8867" to be "Succeeded or Failed"
May  4 12:41:05.818: INFO: Pod "downwardapi-volume-c6d3fb48-0d21-4eab-996a-1adddb76b059": Phase="Pending", Reason="", readiness=false. Elapsed: 4.544089ms
May  4 12:41:07.823: INFO: Pod "downwardapi-volume-c6d3fb48-0d21-4eab-996a-1adddb76b059": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010202698s
May  4 12:41:09.824: INFO: Pod "downwardapi-volume-c6d3fb48-0d21-4eab-996a-1adddb76b059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010855583s
STEP: Saw pod success 05/04/23 12:41:09.824
May  4 12:41:09.824: INFO: Pod "downwardapi-volume-c6d3fb48-0d21-4eab-996a-1adddb76b059" satisfied condition "Succeeded or Failed"
May  4 12:41:09.829: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod downwardapi-volume-c6d3fb48-0d21-4eab-996a-1adddb76b059 container client-container: <nil>
STEP: delete the pod 05/04/23 12:41:09.845
May  4 12:41:09.871: INFO: Waiting for pod downwardapi-volume-c6d3fb48-0d21-4eab-996a-1adddb76b059 to disappear
May  4 12:41:09.875: INFO: Pod downwardapi-volume-c6d3fb48-0d21-4eab-996a-1adddb76b059 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
May  4 12:41:09.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8867" for this suite. 05/04/23 12:41:09.885
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","completed":189,"skipped":3646,"failed":0}
------------------------------
• [4.122 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:41:05.774
    May  4 12:41:05.774: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename downward-api 05/04/23 12:41:05.775
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:41:05.793
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:41:05.798
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:206
    STEP: Creating a pod to test downward API volume plugin 05/04/23 12:41:05.801
    May  4 12:41:05.813: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c6d3fb48-0d21-4eab-996a-1adddb76b059" in namespace "downward-api-8867" to be "Succeeded or Failed"
    May  4 12:41:05.818: INFO: Pod "downwardapi-volume-c6d3fb48-0d21-4eab-996a-1adddb76b059": Phase="Pending", Reason="", readiness=false. Elapsed: 4.544089ms
    May  4 12:41:07.823: INFO: Pod "downwardapi-volume-c6d3fb48-0d21-4eab-996a-1adddb76b059": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010202698s
    May  4 12:41:09.824: INFO: Pod "downwardapi-volume-c6d3fb48-0d21-4eab-996a-1adddb76b059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010855583s
    STEP: Saw pod success 05/04/23 12:41:09.824
    May  4 12:41:09.824: INFO: Pod "downwardapi-volume-c6d3fb48-0d21-4eab-996a-1adddb76b059" satisfied condition "Succeeded or Failed"
    May  4 12:41:09.829: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod downwardapi-volume-c6d3fb48-0d21-4eab-996a-1adddb76b059 container client-container: <nil>
    STEP: delete the pod 05/04/23 12:41:09.845
    May  4 12:41:09.871: INFO: Waiting for pod downwardapi-volume-c6d3fb48-0d21-4eab-996a-1adddb76b059 to disappear
    May  4 12:41:09.875: INFO: Pod downwardapi-volume-c6d3fb48-0d21-4eab-996a-1adddb76b059 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    May  4 12:41:09.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8867" for this suite. 05/04/23 12:41:09.885
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:41:09.897
May  4 12:41:09.897: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename emptydir-wrapper 05/04/23 12:41:09.897
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:41:09.929
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:41:09.932
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
May  4 12:41:09.969: INFO: Waiting up to 5m0s for pod "pod-secrets-8f686fd2-9dc0-4aa6-a7b2-7ff6ac563b17" in namespace "emptydir-wrapper-5096" to be "running and ready"
May  4 12:41:09.974: INFO: Pod "pod-secrets-8f686fd2-9dc0-4aa6-a7b2-7ff6ac563b17": Phase="Pending", Reason="", readiness=false. Elapsed: 5.641868ms
May  4 12:41:09.975: INFO: The phase of Pod pod-secrets-8f686fd2-9dc0-4aa6-a7b2-7ff6ac563b17 is Pending, waiting for it to be Running (with Ready = true)
May  4 12:41:11.979: INFO: Pod "pod-secrets-8f686fd2-9dc0-4aa6-a7b2-7ff6ac563b17": Phase="Running", Reason="", readiness=true. Elapsed: 2.010539158s
May  4 12:41:11.979: INFO: The phase of Pod pod-secrets-8f686fd2-9dc0-4aa6-a7b2-7ff6ac563b17 is Running (Ready = true)
May  4 12:41:11.979: INFO: Pod "pod-secrets-8f686fd2-9dc0-4aa6-a7b2-7ff6ac563b17" satisfied condition "running and ready"
STEP: Cleaning up the secret 05/04/23 12:41:11.985
STEP: Cleaning up the configmap 05/04/23 12:41:11.993
STEP: Cleaning up the pod 05/04/23 12:41:12.001
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
May  4 12:41:12.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5096" for this suite. 05/04/23 12:41:12.036
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","completed":190,"skipped":3649,"failed":0}
------------------------------
• [2.147 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:41:09.897
    May  4 12:41:09.897: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename emptydir-wrapper 05/04/23 12:41:09.897
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:41:09.929
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:41:09.932
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    May  4 12:41:09.969: INFO: Waiting up to 5m0s for pod "pod-secrets-8f686fd2-9dc0-4aa6-a7b2-7ff6ac563b17" in namespace "emptydir-wrapper-5096" to be "running and ready"
    May  4 12:41:09.974: INFO: Pod "pod-secrets-8f686fd2-9dc0-4aa6-a7b2-7ff6ac563b17": Phase="Pending", Reason="", readiness=false. Elapsed: 5.641868ms
    May  4 12:41:09.975: INFO: The phase of Pod pod-secrets-8f686fd2-9dc0-4aa6-a7b2-7ff6ac563b17 is Pending, waiting for it to be Running (with Ready = true)
    May  4 12:41:11.979: INFO: Pod "pod-secrets-8f686fd2-9dc0-4aa6-a7b2-7ff6ac563b17": Phase="Running", Reason="", readiness=true. Elapsed: 2.010539158s
    May  4 12:41:11.979: INFO: The phase of Pod pod-secrets-8f686fd2-9dc0-4aa6-a7b2-7ff6ac563b17 is Running (Ready = true)
    May  4 12:41:11.979: INFO: Pod "pod-secrets-8f686fd2-9dc0-4aa6-a7b2-7ff6ac563b17" satisfied condition "running and ready"
    STEP: Cleaning up the secret 05/04/23 12:41:11.985
    STEP: Cleaning up the configmap 05/04/23 12:41:11.993
    STEP: Cleaning up the pod 05/04/23 12:41:12.001
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    May  4 12:41:12.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-5096" for this suite. 05/04/23 12:41:12.036
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:41:12.044
May  4 12:41:12.044: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename configmap 05/04/23 12:41:12.045
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:41:12.073
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:41:12.077
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
STEP: Creating configMap with name cm-test-opt-del-d1f2d941-abff-4203-b48b-098a948da625 05/04/23 12:41:12.089
STEP: Creating configMap with name cm-test-opt-upd-ffe85db3-c66c-4cf6-b60a-2fa636f4f3fe 05/04/23 12:41:12.094
STEP: Creating the pod 05/04/23 12:41:12.102
May  4 12:41:12.116: INFO: Waiting up to 5m0s for pod "pod-configmaps-ef2a28f6-c1b2-4440-821d-25df1a5a4dbe" in namespace "configmap-2655" to be "running and ready"
May  4 12:41:12.124: INFO: Pod "pod-configmaps-ef2a28f6-c1b2-4440-821d-25df1a5a4dbe": Phase="Pending", Reason="", readiness=false. Elapsed: 7.46517ms
May  4 12:41:12.124: INFO: The phase of Pod pod-configmaps-ef2a28f6-c1b2-4440-821d-25df1a5a4dbe is Pending, waiting for it to be Running (with Ready = true)
May  4 12:41:14.129: INFO: Pod "pod-configmaps-ef2a28f6-c1b2-4440-821d-25df1a5a4dbe": Phase="Running", Reason="", readiness=true. Elapsed: 2.012933754s
May  4 12:41:14.129: INFO: The phase of Pod pod-configmaps-ef2a28f6-c1b2-4440-821d-25df1a5a4dbe is Running (Ready = true)
May  4 12:41:14.129: INFO: Pod "pod-configmaps-ef2a28f6-c1b2-4440-821d-25df1a5a4dbe" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-d1f2d941-abff-4203-b48b-098a948da625 05/04/23 12:41:14.164
STEP: Updating configmap cm-test-opt-upd-ffe85db3-c66c-4cf6-b60a-2fa636f4f3fe 05/04/23 12:41:14.173
STEP: Creating configMap with name cm-test-opt-create-827ea57e-eb24-44a1-9bdb-4b35225ffabf 05/04/23 12:41:14.188
STEP: waiting to observe update in volume 05/04/23 12:41:14.193
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
May  4 12:41:16.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2655" for this suite. 05/04/23 12:41:16.24
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":191,"skipped":3659,"failed":0}
------------------------------
• [4.207 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:41:12.044
    May  4 12:41:12.044: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename configmap 05/04/23 12:41:12.045
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:41:12.073
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:41:12.077
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:239
    STEP: Creating configMap with name cm-test-opt-del-d1f2d941-abff-4203-b48b-098a948da625 05/04/23 12:41:12.089
    STEP: Creating configMap with name cm-test-opt-upd-ffe85db3-c66c-4cf6-b60a-2fa636f4f3fe 05/04/23 12:41:12.094
    STEP: Creating the pod 05/04/23 12:41:12.102
    May  4 12:41:12.116: INFO: Waiting up to 5m0s for pod "pod-configmaps-ef2a28f6-c1b2-4440-821d-25df1a5a4dbe" in namespace "configmap-2655" to be "running and ready"
    May  4 12:41:12.124: INFO: Pod "pod-configmaps-ef2a28f6-c1b2-4440-821d-25df1a5a4dbe": Phase="Pending", Reason="", readiness=false. Elapsed: 7.46517ms
    May  4 12:41:12.124: INFO: The phase of Pod pod-configmaps-ef2a28f6-c1b2-4440-821d-25df1a5a4dbe is Pending, waiting for it to be Running (with Ready = true)
    May  4 12:41:14.129: INFO: Pod "pod-configmaps-ef2a28f6-c1b2-4440-821d-25df1a5a4dbe": Phase="Running", Reason="", readiness=true. Elapsed: 2.012933754s
    May  4 12:41:14.129: INFO: The phase of Pod pod-configmaps-ef2a28f6-c1b2-4440-821d-25df1a5a4dbe is Running (Ready = true)
    May  4 12:41:14.129: INFO: Pod "pod-configmaps-ef2a28f6-c1b2-4440-821d-25df1a5a4dbe" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-d1f2d941-abff-4203-b48b-098a948da625 05/04/23 12:41:14.164
    STEP: Updating configmap cm-test-opt-upd-ffe85db3-c66c-4cf6-b60a-2fa636f4f3fe 05/04/23 12:41:14.173
    STEP: Creating configMap with name cm-test-opt-create-827ea57e-eb24-44a1-9bdb-4b35225ffabf 05/04/23 12:41:14.188
    STEP: waiting to observe update in volume 05/04/23 12:41:14.193
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    May  4 12:41:16.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2655" for this suite. 05/04/23 12:41:16.24
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:41:16.253
May  4 12:41:16.253: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename services 05/04/23 12:41:16.254
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:41:16.295
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:41:16.299
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
STEP: creating a Service 05/04/23 12:41:16.306
STEP: watching for the Service to be added 05/04/23 12:41:16.322
May  4 12:41:16.325: INFO: Found Service test-service-q2t62 in namespace services-256 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
May  4 12:41:16.325: INFO: Service test-service-q2t62 created
STEP: Getting /status 05/04/23 12:41:16.325
May  4 12:41:16.333: INFO: Service test-service-q2t62 has LoadBalancer: {[]}
STEP: patching the ServiceStatus 05/04/23 12:41:16.333
STEP: watching for the Service to be patched 05/04/23 12:41:16.345
May  4 12:41:16.347: INFO: observed Service test-service-q2t62 in namespace services-256 with annotations: map[] & LoadBalancer: {[]}
May  4 12:41:16.347: INFO: Found Service test-service-q2t62 in namespace services-256 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
May  4 12:41:16.347: INFO: Service test-service-q2t62 has service status patched
STEP: updating the ServiceStatus 05/04/23 12:41:16.347
May  4 12:41:16.363: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 05/04/23 12:41:16.363
May  4 12:41:16.366: INFO: Observed Service test-service-q2t62 in namespace services-256 with annotations: map[] & Conditions: {[]}
May  4 12:41:16.366: INFO: Observed event: &Service{ObjectMeta:{test-service-q2t62  services-256  ee46868a-e4ae-48dc-a2a7-25ded1327031 29161 0 2023-05-04 12:41:16 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-05-04 12:41:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-05-04 12:41:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.21.167.208,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.21.167.208],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
May  4 12:41:16.366: INFO: Found Service test-service-q2t62 in namespace services-256 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May  4 12:41:16.366: INFO: Service test-service-q2t62 has service status updated
STEP: patching the service 05/04/23 12:41:16.366
STEP: watching for the Service to be patched 05/04/23 12:41:16.388
May  4 12:41:16.393: INFO: observed Service test-service-q2t62 in namespace services-256 with labels: map[test-service-static:true]
May  4 12:41:16.393: INFO: observed Service test-service-q2t62 in namespace services-256 with labels: map[test-service-static:true]
May  4 12:41:16.393: INFO: observed Service test-service-q2t62 in namespace services-256 with labels: map[test-service-static:true]
May  4 12:41:16.393: INFO: Found Service test-service-q2t62 in namespace services-256 with labels: map[test-service:patched test-service-static:true]
May  4 12:41:16.393: INFO: Service test-service-q2t62 patched
STEP: deleting the service 05/04/23 12:41:16.393
STEP: watching for the Service to be deleted 05/04/23 12:41:16.415
May  4 12:41:16.417: INFO: Observed event: ADDED
May  4 12:41:16.417: INFO: Observed event: MODIFIED
May  4 12:41:16.417: INFO: Observed event: MODIFIED
May  4 12:41:16.417: INFO: Observed event: MODIFIED
May  4 12:41:16.417: INFO: Found Service test-service-q2t62 in namespace services-256 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
May  4 12:41:16.417: INFO: Service test-service-q2t62 deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  4 12:41:16.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-256" for this suite. 05/04/23 12:41:16.426
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","completed":192,"skipped":3680,"failed":0}
------------------------------
• [0.185 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:41:16.253
    May  4 12:41:16.253: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename services 05/04/23 12:41:16.254
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:41:16.295
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:41:16.299
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3415
    STEP: creating a Service 05/04/23 12:41:16.306
    STEP: watching for the Service to be added 05/04/23 12:41:16.322
    May  4 12:41:16.325: INFO: Found Service test-service-q2t62 in namespace services-256 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    May  4 12:41:16.325: INFO: Service test-service-q2t62 created
    STEP: Getting /status 05/04/23 12:41:16.325
    May  4 12:41:16.333: INFO: Service test-service-q2t62 has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 05/04/23 12:41:16.333
    STEP: watching for the Service to be patched 05/04/23 12:41:16.345
    May  4 12:41:16.347: INFO: observed Service test-service-q2t62 in namespace services-256 with annotations: map[] & LoadBalancer: {[]}
    May  4 12:41:16.347: INFO: Found Service test-service-q2t62 in namespace services-256 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    May  4 12:41:16.347: INFO: Service test-service-q2t62 has service status patched
    STEP: updating the ServiceStatus 05/04/23 12:41:16.347
    May  4 12:41:16.363: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 05/04/23 12:41:16.363
    May  4 12:41:16.366: INFO: Observed Service test-service-q2t62 in namespace services-256 with annotations: map[] & Conditions: {[]}
    May  4 12:41:16.366: INFO: Observed event: &Service{ObjectMeta:{test-service-q2t62  services-256  ee46868a-e4ae-48dc-a2a7-25ded1327031 29161 0 2023-05-04 12:41:16 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-05-04 12:41:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-05-04 12:41:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.21.167.208,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.21.167.208],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    May  4 12:41:16.366: INFO: Found Service test-service-q2t62 in namespace services-256 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    May  4 12:41:16.366: INFO: Service test-service-q2t62 has service status updated
    STEP: patching the service 05/04/23 12:41:16.366
    STEP: watching for the Service to be patched 05/04/23 12:41:16.388
    May  4 12:41:16.393: INFO: observed Service test-service-q2t62 in namespace services-256 with labels: map[test-service-static:true]
    May  4 12:41:16.393: INFO: observed Service test-service-q2t62 in namespace services-256 with labels: map[test-service-static:true]
    May  4 12:41:16.393: INFO: observed Service test-service-q2t62 in namespace services-256 with labels: map[test-service-static:true]
    May  4 12:41:16.393: INFO: Found Service test-service-q2t62 in namespace services-256 with labels: map[test-service:patched test-service-static:true]
    May  4 12:41:16.393: INFO: Service test-service-q2t62 patched
    STEP: deleting the service 05/04/23 12:41:16.393
    STEP: watching for the Service to be deleted 05/04/23 12:41:16.415
    May  4 12:41:16.417: INFO: Observed event: ADDED
    May  4 12:41:16.417: INFO: Observed event: MODIFIED
    May  4 12:41:16.417: INFO: Observed event: MODIFIED
    May  4 12:41:16.417: INFO: Observed event: MODIFIED
    May  4 12:41:16.417: INFO: Found Service test-service-q2t62 in namespace services-256 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    May  4 12:41:16.417: INFO: Service test-service-q2t62 deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  4 12:41:16.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-256" for this suite. 05/04/23 12:41:16.426
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:41:16.439
May  4 12:41:16.439: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename emptydir 05/04/23 12:41:16.44
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:41:16.462
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:41:16.468
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
STEP: Creating a pod to test emptydir 0666 on node default medium 05/04/23 12:41:16.472
May  4 12:41:16.484: INFO: Waiting up to 5m0s for pod "pod-d014368b-ac8b-467f-8f8d-972127c9fdc8" in namespace "emptydir-9883" to be "Succeeded or Failed"
May  4 12:41:16.519: INFO: Pod "pod-d014368b-ac8b-467f-8f8d-972127c9fdc8": Phase="Pending", Reason="", readiness=false. Elapsed: 35.303874ms
May  4 12:41:18.524: INFO: Pod "pod-d014368b-ac8b-467f-8f8d-972127c9fdc8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040787003s
May  4 12:41:20.525: INFO: Pod "pod-d014368b-ac8b-467f-8f8d-972127c9fdc8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041750251s
STEP: Saw pod success 05/04/23 12:41:20.526
May  4 12:41:20.526: INFO: Pod "pod-d014368b-ac8b-467f-8f8d-972127c9fdc8" satisfied condition "Succeeded or Failed"
May  4 12:41:20.538: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-d014368b-ac8b-467f-8f8d-972127c9fdc8 container test-container: <nil>
STEP: delete the pod 05/04/23 12:41:20.548
May  4 12:41:20.570: INFO: Waiting for pod pod-d014368b-ac8b-467f-8f8d-972127c9fdc8 to disappear
May  4 12:41:20.575: INFO: Pod pod-d014368b-ac8b-467f-8f8d-972127c9fdc8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May  4 12:41:20.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9883" for this suite. 05/04/23 12:41:20.589
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":193,"skipped":3703,"failed":0}
------------------------------
• [4.161 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:41:16.439
    May  4 12:41:16.439: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename emptydir 05/04/23 12:41:16.44
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:41:16.462
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:41:16.468
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:206
    STEP: Creating a pod to test emptydir 0666 on node default medium 05/04/23 12:41:16.472
    May  4 12:41:16.484: INFO: Waiting up to 5m0s for pod "pod-d014368b-ac8b-467f-8f8d-972127c9fdc8" in namespace "emptydir-9883" to be "Succeeded or Failed"
    May  4 12:41:16.519: INFO: Pod "pod-d014368b-ac8b-467f-8f8d-972127c9fdc8": Phase="Pending", Reason="", readiness=false. Elapsed: 35.303874ms
    May  4 12:41:18.524: INFO: Pod "pod-d014368b-ac8b-467f-8f8d-972127c9fdc8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040787003s
    May  4 12:41:20.525: INFO: Pod "pod-d014368b-ac8b-467f-8f8d-972127c9fdc8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041750251s
    STEP: Saw pod success 05/04/23 12:41:20.526
    May  4 12:41:20.526: INFO: Pod "pod-d014368b-ac8b-467f-8f8d-972127c9fdc8" satisfied condition "Succeeded or Failed"
    May  4 12:41:20.538: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-d014368b-ac8b-467f-8f8d-972127c9fdc8 container test-container: <nil>
    STEP: delete the pod 05/04/23 12:41:20.548
    May  4 12:41:20.570: INFO: Waiting for pod pod-d014368b-ac8b-467f-8f8d-972127c9fdc8 to disappear
    May  4 12:41:20.575: INFO: Pod pod-d014368b-ac8b-467f-8f8d-972127c9fdc8 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May  4 12:41:20.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9883" for this suite. 05/04/23 12:41:20.589
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:41:20.6
May  4 12:41:20.600: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename webhook 05/04/23 12:41:20.601
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:41:20.637
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:41:20.643
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/04/23 12:41:20.673
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/04/23 12:41:20.94
STEP: Deploying the webhook pod 05/04/23 12:41:20.961
STEP: Wait for the deployment to be ready 05/04/23 12:41:20.975
May  4 12:41:20.993: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/04/23 12:41:23.006
STEP: Verifying the service has paired with the endpoint 05/04/23 12:41:23.019
May  4 12:41:24.019: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
STEP: Creating a validating webhook configuration 05/04/23 12:41:24.025
STEP: Creating a configMap that does not comply to the validation webhook rules 05/04/23 12:41:24.05
STEP: Updating a validating webhook configuration's rules to not include the create operation 05/04/23 12:41:24.063
STEP: Creating a configMap that does not comply to the validation webhook rules 05/04/23 12:41:24.078
STEP: Patching a validating webhook configuration's rules to include the create operation 05/04/23 12:41:24.09
STEP: Creating a configMap that does not comply to the validation webhook rules 05/04/23 12:41:24.101
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  4 12:41:24.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3521" for this suite. 05/04/23 12:41:24.12
STEP: Destroying namespace "webhook-3521-markers" for this suite. 05/04/23 12:41:24.129
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","completed":194,"skipped":3711,"failed":0}
------------------------------
• [3.598 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:41:20.6
    May  4 12:41:20.600: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename webhook 05/04/23 12:41:20.601
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:41:20.637
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:41:20.643
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/04/23 12:41:20.673
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/04/23 12:41:20.94
    STEP: Deploying the webhook pod 05/04/23 12:41:20.961
    STEP: Wait for the deployment to be ready 05/04/23 12:41:20.975
    May  4 12:41:20.993: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/04/23 12:41:23.006
    STEP: Verifying the service has paired with the endpoint 05/04/23 12:41:23.019
    May  4 12:41:24.019: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:412
    STEP: Creating a validating webhook configuration 05/04/23 12:41:24.025
    STEP: Creating a configMap that does not comply to the validation webhook rules 05/04/23 12:41:24.05
    STEP: Updating a validating webhook configuration's rules to not include the create operation 05/04/23 12:41:24.063
    STEP: Creating a configMap that does not comply to the validation webhook rules 05/04/23 12:41:24.078
    STEP: Patching a validating webhook configuration's rules to include the create operation 05/04/23 12:41:24.09
    STEP: Creating a configMap that does not comply to the validation webhook rules 05/04/23 12:41:24.101
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  4 12:41:24.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3521" for this suite. 05/04/23 12:41:24.12
    STEP: Destroying namespace "webhook-3521-markers" for this suite. 05/04/23 12:41:24.129
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:41:24.199
May  4 12:41:24.200: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename container-runtime 05/04/23 12:41:24.203
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:41:24.23
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:41:24.235
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
STEP: create the container 05/04/23 12:41:24.238
STEP: wait for the container to reach Succeeded 05/04/23 12:41:24.253
STEP: get the container status 05/04/23 12:41:27.284
STEP: the container should be terminated 05/04/23 12:41:27.289
STEP: the termination message should be set 05/04/23 12:41:27.289
May  4 12:41:27.289: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 05/04/23 12:41:27.289
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
May  4 12:41:27.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7529" for this suite. 05/04/23 12:41:27.323
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","completed":195,"skipped":3722,"failed":0}
------------------------------
• [3.132 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:41:24.199
    May  4 12:41:24.200: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename container-runtime 05/04/23 12:41:24.203
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:41:24.23
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:41:24.235
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194
    STEP: create the container 05/04/23 12:41:24.238
    STEP: wait for the container to reach Succeeded 05/04/23 12:41:24.253
    STEP: get the container status 05/04/23 12:41:27.284
    STEP: the container should be terminated 05/04/23 12:41:27.289
    STEP: the termination message should be set 05/04/23 12:41:27.289
    May  4 12:41:27.289: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 05/04/23 12:41:27.289
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    May  4 12:41:27.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-7529" for this suite. 05/04/23 12:41:27.323
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:41:27.334
May  4 12:41:27.334: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename webhook 05/04/23 12:41:27.335
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:41:27.356
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:41:27.359
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/04/23 12:41:27.382
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/04/23 12:41:27.723
STEP: Deploying the webhook pod 05/04/23 12:41:27.73
STEP: Wait for the deployment to be ready 05/04/23 12:41:27.752
May  4 12:41:27.766: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/04/23 12:41:29.78
STEP: Verifying the service has paired with the endpoint 05/04/23 12:41:29.799
May  4 12:41:30.801: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
STEP: Setting timeout (1s) shorter than webhook latency (5s) 05/04/23 12:41:30.805
STEP: Registering slow webhook via the AdmissionRegistration API 05/04/23 12:41:30.805
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 05/04/23 12:41:30.82
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 05/04/23 12:41:31.831
STEP: Registering slow webhook via the AdmissionRegistration API 05/04/23 12:41:31.831
STEP: Having no error when timeout is longer than webhook latency 05/04/23 12:41:32.906
STEP: Registering slow webhook via the AdmissionRegistration API 05/04/23 12:41:32.906
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 05/04/23 12:41:37.947
STEP: Registering slow webhook via the AdmissionRegistration API 05/04/23 12:41:37.947
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  4 12:41:42.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2183" for this suite. 05/04/23 12:41:42.996
STEP: Destroying namespace "webhook-2183-markers" for this suite. 05/04/23 12:41:43.004
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","completed":196,"skipped":3757,"failed":0}
------------------------------
• [SLOW TEST] [15.759 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:41:27.334
    May  4 12:41:27.334: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename webhook 05/04/23 12:41:27.335
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:41:27.356
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:41:27.359
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/04/23 12:41:27.382
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/04/23 12:41:27.723
    STEP: Deploying the webhook pod 05/04/23 12:41:27.73
    STEP: Wait for the deployment to be ready 05/04/23 12:41:27.752
    May  4 12:41:27.766: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/04/23 12:41:29.78
    STEP: Verifying the service has paired with the endpoint 05/04/23 12:41:29.799
    May  4 12:41:30.801: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:380
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 05/04/23 12:41:30.805
    STEP: Registering slow webhook via the AdmissionRegistration API 05/04/23 12:41:30.805
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 05/04/23 12:41:30.82
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 05/04/23 12:41:31.831
    STEP: Registering slow webhook via the AdmissionRegistration API 05/04/23 12:41:31.831
    STEP: Having no error when timeout is longer than webhook latency 05/04/23 12:41:32.906
    STEP: Registering slow webhook via the AdmissionRegistration API 05/04/23 12:41:32.906
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 05/04/23 12:41:37.947
    STEP: Registering slow webhook via the AdmissionRegistration API 05/04/23 12:41:37.947
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  4 12:41:42.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2183" for this suite. 05/04/23 12:41:42.996
    STEP: Destroying namespace "webhook-2183-markers" for this suite. 05/04/23 12:41:43.004
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:41:43.106
May  4 12:41:43.107: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename crd-watch 05/04/23 12:41:43.108
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:41:43.147
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:41:43.153
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
May  4 12:41:43.158: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Creating first CR  05/04/23 12:41:45.743
May  4 12:41:45.751: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-04T12:41:45Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-04T12:41:45Z]] name:name1 resourceVersion:29474 uid:26894657-d5e3-4d79-a8a4-eae469ff7c52] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 05/04/23 12:41:55.753
May  4 12:41:55.762: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-04T12:41:55Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-04T12:41:55Z]] name:name2 resourceVersion:29512 uid:ab388f86-2631-4233-9c7d-493203f53b21] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 05/04/23 12:42:05.763
May  4 12:42:05.769: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-04T12:41:45Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-04T12:42:05Z]] name:name1 resourceVersion:29535 uid:26894657-d5e3-4d79-a8a4-eae469ff7c52] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 05/04/23 12:42:15.77
May  4 12:42:15.777: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-04T12:41:55Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-04T12:42:15Z]] name:name2 resourceVersion:29561 uid:ab388f86-2631-4233-9c7d-493203f53b21] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 05/04/23 12:42:25.778
May  4 12:42:25.786: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-04T12:41:45Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-04T12:42:05Z]] name:name1 resourceVersion:29584 uid:26894657-d5e3-4d79-a8a4-eae469ff7c52] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 05/04/23 12:42:35.789
May  4 12:42:35.799: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-04T12:41:55Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-04T12:42:15Z]] name:name2 resourceVersion:29606 uid:ab388f86-2631-4233-9c7d-493203f53b21] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  4 12:42:46.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-6726" for this suite. 05/04/23 12:42:46.319
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","completed":197,"skipped":3785,"failed":0}
------------------------------
• [SLOW TEST] [63.220 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:41:43.106
    May  4 12:41:43.107: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename crd-watch 05/04/23 12:41:43.108
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:41:43.147
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:41:43.153
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    May  4 12:41:43.158: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Creating first CR  05/04/23 12:41:45.743
    May  4 12:41:45.751: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-04T12:41:45Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-04T12:41:45Z]] name:name1 resourceVersion:29474 uid:26894657-d5e3-4d79-a8a4-eae469ff7c52] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 05/04/23 12:41:55.753
    May  4 12:41:55.762: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-04T12:41:55Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-04T12:41:55Z]] name:name2 resourceVersion:29512 uid:ab388f86-2631-4233-9c7d-493203f53b21] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 05/04/23 12:42:05.763
    May  4 12:42:05.769: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-04T12:41:45Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-04T12:42:05Z]] name:name1 resourceVersion:29535 uid:26894657-d5e3-4d79-a8a4-eae469ff7c52] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 05/04/23 12:42:15.77
    May  4 12:42:15.777: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-04T12:41:55Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-04T12:42:15Z]] name:name2 resourceVersion:29561 uid:ab388f86-2631-4233-9c7d-493203f53b21] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 05/04/23 12:42:25.778
    May  4 12:42:25.786: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-04T12:41:45Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-04T12:42:05Z]] name:name1 resourceVersion:29584 uid:26894657-d5e3-4d79-a8a4-eae469ff7c52] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 05/04/23 12:42:35.789
    May  4 12:42:35.799: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-04T12:41:55Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-04T12:42:15Z]] name:name2 resourceVersion:29606 uid:ab388f86-2631-4233-9c7d-493203f53b21] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  4 12:42:46.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-watch-6726" for this suite. 05/04/23 12:42:46.319
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:42:46.33
May  4 12:42:46.331: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename webhook 05/04/23 12:42:46.331
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:42:46.355
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:42:46.358
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/04/23 12:42:46.382
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/04/23 12:42:47.488
STEP: Deploying the webhook pod 05/04/23 12:42:47.501
STEP: Wait for the deployment to be ready 05/04/23 12:42:47.522
May  4 12:42:47.537: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/04/23 12:42:49.55
STEP: Verifying the service has paired with the endpoint 05/04/23 12:42:49.568
May  4 12:42:50.568: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
STEP: Listing all of the created validation webhooks 05/04/23 12:42:50.654
STEP: Creating a configMap that should be mutated 05/04/23 12:42:50.671
STEP: Deleting the collection of validation webhooks 05/04/23 12:42:50.71
STEP: Creating a configMap that should not be mutated 05/04/23 12:42:50.777
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  4 12:42:50.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1533" for this suite. 05/04/23 12:42:50.798
STEP: Destroying namespace "webhook-1533-markers" for this suite. 05/04/23 12:42:50.805
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","completed":198,"skipped":3815,"failed":0}
------------------------------
• [4.563 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:42:46.33
    May  4 12:42:46.331: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename webhook 05/04/23 12:42:46.331
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:42:46.355
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:42:46.358
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/04/23 12:42:46.382
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/04/23 12:42:47.488
    STEP: Deploying the webhook pod 05/04/23 12:42:47.501
    STEP: Wait for the deployment to be ready 05/04/23 12:42:47.522
    May  4 12:42:47.537: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/04/23 12:42:49.55
    STEP: Verifying the service has paired with the endpoint 05/04/23 12:42:49.568
    May  4 12:42:50.568: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:655
    STEP: Listing all of the created validation webhooks 05/04/23 12:42:50.654
    STEP: Creating a configMap that should be mutated 05/04/23 12:42:50.671
    STEP: Deleting the collection of validation webhooks 05/04/23 12:42:50.71
    STEP: Creating a configMap that should not be mutated 05/04/23 12:42:50.777
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  4 12:42:50.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1533" for this suite. 05/04/23 12:42:50.798
    STEP: Destroying namespace "webhook-1533-markers" for this suite. 05/04/23 12:42:50.805
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:42:50.899
May  4 12:42:50.899: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename endpointslice 05/04/23 12:42:50.899
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:42:50.974
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:42:50.979
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
May  4 12:42:50.997: INFO: Endpoints addresses: [10.0.1.168 10.0.1.223 10.0.1.58] , ports: [443]
May  4 12:42:50.997: INFO: EndpointSlices addresses: [10.0.1.168 10.0.1.223 10.0.1.58] , ports: [443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
May  4 12:42:50.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-4045" for this suite. 05/04/23 12:42:51.018
{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","completed":199,"skipped":3843,"failed":0}
------------------------------
• [0.127 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:42:50.899
    May  4 12:42:50.899: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename endpointslice 05/04/23 12:42:50.899
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:42:50.974
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:42:50.979
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:65
    May  4 12:42:50.997: INFO: Endpoints addresses: [10.0.1.168 10.0.1.223 10.0.1.58] , ports: [443]
    May  4 12:42:50.997: INFO: EndpointSlices addresses: [10.0.1.168 10.0.1.223 10.0.1.58] , ports: [443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    May  4 12:42:50.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-4045" for this suite. 05/04/23 12:42:51.018
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:42:51.027
May  4 12:42:51.027: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename emptydir 05/04/23 12:42:51.028
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:42:51.046
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:42:51.05
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
STEP: Creating Pod 05/04/23 12:42:51.052
May  4 12:42:51.062: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-4ddda4e4-97c8-404e-812c-26c29c41c1d8" in namespace "emptydir-7327" to be "running"
May  4 12:42:51.066: INFO: Pod "pod-sharedvolume-4ddda4e4-97c8-404e-812c-26c29c41c1d8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.693858ms
May  4 12:42:53.071: INFO: Pod "pod-sharedvolume-4ddda4e4-97c8-404e-812c-26c29c41c1d8": Phase="Running", Reason="", readiness=false. Elapsed: 2.00907402s
May  4 12:42:53.071: INFO: Pod "pod-sharedvolume-4ddda4e4-97c8-404e-812c-26c29c41c1d8" satisfied condition "running"
STEP: Reading file content from the nginx-container 05/04/23 12:42:53.071
May  4 12:42:53.071: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-7327 PodName:pod-sharedvolume-4ddda4e4-97c8-404e-812c-26c29c41c1d8 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  4 12:42:53.071: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
May  4 12:42:53.072: INFO: ExecWithOptions: Clientset creation
May  4 12:42:53.072: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/emptydir-7327/pods/pod-sharedvolume-4ddda4e4-97c8-404e-812c-26c29c41c1d8/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
May  4 12:42:53.138: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May  4 12:42:53.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7327" for this suite. 05/04/23 12:42:53.146
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","completed":200,"skipped":3846,"failed":0}
------------------------------
• [2.128 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:42:51.027
    May  4 12:42:51.027: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename emptydir 05/04/23 12:42:51.028
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:42:51.046
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:42:51.05
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:226
    STEP: Creating Pod 05/04/23 12:42:51.052
    May  4 12:42:51.062: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-4ddda4e4-97c8-404e-812c-26c29c41c1d8" in namespace "emptydir-7327" to be "running"
    May  4 12:42:51.066: INFO: Pod "pod-sharedvolume-4ddda4e4-97c8-404e-812c-26c29c41c1d8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.693858ms
    May  4 12:42:53.071: INFO: Pod "pod-sharedvolume-4ddda4e4-97c8-404e-812c-26c29c41c1d8": Phase="Running", Reason="", readiness=false. Elapsed: 2.00907402s
    May  4 12:42:53.071: INFO: Pod "pod-sharedvolume-4ddda4e4-97c8-404e-812c-26c29c41c1d8" satisfied condition "running"
    STEP: Reading file content from the nginx-container 05/04/23 12:42:53.071
    May  4 12:42:53.071: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-7327 PodName:pod-sharedvolume-4ddda4e4-97c8-404e-812c-26c29c41c1d8 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  4 12:42:53.071: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    May  4 12:42:53.072: INFO: ExecWithOptions: Clientset creation
    May  4 12:42:53.072: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/emptydir-7327/pods/pod-sharedvolume-4ddda4e4-97c8-404e-812c-26c29c41c1d8/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    May  4 12:42:53.138: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May  4 12:42:53.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7327" for this suite. 05/04/23 12:42:53.146
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:42:53.156
May  4 12:42:53.156: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename crd-publish-openapi 05/04/23 12:42:53.157
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:42:53.176
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:42:53.178
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
May  4 12:42:53.182: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 05/04/23 12:42:59.472
May  4 12:42:59.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-229 --namespace=crd-publish-openapi-229 create -f -'
May  4 12:43:00.583: INFO: stderr: ""
May  4 12:43:00.583: INFO: stdout: "e2e-test-crd-publish-openapi-2072-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May  4 12:43:00.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-229 --namespace=crd-publish-openapi-229 delete e2e-test-crd-publish-openapi-2072-crds test-cr'
May  4 12:43:00.743: INFO: stderr: ""
May  4 12:43:00.743: INFO: stdout: "e2e-test-crd-publish-openapi-2072-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
May  4 12:43:00.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-229 --namespace=crd-publish-openapi-229 apply -f -'
May  4 12:43:01.727: INFO: stderr: ""
May  4 12:43:01.727: INFO: stdout: "e2e-test-crd-publish-openapi-2072-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May  4 12:43:01.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-229 --namespace=crd-publish-openapi-229 delete e2e-test-crd-publish-openapi-2072-crds test-cr'
May  4 12:43:01.811: INFO: stderr: ""
May  4 12:43:01.811: INFO: stdout: "e2e-test-crd-publish-openapi-2072-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 05/04/23 12:43:01.811
May  4 12:43:01.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-229 explain e2e-test-crd-publish-openapi-2072-crds'
May  4 12:43:02.978: INFO: stderr: ""
May  4 12:43:02.978: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2072-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  4 12:43:09.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-229" for this suite. 05/04/23 12:43:09.199
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","completed":201,"skipped":3850,"failed":0}
------------------------------
• [SLOW TEST] [16.052 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:42:53.156
    May  4 12:42:53.156: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename crd-publish-openapi 05/04/23 12:42:53.157
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:42:53.176
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:42:53.178
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:193
    May  4 12:42:53.182: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 05/04/23 12:42:59.472
    May  4 12:42:59.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-229 --namespace=crd-publish-openapi-229 create -f -'
    May  4 12:43:00.583: INFO: stderr: ""
    May  4 12:43:00.583: INFO: stdout: "e2e-test-crd-publish-openapi-2072-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    May  4 12:43:00.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-229 --namespace=crd-publish-openapi-229 delete e2e-test-crd-publish-openapi-2072-crds test-cr'
    May  4 12:43:00.743: INFO: stderr: ""
    May  4 12:43:00.743: INFO: stdout: "e2e-test-crd-publish-openapi-2072-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    May  4 12:43:00.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-229 --namespace=crd-publish-openapi-229 apply -f -'
    May  4 12:43:01.727: INFO: stderr: ""
    May  4 12:43:01.727: INFO: stdout: "e2e-test-crd-publish-openapi-2072-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    May  4 12:43:01.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-229 --namespace=crd-publish-openapi-229 delete e2e-test-crd-publish-openapi-2072-crds test-cr'
    May  4 12:43:01.811: INFO: stderr: ""
    May  4 12:43:01.811: INFO: stdout: "e2e-test-crd-publish-openapi-2072-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 05/04/23 12:43:01.811
    May  4 12:43:01.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-229 explain e2e-test-crd-publish-openapi-2072-crds'
    May  4 12:43:02.978: INFO: stderr: ""
    May  4 12:43:02.978: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2072-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  4 12:43:09.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-229" for this suite. 05/04/23 12:43:09.199
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:43:09.208
May  4 12:43:09.209: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename emptydir 05/04/23 12:43:09.211
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:43:09.235
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:43:09.242
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
STEP: Creating a pod to test emptydir 0644 on node default medium 05/04/23 12:43:09.244
May  4 12:43:09.254: INFO: Waiting up to 5m0s for pod "pod-56ae3882-bc8a-491f-a970-1c49b2ac94f9" in namespace "emptydir-9985" to be "Succeeded or Failed"
May  4 12:43:09.258: INFO: Pod "pod-56ae3882-bc8a-491f-a970-1c49b2ac94f9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.610105ms
May  4 12:43:11.262: INFO: Pod "pod-56ae3882-bc8a-491f-a970-1c49b2ac94f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008352088s
May  4 12:43:13.263: INFO: Pod "pod-56ae3882-bc8a-491f-a970-1c49b2ac94f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008766906s
STEP: Saw pod success 05/04/23 12:43:13.263
May  4 12:43:13.263: INFO: Pod "pod-56ae3882-bc8a-491f-a970-1c49b2ac94f9" satisfied condition "Succeeded or Failed"
May  4 12:43:13.266: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-56ae3882-bc8a-491f-a970-1c49b2ac94f9 container test-container: <nil>
STEP: delete the pod 05/04/23 12:43:13.283
May  4 12:43:13.299: INFO: Waiting for pod pod-56ae3882-bc8a-491f-a970-1c49b2ac94f9 to disappear
May  4 12:43:13.303: INFO: Pod pod-56ae3882-bc8a-491f-a970-1c49b2ac94f9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May  4 12:43:13.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9985" for this suite. 05/04/23 12:43:13.311
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":202,"skipped":3858,"failed":0}
------------------------------
• [4.111 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:43:09.208
    May  4 12:43:09.209: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename emptydir 05/04/23 12:43:09.211
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:43:09.235
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:43:09.242
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:196
    STEP: Creating a pod to test emptydir 0644 on node default medium 05/04/23 12:43:09.244
    May  4 12:43:09.254: INFO: Waiting up to 5m0s for pod "pod-56ae3882-bc8a-491f-a970-1c49b2ac94f9" in namespace "emptydir-9985" to be "Succeeded or Failed"
    May  4 12:43:09.258: INFO: Pod "pod-56ae3882-bc8a-491f-a970-1c49b2ac94f9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.610105ms
    May  4 12:43:11.262: INFO: Pod "pod-56ae3882-bc8a-491f-a970-1c49b2ac94f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008352088s
    May  4 12:43:13.263: INFO: Pod "pod-56ae3882-bc8a-491f-a970-1c49b2ac94f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008766906s
    STEP: Saw pod success 05/04/23 12:43:13.263
    May  4 12:43:13.263: INFO: Pod "pod-56ae3882-bc8a-491f-a970-1c49b2ac94f9" satisfied condition "Succeeded or Failed"
    May  4 12:43:13.266: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-56ae3882-bc8a-491f-a970-1c49b2ac94f9 container test-container: <nil>
    STEP: delete the pod 05/04/23 12:43:13.283
    May  4 12:43:13.299: INFO: Waiting for pod pod-56ae3882-bc8a-491f-a970-1c49b2ac94f9 to disappear
    May  4 12:43:13.303: INFO: Pod pod-56ae3882-bc8a-491f-a970-1c49b2ac94f9 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May  4 12:43:13.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9985" for this suite. 05/04/23 12:43:13.311
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:43:13.32
May  4 12:43:13.320: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename sched-preemption 05/04/23 12:43:13.321
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:43:13.339
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:43:13.345
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
May  4 12:43:13.370: INFO: Waiting up to 1m0s for all nodes to be ready
May  4 12:44:13.428: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
STEP: Create pods that use 4/5 of node resources. 05/04/23 12:44:13.432
May  4 12:44:13.456: INFO: Created pod: pod0-0-sched-preemption-low-priority
May  4 12:44:13.465: INFO: Created pod: pod0-1-sched-preemption-medium-priority
May  4 12:44:13.491: INFO: Created pod: pod1-0-sched-preemption-medium-priority
May  4 12:44:13.499: INFO: Created pod: pod1-1-sched-preemption-medium-priority
May  4 12:44:13.527: INFO: Created pod: pod2-0-sched-preemption-medium-priority
May  4 12:44:13.536: INFO: Created pod: pod2-1-sched-preemption-medium-priority
May  4 12:44:13.559: INFO: Created pod: pod3-0-sched-preemption-medium-priority
May  4 12:44:13.567: INFO: Created pod: pod3-1-sched-preemption-medium-priority
May  4 12:44:13.697: INFO: Created pod: pod4-0-sched-preemption-medium-priority
May  4 12:44:13.708: INFO: Created pod: pod4-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 05/04/23 12:44:13.708
May  4 12:44:13.708: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-4358" to be "running"
May  4 12:44:13.718: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 9.990183ms
May  4 12:44:15.727: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018824036s
May  4 12:44:17.725: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016407528s
May  4 12:44:19.722: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014065969s
May  4 12:44:21.725: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.016980548s
May  4 12:44:23.724: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 10.015841075s
May  4 12:44:23.724: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
May  4 12:44:23.724: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-4358" to be "running"
May  4 12:44:23.729: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.373562ms
May  4 12:44:23.729: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
May  4 12:44:23.729: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-4358" to be "running"
May  4 12:44:23.732: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.371434ms
May  4 12:44:23.732: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
May  4 12:44:23.732: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-4358" to be "running"
May  4 12:44:23.736: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.209205ms
May  4 12:44:23.736: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
May  4 12:44:23.736: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-4358" to be "running"
May  4 12:44:23.740: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.474917ms
May  4 12:44:23.740: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
May  4 12:44:23.740: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-4358" to be "running"
May  4 12:44:23.745: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.033261ms
May  4 12:44:23.745: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
May  4 12:44:23.745: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-4358" to be "running"
May  4 12:44:23.749: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.042991ms
May  4 12:44:23.749: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
May  4 12:44:23.749: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-4358" to be "running"
May  4 12:44:23.756: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.490936ms
May  4 12:44:23.756: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
May  4 12:44:23.756: INFO: Waiting up to 5m0s for pod "pod4-0-sched-preemption-medium-priority" in namespace "sched-preemption-4358" to be "running"
May  4 12:44:23.759: INFO: Pod "pod4-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.588244ms
May  4 12:44:23.759: INFO: Pod "pod4-0-sched-preemption-medium-priority" satisfied condition "running"
May  4 12:44:23.759: INFO: Waiting up to 5m0s for pod "pod4-1-sched-preemption-medium-priority" in namespace "sched-preemption-4358" to be "running"
May  4 12:44:23.765: INFO: Pod "pod4-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.48214ms
May  4 12:44:23.765: INFO: Pod "pod4-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 05/04/23 12:44:23.765
May  4 12:44:23.770: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-4358" to be "running"
May  4 12:44:23.778: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.204614ms
May  4 12:44:25.783: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013110522s
May  4 12:44:27.782: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.012006845s
May  4 12:44:27.782: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
May  4 12:44:27.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4358" for this suite. 05/04/23 12:44:27.846
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","completed":203,"skipped":3872,"failed":0}
------------------------------
• [SLOW TEST] [74.631 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:43:13.32
    May  4 12:43:13.320: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename sched-preemption 05/04/23 12:43:13.321
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:43:13.339
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:43:13.345
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    May  4 12:43:13.370: INFO: Waiting up to 1m0s for all nodes to be ready
    May  4 12:44:13.428: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:125
    STEP: Create pods that use 4/5 of node resources. 05/04/23 12:44:13.432
    May  4 12:44:13.456: INFO: Created pod: pod0-0-sched-preemption-low-priority
    May  4 12:44:13.465: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    May  4 12:44:13.491: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    May  4 12:44:13.499: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    May  4 12:44:13.527: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    May  4 12:44:13.536: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    May  4 12:44:13.559: INFO: Created pod: pod3-0-sched-preemption-medium-priority
    May  4 12:44:13.567: INFO: Created pod: pod3-1-sched-preemption-medium-priority
    May  4 12:44:13.697: INFO: Created pod: pod4-0-sched-preemption-medium-priority
    May  4 12:44:13.708: INFO: Created pod: pod4-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 05/04/23 12:44:13.708
    May  4 12:44:13.708: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-4358" to be "running"
    May  4 12:44:13.718: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 9.990183ms
    May  4 12:44:15.727: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018824036s
    May  4 12:44:17.725: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016407528s
    May  4 12:44:19.722: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014065969s
    May  4 12:44:21.725: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.016980548s
    May  4 12:44:23.724: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 10.015841075s
    May  4 12:44:23.724: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    May  4 12:44:23.724: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-4358" to be "running"
    May  4 12:44:23.729: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.373562ms
    May  4 12:44:23.729: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    May  4 12:44:23.729: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-4358" to be "running"
    May  4 12:44:23.732: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.371434ms
    May  4 12:44:23.732: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    May  4 12:44:23.732: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-4358" to be "running"
    May  4 12:44:23.736: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.209205ms
    May  4 12:44:23.736: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    May  4 12:44:23.736: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-4358" to be "running"
    May  4 12:44:23.740: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.474917ms
    May  4 12:44:23.740: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    May  4 12:44:23.740: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-4358" to be "running"
    May  4 12:44:23.745: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.033261ms
    May  4 12:44:23.745: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    May  4 12:44:23.745: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-4358" to be "running"
    May  4 12:44:23.749: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.042991ms
    May  4 12:44:23.749: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
    May  4 12:44:23.749: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-4358" to be "running"
    May  4 12:44:23.756: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.490936ms
    May  4 12:44:23.756: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
    May  4 12:44:23.756: INFO: Waiting up to 5m0s for pod "pod4-0-sched-preemption-medium-priority" in namespace "sched-preemption-4358" to be "running"
    May  4 12:44:23.759: INFO: Pod "pod4-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.588244ms
    May  4 12:44:23.759: INFO: Pod "pod4-0-sched-preemption-medium-priority" satisfied condition "running"
    May  4 12:44:23.759: INFO: Waiting up to 5m0s for pod "pod4-1-sched-preemption-medium-priority" in namespace "sched-preemption-4358" to be "running"
    May  4 12:44:23.765: INFO: Pod "pod4-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.48214ms
    May  4 12:44:23.765: INFO: Pod "pod4-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 05/04/23 12:44:23.765
    May  4 12:44:23.770: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-4358" to be "running"
    May  4 12:44:23.778: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.204614ms
    May  4 12:44:25.783: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013110522s
    May  4 12:44:27.782: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.012006845s
    May  4 12:44:27.782: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    May  4 12:44:27.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-4358" for this suite. 05/04/23 12:44:27.846
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:44:27.952
May  4 12:44:27.952: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename container-lifecycle-hook 05/04/23 12:44:27.953
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:44:27.974
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:44:27.978
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 05/04/23 12:44:27.989
May  4 12:44:28.004: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8052" to be "running and ready"
May  4 12:44:28.018: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 14.764449ms
May  4 12:44:28.018: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May  4 12:44:30.025: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.021368948s
May  4 12:44:30.025: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
May  4 12:44:30.025: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
STEP: create the pod with lifecycle hook 05/04/23 12:44:30.03
May  4 12:44:30.039: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-8052" to be "running and ready"
May  4 12:44:30.044: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.675582ms
May  4 12:44:30.044: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
May  4 12:44:32.052: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.013679707s
May  4 12:44:32.052: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
May  4 12:44:32.052: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 05/04/23 12:44:32.058
STEP: delete the pod with lifecycle hook 05/04/23 12:44:32.075
May  4 12:44:32.093: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May  4 12:44:32.098: INFO: Pod pod-with-poststart-http-hook still exists
May  4 12:44:34.099: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May  4 12:44:34.115: INFO: Pod pod-with-poststart-http-hook still exists
May  4 12:44:36.100: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May  4 12:44:36.104: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
May  4 12:44:36.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8052" for this suite. 05/04/23 12:44:36.122
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","completed":204,"skipped":3884,"failed":0}
------------------------------
• [SLOW TEST] [8.182 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:44:27.952
    May  4 12:44:27.952: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename container-lifecycle-hook 05/04/23 12:44:27.953
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:44:27.974
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:44:27.978
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 05/04/23 12:44:27.989
    May  4 12:44:28.004: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8052" to be "running and ready"
    May  4 12:44:28.018: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 14.764449ms
    May  4 12:44:28.018: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    May  4 12:44:30.025: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.021368948s
    May  4 12:44:30.025: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    May  4 12:44:30.025: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:130
    STEP: create the pod with lifecycle hook 05/04/23 12:44:30.03
    May  4 12:44:30.039: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-8052" to be "running and ready"
    May  4 12:44:30.044: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.675582ms
    May  4 12:44:30.044: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    May  4 12:44:32.052: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.013679707s
    May  4 12:44:32.052: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    May  4 12:44:32.052: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 05/04/23 12:44:32.058
    STEP: delete the pod with lifecycle hook 05/04/23 12:44:32.075
    May  4 12:44:32.093: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    May  4 12:44:32.098: INFO: Pod pod-with-poststart-http-hook still exists
    May  4 12:44:34.099: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    May  4 12:44:34.115: INFO: Pod pod-with-poststart-http-hook still exists
    May  4 12:44:36.100: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    May  4 12:44:36.104: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    May  4 12:44:36.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-8052" for this suite. 05/04/23 12:44:36.122
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:44:36.135
May  4 12:44:36.135: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename projected 05/04/23 12:44:36.136
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:44:36.155
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:44:36.158
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
STEP: Creating a pod to test downward API volume plugin 05/04/23 12:44:36.161
May  4 12:44:36.172: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5edda6e4-4374-4967-8e9c-2d19c1d3f1a6" in namespace "projected-3261" to be "Succeeded or Failed"
May  4 12:44:36.176: INFO: Pod "downwardapi-volume-5edda6e4-4374-4967-8e9c-2d19c1d3f1a6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.743548ms
May  4 12:44:38.182: INFO: Pod "downwardapi-volume-5edda6e4-4374-4967-8e9c-2d19c1d3f1a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010461638s
May  4 12:44:40.181: INFO: Pod "downwardapi-volume-5edda6e4-4374-4967-8e9c-2d19c1d3f1a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009782506s
STEP: Saw pod success 05/04/23 12:44:40.182
May  4 12:44:40.182: INFO: Pod "downwardapi-volume-5edda6e4-4374-4967-8e9c-2d19c1d3f1a6" satisfied condition "Succeeded or Failed"
May  4 12:44:40.186: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod downwardapi-volume-5edda6e4-4374-4967-8e9c-2d19c1d3f1a6 container client-container: <nil>
STEP: delete the pod 05/04/23 12:44:40.195
May  4 12:44:40.222: INFO: Waiting for pod downwardapi-volume-5edda6e4-4374-4967-8e9c-2d19c1d3f1a6 to disappear
May  4 12:44:40.226: INFO: Pod downwardapi-volume-5edda6e4-4374-4967-8e9c-2d19c1d3f1a6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
May  4 12:44:40.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3261" for this suite. 05/04/23 12:44:40.235
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":205,"skipped":3898,"failed":0}
------------------------------
• [4.109 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:44:36.135
    May  4 12:44:36.135: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename projected 05/04/23 12:44:36.136
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:44:36.155
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:44:36.158
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:83
    STEP: Creating a pod to test downward API volume plugin 05/04/23 12:44:36.161
    May  4 12:44:36.172: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5edda6e4-4374-4967-8e9c-2d19c1d3f1a6" in namespace "projected-3261" to be "Succeeded or Failed"
    May  4 12:44:36.176: INFO: Pod "downwardapi-volume-5edda6e4-4374-4967-8e9c-2d19c1d3f1a6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.743548ms
    May  4 12:44:38.182: INFO: Pod "downwardapi-volume-5edda6e4-4374-4967-8e9c-2d19c1d3f1a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010461638s
    May  4 12:44:40.181: INFO: Pod "downwardapi-volume-5edda6e4-4374-4967-8e9c-2d19c1d3f1a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009782506s
    STEP: Saw pod success 05/04/23 12:44:40.182
    May  4 12:44:40.182: INFO: Pod "downwardapi-volume-5edda6e4-4374-4967-8e9c-2d19c1d3f1a6" satisfied condition "Succeeded or Failed"
    May  4 12:44:40.186: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod downwardapi-volume-5edda6e4-4374-4967-8e9c-2d19c1d3f1a6 container client-container: <nil>
    STEP: delete the pod 05/04/23 12:44:40.195
    May  4 12:44:40.222: INFO: Waiting for pod downwardapi-volume-5edda6e4-4374-4967-8e9c-2d19c1d3f1a6 to disappear
    May  4 12:44:40.226: INFO: Pod downwardapi-volume-5edda6e4-4374-4967-8e9c-2d19c1d3f1a6 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    May  4 12:44:40.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3261" for this suite. 05/04/23 12:44:40.235
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:44:40.255
May  4 12:44:40.255: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename projected 05/04/23 12:44:40.257
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:44:40.279
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:44:40.283
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
STEP: Creating a pod to test downward API volume plugin 05/04/23 12:44:40.286
May  4 12:44:40.297: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3234259c-f7a7-45a6-9970-0062391beb73" in namespace "projected-8150" to be "Succeeded or Failed"
May  4 12:44:40.319: INFO: Pod "downwardapi-volume-3234259c-f7a7-45a6-9970-0062391beb73": Phase="Pending", Reason="", readiness=false. Elapsed: 21.494535ms
May  4 12:44:42.325: INFO: Pod "downwardapi-volume-3234259c-f7a7-45a6-9970-0062391beb73": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027996444s
May  4 12:44:44.324: INFO: Pod "downwardapi-volume-3234259c-f7a7-45a6-9970-0062391beb73": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026903831s
STEP: Saw pod success 05/04/23 12:44:44.324
May  4 12:44:44.324: INFO: Pod "downwardapi-volume-3234259c-f7a7-45a6-9970-0062391beb73" satisfied condition "Succeeded or Failed"
May  4 12:44:44.329: INFO: Trying to get logs from node ip-10-0-1-232.us-west-2.compute.internal pod downwardapi-volume-3234259c-f7a7-45a6-9970-0062391beb73 container client-container: <nil>
STEP: delete the pod 05/04/23 12:44:44.346
May  4 12:44:44.360: INFO: Waiting for pod downwardapi-volume-3234259c-f7a7-45a6-9970-0062391beb73 to disappear
May  4 12:44:44.363: INFO: Pod downwardapi-volume-3234259c-f7a7-45a6-9970-0062391beb73 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
May  4 12:44:44.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8150" for this suite. 05/04/23 12:44:44.37
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","completed":206,"skipped":3936,"failed":0}
------------------------------
• [4.125 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:44:40.255
    May  4 12:44:40.255: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename projected 05/04/23 12:44:40.257
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:44:40.279
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:44:40.283
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:220
    STEP: Creating a pod to test downward API volume plugin 05/04/23 12:44:40.286
    May  4 12:44:40.297: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3234259c-f7a7-45a6-9970-0062391beb73" in namespace "projected-8150" to be "Succeeded or Failed"
    May  4 12:44:40.319: INFO: Pod "downwardapi-volume-3234259c-f7a7-45a6-9970-0062391beb73": Phase="Pending", Reason="", readiness=false. Elapsed: 21.494535ms
    May  4 12:44:42.325: INFO: Pod "downwardapi-volume-3234259c-f7a7-45a6-9970-0062391beb73": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027996444s
    May  4 12:44:44.324: INFO: Pod "downwardapi-volume-3234259c-f7a7-45a6-9970-0062391beb73": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026903831s
    STEP: Saw pod success 05/04/23 12:44:44.324
    May  4 12:44:44.324: INFO: Pod "downwardapi-volume-3234259c-f7a7-45a6-9970-0062391beb73" satisfied condition "Succeeded or Failed"
    May  4 12:44:44.329: INFO: Trying to get logs from node ip-10-0-1-232.us-west-2.compute.internal pod downwardapi-volume-3234259c-f7a7-45a6-9970-0062391beb73 container client-container: <nil>
    STEP: delete the pod 05/04/23 12:44:44.346
    May  4 12:44:44.360: INFO: Waiting for pod downwardapi-volume-3234259c-f7a7-45a6-9970-0062391beb73 to disappear
    May  4 12:44:44.363: INFO: Pod downwardapi-volume-3234259c-f7a7-45a6-9970-0062391beb73 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    May  4 12:44:44.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8150" for this suite. 05/04/23 12:44:44.37
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:44:44.383
May  4 12:44:44.383: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename emptydir 05/04/23 12:44:44.384
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:44:44.407
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:44:44.414
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
STEP: Creating a pod to test emptydir 0644 on tmpfs 05/04/23 12:44:44.418
May  4 12:44:44.428: INFO: Waiting up to 5m0s for pod "pod-4386f8b6-6560-4d5e-82c6-f741d37d0fc0" in namespace "emptydir-9422" to be "Succeeded or Failed"
May  4 12:44:44.433: INFO: Pod "pod-4386f8b6-6560-4d5e-82c6-f741d37d0fc0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.380007ms
May  4 12:44:46.438: INFO: Pod "pod-4386f8b6-6560-4d5e-82c6-f741d37d0fc0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010559084s
May  4 12:44:48.438: INFO: Pod "pod-4386f8b6-6560-4d5e-82c6-f741d37d0fc0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010690464s
STEP: Saw pod success 05/04/23 12:44:48.438
May  4 12:44:48.438: INFO: Pod "pod-4386f8b6-6560-4d5e-82c6-f741d37d0fc0" satisfied condition "Succeeded or Failed"
May  4 12:44:48.443: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-4386f8b6-6560-4d5e-82c6-f741d37d0fc0 container test-container: <nil>
STEP: delete the pod 05/04/23 12:44:48.452
May  4 12:44:48.471: INFO: Waiting for pod pod-4386f8b6-6560-4d5e-82c6-f741d37d0fc0 to disappear
May  4 12:44:48.475: INFO: Pod pod-4386f8b6-6560-4d5e-82c6-f741d37d0fc0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May  4 12:44:48.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9422" for this suite. 05/04/23 12:44:48.482
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":207,"skipped":3949,"failed":0}
------------------------------
• [4.107 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:44:44.383
    May  4 12:44:44.383: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename emptydir 05/04/23 12:44:44.384
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:44:44.407
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:44:44.414
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:126
    STEP: Creating a pod to test emptydir 0644 on tmpfs 05/04/23 12:44:44.418
    May  4 12:44:44.428: INFO: Waiting up to 5m0s for pod "pod-4386f8b6-6560-4d5e-82c6-f741d37d0fc0" in namespace "emptydir-9422" to be "Succeeded or Failed"
    May  4 12:44:44.433: INFO: Pod "pod-4386f8b6-6560-4d5e-82c6-f741d37d0fc0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.380007ms
    May  4 12:44:46.438: INFO: Pod "pod-4386f8b6-6560-4d5e-82c6-f741d37d0fc0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010559084s
    May  4 12:44:48.438: INFO: Pod "pod-4386f8b6-6560-4d5e-82c6-f741d37d0fc0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010690464s
    STEP: Saw pod success 05/04/23 12:44:48.438
    May  4 12:44:48.438: INFO: Pod "pod-4386f8b6-6560-4d5e-82c6-f741d37d0fc0" satisfied condition "Succeeded or Failed"
    May  4 12:44:48.443: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-4386f8b6-6560-4d5e-82c6-f741d37d0fc0 container test-container: <nil>
    STEP: delete the pod 05/04/23 12:44:48.452
    May  4 12:44:48.471: INFO: Waiting for pod pod-4386f8b6-6560-4d5e-82c6-f741d37d0fc0 to disappear
    May  4 12:44:48.475: INFO: Pod pod-4386f8b6-6560-4d5e-82c6-f741d37d0fc0 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May  4 12:44:48.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9422" for this suite. 05/04/23 12:44:48.482
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:44:48.491
May  4 12:44:48.491: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename configmap 05/04/23 12:44:48.492
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:44:48.51
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:44:48.518
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
STEP: Creating configMap with name configmap-test-volume-map-a50bf1ce-eb4f-4d00-a1cd-74ccfe1128d8 05/04/23 12:44:48.522
STEP: Creating a pod to test consume configMaps 05/04/23 12:44:48.528
May  4 12:44:48.537: INFO: Waiting up to 5m0s for pod "pod-configmaps-b6ce9204-081a-4ea7-a69c-b7a2700c273d" in namespace "configmap-4226" to be "Succeeded or Failed"
May  4 12:44:48.545: INFO: Pod "pod-configmaps-b6ce9204-081a-4ea7-a69c-b7a2700c273d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.375856ms
May  4 12:44:50.549: INFO: Pod "pod-configmaps-b6ce9204-081a-4ea7-a69c-b7a2700c273d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011648212s
May  4 12:44:52.552: INFO: Pod "pod-configmaps-b6ce9204-081a-4ea7-a69c-b7a2700c273d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014402321s
STEP: Saw pod success 05/04/23 12:44:52.552
May  4 12:44:52.552: INFO: Pod "pod-configmaps-b6ce9204-081a-4ea7-a69c-b7a2700c273d" satisfied condition "Succeeded or Failed"
May  4 12:44:52.555: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod pod-configmaps-b6ce9204-081a-4ea7-a69c-b7a2700c273d container agnhost-container: <nil>
STEP: delete the pod 05/04/23 12:44:52.562
May  4 12:44:52.578: INFO: Waiting for pod pod-configmaps-b6ce9204-081a-4ea7-a69c-b7a2700c273d to disappear
May  4 12:44:52.581: INFO: Pod pod-configmaps-b6ce9204-081a-4ea7-a69c-b7a2700c273d no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
May  4 12:44:52.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4226" for this suite. 05/04/23 12:44:52.588
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":208,"skipped":3957,"failed":0}
------------------------------
• [4.105 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:44:48.491
    May  4 12:44:48.491: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename configmap 05/04/23 12:44:48.492
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:44:48.51
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:44:48.518
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:108
    STEP: Creating configMap with name configmap-test-volume-map-a50bf1ce-eb4f-4d00-a1cd-74ccfe1128d8 05/04/23 12:44:48.522
    STEP: Creating a pod to test consume configMaps 05/04/23 12:44:48.528
    May  4 12:44:48.537: INFO: Waiting up to 5m0s for pod "pod-configmaps-b6ce9204-081a-4ea7-a69c-b7a2700c273d" in namespace "configmap-4226" to be "Succeeded or Failed"
    May  4 12:44:48.545: INFO: Pod "pod-configmaps-b6ce9204-081a-4ea7-a69c-b7a2700c273d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.375856ms
    May  4 12:44:50.549: INFO: Pod "pod-configmaps-b6ce9204-081a-4ea7-a69c-b7a2700c273d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011648212s
    May  4 12:44:52.552: INFO: Pod "pod-configmaps-b6ce9204-081a-4ea7-a69c-b7a2700c273d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014402321s
    STEP: Saw pod success 05/04/23 12:44:52.552
    May  4 12:44:52.552: INFO: Pod "pod-configmaps-b6ce9204-081a-4ea7-a69c-b7a2700c273d" satisfied condition "Succeeded or Failed"
    May  4 12:44:52.555: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod pod-configmaps-b6ce9204-081a-4ea7-a69c-b7a2700c273d container agnhost-container: <nil>
    STEP: delete the pod 05/04/23 12:44:52.562
    May  4 12:44:52.578: INFO: Waiting for pod pod-configmaps-b6ce9204-081a-4ea7-a69c-b7a2700c273d to disappear
    May  4 12:44:52.581: INFO: Pod pod-configmaps-b6ce9204-081a-4ea7-a69c-b7a2700c273d no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    May  4 12:44:52.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4226" for this suite. 05/04/23 12:44:52.588
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:44:52.6
May  4 12:44:52.600: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename emptydir 05/04/23 12:44:52.601
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:44:52.621
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:44:52.625
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
STEP: Creating a pod to test emptydir 0666 on node default medium 05/04/23 12:44:52.628
May  4 12:44:52.637: INFO: Waiting up to 5m0s for pod "pod-033b2218-be3b-46e3-8814-cc7fbee75ad6" in namespace "emptydir-9831" to be "Succeeded or Failed"
May  4 12:44:52.640: INFO: Pod "pod-033b2218-be3b-46e3-8814-cc7fbee75ad6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.393372ms
May  4 12:44:54.645: INFO: Pod "pod-033b2218-be3b-46e3-8814-cc7fbee75ad6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007780918s
May  4 12:44:56.646: INFO: Pod "pod-033b2218-be3b-46e3-8814-cc7fbee75ad6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009597785s
STEP: Saw pod success 05/04/23 12:44:56.647
May  4 12:44:56.647: INFO: Pod "pod-033b2218-be3b-46e3-8814-cc7fbee75ad6" satisfied condition "Succeeded or Failed"
May  4 12:44:56.650: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod pod-033b2218-be3b-46e3-8814-cc7fbee75ad6 container test-container: <nil>
STEP: delete the pod 05/04/23 12:44:56.659
May  4 12:44:56.689: INFO: Waiting for pod pod-033b2218-be3b-46e3-8814-cc7fbee75ad6 to disappear
May  4 12:44:56.703: INFO: Pod pod-033b2218-be3b-46e3-8814-cc7fbee75ad6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May  4 12:44:56.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9831" for this suite. 05/04/23 12:44:56.736
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":209,"skipped":3988,"failed":0}
------------------------------
• [4.143 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:44:52.6
    May  4 12:44:52.600: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename emptydir 05/04/23 12:44:52.601
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:44:52.621
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:44:52.625
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:176
    STEP: Creating a pod to test emptydir 0666 on node default medium 05/04/23 12:44:52.628
    May  4 12:44:52.637: INFO: Waiting up to 5m0s for pod "pod-033b2218-be3b-46e3-8814-cc7fbee75ad6" in namespace "emptydir-9831" to be "Succeeded or Failed"
    May  4 12:44:52.640: INFO: Pod "pod-033b2218-be3b-46e3-8814-cc7fbee75ad6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.393372ms
    May  4 12:44:54.645: INFO: Pod "pod-033b2218-be3b-46e3-8814-cc7fbee75ad6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007780918s
    May  4 12:44:56.646: INFO: Pod "pod-033b2218-be3b-46e3-8814-cc7fbee75ad6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009597785s
    STEP: Saw pod success 05/04/23 12:44:56.647
    May  4 12:44:56.647: INFO: Pod "pod-033b2218-be3b-46e3-8814-cc7fbee75ad6" satisfied condition "Succeeded or Failed"
    May  4 12:44:56.650: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod pod-033b2218-be3b-46e3-8814-cc7fbee75ad6 container test-container: <nil>
    STEP: delete the pod 05/04/23 12:44:56.659
    May  4 12:44:56.689: INFO: Waiting for pod pod-033b2218-be3b-46e3-8814-cc7fbee75ad6 to disappear
    May  4 12:44:56.703: INFO: Pod pod-033b2218-be3b-46e3-8814-cc7fbee75ad6 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May  4 12:44:56.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9831" for this suite. 05/04/23 12:44:56.736
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:44:56.746
May  4 12:44:56.746: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename var-expansion 05/04/23 12:44:56.746
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:44:56.766
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:44:56.77
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
STEP: creating the pod with failed condition 05/04/23 12:44:56.773
May  4 12:44:56.786: INFO: Waiting up to 2m0s for pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9" in namespace "var-expansion-1085" to be "running"
May  4 12:44:56.792: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.844603ms
May  4 12:44:58.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011053136s
May  4 12:45:00.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010343312s
May  4 12:45:02.798: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011330669s
May  4 12:45:04.796: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010220737s
May  4 12:45:06.798: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.011254951s
May  4 12:45:08.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.010576584s
May  4 12:45:10.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 14.01092193s
May  4 12:45:12.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 16.011024485s
May  4 12:45:14.798: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 18.011384599s
May  4 12:45:16.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 20.010797838s
May  4 12:45:18.796: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 22.01022684s
May  4 12:45:20.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 24.010901958s
May  4 12:45:22.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 26.011231971s
May  4 12:45:24.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 28.010970769s
May  4 12:45:26.798: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 30.012127531s
May  4 12:45:28.796: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 32.010244129s
May  4 12:45:30.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 34.010843111s
May  4 12:45:32.798: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 36.011359932s
May  4 12:45:34.799: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 38.012276311s
May  4 12:45:36.798: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 40.011959776s
May  4 12:45:38.799: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 42.012847289s
May  4 12:45:40.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 44.011057493s
May  4 12:45:42.796: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 46.009968305s
May  4 12:45:44.798: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 48.011442355s
May  4 12:45:46.798: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 50.011886961s
May  4 12:45:48.798: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 52.011336441s
May  4 12:45:50.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 54.010893978s
May  4 12:45:52.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 56.010298336s
May  4 12:45:54.807: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 58.020554794s
May  4 12:45:56.800: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.013398223s
May  4 12:45:58.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.010968835s
May  4 12:46:00.799: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.013232678s
May  4 12:46:02.799: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.012351037s
May  4 12:46:04.798: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.012213779s
May  4 12:46:06.799: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.012311455s
May  4 12:46:08.803: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.017095215s
May  4 12:46:10.798: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.011372756s
May  4 12:46:12.799: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.012391305s
May  4 12:46:14.798: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.011446103s
May  4 12:46:16.798: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.011382449s
May  4 12:46:18.805: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.018836578s
May  4 12:46:20.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.011141459s
May  4 12:46:22.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.011187426s
May  4 12:46:24.798: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.011341775s
May  4 12:46:26.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.01122778s
May  4 12:46:28.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.011043359s
May  4 12:46:30.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.010845629s
May  4 12:46:32.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.010646167s
May  4 12:46:34.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.01050383s
May  4 12:46:36.798: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.011430584s
May  4 12:46:38.796: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.010192172s
May  4 12:46:40.801: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.014280166s
May  4 12:46:42.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.010977655s
May  4 12:46:44.798: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.011414556s
May  4 12:46:46.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.011027703s
May  4 12:46:48.801: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.014267884s
May  4 12:46:50.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.011007433s
May  4 12:46:52.796: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.010076442s
May  4 12:46:54.800: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.013831232s
May  4 12:46:56.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.011131298s
May  4 12:46:56.805: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.018962066s
STEP: updating the pod 05/04/23 12:46:56.805
May  4 12:46:57.320: INFO: Successfully updated pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9"
STEP: waiting for pod running 05/04/23 12:46:57.32
May  4 12:46:57.320: INFO: Waiting up to 2m0s for pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9" in namespace "var-expansion-1085" to be "running"
May  4 12:46:57.325: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.286101ms
May  4 12:46:59.330: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Running", Reason="", readiness=true. Elapsed: 2.009402258s
May  4 12:46:59.330: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9" satisfied condition "running"
STEP: deleting the pod gracefully 05/04/23 12:46:59.33
May  4 12:46:59.330: INFO: Deleting pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9" in namespace "var-expansion-1085"
May  4 12:46:59.339: INFO: Wait up to 5m0s for pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
May  4 12:47:31.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1085" for this suite. 05/04/23 12:47:31.358
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","completed":210,"skipped":4032,"failed":0}
------------------------------
• [SLOW TEST] [154.622 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:44:56.746
    May  4 12:44:56.746: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename var-expansion 05/04/23 12:44:56.746
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:44:56.766
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:44:56.77
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:224
    STEP: creating the pod with failed condition 05/04/23 12:44:56.773
    May  4 12:44:56.786: INFO: Waiting up to 2m0s for pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9" in namespace "var-expansion-1085" to be "running"
    May  4 12:44:56.792: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.844603ms
    May  4 12:44:58.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011053136s
    May  4 12:45:00.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010343312s
    May  4 12:45:02.798: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011330669s
    May  4 12:45:04.796: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010220737s
    May  4 12:45:06.798: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.011254951s
    May  4 12:45:08.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.010576584s
    May  4 12:45:10.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 14.01092193s
    May  4 12:45:12.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 16.011024485s
    May  4 12:45:14.798: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 18.011384599s
    May  4 12:45:16.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 20.010797838s
    May  4 12:45:18.796: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 22.01022684s
    May  4 12:45:20.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 24.010901958s
    May  4 12:45:22.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 26.011231971s
    May  4 12:45:24.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 28.010970769s
    May  4 12:45:26.798: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 30.012127531s
    May  4 12:45:28.796: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 32.010244129s
    May  4 12:45:30.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 34.010843111s
    May  4 12:45:32.798: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 36.011359932s
    May  4 12:45:34.799: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 38.012276311s
    May  4 12:45:36.798: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 40.011959776s
    May  4 12:45:38.799: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 42.012847289s
    May  4 12:45:40.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 44.011057493s
    May  4 12:45:42.796: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 46.009968305s
    May  4 12:45:44.798: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 48.011442355s
    May  4 12:45:46.798: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 50.011886961s
    May  4 12:45:48.798: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 52.011336441s
    May  4 12:45:50.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 54.010893978s
    May  4 12:45:52.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 56.010298336s
    May  4 12:45:54.807: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 58.020554794s
    May  4 12:45:56.800: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.013398223s
    May  4 12:45:58.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.010968835s
    May  4 12:46:00.799: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.013232678s
    May  4 12:46:02.799: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.012351037s
    May  4 12:46:04.798: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.012213779s
    May  4 12:46:06.799: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.012311455s
    May  4 12:46:08.803: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.017095215s
    May  4 12:46:10.798: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.011372756s
    May  4 12:46:12.799: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.012391305s
    May  4 12:46:14.798: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.011446103s
    May  4 12:46:16.798: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.011382449s
    May  4 12:46:18.805: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.018836578s
    May  4 12:46:20.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.011141459s
    May  4 12:46:22.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.011187426s
    May  4 12:46:24.798: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.011341775s
    May  4 12:46:26.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.01122778s
    May  4 12:46:28.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.011043359s
    May  4 12:46:30.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.010845629s
    May  4 12:46:32.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.010646167s
    May  4 12:46:34.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.01050383s
    May  4 12:46:36.798: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.011430584s
    May  4 12:46:38.796: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.010192172s
    May  4 12:46:40.801: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.014280166s
    May  4 12:46:42.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.010977655s
    May  4 12:46:44.798: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.011414556s
    May  4 12:46:46.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.011027703s
    May  4 12:46:48.801: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.014267884s
    May  4 12:46:50.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.011007433s
    May  4 12:46:52.796: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.010076442s
    May  4 12:46:54.800: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.013831232s
    May  4 12:46:56.797: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.011131298s
    May  4 12:46:56.805: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.018962066s
    STEP: updating the pod 05/04/23 12:46:56.805
    May  4 12:46:57.320: INFO: Successfully updated pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9"
    STEP: waiting for pod running 05/04/23 12:46:57.32
    May  4 12:46:57.320: INFO: Waiting up to 2m0s for pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9" in namespace "var-expansion-1085" to be "running"
    May  4 12:46:57.325: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.286101ms
    May  4 12:46:59.330: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9": Phase="Running", Reason="", readiness=true. Elapsed: 2.009402258s
    May  4 12:46:59.330: INFO: Pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9" satisfied condition "running"
    STEP: deleting the pod gracefully 05/04/23 12:46:59.33
    May  4 12:46:59.330: INFO: Deleting pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9" in namespace "var-expansion-1085"
    May  4 12:46:59.339: INFO: Wait up to 5m0s for pod "var-expansion-7c3f4ea8-ea50-4fa2-9561-9bf78221c9e9" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    May  4 12:47:31.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-1085" for this suite. 05/04/23 12:47:31.358
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:47:31.369
May  4 12:47:31.369: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename deployment 05/04/23 12:47:31.37
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:47:31.389
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:47:31.394
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
May  4 12:47:31.397: INFO: Creating deployment "test-recreate-deployment"
May  4 12:47:31.404: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
May  4 12:47:31.421: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
May  4 12:47:33.430: INFO: Waiting deployment "test-recreate-deployment" to complete
May  4 12:47:33.434: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
May  4 12:47:33.444: INFO: Updating deployment test-recreate-deployment
May  4 12:47:33.444: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May  4 12:47:33.556: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-432  c1d216d7-67d9-4153-ae89-a657cf636c33 31054 2 2023-05-04 12:47:31 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-04 12:47:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 12:47:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00809c288 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-04 12:47:33 +0000 UTC,LastTransitionTime:2023-05-04 12:47:33 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-05-04 12:47:33 +0000 UTC,LastTransitionTime:2023-05-04 12:47:31 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

May  4 12:47:33.561: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-432  b68d553f-82dc-4f8f-9ee9-ef2cde6c2738 31052 1 2023-05-04 12:47:33 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment c1d216d7-67d9-4153-ae89-a657cf636c33 0xc00341e170 0xc00341e171}] [] [{kube-controller-manager Update apps/v1 2023-05-04 12:47:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c1d216d7-67d9-4153-ae89-a657cf636c33\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 12:47:33 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00341e208 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  4 12:47:33.561: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
May  4 12:47:33.561: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-432  83f2b1b1-0474-48c4-899c-10b4f489bffb 31043 2 2023-05-04 12:47:31 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment c1d216d7-67d9-4153-ae89-a657cf636c33 0xc00341e057 0xc00341e058}] [] [{kube-controller-manager Update apps/v1 2023-05-04 12:47:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c1d216d7-67d9-4153-ae89-a657cf636c33\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 12:47:33 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00341e108 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  4 12:47:33.567: INFO: Pod "test-recreate-deployment-9d58999df-mbpgf" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-9d58999df-mbpgf test-recreate-deployment-9d58999df- deployment-432  58a0df1b-1624-42f4-919d-9540d42dc2be 31055 0 2023-05-04 12:47:33 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df b68d553f-82dc-4f8f-9ee9-ef2cde6c2738 0xc00809c620 0xc00809c621}] [] [{kube-controller-manager Update v1 2023-05-04 12:47:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b68d553f-82dc-4f8f-9ee9-ef2cde6c2738\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-04 12:47:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cqjhv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cqjhv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-224.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 12:47:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 12:47:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 12:47:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 12:47:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.224,PodIP:,StartTime:2023-05-04 12:47:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
May  4 12:47:33.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-432" for this suite. 05/04/23 12:47:33.575
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","completed":211,"skipped":4036,"failed":0}
------------------------------
• [2.215 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:47:31.369
    May  4 12:47:31.369: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename deployment 05/04/23 12:47:31.37
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:47:31.389
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:47:31.394
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    May  4 12:47:31.397: INFO: Creating deployment "test-recreate-deployment"
    May  4 12:47:31.404: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    May  4 12:47:31.421: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    May  4 12:47:33.430: INFO: Waiting deployment "test-recreate-deployment" to complete
    May  4 12:47:33.434: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    May  4 12:47:33.444: INFO: Updating deployment test-recreate-deployment
    May  4 12:47:33.444: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May  4 12:47:33.556: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-432  c1d216d7-67d9-4153-ae89-a657cf636c33 31054 2 2023-05-04 12:47:31 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-04 12:47:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 12:47:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00809c288 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-04 12:47:33 +0000 UTC,LastTransitionTime:2023-05-04 12:47:33 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-05-04 12:47:33 +0000 UTC,LastTransitionTime:2023-05-04 12:47:31 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    May  4 12:47:33.561: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-432  b68d553f-82dc-4f8f-9ee9-ef2cde6c2738 31052 1 2023-05-04 12:47:33 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment c1d216d7-67d9-4153-ae89-a657cf636c33 0xc00341e170 0xc00341e171}] [] [{kube-controller-manager Update apps/v1 2023-05-04 12:47:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c1d216d7-67d9-4153-ae89-a657cf636c33\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 12:47:33 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00341e208 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    May  4 12:47:33.561: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    May  4 12:47:33.561: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-432  83f2b1b1-0474-48c4-899c-10b4f489bffb 31043 2 2023-05-04 12:47:31 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment c1d216d7-67d9-4153-ae89-a657cf636c33 0xc00341e057 0xc00341e058}] [] [{kube-controller-manager Update apps/v1 2023-05-04 12:47:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c1d216d7-67d9-4153-ae89-a657cf636c33\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 12:47:33 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00341e108 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    May  4 12:47:33.567: INFO: Pod "test-recreate-deployment-9d58999df-mbpgf" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-9d58999df-mbpgf test-recreate-deployment-9d58999df- deployment-432  58a0df1b-1624-42f4-919d-9540d42dc2be 31055 0 2023-05-04 12:47:33 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df b68d553f-82dc-4f8f-9ee9-ef2cde6c2738 0xc00809c620 0xc00809c621}] [] [{kube-controller-manager Update v1 2023-05-04 12:47:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b68d553f-82dc-4f8f-9ee9-ef2cde6c2738\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-04 12:47:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cqjhv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cqjhv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-224.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 12:47:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 12:47:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 12:47:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 12:47:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.224,PodIP:,StartTime:2023-05-04 12:47:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    May  4 12:47:33.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-432" for this suite. 05/04/23 12:47:33.575
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:47:33.586
May  4 12:47:33.586: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename events 05/04/23 12:47:33.587
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:47:33.603
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:47:33.606
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 05/04/23 12:47:33.609
May  4 12:47:33.617: INFO: created test-event-1
May  4 12:47:33.623: INFO: created test-event-2
May  4 12:47:33.628: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 05/04/23 12:47:33.628
STEP: delete collection of events 05/04/23 12:47:33.634
May  4 12:47:33.634: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 05/04/23 12:47:33.658
May  4 12:47:33.658: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
May  4 12:47:33.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5468" for this suite. 05/04/23 12:47:33.67
{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","completed":212,"skipped":4076,"failed":0}
------------------------------
• [0.094 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:47:33.586
    May  4 12:47:33.586: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename events 05/04/23 12:47:33.587
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:47:33.603
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:47:33.606
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 05/04/23 12:47:33.609
    May  4 12:47:33.617: INFO: created test-event-1
    May  4 12:47:33.623: INFO: created test-event-2
    May  4 12:47:33.628: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 05/04/23 12:47:33.628
    STEP: delete collection of events 05/04/23 12:47:33.634
    May  4 12:47:33.634: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 05/04/23 12:47:33.658
    May  4 12:47:33.658: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    May  4 12:47:33.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-5468" for this suite. 05/04/23 12:47:33.67
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:47:33.68
May  4 12:47:33.680: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename replication-controller 05/04/23 12:47:33.681
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:47:33.696
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:47:33.699
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
STEP: Given a ReplicationController is created 05/04/23 12:47:33.702
STEP: When the matched label of one of its pods change 05/04/23 12:47:33.709
May  4 12:47:33.713: INFO: Pod name pod-release: Found 0 pods out of 1
May  4 12:47:38.719: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 05/04/23 12:47:38.738
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
May  4 12:47:39.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1070" for this suite. 05/04/23 12:47:39.756
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","completed":213,"skipped":4078,"failed":0}
------------------------------
• [SLOW TEST] [6.087 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:47:33.68
    May  4 12:47:33.680: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename replication-controller 05/04/23 12:47:33.681
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:47:33.696
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:47:33.699
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:100
    STEP: Given a ReplicationController is created 05/04/23 12:47:33.702
    STEP: When the matched label of one of its pods change 05/04/23 12:47:33.709
    May  4 12:47:33.713: INFO: Pod name pod-release: Found 0 pods out of 1
    May  4 12:47:38.719: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 05/04/23 12:47:38.738
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    May  4 12:47:39.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-1070" for this suite. 05/04/23 12:47:39.756
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:47:39.767
May  4 12:47:39.767: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename proxy 05/04/23 12:47:39.769
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:47:39.786
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:47:39.792
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
May  4 12:47:39.795: INFO: Creating pod...
May  4 12:47:39.805: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-916" to be "running"
May  4 12:47:39.809: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 3.583346ms
May  4 12:47:41.815: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.009285712s
May  4 12:47:41.815: INFO: Pod "agnhost" satisfied condition "running"
May  4 12:47:41.815: INFO: Creating service...
May  4 12:47:41.830: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-916/pods/agnhost/proxy?method=DELETE
May  4 12:47:41.841: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
May  4 12:47:41.841: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-916/pods/agnhost/proxy?method=OPTIONS
May  4 12:47:41.849: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
May  4 12:47:41.849: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-916/pods/agnhost/proxy?method=PATCH
May  4 12:47:41.857: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
May  4 12:47:41.857: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-916/pods/agnhost/proxy?method=POST
May  4 12:47:41.863: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
May  4 12:47:41.863: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-916/pods/agnhost/proxy?method=PUT
May  4 12:47:41.868: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
May  4 12:47:41.868: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-916/services/e2e-proxy-test-service/proxy?method=DELETE
May  4 12:47:41.875: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
May  4 12:47:41.875: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-916/services/e2e-proxy-test-service/proxy?method=OPTIONS
May  4 12:47:41.883: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
May  4 12:47:41.883: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-916/services/e2e-proxy-test-service/proxy?method=PATCH
May  4 12:47:41.890: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
May  4 12:47:41.890: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-916/services/e2e-proxy-test-service/proxy?method=POST
May  4 12:47:41.898: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
May  4 12:47:41.898: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-916/services/e2e-proxy-test-service/proxy?method=PUT
May  4 12:47:41.908: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
May  4 12:47:41.908: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-916/pods/agnhost/proxy?method=GET
May  4 12:47:41.912: INFO: http.Client request:GET StatusCode:301
May  4 12:47:41.912: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-916/services/e2e-proxy-test-service/proxy?method=GET
May  4 12:47:41.922: INFO: http.Client request:GET StatusCode:301
May  4 12:47:41.922: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-916/pods/agnhost/proxy?method=HEAD
May  4 12:47:41.926: INFO: http.Client request:HEAD StatusCode:301
May  4 12:47:41.926: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-916/services/e2e-proxy-test-service/proxy?method=HEAD
May  4 12:47:41.932: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:187
May  4 12:47:41.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-916" for this suite. 05/04/23 12:47:41.941
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","completed":214,"skipped":4083,"failed":0}
------------------------------
• [2.184 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:47:39.767
    May  4 12:47:39.767: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename proxy 05/04/23 12:47:39.769
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:47:39.786
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:47:39.792
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    May  4 12:47:39.795: INFO: Creating pod...
    May  4 12:47:39.805: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-916" to be "running"
    May  4 12:47:39.809: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 3.583346ms
    May  4 12:47:41.815: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.009285712s
    May  4 12:47:41.815: INFO: Pod "agnhost" satisfied condition "running"
    May  4 12:47:41.815: INFO: Creating service...
    May  4 12:47:41.830: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-916/pods/agnhost/proxy?method=DELETE
    May  4 12:47:41.841: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    May  4 12:47:41.841: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-916/pods/agnhost/proxy?method=OPTIONS
    May  4 12:47:41.849: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    May  4 12:47:41.849: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-916/pods/agnhost/proxy?method=PATCH
    May  4 12:47:41.857: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    May  4 12:47:41.857: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-916/pods/agnhost/proxy?method=POST
    May  4 12:47:41.863: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    May  4 12:47:41.863: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-916/pods/agnhost/proxy?method=PUT
    May  4 12:47:41.868: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    May  4 12:47:41.868: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-916/services/e2e-proxy-test-service/proxy?method=DELETE
    May  4 12:47:41.875: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    May  4 12:47:41.875: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-916/services/e2e-proxy-test-service/proxy?method=OPTIONS
    May  4 12:47:41.883: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    May  4 12:47:41.883: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-916/services/e2e-proxy-test-service/proxy?method=PATCH
    May  4 12:47:41.890: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    May  4 12:47:41.890: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-916/services/e2e-proxy-test-service/proxy?method=POST
    May  4 12:47:41.898: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    May  4 12:47:41.898: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-916/services/e2e-proxy-test-service/proxy?method=PUT
    May  4 12:47:41.908: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    May  4 12:47:41.908: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-916/pods/agnhost/proxy?method=GET
    May  4 12:47:41.912: INFO: http.Client request:GET StatusCode:301
    May  4 12:47:41.912: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-916/services/e2e-proxy-test-service/proxy?method=GET
    May  4 12:47:41.922: INFO: http.Client request:GET StatusCode:301
    May  4 12:47:41.922: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-916/pods/agnhost/proxy?method=HEAD
    May  4 12:47:41.926: INFO: http.Client request:HEAD StatusCode:301
    May  4 12:47:41.926: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-916/services/e2e-proxy-test-service/proxy?method=HEAD
    May  4 12:47:41.932: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    May  4 12:47:41.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-916" for this suite. 05/04/23 12:47:41.941
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:47:41.953
May  4 12:47:41.953: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename configmap 05/04/23 12:47:41.954
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:47:41.976
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:47:41.981
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
STEP: Creating configMap configmap-2811/configmap-test-ef04d3f1-627b-40ba-9b0b-21626f455504 05/04/23 12:47:41.984
STEP: Creating a pod to test consume configMaps 05/04/23 12:47:41.991
May  4 12:47:42.003: INFO: Waiting up to 5m0s for pod "pod-configmaps-d6cb33fb-3f0e-40c0-aef3-49c871ba1f60" in namespace "configmap-2811" to be "Succeeded or Failed"
May  4 12:47:42.014: INFO: Pod "pod-configmaps-d6cb33fb-3f0e-40c0-aef3-49c871ba1f60": Phase="Pending", Reason="", readiness=false. Elapsed: 11.185131ms
May  4 12:47:44.019: INFO: Pod "pod-configmaps-d6cb33fb-3f0e-40c0-aef3-49c871ba1f60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016201175s
May  4 12:47:46.020: INFO: Pod "pod-configmaps-d6cb33fb-3f0e-40c0-aef3-49c871ba1f60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016895915s
STEP: Saw pod success 05/04/23 12:47:46.02
May  4 12:47:46.020: INFO: Pod "pod-configmaps-d6cb33fb-3f0e-40c0-aef3-49c871ba1f60" satisfied condition "Succeeded or Failed"
May  4 12:47:46.024: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-configmaps-d6cb33fb-3f0e-40c0-aef3-49c871ba1f60 container env-test: <nil>
STEP: delete the pod 05/04/23 12:47:46.042
May  4 12:47:46.060: INFO: Waiting for pod pod-configmaps-d6cb33fb-3f0e-40c0-aef3-49c871ba1f60 to disappear
May  4 12:47:46.064: INFO: Pod pod-configmaps-d6cb33fb-3f0e-40c0-aef3-49c871ba1f60 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
May  4 12:47:46.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2811" for this suite. 05/04/23 12:47:46.071
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","completed":215,"skipped":4098,"failed":0}
------------------------------
• [4.125 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:47:41.953
    May  4 12:47:41.953: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename configmap 05/04/23 12:47:41.954
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:47:41.976
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:47:41.981
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:92
    STEP: Creating configMap configmap-2811/configmap-test-ef04d3f1-627b-40ba-9b0b-21626f455504 05/04/23 12:47:41.984
    STEP: Creating a pod to test consume configMaps 05/04/23 12:47:41.991
    May  4 12:47:42.003: INFO: Waiting up to 5m0s for pod "pod-configmaps-d6cb33fb-3f0e-40c0-aef3-49c871ba1f60" in namespace "configmap-2811" to be "Succeeded or Failed"
    May  4 12:47:42.014: INFO: Pod "pod-configmaps-d6cb33fb-3f0e-40c0-aef3-49c871ba1f60": Phase="Pending", Reason="", readiness=false. Elapsed: 11.185131ms
    May  4 12:47:44.019: INFO: Pod "pod-configmaps-d6cb33fb-3f0e-40c0-aef3-49c871ba1f60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016201175s
    May  4 12:47:46.020: INFO: Pod "pod-configmaps-d6cb33fb-3f0e-40c0-aef3-49c871ba1f60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016895915s
    STEP: Saw pod success 05/04/23 12:47:46.02
    May  4 12:47:46.020: INFO: Pod "pod-configmaps-d6cb33fb-3f0e-40c0-aef3-49c871ba1f60" satisfied condition "Succeeded or Failed"
    May  4 12:47:46.024: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-configmaps-d6cb33fb-3f0e-40c0-aef3-49c871ba1f60 container env-test: <nil>
    STEP: delete the pod 05/04/23 12:47:46.042
    May  4 12:47:46.060: INFO: Waiting for pod pod-configmaps-d6cb33fb-3f0e-40c0-aef3-49c871ba1f60 to disappear
    May  4 12:47:46.064: INFO: Pod pod-configmaps-d6cb33fb-3f0e-40c0-aef3-49c871ba1f60 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    May  4 12:47:46.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2811" for this suite. 05/04/23 12:47:46.071
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:47:46.083
May  4 12:47:46.083: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename dns 05/04/23 12:47:46.083
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:47:46.103
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:47:46.106
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 05/04/23 12:47:46.109
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8556.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8556.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8556.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8556.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8556.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8556.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8556.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8556.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8556.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8556.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 127.240.21.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.21.240.127_udp@PTR;check="$$(dig +tcp +noall +answer +search 127.240.21.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.21.240.127_tcp@PTR;sleep 1; done
 05/04/23 12:47:46.146
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8556.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8556.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8556.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8556.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8556.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8556.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8556.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8556.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8556.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8556.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 127.240.21.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.21.240.127_udp@PTR;check="$$(dig +tcp +noall +answer +search 127.240.21.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.21.240.127_tcp@PTR;sleep 1; done
 05/04/23 12:47:46.146
STEP: creating a pod to probe DNS 05/04/23 12:47:46.146
STEP: submitting the pod to kubernetes 05/04/23 12:47:46.147
May  4 12:47:46.160: INFO: Waiting up to 15m0s for pod "dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d" in namespace "dns-8556" to be "running"
May  4 12:47:46.164: INFO: Pod "dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.074377ms
May  4 12:47:48.170: INFO: Pod "dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009286822s
May  4 12:47:50.172: INFO: Pod "dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d": Phase="Running", Reason="", readiness=true. Elapsed: 4.011102785s
May  4 12:47:50.172: INFO: Pod "dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d" satisfied condition "running"
STEP: retrieving the pod 05/04/23 12:47:50.172
STEP: looking for the results for each expected name from probers 05/04/23 12:47:50.176
May  4 12:47:50.187: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
May  4 12:47:50.193: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
May  4 12:47:50.201: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
May  4 12:47:50.228: INFO: Unable to read jessie_udp@dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
May  4 12:47:50.235: INFO: Unable to read jessie_tcp@dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
May  4 12:47:50.239: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
May  4 12:47:50.243: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
May  4 12:47:50.266: INFO: Lookups using dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d failed for: [wheezy_tcp@dns-test-service.dns-8556.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local jessie_udp@dns-test-service.dns-8556.svc.cluster.local jessie_tcp@dns-test-service.dns-8556.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local]

May  4 12:47:55.280: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
May  4 12:47:55.284: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
May  4 12:47:55.289: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
May  4 12:47:55.310: INFO: Unable to read jessie_udp@dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
May  4 12:47:55.313: INFO: Unable to read jessie_tcp@dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
May  4 12:47:55.318: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
May  4 12:47:55.322: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
May  4 12:47:55.342: INFO: Lookups using dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d failed for: [wheezy_tcp@dns-test-service.dns-8556.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local jessie_udp@dns-test-service.dns-8556.svc.cluster.local jessie_tcp@dns-test-service.dns-8556.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local]

May  4 12:48:00.275: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
May  4 12:48:00.281: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
May  4 12:48:00.286: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
May  4 12:48:00.316: INFO: Unable to read jessie_udp@dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
May  4 12:48:00.327: INFO: Unable to read jessie_tcp@dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
May  4 12:48:00.334: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
May  4 12:48:00.341: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
May  4 12:48:00.364: INFO: Lookups using dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d failed for: [wheezy_tcp@dns-test-service.dns-8556.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local jessie_udp@dns-test-service.dns-8556.svc.cluster.local jessie_tcp@dns-test-service.dns-8556.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local]

May  4 12:48:05.287: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
May  4 12:48:05.308: INFO: Unable to read jessie_udp@dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
May  4 12:48:05.318: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
May  4 12:48:05.354: INFO: Lookups using dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d failed for: [wheezy_tcp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local jessie_udp@dns-test-service.dns-8556.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local]

May  4 12:48:10.285: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
May  4 12:48:10.342: INFO: Lookups using dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d failed for: [wheezy_tcp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local]

May  4 12:48:15.290: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
May  4 12:48:15.372: INFO: Lookups using dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d failed for: [wheezy_tcp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local]

May  4 12:48:20.358: INFO: DNS probes using dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d succeeded

STEP: deleting the pod 05/04/23 12:48:20.358
STEP: deleting the test service 05/04/23 12:48:20.387
STEP: deleting the test headless service 05/04/23 12:48:20.453
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
May  4 12:48:20.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8556" for this suite. 05/04/23 12:48:20.485
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","completed":216,"skipped":4134,"failed":0}
------------------------------
• [SLOW TEST] [34.422 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:47:46.083
    May  4 12:47:46.083: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename dns 05/04/23 12:47:46.083
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:47:46.103
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:47:46.106
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 05/04/23 12:47:46.109
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8556.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8556.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8556.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8556.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8556.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8556.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8556.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8556.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8556.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8556.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 127.240.21.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.21.240.127_udp@PTR;check="$$(dig +tcp +noall +answer +search 127.240.21.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.21.240.127_tcp@PTR;sleep 1; done
     05/04/23 12:47:46.146
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8556.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8556.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8556.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8556.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8556.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8556.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8556.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8556.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8556.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8556.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 127.240.21.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.21.240.127_udp@PTR;check="$$(dig +tcp +noall +answer +search 127.240.21.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.21.240.127_tcp@PTR;sleep 1; done
     05/04/23 12:47:46.146
    STEP: creating a pod to probe DNS 05/04/23 12:47:46.146
    STEP: submitting the pod to kubernetes 05/04/23 12:47:46.147
    May  4 12:47:46.160: INFO: Waiting up to 15m0s for pod "dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d" in namespace "dns-8556" to be "running"
    May  4 12:47:46.164: INFO: Pod "dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.074377ms
    May  4 12:47:48.170: INFO: Pod "dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009286822s
    May  4 12:47:50.172: INFO: Pod "dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d": Phase="Running", Reason="", readiness=true. Elapsed: 4.011102785s
    May  4 12:47:50.172: INFO: Pod "dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d" satisfied condition "running"
    STEP: retrieving the pod 05/04/23 12:47:50.172
    STEP: looking for the results for each expected name from probers 05/04/23 12:47:50.176
    May  4 12:47:50.187: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
    May  4 12:47:50.193: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
    May  4 12:47:50.201: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
    May  4 12:47:50.228: INFO: Unable to read jessie_udp@dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
    May  4 12:47:50.235: INFO: Unable to read jessie_tcp@dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
    May  4 12:47:50.239: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
    May  4 12:47:50.243: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
    May  4 12:47:50.266: INFO: Lookups using dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d failed for: [wheezy_tcp@dns-test-service.dns-8556.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local jessie_udp@dns-test-service.dns-8556.svc.cluster.local jessie_tcp@dns-test-service.dns-8556.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local]

    May  4 12:47:55.280: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
    May  4 12:47:55.284: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
    May  4 12:47:55.289: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
    May  4 12:47:55.310: INFO: Unable to read jessie_udp@dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
    May  4 12:47:55.313: INFO: Unable to read jessie_tcp@dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
    May  4 12:47:55.318: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
    May  4 12:47:55.322: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
    May  4 12:47:55.342: INFO: Lookups using dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d failed for: [wheezy_tcp@dns-test-service.dns-8556.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local jessie_udp@dns-test-service.dns-8556.svc.cluster.local jessie_tcp@dns-test-service.dns-8556.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local]

    May  4 12:48:00.275: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
    May  4 12:48:00.281: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
    May  4 12:48:00.286: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
    May  4 12:48:00.316: INFO: Unable to read jessie_udp@dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
    May  4 12:48:00.327: INFO: Unable to read jessie_tcp@dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
    May  4 12:48:00.334: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
    May  4 12:48:00.341: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
    May  4 12:48:00.364: INFO: Lookups using dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d failed for: [wheezy_tcp@dns-test-service.dns-8556.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local jessie_udp@dns-test-service.dns-8556.svc.cluster.local jessie_tcp@dns-test-service.dns-8556.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local]

    May  4 12:48:05.287: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
    May  4 12:48:05.308: INFO: Unable to read jessie_udp@dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
    May  4 12:48:05.318: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
    May  4 12:48:05.354: INFO: Lookups using dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d failed for: [wheezy_tcp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local jessie_udp@dns-test-service.dns-8556.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local]

    May  4 12:48:10.285: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
    May  4 12:48:10.342: INFO: Lookups using dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d failed for: [wheezy_tcp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local]

    May  4 12:48:15.290: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local from pod dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d: the server could not find the requested resource (get pods dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d)
    May  4 12:48:15.372: INFO: Lookups using dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d failed for: [wheezy_tcp@_http._tcp.dns-test-service.dns-8556.svc.cluster.local]

    May  4 12:48:20.358: INFO: DNS probes using dns-8556/dns-test-da42d2a1-9867-4fa1-9712-382f621afc3d succeeded

    STEP: deleting the pod 05/04/23 12:48:20.358
    STEP: deleting the test service 05/04/23 12:48:20.387
    STEP: deleting the test headless service 05/04/23 12:48:20.453
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    May  4 12:48:20.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-8556" for this suite. 05/04/23 12:48:20.485
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:48:20.506
May  4 12:48:20.506: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename crd-publish-openapi 05/04/23 12:48:20.507
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:48:20.622
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:48:20.63
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
May  4 12:48:20.644: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 05/04/23 12:48:27.348
May  4 12:48:27.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-7110 --namespace=crd-publish-openapi-7110 create -f -'
May  4 12:48:28.350: INFO: stderr: ""
May  4 12:48:28.350: INFO: stdout: "e2e-test-crd-publish-openapi-4456-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May  4 12:48:28.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-7110 --namespace=crd-publish-openapi-7110 delete e2e-test-crd-publish-openapi-4456-crds test-cr'
May  4 12:48:28.514: INFO: stderr: ""
May  4 12:48:28.514: INFO: stdout: "e2e-test-crd-publish-openapi-4456-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
May  4 12:48:28.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-7110 --namespace=crd-publish-openapi-7110 apply -f -'
May  4 12:48:29.487: INFO: stderr: ""
May  4 12:48:29.487: INFO: stdout: "e2e-test-crd-publish-openapi-4456-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May  4 12:48:29.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-7110 --namespace=crd-publish-openapi-7110 delete e2e-test-crd-publish-openapi-4456-crds test-cr'
May  4 12:48:29.583: INFO: stderr: ""
May  4 12:48:29.583: INFO: stdout: "e2e-test-crd-publish-openapi-4456-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 05/04/23 12:48:29.584
May  4 12:48:29.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-7110 explain e2e-test-crd-publish-openapi-4456-crds'
May  4 12:48:30.593: INFO: stderr: ""
May  4 12:48:30.593: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4456-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  4 12:48:37.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7110" for this suite. 05/04/23 12:48:37.019
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","completed":217,"skipped":4149,"failed":0}
------------------------------
• [SLOW TEST] [16.521 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:48:20.506
    May  4 12:48:20.506: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename crd-publish-openapi 05/04/23 12:48:20.507
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:48:20.622
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:48:20.63
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:152
    May  4 12:48:20.644: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 05/04/23 12:48:27.348
    May  4 12:48:27.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-7110 --namespace=crd-publish-openapi-7110 create -f -'
    May  4 12:48:28.350: INFO: stderr: ""
    May  4 12:48:28.350: INFO: stdout: "e2e-test-crd-publish-openapi-4456-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    May  4 12:48:28.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-7110 --namespace=crd-publish-openapi-7110 delete e2e-test-crd-publish-openapi-4456-crds test-cr'
    May  4 12:48:28.514: INFO: stderr: ""
    May  4 12:48:28.514: INFO: stdout: "e2e-test-crd-publish-openapi-4456-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    May  4 12:48:28.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-7110 --namespace=crd-publish-openapi-7110 apply -f -'
    May  4 12:48:29.487: INFO: stderr: ""
    May  4 12:48:29.487: INFO: stdout: "e2e-test-crd-publish-openapi-4456-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    May  4 12:48:29.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-7110 --namespace=crd-publish-openapi-7110 delete e2e-test-crd-publish-openapi-4456-crds test-cr'
    May  4 12:48:29.583: INFO: stderr: ""
    May  4 12:48:29.583: INFO: stdout: "e2e-test-crd-publish-openapi-4456-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 05/04/23 12:48:29.584
    May  4 12:48:29.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=crd-publish-openapi-7110 explain e2e-test-crd-publish-openapi-4456-crds'
    May  4 12:48:30.593: INFO: stderr: ""
    May  4 12:48:30.593: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4456-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  4 12:48:37.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-7110" for this suite. 05/04/23 12:48:37.019
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:48:37.028
May  4 12:48:37.028: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename custom-resource-definition 05/04/23 12:48:37.029
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:48:37.046
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:48:37.051
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
May  4 12:48:37.054: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  4 12:48:37.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6596" for this suite. 05/04/23 12:48:37.63
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","completed":218,"skipped":4149,"failed":0}
------------------------------
• [0.616 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:48:37.028
    May  4 12:48:37.028: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename custom-resource-definition 05/04/23 12:48:37.029
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:48:37.046
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:48:37.051
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    May  4 12:48:37.054: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  4 12:48:37.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-6596" for this suite. 05/04/23 12:48:37.63
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:48:37.644
May  4 12:48:37.644: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename containers 05/04/23 12:48:37.645
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:48:37.669
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:48:37.678
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
STEP: Creating a pod to test override all 05/04/23 12:48:37.682
May  4 12:48:37.697: INFO: Waiting up to 5m0s for pod "client-containers-15270815-e1bb-405f-ac54-fc7405285e54" in namespace "containers-5708" to be "Succeeded or Failed"
May  4 12:48:37.703: INFO: Pod "client-containers-15270815-e1bb-405f-ac54-fc7405285e54": Phase="Pending", Reason="", readiness=false. Elapsed: 5.295166ms
May  4 12:48:39.708: INFO: Pod "client-containers-15270815-e1bb-405f-ac54-fc7405285e54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010601294s
May  4 12:48:41.708: INFO: Pod "client-containers-15270815-e1bb-405f-ac54-fc7405285e54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010286181s
STEP: Saw pod success 05/04/23 12:48:41.708
May  4 12:48:41.708: INFO: Pod "client-containers-15270815-e1bb-405f-ac54-fc7405285e54" satisfied condition "Succeeded or Failed"
May  4 12:48:41.712: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod client-containers-15270815-e1bb-405f-ac54-fc7405285e54 container agnhost-container: <nil>
STEP: delete the pod 05/04/23 12:48:41.727
May  4 12:48:41.749: INFO: Waiting for pod client-containers-15270815-e1bb-405f-ac54-fc7405285e54 to disappear
May  4 12:48:41.752: INFO: Pod client-containers-15270815-e1bb-405f-ac54-fc7405285e54 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
May  4 12:48:41.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5708" for this suite. 05/04/23 12:48:41.761
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","completed":219,"skipped":4154,"failed":0}
------------------------------
• [4.124 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:48:37.644
    May  4 12:48:37.644: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename containers 05/04/23 12:48:37.645
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:48:37.669
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:48:37.678
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:86
    STEP: Creating a pod to test override all 05/04/23 12:48:37.682
    May  4 12:48:37.697: INFO: Waiting up to 5m0s for pod "client-containers-15270815-e1bb-405f-ac54-fc7405285e54" in namespace "containers-5708" to be "Succeeded or Failed"
    May  4 12:48:37.703: INFO: Pod "client-containers-15270815-e1bb-405f-ac54-fc7405285e54": Phase="Pending", Reason="", readiness=false. Elapsed: 5.295166ms
    May  4 12:48:39.708: INFO: Pod "client-containers-15270815-e1bb-405f-ac54-fc7405285e54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010601294s
    May  4 12:48:41.708: INFO: Pod "client-containers-15270815-e1bb-405f-ac54-fc7405285e54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010286181s
    STEP: Saw pod success 05/04/23 12:48:41.708
    May  4 12:48:41.708: INFO: Pod "client-containers-15270815-e1bb-405f-ac54-fc7405285e54" satisfied condition "Succeeded or Failed"
    May  4 12:48:41.712: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod client-containers-15270815-e1bb-405f-ac54-fc7405285e54 container agnhost-container: <nil>
    STEP: delete the pod 05/04/23 12:48:41.727
    May  4 12:48:41.749: INFO: Waiting for pod client-containers-15270815-e1bb-405f-ac54-fc7405285e54 to disappear
    May  4 12:48:41.752: INFO: Pod client-containers-15270815-e1bb-405f-ac54-fc7405285e54 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    May  4 12:48:41.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-5708" for this suite. 05/04/23 12:48:41.761
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:48:41.769
May  4 12:48:41.769: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename proxy 05/04/23 12:48:41.77
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:48:41.792
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:48:41.795
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 05/04/23 12:48:41.817
STEP: creating replication controller proxy-service-p68ns in namespace proxy-6780 05/04/23 12:48:41.817
I0504 12:48:41.836549      21 runners.go:193] Created replication controller with name: proxy-service-p68ns, namespace: proxy-6780, replica count: 1
I0504 12:48:42.887790      21 runners.go:193] proxy-service-p68ns Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0504 12:48:43.887983      21 runners.go:193] proxy-service-p68ns Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  4 12:48:43.891: INFO: setup took 2.092614921s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 05/04/23 12:48:43.891
May  4 12:48:43.904: INFO: (0) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 12.919351ms)
May  4 12:48:43.904: INFO: (0) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">... (200; 13.037971ms)
May  4 12:48:43.905: INFO: (0) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 13.452237ms)
May  4 12:48:43.905: INFO: (0) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 13.538841ms)
May  4 12:48:43.905: INFO: (0) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 14.002023ms)
May  4 12:48:43.905: INFO: (0) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">test<... (200; 13.957956ms)
May  4 12:48:43.907: INFO: (0) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/rewriteme">test</a> (200; 15.847026ms)
May  4 12:48:43.911: INFO: (0) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:462/proxy/: tls qux (200; 19.899334ms)
May  4 12:48:43.911: INFO: (0) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:460/proxy/: tls baz (200; 19.908699ms)
May  4 12:48:43.911: INFO: (0) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/tlsrewritem... (200; 19.92727ms)
May  4 12:48:43.911: INFO: (0) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname1/proxy/: foo (200; 20.010127ms)
May  4 12:48:43.911: INFO: (0) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname2/proxy/: bar (200; 19.931673ms)
May  4 12:48:43.911: INFO: (0) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname2/proxy/: bar (200; 20.080711ms)
May  4 12:48:43.911: INFO: (0) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname2/proxy/: tls qux (200; 20.008085ms)
May  4 12:48:43.911: INFO: (0) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname1/proxy/: foo (200; 20.081786ms)
May  4 12:48:43.911: INFO: (0) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname1/proxy/: tls baz (200; 20.010556ms)
May  4 12:48:43.925: INFO: (1) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">test<... (200; 13.942093ms)
May  4 12:48:43.925: INFO: (1) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 13.744374ms)
May  4 12:48:43.931: INFO: (1) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/tlsrewritem... (200; 19.02159ms)
May  4 12:48:43.931: INFO: (1) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 19.084652ms)
May  4 12:48:43.931: INFO: (1) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:460/proxy/: tls baz (200; 19.004034ms)
May  4 12:48:43.931: INFO: (1) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">... (200; 19.038367ms)
May  4 12:48:43.931: INFO: (1) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 18.903664ms)
May  4 12:48:43.931: INFO: (1) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/rewriteme">test</a> (200; 18.994737ms)
May  4 12:48:43.932: INFO: (1) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:462/proxy/: tls qux (200; 20.27354ms)
May  4 12:48:43.932: INFO: (1) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 20.277902ms)
May  4 12:48:43.937: INFO: (1) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname2/proxy/: tls qux (200; 25.382628ms)
May  4 12:48:43.942: INFO: (1) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname1/proxy/: foo (200; 30.780286ms)
May  4 12:48:43.942: INFO: (1) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname1/proxy/: tls baz (200; 30.746936ms)
May  4 12:48:43.942: INFO: (1) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname1/proxy/: foo (200; 30.795089ms)
May  4 12:48:43.943: INFO: (1) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname2/proxy/: bar (200; 31.244246ms)
May  4 12:48:43.943: INFO: (1) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname2/proxy/: bar (200; 31.329511ms)
May  4 12:48:43.955: INFO: (2) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 10.808513ms)
May  4 12:48:43.955: INFO: (2) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 11.607014ms)
May  4 12:48:43.957: INFO: (2) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">test<... (200; 12.731337ms)
May  4 12:48:43.957: INFO: (2) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:462/proxy/: tls qux (200; 13.861294ms)
May  4 12:48:43.957: INFO: (2) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/tlsrewritem... (200; 14.269053ms)
May  4 12:48:43.958: INFO: (2) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">... (200; 13.686706ms)
May  4 12:48:43.962: INFO: (2) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:460/proxy/: tls baz (200; 18.914941ms)
May  4 12:48:43.962: INFO: (2) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 19.014832ms)
May  4 12:48:43.962: INFO: (2) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/rewriteme">test</a> (200; 19.193833ms)
May  4 12:48:43.962: INFO: (2) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname2/proxy/: bar (200; 18.823908ms)
May  4 12:48:43.962: INFO: (2) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname1/proxy/: foo (200; 18.426244ms)
May  4 12:48:43.962: INFO: (2) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 18.748894ms)
May  4 12:48:43.964: INFO: (2) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname2/proxy/: bar (200; 20.609022ms)
May  4 12:48:43.964: INFO: (2) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname1/proxy/: foo (200; 20.227402ms)
May  4 12:48:43.964: INFO: (2) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname1/proxy/: tls baz (200; 20.600521ms)
May  4 12:48:43.965: INFO: (2) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname2/proxy/: tls qux (200; 21.209678ms)
May  4 12:48:43.974: INFO: (3) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 8.158053ms)
May  4 12:48:43.977: INFO: (3) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">... (200; 11.186349ms)
May  4 12:48:43.977: INFO: (3) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/rewriteme">test</a> (200; 11.698574ms)
May  4 12:48:43.977: INFO: (3) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">test<... (200; 11.147371ms)
May  4 12:48:43.977: INFO: (3) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 11.196405ms)
May  4 12:48:43.977: INFO: (3) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:460/proxy/: tls baz (200; 11.589585ms)
May  4 12:48:43.977: INFO: (3) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 11.30288ms)
May  4 12:48:43.980: INFO: (3) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 14.166566ms)
May  4 12:48:43.981: INFO: (3) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:462/proxy/: tls qux (200; 14.956907ms)
May  4 12:48:43.982: INFO: (3) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/tlsrewritem... (200; 16.888313ms)
May  4 12:48:43.982: INFO: (3) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname2/proxy/: bar (200; 16.431843ms)
May  4 12:48:43.983: INFO: (3) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname2/proxy/: bar (200; 17.331657ms)
May  4 12:48:43.984: INFO: (3) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname1/proxy/: tls baz (200; 18.500472ms)
May  4 12:48:43.985: INFO: (3) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname1/proxy/: foo (200; 19.624434ms)
May  4 12:48:43.986: INFO: (3) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname2/proxy/: tls qux (200; 19.617492ms)
May  4 12:48:43.987: INFO: (3) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname1/proxy/: foo (200; 21.405205ms)
May  4 12:48:43.993: INFO: (4) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:462/proxy/: tls qux (200; 5.317405ms)
May  4 12:48:43.999: INFO: (4) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">test<... (200; 11.670244ms)
May  4 12:48:43.999: INFO: (4) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:460/proxy/: tls baz (200; 11.672626ms)
May  4 12:48:43.999: INFO: (4) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 11.639023ms)
May  4 12:48:43.999: INFO: (4) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 11.716416ms)
May  4 12:48:43.999: INFO: (4) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/tlsrewritem... (200; 11.725065ms)
May  4 12:48:43.999: INFO: (4) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/rewriteme">test</a> (200; 11.876896ms)
May  4 12:48:43.999: INFO: (4) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 11.84337ms)
May  4 12:48:43.999: INFO: (4) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 11.867946ms)
May  4 12:48:43.999: INFO: (4) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">... (200; 11.921082ms)
May  4 12:48:44.002: INFO: (4) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname1/proxy/: foo (200; 14.149029ms)
May  4 12:48:44.003: INFO: (4) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname1/proxy/: tls baz (200; 15.809549ms)
May  4 12:48:44.004: INFO: (4) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname1/proxy/: foo (200; 16.832055ms)
May  4 12:48:44.005: INFO: (4) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname2/proxy/: tls qux (200; 17.23936ms)
May  4 12:48:44.005: INFO: (4) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname2/proxy/: bar (200; 17.271526ms)
May  4 12:48:44.005: INFO: (4) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname2/proxy/: bar (200; 17.33085ms)
May  4 12:48:44.015: INFO: (5) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 9.815286ms)
May  4 12:48:44.015: INFO: (5) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">... (200; 9.846027ms)
May  4 12:48:44.018: INFO: (5) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 13.111261ms)
May  4 12:48:44.018: INFO: (5) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 13.051482ms)
May  4 12:48:44.020: INFO: (5) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:460/proxy/: tls baz (200; 14.435602ms)
May  4 12:48:44.020: INFO: (5) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:462/proxy/: tls qux (200; 14.824213ms)
May  4 12:48:44.020: INFO: (5) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 14.913627ms)
May  4 12:48:44.021: INFO: (5) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/rewriteme">test</a> (200; 15.706784ms)
May  4 12:48:44.021: INFO: (5) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/tlsrewritem... (200; 16.036897ms)
May  4 12:48:44.022: INFO: (5) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">test<... (200; 16.446894ms)
May  4 12:48:44.023: INFO: (5) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname1/proxy/: tls baz (200; 17.475753ms)
May  4 12:48:44.026: INFO: (5) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname1/proxy/: foo (200; 21.028408ms)
May  4 12:48:44.026: INFO: (5) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname2/proxy/: bar (200; 21.177995ms)
May  4 12:48:44.027: INFO: (5) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname1/proxy/: foo (200; 21.569809ms)
May  4 12:48:44.027: INFO: (5) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname2/proxy/: bar (200; 22.116497ms)
May  4 12:48:44.029: INFO: (5) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname2/proxy/: tls qux (200; 24.212815ms)
May  4 12:48:44.040: INFO: (6) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">test<... (200; 10.970431ms)
May  4 12:48:44.043: INFO: (6) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 13.209724ms)
May  4 12:48:44.044: INFO: (6) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname1/proxy/: foo (200; 13.911079ms)
May  4 12:48:44.044: INFO: (6) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 14.110507ms)
May  4 12:48:44.044: INFO: (6) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:462/proxy/: tls qux (200; 14.041969ms)
May  4 12:48:44.044: INFO: (6) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 14.007933ms)
May  4 12:48:44.044: INFO: (6) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 14.149829ms)
May  4 12:48:44.044: INFO: (6) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">... (200; 14.016366ms)
May  4 12:48:44.044: INFO: (6) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/rewriteme">test</a> (200; 14.670855ms)
May  4 12:48:44.045: INFO: (6) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/tlsrewritem... (200; 15.879505ms)
May  4 12:48:44.045: INFO: (6) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:460/proxy/: tls baz (200; 15.712092ms)
May  4 12:48:44.048: INFO: (6) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname1/proxy/: foo (200; 18.108651ms)
May  4 12:48:44.056: INFO: (6) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname2/proxy/: tls qux (200; 26.203725ms)
May  4 12:48:44.056: INFO: (6) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname2/proxy/: bar (200; 26.406708ms)
May  4 12:48:44.061: INFO: (6) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname1/proxy/: tls baz (200; 31.523572ms)
May  4 12:48:44.061: INFO: (6) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname2/proxy/: bar (200; 31.639775ms)
May  4 12:48:44.081: INFO: (7) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/tlsrewritem... (200; 19.610298ms)
May  4 12:48:44.087: INFO: (7) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/rewriteme">test</a> (200; 25.141394ms)
May  4 12:48:44.090: INFO: (7) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 28.215602ms)
May  4 12:48:44.090: INFO: (7) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">... (200; 27.935474ms)
May  4 12:48:44.090: INFO: (7) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 28.010792ms)
May  4 12:48:44.090: INFO: (7) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">test<... (200; 28.320148ms)
May  4 12:48:44.090: INFO: (7) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 28.576448ms)
May  4 12:48:44.090: INFO: (7) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:462/proxy/: tls qux (200; 28.441898ms)
May  4 12:48:44.091: INFO: (7) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:460/proxy/: tls baz (200; 28.971297ms)
May  4 12:48:44.091: INFO: (7) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 29.290436ms)
May  4 12:48:44.095: INFO: (7) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname1/proxy/: tls baz (200; 33.472504ms)
May  4 12:48:44.099: INFO: (7) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname1/proxy/: foo (200; 37.19591ms)
May  4 12:48:44.099: INFO: (7) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname2/proxy/: tls qux (200; 37.0841ms)
May  4 12:48:44.099: INFO: (7) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname2/proxy/: bar (200; 37.130503ms)
May  4 12:48:44.099: INFO: (7) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname1/proxy/: foo (200; 37.34562ms)
May  4 12:48:44.099: INFO: (7) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname2/proxy/: bar (200; 37.588633ms)
May  4 12:48:44.115: INFO: (8) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 15.70388ms)
May  4 12:48:44.115: INFO: (8) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 15.718611ms)
May  4 12:48:44.115: INFO: (8) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 15.764067ms)
May  4 12:48:44.115: INFO: (8) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 15.812522ms)
May  4 12:48:44.115: INFO: (8) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:460/proxy/: tls baz (200; 15.980139ms)
May  4 12:48:44.115: INFO: (8) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/rewriteme">test</a> (200; 15.941345ms)
May  4 12:48:44.115: INFO: (8) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:462/proxy/: tls qux (200; 16.08595ms)
May  4 12:48:44.116: INFO: (8) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">... (200; 16.705899ms)
May  4 12:48:44.116: INFO: (8) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">test<... (200; 16.744011ms)
May  4 12:48:44.116: INFO: (8) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/tlsrewritem... (200; 17.087342ms)
May  4 12:48:44.117: INFO: (8) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname2/proxy/: bar (200; 18.100764ms)
May  4 12:48:44.119: INFO: (8) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname2/proxy/: bar (200; 19.839349ms)
May  4 12:48:44.120: INFO: (8) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname1/proxy/: foo (200; 20.252148ms)
May  4 12:48:44.120: INFO: (8) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname1/proxy/: foo (200; 20.102554ms)
May  4 12:48:44.120: INFO: (8) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname1/proxy/: tls baz (200; 20.000206ms)
May  4 12:48:44.120: INFO: (8) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname2/proxy/: tls qux (200; 20.150256ms)
May  4 12:48:44.132: INFO: (9) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">test<... (200; 11.550425ms)
May  4 12:48:44.133: INFO: (9) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 13.374639ms)
May  4 12:48:44.133: INFO: (9) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/tlsrewritem... (200; 13.076874ms)
May  4 12:48:44.133: INFO: (9) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:460/proxy/: tls baz (200; 13.542408ms)
May  4 12:48:44.133: INFO: (9) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 13.594357ms)
May  4 12:48:44.133: INFO: (9) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 13.418731ms)
May  4 12:48:44.133: INFO: (9) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">... (200; 13.627125ms)
May  4 12:48:44.133: INFO: (9) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/rewriteme">test</a> (200; 13.113804ms)
May  4 12:48:44.133: INFO: (9) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 13.251413ms)
May  4 12:48:44.133: INFO: (9) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:462/proxy/: tls qux (200; 13.624311ms)
May  4 12:48:44.136: INFO: (9) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname2/proxy/: bar (200; 16.775393ms)
May  4 12:48:44.139: INFO: (9) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname1/proxy/: foo (200; 19.294818ms)
May  4 12:48:44.140: INFO: (9) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname2/proxy/: tls qux (200; 19.524336ms)
May  4 12:48:44.140: INFO: (9) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname1/proxy/: tls baz (200; 19.748165ms)
May  4 12:48:44.140: INFO: (9) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname1/proxy/: foo (200; 19.978925ms)
May  4 12:48:44.140: INFO: (9) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname2/proxy/: bar (200; 19.791987ms)
May  4 12:48:44.152: INFO: (10) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/rewriteme">test</a> (200; 12.15117ms)
May  4 12:48:44.152: INFO: (10) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 12.114899ms)
May  4 12:48:44.156: INFO: (10) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 15.632078ms)
May  4 12:48:44.156: INFO: (10) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/tlsrewritem... (200; 15.934072ms)
May  4 12:48:44.156: INFO: (10) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">test<... (200; 16.030966ms)
May  4 12:48:44.157: INFO: (10) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 16.177559ms)
May  4 12:48:44.157: INFO: (10) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">... (200; 16.294827ms)
May  4 12:48:44.157: INFO: (10) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:460/proxy/: tls baz (200; 16.601914ms)
May  4 12:48:44.157: INFO: (10) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:462/proxy/: tls qux (200; 16.657327ms)
May  4 12:48:44.157: INFO: (10) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname1/proxy/: foo (200; 17.140402ms)
May  4 12:48:44.161: INFO: (10) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 20.672753ms)
May  4 12:48:44.162: INFO: (10) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname2/proxy/: tls qux (200; 21.861478ms)
May  4 12:48:44.163: INFO: (10) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname2/proxy/: bar (200; 22.45759ms)
May  4 12:48:44.164: INFO: (10) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname1/proxy/: tls baz (200; 23.203317ms)
May  4 12:48:44.164: INFO: (10) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname1/proxy/: foo (200; 23.135627ms)
May  4 12:48:44.164: INFO: (10) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname2/proxy/: bar (200; 23.205011ms)
May  4 12:48:44.173: INFO: (11) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 9.486912ms)
May  4 12:48:44.173: INFO: (11) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 9.644175ms)
May  4 12:48:44.173: INFO: (11) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:460/proxy/: tls baz (200; 9.804631ms)
May  4 12:48:44.173: INFO: (11) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 9.715833ms)
May  4 12:48:44.176: INFO: (11) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/tlsrewritem... (200; 12.494636ms)
May  4 12:48:44.176: INFO: (11) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">test<... (200; 12.301789ms)
May  4 12:48:44.176: INFO: (11) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/rewriteme">test</a> (200; 12.50169ms)
May  4 12:48:44.176: INFO: (11) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">... (200; 12.663529ms)
May  4 12:48:44.176: INFO: (11) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 12.508755ms)
May  4 12:48:44.177: INFO: (11) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:462/proxy/: tls qux (200; 12.722362ms)
May  4 12:48:44.179: INFO: (11) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname2/proxy/: bar (200; 15.269833ms)
May  4 12:48:44.186: INFO: (11) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname2/proxy/: tls qux (200; 22.567538ms)
May  4 12:48:44.186: INFO: (11) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname1/proxy/: foo (200; 22.616785ms)
May  4 12:48:44.187: INFO: (11) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname2/proxy/: bar (200; 23.484342ms)
May  4 12:48:44.187: INFO: (11) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname1/proxy/: tls baz (200; 23.795063ms)
May  4 12:48:44.187: INFO: (11) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname1/proxy/: foo (200; 23.685235ms)
May  4 12:48:44.201: INFO: (12) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 13.01941ms)
May  4 12:48:44.201: INFO: (12) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 13.059764ms)
May  4 12:48:44.201: INFO: (12) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname2/proxy/: tls qux (200; 13.176564ms)
May  4 12:48:44.205: INFO: (12) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/rewriteme">test</a> (200; 17.763741ms)
May  4 12:48:44.206: INFO: (12) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:460/proxy/: tls baz (200; 17.983217ms)
May  4 12:48:44.206: INFO: (12) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">test<... (200; 18.842356ms)
May  4 12:48:44.207: INFO: (12) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">... (200; 19.847904ms)
May  4 12:48:44.207: INFO: (12) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname2/proxy/: bar (200; 19.807604ms)
May  4 12:48:44.207: INFO: (12) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 19.737111ms)
May  4 12:48:44.208: INFO: (12) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 19.947943ms)
May  4 12:48:44.208: INFO: (12) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/tlsrewritem... (200; 20.169943ms)
May  4 12:48:44.208: INFO: (12) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname1/proxy/: foo (200; 20.144951ms)
May  4 12:48:44.208: INFO: (12) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:462/proxy/: tls qux (200; 20.21608ms)
May  4 12:48:44.208: INFO: (12) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname1/proxy/: foo (200; 20.293595ms)
May  4 12:48:44.208: INFO: (12) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname1/proxy/: tls baz (200; 20.457493ms)
May  4 12:48:44.208: INFO: (12) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname2/proxy/: bar (200; 20.958726ms)
May  4 12:48:44.243: INFO: (13) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/rewriteme">test</a> (200; 34.970257ms)
May  4 12:48:44.259: INFO: (13) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:460/proxy/: tls baz (200; 50.198318ms)
May  4 12:48:44.259: INFO: (13) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 50.236173ms)
May  4 12:48:44.259: INFO: (13) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">test<... (200; 49.822741ms)
May  4 12:48:44.259: INFO: (13) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 50.15489ms)
May  4 12:48:44.259: INFO: (13) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">... (200; 50.188765ms)
May  4 12:48:44.261: INFO: (13) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/tlsrewritem... (200; 52.915745ms)
May  4 12:48:44.261: INFO: (13) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 52.678945ms)
May  4 12:48:44.261: INFO: (13) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 52.650346ms)
May  4 12:48:44.262: INFO: (13) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:462/proxy/: tls qux (200; 52.853829ms)
May  4 12:48:44.266: INFO: (13) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname2/proxy/: bar (200; 57.636477ms)
May  4 12:48:44.266: INFO: (13) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname1/proxy/: foo (200; 57.453546ms)
May  4 12:48:44.267: INFO: (13) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname1/proxy/: foo (200; 57.785228ms)
May  4 12:48:44.267: INFO: (13) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname1/proxy/: tls baz (200; 57.955997ms)
May  4 12:48:44.267: INFO: (13) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname2/proxy/: bar (200; 58.215817ms)
May  4 12:48:44.267: INFO: (13) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname2/proxy/: tls qux (200; 58.179841ms)
May  4 12:48:44.290: INFO: (14) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">test<... (200; 22.133884ms)
May  4 12:48:44.290: INFO: (14) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:462/proxy/: tls qux (200; 22.024193ms)
May  4 12:48:44.290: INFO: (14) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:460/proxy/: tls baz (200; 22.060995ms)
May  4 12:48:44.290: INFO: (14) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 22.261695ms)
May  4 12:48:44.290: INFO: (14) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 22.492845ms)
May  4 12:48:44.290: INFO: (14) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 22.52325ms)
May  4 12:48:44.291: INFO: (14) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/rewriteme">test</a> (200; 23.596671ms)
May  4 12:48:44.292: INFO: (14) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 25.145759ms)
May  4 12:48:44.292: INFO: (14) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">... (200; 25.031809ms)
May  4 12:48:44.292: INFO: (14) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/tlsrewritem... (200; 24.971958ms)
May  4 12:48:44.292: INFO: (14) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname1/proxy/: foo (200; 25.253483ms)
May  4 12:48:44.292: INFO: (14) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname1/proxy/: tls baz (200; 24.854773ms)
May  4 12:48:44.293: INFO: (14) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname2/proxy/: bar (200; 25.465403ms)
May  4 12:48:44.294: INFO: (14) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname2/proxy/: bar (200; 25.916259ms)
May  4 12:48:44.294: INFO: (14) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname2/proxy/: tls qux (200; 26.469844ms)
May  4 12:48:44.294: INFO: (14) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname1/proxy/: foo (200; 26.781731ms)
May  4 12:48:44.323: INFO: (15) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">test<... (200; 28.337319ms)
May  4 12:48:44.323: INFO: (15) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 28.27784ms)
May  4 12:48:44.323: INFO: (15) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 28.514669ms)
May  4 12:48:44.324: INFO: (15) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 28.989227ms)
May  4 12:48:44.324: INFO: (15) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:460/proxy/: tls baz (200; 29.178517ms)
May  4 12:48:44.324: INFO: (15) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:462/proxy/: tls qux (200; 29.195867ms)
May  4 12:48:44.324: INFO: (15) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/rewriteme">test</a> (200; 29.585984ms)
May  4 12:48:44.324: INFO: (15) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/tlsrewritem... (200; 29.837334ms)
May  4 12:48:44.325: INFO: (15) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname1/proxy/: foo (200; 30.569753ms)
May  4 12:48:44.325: INFO: (15) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">... (200; 30.747265ms)
May  4 12:48:44.327: INFO: (15) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 31.994015ms)
May  4 12:48:44.331: INFO: (15) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname2/proxy/: bar (200; 36.380092ms)
May  4 12:48:44.331: INFO: (15) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname2/proxy/: tls qux (200; 36.640639ms)
May  4 12:48:44.331: INFO: (15) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname2/proxy/: bar (200; 36.736452ms)
May  4 12:48:44.333: INFO: (15) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname1/proxy/: tls baz (200; 37.946054ms)
May  4 12:48:44.333: INFO: (15) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname1/proxy/: foo (200; 38.105402ms)
May  4 12:48:44.349: INFO: (16) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 16.037197ms)
May  4 12:48:44.350: INFO: (16) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:462/proxy/: tls qux (200; 16.897346ms)
May  4 12:48:44.351: INFO: (16) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/rewriteme">test</a> (200; 17.927219ms)
May  4 12:48:44.351: INFO: (16) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 18.044297ms)
May  4 12:48:44.351: INFO: (16) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/tlsrewritem... (200; 17.955957ms)
May  4 12:48:44.351: INFO: (16) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:460/proxy/: tls baz (200; 17.856873ms)
May  4 12:48:44.351: INFO: (16) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname1/proxy/: foo (200; 18.215336ms)
May  4 12:48:44.351: INFO: (16) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 18.333041ms)
May  4 12:48:44.351: INFO: (16) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">test<... (200; 18.092578ms)
May  4 12:48:44.352: INFO: (16) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 18.666412ms)
May  4 12:48:44.353: INFO: (16) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname1/proxy/: foo (200; 19.356876ms)
May  4 12:48:44.353: INFO: (16) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">... (200; 19.187687ms)
May  4 12:48:44.353: INFO: (16) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname2/proxy/: bar (200; 19.636466ms)
May  4 12:48:44.353: INFO: (16) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname1/proxy/: tls baz (200; 19.734576ms)
May  4 12:48:44.353: INFO: (16) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname2/proxy/: bar (200; 20.159416ms)
May  4 12:48:44.353: INFO: (16) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname2/proxy/: tls qux (200; 19.936776ms)
May  4 12:48:44.358: INFO: (17) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 4.721425ms)
May  4 12:48:44.360: INFO: (17) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:460/proxy/: tls baz (200; 6.856701ms)
May  4 12:48:44.363: INFO: (17) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/rewriteme">test</a> (200; 9.822088ms)
May  4 12:48:44.366: INFO: (17) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">... (200; 12.853866ms)
May  4 12:48:44.367: INFO: (17) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:462/proxy/: tls qux (200; 13.107572ms)
May  4 12:48:44.367: INFO: (17) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">test<... (200; 13.262641ms)
May  4 12:48:44.367: INFO: (17) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 13.236891ms)
May  4 12:48:44.367: INFO: (17) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/tlsrewritem... (200; 13.446236ms)
May  4 12:48:44.368: INFO: (17) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 14.445663ms)
May  4 12:48:44.368: INFO: (17) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 15.018338ms)
May  4 12:48:44.369: INFO: (17) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname2/proxy/: bar (200; 15.964093ms)
May  4 12:48:44.373: INFO: (17) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname1/proxy/: tls baz (200; 19.13851ms)
May  4 12:48:44.373: INFO: (17) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname1/proxy/: foo (200; 19.336428ms)
May  4 12:48:44.373: INFO: (17) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname2/proxy/: bar (200; 19.365131ms)
May  4 12:48:44.373: INFO: (17) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname1/proxy/: foo (200; 19.322753ms)
May  4 12:48:44.373: INFO: (17) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname2/proxy/: tls qux (200; 19.398799ms)
May  4 12:48:44.389: INFO: (18) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/tlsrewritem... (200; 16.220694ms)
May  4 12:48:44.390: INFO: (18) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 16.704117ms)
May  4 12:48:44.390: INFO: (18) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">test<... (200; 16.565804ms)
May  4 12:48:44.390: INFO: (18) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 16.617038ms)
May  4 12:48:44.390: INFO: (18) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/rewriteme">test</a> (200; 16.633017ms)
May  4 12:48:44.390: INFO: (18) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 16.757682ms)
May  4 12:48:44.390: INFO: (18) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname2/proxy/: tls qux (200; 16.689935ms)
May  4 12:48:44.390: INFO: (18) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:460/proxy/: tls baz (200; 16.750131ms)
May  4 12:48:44.390: INFO: (18) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 17.005356ms)
May  4 12:48:44.390: INFO: (18) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:462/proxy/: tls qux (200; 17.025304ms)
May  4 12:48:44.391: INFO: (18) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">... (200; 17.398521ms)
May  4 12:48:44.393: INFO: (18) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname1/proxy/: foo (200; 19.859934ms)
May  4 12:48:44.395: INFO: (18) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname1/proxy/: foo (200; 21.381968ms)
May  4 12:48:44.395: INFO: (18) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname2/proxy/: bar (200; 21.719551ms)
May  4 12:48:44.395: INFO: (18) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname2/proxy/: bar (200; 21.898915ms)
May  4 12:48:44.396: INFO: (18) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname1/proxy/: tls baz (200; 22.519541ms)
May  4 12:48:44.402: INFO: (19) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 6.735168ms)
May  4 12:48:44.406: INFO: (19) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname1/proxy/: foo (200; 10.484223ms)
May  4 12:48:44.407: INFO: (19) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">... (200; 10.903328ms)
May  4 12:48:44.407: INFO: (19) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">test<... (200; 11.050725ms)
May  4 12:48:44.407: INFO: (19) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 11.07556ms)
May  4 12:48:44.407: INFO: (19) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:460/proxy/: tls baz (200; 11.298748ms)
May  4 12:48:44.407: INFO: (19) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/tlsrewritem... (200; 11.403889ms)
May  4 12:48:44.407: INFO: (19) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:462/proxy/: tls qux (200; 11.37391ms)
May  4 12:48:44.408: INFO: (19) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/rewriteme">test</a> (200; 12.48594ms)
May  4 12:48:44.408: INFO: (19) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 12.619248ms)
May  4 12:48:44.408: INFO: (19) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 12.636655ms)
May  4 12:48:44.410: INFO: (19) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname1/proxy/: tls baz (200; 14.190945ms)
May  4 12:48:44.410: INFO: (19) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname2/proxy/: bar (200; 14.359576ms)
May  4 12:48:44.411: INFO: (19) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname2/proxy/: tls qux (200; 15.231423ms)
May  4 12:48:44.411: INFO: (19) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname2/proxy/: bar (200; 15.188484ms)
May  4 12:48:44.411: INFO: (19) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname1/proxy/: foo (200; 15.438925ms)
STEP: deleting ReplicationController proxy-service-p68ns in namespace proxy-6780, will wait for the garbage collector to delete the pods 05/04/23 12:48:44.411
May  4 12:48:44.476: INFO: Deleting ReplicationController proxy-service-p68ns took: 9.385584ms
May  4 12:48:44.576: INFO: Terminating ReplicationController proxy-service-p68ns pods took: 100.355225ms
[AfterEach] version v1
  test/e2e/framework/framework.go:187
May  4 12:48:47.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6780" for this suite. 05/04/23 12:48:47.289
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","completed":220,"skipped":4156,"failed":0}
------------------------------
• [SLOW TEST] [5.529 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:48:41.769
    May  4 12:48:41.769: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename proxy 05/04/23 12:48:41.77
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:48:41.792
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:48:41.795
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 05/04/23 12:48:41.817
    STEP: creating replication controller proxy-service-p68ns in namespace proxy-6780 05/04/23 12:48:41.817
    I0504 12:48:41.836549      21 runners.go:193] Created replication controller with name: proxy-service-p68ns, namespace: proxy-6780, replica count: 1
    I0504 12:48:42.887790      21 runners.go:193] proxy-service-p68ns Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0504 12:48:43.887983      21 runners.go:193] proxy-service-p68ns Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  4 12:48:43.891: INFO: setup took 2.092614921s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 05/04/23 12:48:43.891
    May  4 12:48:43.904: INFO: (0) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 12.919351ms)
    May  4 12:48:43.904: INFO: (0) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">... (200; 13.037971ms)
    May  4 12:48:43.905: INFO: (0) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 13.452237ms)
    May  4 12:48:43.905: INFO: (0) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 13.538841ms)
    May  4 12:48:43.905: INFO: (0) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 14.002023ms)
    May  4 12:48:43.905: INFO: (0) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">test<... (200; 13.957956ms)
    May  4 12:48:43.907: INFO: (0) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/rewriteme">test</a> (200; 15.847026ms)
    May  4 12:48:43.911: INFO: (0) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:462/proxy/: tls qux (200; 19.899334ms)
    May  4 12:48:43.911: INFO: (0) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:460/proxy/: tls baz (200; 19.908699ms)
    May  4 12:48:43.911: INFO: (0) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/tlsrewritem... (200; 19.92727ms)
    May  4 12:48:43.911: INFO: (0) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname1/proxy/: foo (200; 20.010127ms)
    May  4 12:48:43.911: INFO: (0) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname2/proxy/: bar (200; 19.931673ms)
    May  4 12:48:43.911: INFO: (0) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname2/proxy/: bar (200; 20.080711ms)
    May  4 12:48:43.911: INFO: (0) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname2/proxy/: tls qux (200; 20.008085ms)
    May  4 12:48:43.911: INFO: (0) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname1/proxy/: foo (200; 20.081786ms)
    May  4 12:48:43.911: INFO: (0) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname1/proxy/: tls baz (200; 20.010556ms)
    May  4 12:48:43.925: INFO: (1) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">test<... (200; 13.942093ms)
    May  4 12:48:43.925: INFO: (1) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 13.744374ms)
    May  4 12:48:43.931: INFO: (1) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/tlsrewritem... (200; 19.02159ms)
    May  4 12:48:43.931: INFO: (1) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 19.084652ms)
    May  4 12:48:43.931: INFO: (1) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:460/proxy/: tls baz (200; 19.004034ms)
    May  4 12:48:43.931: INFO: (1) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">... (200; 19.038367ms)
    May  4 12:48:43.931: INFO: (1) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 18.903664ms)
    May  4 12:48:43.931: INFO: (1) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/rewriteme">test</a> (200; 18.994737ms)
    May  4 12:48:43.932: INFO: (1) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:462/proxy/: tls qux (200; 20.27354ms)
    May  4 12:48:43.932: INFO: (1) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 20.277902ms)
    May  4 12:48:43.937: INFO: (1) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname2/proxy/: tls qux (200; 25.382628ms)
    May  4 12:48:43.942: INFO: (1) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname1/proxy/: foo (200; 30.780286ms)
    May  4 12:48:43.942: INFO: (1) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname1/proxy/: tls baz (200; 30.746936ms)
    May  4 12:48:43.942: INFO: (1) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname1/proxy/: foo (200; 30.795089ms)
    May  4 12:48:43.943: INFO: (1) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname2/proxy/: bar (200; 31.244246ms)
    May  4 12:48:43.943: INFO: (1) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname2/proxy/: bar (200; 31.329511ms)
    May  4 12:48:43.955: INFO: (2) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 10.808513ms)
    May  4 12:48:43.955: INFO: (2) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 11.607014ms)
    May  4 12:48:43.957: INFO: (2) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">test<... (200; 12.731337ms)
    May  4 12:48:43.957: INFO: (2) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:462/proxy/: tls qux (200; 13.861294ms)
    May  4 12:48:43.957: INFO: (2) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/tlsrewritem... (200; 14.269053ms)
    May  4 12:48:43.958: INFO: (2) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">... (200; 13.686706ms)
    May  4 12:48:43.962: INFO: (2) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:460/proxy/: tls baz (200; 18.914941ms)
    May  4 12:48:43.962: INFO: (2) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 19.014832ms)
    May  4 12:48:43.962: INFO: (2) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/rewriteme">test</a> (200; 19.193833ms)
    May  4 12:48:43.962: INFO: (2) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname2/proxy/: bar (200; 18.823908ms)
    May  4 12:48:43.962: INFO: (2) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname1/proxy/: foo (200; 18.426244ms)
    May  4 12:48:43.962: INFO: (2) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 18.748894ms)
    May  4 12:48:43.964: INFO: (2) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname2/proxy/: bar (200; 20.609022ms)
    May  4 12:48:43.964: INFO: (2) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname1/proxy/: foo (200; 20.227402ms)
    May  4 12:48:43.964: INFO: (2) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname1/proxy/: tls baz (200; 20.600521ms)
    May  4 12:48:43.965: INFO: (2) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname2/proxy/: tls qux (200; 21.209678ms)
    May  4 12:48:43.974: INFO: (3) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 8.158053ms)
    May  4 12:48:43.977: INFO: (3) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">... (200; 11.186349ms)
    May  4 12:48:43.977: INFO: (3) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/rewriteme">test</a> (200; 11.698574ms)
    May  4 12:48:43.977: INFO: (3) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">test<... (200; 11.147371ms)
    May  4 12:48:43.977: INFO: (3) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 11.196405ms)
    May  4 12:48:43.977: INFO: (3) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:460/proxy/: tls baz (200; 11.589585ms)
    May  4 12:48:43.977: INFO: (3) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 11.30288ms)
    May  4 12:48:43.980: INFO: (3) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 14.166566ms)
    May  4 12:48:43.981: INFO: (3) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:462/proxy/: tls qux (200; 14.956907ms)
    May  4 12:48:43.982: INFO: (3) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/tlsrewritem... (200; 16.888313ms)
    May  4 12:48:43.982: INFO: (3) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname2/proxy/: bar (200; 16.431843ms)
    May  4 12:48:43.983: INFO: (3) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname2/proxy/: bar (200; 17.331657ms)
    May  4 12:48:43.984: INFO: (3) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname1/proxy/: tls baz (200; 18.500472ms)
    May  4 12:48:43.985: INFO: (3) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname1/proxy/: foo (200; 19.624434ms)
    May  4 12:48:43.986: INFO: (3) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname2/proxy/: tls qux (200; 19.617492ms)
    May  4 12:48:43.987: INFO: (3) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname1/proxy/: foo (200; 21.405205ms)
    May  4 12:48:43.993: INFO: (4) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:462/proxy/: tls qux (200; 5.317405ms)
    May  4 12:48:43.999: INFO: (4) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">test<... (200; 11.670244ms)
    May  4 12:48:43.999: INFO: (4) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:460/proxy/: tls baz (200; 11.672626ms)
    May  4 12:48:43.999: INFO: (4) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 11.639023ms)
    May  4 12:48:43.999: INFO: (4) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 11.716416ms)
    May  4 12:48:43.999: INFO: (4) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/tlsrewritem... (200; 11.725065ms)
    May  4 12:48:43.999: INFO: (4) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/rewriteme">test</a> (200; 11.876896ms)
    May  4 12:48:43.999: INFO: (4) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 11.84337ms)
    May  4 12:48:43.999: INFO: (4) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 11.867946ms)
    May  4 12:48:43.999: INFO: (4) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">... (200; 11.921082ms)
    May  4 12:48:44.002: INFO: (4) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname1/proxy/: foo (200; 14.149029ms)
    May  4 12:48:44.003: INFO: (4) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname1/proxy/: tls baz (200; 15.809549ms)
    May  4 12:48:44.004: INFO: (4) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname1/proxy/: foo (200; 16.832055ms)
    May  4 12:48:44.005: INFO: (4) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname2/proxy/: tls qux (200; 17.23936ms)
    May  4 12:48:44.005: INFO: (4) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname2/proxy/: bar (200; 17.271526ms)
    May  4 12:48:44.005: INFO: (4) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname2/proxy/: bar (200; 17.33085ms)
    May  4 12:48:44.015: INFO: (5) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 9.815286ms)
    May  4 12:48:44.015: INFO: (5) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">... (200; 9.846027ms)
    May  4 12:48:44.018: INFO: (5) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 13.111261ms)
    May  4 12:48:44.018: INFO: (5) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 13.051482ms)
    May  4 12:48:44.020: INFO: (5) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:460/proxy/: tls baz (200; 14.435602ms)
    May  4 12:48:44.020: INFO: (5) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:462/proxy/: tls qux (200; 14.824213ms)
    May  4 12:48:44.020: INFO: (5) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 14.913627ms)
    May  4 12:48:44.021: INFO: (5) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/rewriteme">test</a> (200; 15.706784ms)
    May  4 12:48:44.021: INFO: (5) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/tlsrewritem... (200; 16.036897ms)
    May  4 12:48:44.022: INFO: (5) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">test<... (200; 16.446894ms)
    May  4 12:48:44.023: INFO: (5) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname1/proxy/: tls baz (200; 17.475753ms)
    May  4 12:48:44.026: INFO: (5) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname1/proxy/: foo (200; 21.028408ms)
    May  4 12:48:44.026: INFO: (5) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname2/proxy/: bar (200; 21.177995ms)
    May  4 12:48:44.027: INFO: (5) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname1/proxy/: foo (200; 21.569809ms)
    May  4 12:48:44.027: INFO: (5) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname2/proxy/: bar (200; 22.116497ms)
    May  4 12:48:44.029: INFO: (5) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname2/proxy/: tls qux (200; 24.212815ms)
    May  4 12:48:44.040: INFO: (6) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">test<... (200; 10.970431ms)
    May  4 12:48:44.043: INFO: (6) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 13.209724ms)
    May  4 12:48:44.044: INFO: (6) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname1/proxy/: foo (200; 13.911079ms)
    May  4 12:48:44.044: INFO: (6) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 14.110507ms)
    May  4 12:48:44.044: INFO: (6) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:462/proxy/: tls qux (200; 14.041969ms)
    May  4 12:48:44.044: INFO: (6) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 14.007933ms)
    May  4 12:48:44.044: INFO: (6) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 14.149829ms)
    May  4 12:48:44.044: INFO: (6) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">... (200; 14.016366ms)
    May  4 12:48:44.044: INFO: (6) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/rewriteme">test</a> (200; 14.670855ms)
    May  4 12:48:44.045: INFO: (6) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/tlsrewritem... (200; 15.879505ms)
    May  4 12:48:44.045: INFO: (6) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:460/proxy/: tls baz (200; 15.712092ms)
    May  4 12:48:44.048: INFO: (6) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname1/proxy/: foo (200; 18.108651ms)
    May  4 12:48:44.056: INFO: (6) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname2/proxy/: tls qux (200; 26.203725ms)
    May  4 12:48:44.056: INFO: (6) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname2/proxy/: bar (200; 26.406708ms)
    May  4 12:48:44.061: INFO: (6) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname1/proxy/: tls baz (200; 31.523572ms)
    May  4 12:48:44.061: INFO: (6) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname2/proxy/: bar (200; 31.639775ms)
    May  4 12:48:44.081: INFO: (7) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/tlsrewritem... (200; 19.610298ms)
    May  4 12:48:44.087: INFO: (7) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/rewriteme">test</a> (200; 25.141394ms)
    May  4 12:48:44.090: INFO: (7) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 28.215602ms)
    May  4 12:48:44.090: INFO: (7) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">... (200; 27.935474ms)
    May  4 12:48:44.090: INFO: (7) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 28.010792ms)
    May  4 12:48:44.090: INFO: (7) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">test<... (200; 28.320148ms)
    May  4 12:48:44.090: INFO: (7) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 28.576448ms)
    May  4 12:48:44.090: INFO: (7) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:462/proxy/: tls qux (200; 28.441898ms)
    May  4 12:48:44.091: INFO: (7) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:460/proxy/: tls baz (200; 28.971297ms)
    May  4 12:48:44.091: INFO: (7) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 29.290436ms)
    May  4 12:48:44.095: INFO: (7) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname1/proxy/: tls baz (200; 33.472504ms)
    May  4 12:48:44.099: INFO: (7) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname1/proxy/: foo (200; 37.19591ms)
    May  4 12:48:44.099: INFO: (7) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname2/proxy/: tls qux (200; 37.0841ms)
    May  4 12:48:44.099: INFO: (7) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname2/proxy/: bar (200; 37.130503ms)
    May  4 12:48:44.099: INFO: (7) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname1/proxy/: foo (200; 37.34562ms)
    May  4 12:48:44.099: INFO: (7) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname2/proxy/: bar (200; 37.588633ms)
    May  4 12:48:44.115: INFO: (8) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 15.70388ms)
    May  4 12:48:44.115: INFO: (8) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 15.718611ms)
    May  4 12:48:44.115: INFO: (8) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 15.764067ms)
    May  4 12:48:44.115: INFO: (8) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 15.812522ms)
    May  4 12:48:44.115: INFO: (8) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:460/proxy/: tls baz (200; 15.980139ms)
    May  4 12:48:44.115: INFO: (8) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/rewriteme">test</a> (200; 15.941345ms)
    May  4 12:48:44.115: INFO: (8) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:462/proxy/: tls qux (200; 16.08595ms)
    May  4 12:48:44.116: INFO: (8) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">... (200; 16.705899ms)
    May  4 12:48:44.116: INFO: (8) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">test<... (200; 16.744011ms)
    May  4 12:48:44.116: INFO: (8) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/tlsrewritem... (200; 17.087342ms)
    May  4 12:48:44.117: INFO: (8) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname2/proxy/: bar (200; 18.100764ms)
    May  4 12:48:44.119: INFO: (8) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname2/proxy/: bar (200; 19.839349ms)
    May  4 12:48:44.120: INFO: (8) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname1/proxy/: foo (200; 20.252148ms)
    May  4 12:48:44.120: INFO: (8) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname1/proxy/: foo (200; 20.102554ms)
    May  4 12:48:44.120: INFO: (8) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname1/proxy/: tls baz (200; 20.000206ms)
    May  4 12:48:44.120: INFO: (8) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname2/proxy/: tls qux (200; 20.150256ms)
    May  4 12:48:44.132: INFO: (9) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">test<... (200; 11.550425ms)
    May  4 12:48:44.133: INFO: (9) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 13.374639ms)
    May  4 12:48:44.133: INFO: (9) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/tlsrewritem... (200; 13.076874ms)
    May  4 12:48:44.133: INFO: (9) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:460/proxy/: tls baz (200; 13.542408ms)
    May  4 12:48:44.133: INFO: (9) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 13.594357ms)
    May  4 12:48:44.133: INFO: (9) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 13.418731ms)
    May  4 12:48:44.133: INFO: (9) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">... (200; 13.627125ms)
    May  4 12:48:44.133: INFO: (9) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/rewriteme">test</a> (200; 13.113804ms)
    May  4 12:48:44.133: INFO: (9) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 13.251413ms)
    May  4 12:48:44.133: INFO: (9) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:462/proxy/: tls qux (200; 13.624311ms)
    May  4 12:48:44.136: INFO: (9) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname2/proxy/: bar (200; 16.775393ms)
    May  4 12:48:44.139: INFO: (9) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname1/proxy/: foo (200; 19.294818ms)
    May  4 12:48:44.140: INFO: (9) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname2/proxy/: tls qux (200; 19.524336ms)
    May  4 12:48:44.140: INFO: (9) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname1/proxy/: tls baz (200; 19.748165ms)
    May  4 12:48:44.140: INFO: (9) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname1/proxy/: foo (200; 19.978925ms)
    May  4 12:48:44.140: INFO: (9) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname2/proxy/: bar (200; 19.791987ms)
    May  4 12:48:44.152: INFO: (10) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/rewriteme">test</a> (200; 12.15117ms)
    May  4 12:48:44.152: INFO: (10) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 12.114899ms)
    May  4 12:48:44.156: INFO: (10) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 15.632078ms)
    May  4 12:48:44.156: INFO: (10) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/tlsrewritem... (200; 15.934072ms)
    May  4 12:48:44.156: INFO: (10) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">test<... (200; 16.030966ms)
    May  4 12:48:44.157: INFO: (10) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 16.177559ms)
    May  4 12:48:44.157: INFO: (10) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">... (200; 16.294827ms)
    May  4 12:48:44.157: INFO: (10) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:460/proxy/: tls baz (200; 16.601914ms)
    May  4 12:48:44.157: INFO: (10) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:462/proxy/: tls qux (200; 16.657327ms)
    May  4 12:48:44.157: INFO: (10) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname1/proxy/: foo (200; 17.140402ms)
    May  4 12:48:44.161: INFO: (10) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 20.672753ms)
    May  4 12:48:44.162: INFO: (10) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname2/proxy/: tls qux (200; 21.861478ms)
    May  4 12:48:44.163: INFO: (10) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname2/proxy/: bar (200; 22.45759ms)
    May  4 12:48:44.164: INFO: (10) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname1/proxy/: tls baz (200; 23.203317ms)
    May  4 12:48:44.164: INFO: (10) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname1/proxy/: foo (200; 23.135627ms)
    May  4 12:48:44.164: INFO: (10) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname2/proxy/: bar (200; 23.205011ms)
    May  4 12:48:44.173: INFO: (11) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 9.486912ms)
    May  4 12:48:44.173: INFO: (11) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 9.644175ms)
    May  4 12:48:44.173: INFO: (11) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:460/proxy/: tls baz (200; 9.804631ms)
    May  4 12:48:44.173: INFO: (11) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 9.715833ms)
    May  4 12:48:44.176: INFO: (11) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/tlsrewritem... (200; 12.494636ms)
    May  4 12:48:44.176: INFO: (11) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">test<... (200; 12.301789ms)
    May  4 12:48:44.176: INFO: (11) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/rewriteme">test</a> (200; 12.50169ms)
    May  4 12:48:44.176: INFO: (11) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">... (200; 12.663529ms)
    May  4 12:48:44.176: INFO: (11) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 12.508755ms)
    May  4 12:48:44.177: INFO: (11) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:462/proxy/: tls qux (200; 12.722362ms)
    May  4 12:48:44.179: INFO: (11) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname2/proxy/: bar (200; 15.269833ms)
    May  4 12:48:44.186: INFO: (11) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname2/proxy/: tls qux (200; 22.567538ms)
    May  4 12:48:44.186: INFO: (11) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname1/proxy/: foo (200; 22.616785ms)
    May  4 12:48:44.187: INFO: (11) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname2/proxy/: bar (200; 23.484342ms)
    May  4 12:48:44.187: INFO: (11) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname1/proxy/: tls baz (200; 23.795063ms)
    May  4 12:48:44.187: INFO: (11) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname1/proxy/: foo (200; 23.685235ms)
    May  4 12:48:44.201: INFO: (12) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 13.01941ms)
    May  4 12:48:44.201: INFO: (12) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 13.059764ms)
    May  4 12:48:44.201: INFO: (12) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname2/proxy/: tls qux (200; 13.176564ms)
    May  4 12:48:44.205: INFO: (12) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/rewriteme">test</a> (200; 17.763741ms)
    May  4 12:48:44.206: INFO: (12) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:460/proxy/: tls baz (200; 17.983217ms)
    May  4 12:48:44.206: INFO: (12) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">test<... (200; 18.842356ms)
    May  4 12:48:44.207: INFO: (12) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">... (200; 19.847904ms)
    May  4 12:48:44.207: INFO: (12) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname2/proxy/: bar (200; 19.807604ms)
    May  4 12:48:44.207: INFO: (12) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 19.737111ms)
    May  4 12:48:44.208: INFO: (12) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 19.947943ms)
    May  4 12:48:44.208: INFO: (12) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/tlsrewritem... (200; 20.169943ms)
    May  4 12:48:44.208: INFO: (12) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname1/proxy/: foo (200; 20.144951ms)
    May  4 12:48:44.208: INFO: (12) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:462/proxy/: tls qux (200; 20.21608ms)
    May  4 12:48:44.208: INFO: (12) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname1/proxy/: foo (200; 20.293595ms)
    May  4 12:48:44.208: INFO: (12) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname1/proxy/: tls baz (200; 20.457493ms)
    May  4 12:48:44.208: INFO: (12) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname2/proxy/: bar (200; 20.958726ms)
    May  4 12:48:44.243: INFO: (13) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/rewriteme">test</a> (200; 34.970257ms)
    May  4 12:48:44.259: INFO: (13) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:460/proxy/: tls baz (200; 50.198318ms)
    May  4 12:48:44.259: INFO: (13) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 50.236173ms)
    May  4 12:48:44.259: INFO: (13) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">test<... (200; 49.822741ms)
    May  4 12:48:44.259: INFO: (13) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 50.15489ms)
    May  4 12:48:44.259: INFO: (13) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">... (200; 50.188765ms)
    May  4 12:48:44.261: INFO: (13) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/tlsrewritem... (200; 52.915745ms)
    May  4 12:48:44.261: INFO: (13) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 52.678945ms)
    May  4 12:48:44.261: INFO: (13) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 52.650346ms)
    May  4 12:48:44.262: INFO: (13) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:462/proxy/: tls qux (200; 52.853829ms)
    May  4 12:48:44.266: INFO: (13) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname2/proxy/: bar (200; 57.636477ms)
    May  4 12:48:44.266: INFO: (13) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname1/proxy/: foo (200; 57.453546ms)
    May  4 12:48:44.267: INFO: (13) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname1/proxy/: foo (200; 57.785228ms)
    May  4 12:48:44.267: INFO: (13) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname1/proxy/: tls baz (200; 57.955997ms)
    May  4 12:48:44.267: INFO: (13) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname2/proxy/: bar (200; 58.215817ms)
    May  4 12:48:44.267: INFO: (13) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname2/proxy/: tls qux (200; 58.179841ms)
    May  4 12:48:44.290: INFO: (14) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">test<... (200; 22.133884ms)
    May  4 12:48:44.290: INFO: (14) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:462/proxy/: tls qux (200; 22.024193ms)
    May  4 12:48:44.290: INFO: (14) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:460/proxy/: tls baz (200; 22.060995ms)
    May  4 12:48:44.290: INFO: (14) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 22.261695ms)
    May  4 12:48:44.290: INFO: (14) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 22.492845ms)
    May  4 12:48:44.290: INFO: (14) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 22.52325ms)
    May  4 12:48:44.291: INFO: (14) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/rewriteme">test</a> (200; 23.596671ms)
    May  4 12:48:44.292: INFO: (14) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 25.145759ms)
    May  4 12:48:44.292: INFO: (14) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">... (200; 25.031809ms)
    May  4 12:48:44.292: INFO: (14) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/tlsrewritem... (200; 24.971958ms)
    May  4 12:48:44.292: INFO: (14) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname1/proxy/: foo (200; 25.253483ms)
    May  4 12:48:44.292: INFO: (14) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname1/proxy/: tls baz (200; 24.854773ms)
    May  4 12:48:44.293: INFO: (14) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname2/proxy/: bar (200; 25.465403ms)
    May  4 12:48:44.294: INFO: (14) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname2/proxy/: bar (200; 25.916259ms)
    May  4 12:48:44.294: INFO: (14) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname2/proxy/: tls qux (200; 26.469844ms)
    May  4 12:48:44.294: INFO: (14) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname1/proxy/: foo (200; 26.781731ms)
    May  4 12:48:44.323: INFO: (15) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">test<... (200; 28.337319ms)
    May  4 12:48:44.323: INFO: (15) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 28.27784ms)
    May  4 12:48:44.323: INFO: (15) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 28.514669ms)
    May  4 12:48:44.324: INFO: (15) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 28.989227ms)
    May  4 12:48:44.324: INFO: (15) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:460/proxy/: tls baz (200; 29.178517ms)
    May  4 12:48:44.324: INFO: (15) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:462/proxy/: tls qux (200; 29.195867ms)
    May  4 12:48:44.324: INFO: (15) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/rewriteme">test</a> (200; 29.585984ms)
    May  4 12:48:44.324: INFO: (15) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/tlsrewritem... (200; 29.837334ms)
    May  4 12:48:44.325: INFO: (15) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname1/proxy/: foo (200; 30.569753ms)
    May  4 12:48:44.325: INFO: (15) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">... (200; 30.747265ms)
    May  4 12:48:44.327: INFO: (15) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 31.994015ms)
    May  4 12:48:44.331: INFO: (15) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname2/proxy/: bar (200; 36.380092ms)
    May  4 12:48:44.331: INFO: (15) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname2/proxy/: tls qux (200; 36.640639ms)
    May  4 12:48:44.331: INFO: (15) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname2/proxy/: bar (200; 36.736452ms)
    May  4 12:48:44.333: INFO: (15) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname1/proxy/: tls baz (200; 37.946054ms)
    May  4 12:48:44.333: INFO: (15) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname1/proxy/: foo (200; 38.105402ms)
    May  4 12:48:44.349: INFO: (16) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 16.037197ms)
    May  4 12:48:44.350: INFO: (16) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:462/proxy/: tls qux (200; 16.897346ms)
    May  4 12:48:44.351: INFO: (16) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/rewriteme">test</a> (200; 17.927219ms)
    May  4 12:48:44.351: INFO: (16) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 18.044297ms)
    May  4 12:48:44.351: INFO: (16) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/tlsrewritem... (200; 17.955957ms)
    May  4 12:48:44.351: INFO: (16) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:460/proxy/: tls baz (200; 17.856873ms)
    May  4 12:48:44.351: INFO: (16) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname1/proxy/: foo (200; 18.215336ms)
    May  4 12:48:44.351: INFO: (16) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 18.333041ms)
    May  4 12:48:44.351: INFO: (16) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">test<... (200; 18.092578ms)
    May  4 12:48:44.352: INFO: (16) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 18.666412ms)
    May  4 12:48:44.353: INFO: (16) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname1/proxy/: foo (200; 19.356876ms)
    May  4 12:48:44.353: INFO: (16) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">... (200; 19.187687ms)
    May  4 12:48:44.353: INFO: (16) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname2/proxy/: bar (200; 19.636466ms)
    May  4 12:48:44.353: INFO: (16) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname1/proxy/: tls baz (200; 19.734576ms)
    May  4 12:48:44.353: INFO: (16) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname2/proxy/: bar (200; 20.159416ms)
    May  4 12:48:44.353: INFO: (16) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname2/proxy/: tls qux (200; 19.936776ms)
    May  4 12:48:44.358: INFO: (17) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 4.721425ms)
    May  4 12:48:44.360: INFO: (17) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:460/proxy/: tls baz (200; 6.856701ms)
    May  4 12:48:44.363: INFO: (17) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/rewriteme">test</a> (200; 9.822088ms)
    May  4 12:48:44.366: INFO: (17) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">... (200; 12.853866ms)
    May  4 12:48:44.367: INFO: (17) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:462/proxy/: tls qux (200; 13.107572ms)
    May  4 12:48:44.367: INFO: (17) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">test<... (200; 13.262641ms)
    May  4 12:48:44.367: INFO: (17) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 13.236891ms)
    May  4 12:48:44.367: INFO: (17) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/tlsrewritem... (200; 13.446236ms)
    May  4 12:48:44.368: INFO: (17) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 14.445663ms)
    May  4 12:48:44.368: INFO: (17) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 15.018338ms)
    May  4 12:48:44.369: INFO: (17) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname2/proxy/: bar (200; 15.964093ms)
    May  4 12:48:44.373: INFO: (17) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname1/proxy/: tls baz (200; 19.13851ms)
    May  4 12:48:44.373: INFO: (17) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname1/proxy/: foo (200; 19.336428ms)
    May  4 12:48:44.373: INFO: (17) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname2/proxy/: bar (200; 19.365131ms)
    May  4 12:48:44.373: INFO: (17) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname1/proxy/: foo (200; 19.322753ms)
    May  4 12:48:44.373: INFO: (17) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname2/proxy/: tls qux (200; 19.398799ms)
    May  4 12:48:44.389: INFO: (18) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/tlsrewritem... (200; 16.220694ms)
    May  4 12:48:44.390: INFO: (18) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 16.704117ms)
    May  4 12:48:44.390: INFO: (18) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">test<... (200; 16.565804ms)
    May  4 12:48:44.390: INFO: (18) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 16.617038ms)
    May  4 12:48:44.390: INFO: (18) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/rewriteme">test</a> (200; 16.633017ms)
    May  4 12:48:44.390: INFO: (18) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 16.757682ms)
    May  4 12:48:44.390: INFO: (18) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname2/proxy/: tls qux (200; 16.689935ms)
    May  4 12:48:44.390: INFO: (18) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:460/proxy/: tls baz (200; 16.750131ms)
    May  4 12:48:44.390: INFO: (18) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 17.005356ms)
    May  4 12:48:44.390: INFO: (18) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:462/proxy/: tls qux (200; 17.025304ms)
    May  4 12:48:44.391: INFO: (18) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">... (200; 17.398521ms)
    May  4 12:48:44.393: INFO: (18) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname1/proxy/: foo (200; 19.859934ms)
    May  4 12:48:44.395: INFO: (18) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname1/proxy/: foo (200; 21.381968ms)
    May  4 12:48:44.395: INFO: (18) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname2/proxy/: bar (200; 21.719551ms)
    May  4 12:48:44.395: INFO: (18) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname2/proxy/: bar (200; 21.898915ms)
    May  4 12:48:44.396: INFO: (18) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname1/proxy/: tls baz (200; 22.519541ms)
    May  4 12:48:44.402: INFO: (19) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 6.735168ms)
    May  4 12:48:44.406: INFO: (19) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname1/proxy/: foo (200; 10.484223ms)
    May  4 12:48:44.407: INFO: (19) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">... (200; 10.903328ms)
    May  4 12:48:44.407: INFO: (19) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:1080/proxy/rewriteme">test<... (200; 11.050725ms)
    May  4 12:48:44.407: INFO: (19) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 11.07556ms)
    May  4 12:48:44.407: INFO: (19) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:460/proxy/: tls baz (200; 11.298748ms)
    May  4 12:48:44.407: INFO: (19) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:443/proxy/tlsrewritem... (200; 11.403889ms)
    May  4 12:48:44.407: INFO: (19) /api/v1/namespaces/proxy-6780/pods/https:proxy-service-p68ns-xgbc5:462/proxy/: tls qux (200; 11.37391ms)
    May  4 12:48:44.408: INFO: (19) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/: <a href="/api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5/proxy/rewriteme">test</a> (200; 12.48594ms)
    May  4 12:48:44.408: INFO: (19) /api/v1/namespaces/proxy-6780/pods/proxy-service-p68ns-xgbc5:160/proxy/: foo (200; 12.619248ms)
    May  4 12:48:44.408: INFO: (19) /api/v1/namespaces/proxy-6780/pods/http:proxy-service-p68ns-xgbc5:162/proxy/: bar (200; 12.636655ms)
    May  4 12:48:44.410: INFO: (19) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname1/proxy/: tls baz (200; 14.190945ms)
    May  4 12:48:44.410: INFO: (19) /api/v1/namespaces/proxy-6780/services/proxy-service-p68ns:portname2/proxy/: bar (200; 14.359576ms)
    May  4 12:48:44.411: INFO: (19) /api/v1/namespaces/proxy-6780/services/https:proxy-service-p68ns:tlsportname2/proxy/: tls qux (200; 15.231423ms)
    May  4 12:48:44.411: INFO: (19) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname2/proxy/: bar (200; 15.188484ms)
    May  4 12:48:44.411: INFO: (19) /api/v1/namespaces/proxy-6780/services/http:proxy-service-p68ns:portname1/proxy/: foo (200; 15.438925ms)
    STEP: deleting ReplicationController proxy-service-p68ns in namespace proxy-6780, will wait for the garbage collector to delete the pods 05/04/23 12:48:44.411
    May  4 12:48:44.476: INFO: Deleting ReplicationController proxy-service-p68ns took: 9.385584ms
    May  4 12:48:44.576: INFO: Terminating ReplicationController proxy-service-p68ns pods took: 100.355225ms
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    May  4 12:48:47.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-6780" for this suite. 05/04/23 12:48:47.289
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:48:47.3
May  4 12:48:47.300: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename lease-test 05/04/23 12:48:47.301
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:48:47.324
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:48:47.328
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
May  4 12:48:47.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-8610" for this suite. 05/04/23 12:48:47.414
{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","completed":221,"skipped":4169,"failed":0}
------------------------------
• [0.130 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:48:47.3
    May  4 12:48:47.300: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename lease-test 05/04/23 12:48:47.301
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:48:47.324
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:48:47.328
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/framework.go:187
    May  4 12:48:47.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "lease-test-8610" for this suite. 05/04/23 12:48:47.414
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:48:47.43
May  4 12:48:47.431: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename containers 05/04/23 12:48:47.432
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:48:47.449
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:48:47.453
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
STEP: Creating a pod to test override arguments 05/04/23 12:48:47.456
May  4 12:48:47.467: INFO: Waiting up to 5m0s for pod "client-containers-74b9fb76-86a6-4304-a75f-3569a8b4669b" in namespace "containers-9557" to be "Succeeded or Failed"
May  4 12:48:47.470: INFO: Pod "client-containers-74b9fb76-86a6-4304-a75f-3569a8b4669b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.724688ms
May  4 12:48:49.476: INFO: Pod "client-containers-74b9fb76-86a6-4304-a75f-3569a8b4669b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009088582s
May  4 12:48:51.476: INFO: Pod "client-containers-74b9fb76-86a6-4304-a75f-3569a8b4669b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009481773s
STEP: Saw pod success 05/04/23 12:48:51.476
May  4 12:48:51.476: INFO: Pod "client-containers-74b9fb76-86a6-4304-a75f-3569a8b4669b" satisfied condition "Succeeded or Failed"
May  4 12:48:51.480: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod client-containers-74b9fb76-86a6-4304-a75f-3569a8b4669b container agnhost-container: <nil>
STEP: delete the pod 05/04/23 12:48:51.487
May  4 12:48:51.506: INFO: Waiting for pod client-containers-74b9fb76-86a6-4304-a75f-3569a8b4669b to disappear
May  4 12:48:51.509: INFO: Pod client-containers-74b9fb76-86a6-4304-a75f-3569a8b4669b no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
May  4 12:48:51.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9557" for this suite. 05/04/23 12:48:51.517
{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","completed":222,"skipped":4191,"failed":0}
------------------------------
• [4.094 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:48:47.43
    May  4 12:48:47.431: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename containers 05/04/23 12:48:47.432
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:48:47.449
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:48:47.453
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:58
    STEP: Creating a pod to test override arguments 05/04/23 12:48:47.456
    May  4 12:48:47.467: INFO: Waiting up to 5m0s for pod "client-containers-74b9fb76-86a6-4304-a75f-3569a8b4669b" in namespace "containers-9557" to be "Succeeded or Failed"
    May  4 12:48:47.470: INFO: Pod "client-containers-74b9fb76-86a6-4304-a75f-3569a8b4669b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.724688ms
    May  4 12:48:49.476: INFO: Pod "client-containers-74b9fb76-86a6-4304-a75f-3569a8b4669b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009088582s
    May  4 12:48:51.476: INFO: Pod "client-containers-74b9fb76-86a6-4304-a75f-3569a8b4669b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009481773s
    STEP: Saw pod success 05/04/23 12:48:51.476
    May  4 12:48:51.476: INFO: Pod "client-containers-74b9fb76-86a6-4304-a75f-3569a8b4669b" satisfied condition "Succeeded or Failed"
    May  4 12:48:51.480: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod client-containers-74b9fb76-86a6-4304-a75f-3569a8b4669b container agnhost-container: <nil>
    STEP: delete the pod 05/04/23 12:48:51.487
    May  4 12:48:51.506: INFO: Waiting for pod client-containers-74b9fb76-86a6-4304-a75f-3569a8b4669b to disappear
    May  4 12:48:51.509: INFO: Pod client-containers-74b9fb76-86a6-4304-a75f-3569a8b4669b no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    May  4 12:48:51.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-9557" for this suite. 05/04/23 12:48:51.517
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:48:51.526
May  4 12:48:51.527: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename csistoragecapacity 05/04/23 12:48:51.527
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:48:51.545
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:48:51.548
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 05/04/23 12:48:51.551
STEP: getting /apis/storage.k8s.io 05/04/23 12:48:51.554
STEP: getting /apis/storage.k8s.io/v1 05/04/23 12:48:51.555
STEP: creating 05/04/23 12:48:51.557
STEP: watching 05/04/23 12:48:51.58
May  4 12:48:51.580: INFO: starting watch
STEP: getting 05/04/23 12:48:51.587
STEP: listing in namespace 05/04/23 12:48:51.591
STEP: listing across namespaces 05/04/23 12:48:51.594
STEP: patching 05/04/23 12:48:51.598
STEP: updating 05/04/23 12:48:51.604
May  4 12:48:51.610: INFO: waiting for watch events with expected annotations in namespace
May  4 12:48:51.610: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 05/04/23 12:48:51.61
STEP: deleting a collection 05/04/23 12:48:51.623
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
May  4 12:48:51.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-1228" for this suite. 05/04/23 12:48:51.649
{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","completed":223,"skipped":4208,"failed":0}
------------------------------
• [0.133 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:48:51.526
    May  4 12:48:51.527: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename csistoragecapacity 05/04/23 12:48:51.527
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:48:51.545
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:48:51.548
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 05/04/23 12:48:51.551
    STEP: getting /apis/storage.k8s.io 05/04/23 12:48:51.554
    STEP: getting /apis/storage.k8s.io/v1 05/04/23 12:48:51.555
    STEP: creating 05/04/23 12:48:51.557
    STEP: watching 05/04/23 12:48:51.58
    May  4 12:48:51.580: INFO: starting watch
    STEP: getting 05/04/23 12:48:51.587
    STEP: listing in namespace 05/04/23 12:48:51.591
    STEP: listing across namespaces 05/04/23 12:48:51.594
    STEP: patching 05/04/23 12:48:51.598
    STEP: updating 05/04/23 12:48:51.604
    May  4 12:48:51.610: INFO: waiting for watch events with expected annotations in namespace
    May  4 12:48:51.610: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 05/04/23 12:48:51.61
    STEP: deleting a collection 05/04/23 12:48:51.623
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:187
    May  4 12:48:51.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "csistoragecapacity-1228" for this suite. 05/04/23 12:48:51.649
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:48:51.66
May  4 12:48:51.660: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename dns 05/04/23 12:48:51.661
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:48:51.681
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:48:51.686
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 05/04/23 12:48:51.688
May  4 12:48:51.700: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-6188  079df477-da0b-47f1-a81e-eebd1194c6ed 31635 0 2023-05-04 12:48:51 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-05-04 12:48:51 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cm8c2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cm8c2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  4 12:48:51.700: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-6188" to be "running and ready"
May  4 12:48:51.707: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 6.748496ms
May  4 12:48:51.707: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
May  4 12:48:53.712: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.011905804s
May  4 12:48:53.712: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
May  4 12:48:53.712: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 05/04/23 12:48:53.712
May  4 12:48:53.712: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-6188 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  4 12:48:53.713: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
May  4 12:48:53.713: INFO: ExecWithOptions: Clientset creation
May  4 12:48:53.713: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/dns-6188/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 05/04/23 12:48:53.804
May  4 12:48:53.805: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-6188 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  4 12:48:53.805: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
May  4 12:48:53.805: INFO: ExecWithOptions: Clientset creation
May  4 12:48:53.805: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/dns-6188/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May  4 12:48:53.906: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
May  4 12:48:53.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6188" for this suite. 05/04/23 12:48:53.929
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","completed":224,"skipped":4219,"failed":0}
------------------------------
• [2.276 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:48:51.66
    May  4 12:48:51.660: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename dns 05/04/23 12:48:51.661
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:48:51.681
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:48:51.686
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 05/04/23 12:48:51.688
    May  4 12:48:51.700: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-6188  079df477-da0b-47f1-a81e-eebd1194c6ed 31635 0 2023-05-04 12:48:51 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-05-04 12:48:51 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cm8c2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cm8c2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  4 12:48:51.700: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-6188" to be "running and ready"
    May  4 12:48:51.707: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 6.748496ms
    May  4 12:48:51.707: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    May  4 12:48:53.712: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.011905804s
    May  4 12:48:53.712: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    May  4 12:48:53.712: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 05/04/23 12:48:53.712
    May  4 12:48:53.712: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-6188 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  4 12:48:53.713: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    May  4 12:48:53.713: INFO: ExecWithOptions: Clientset creation
    May  4 12:48:53.713: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/dns-6188/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 05/04/23 12:48:53.804
    May  4 12:48:53.805: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-6188 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  4 12:48:53.805: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    May  4 12:48:53.805: INFO: ExecWithOptions: Clientset creation
    May  4 12:48:53.805: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/dns-6188/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May  4 12:48:53.906: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    May  4 12:48:53.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-6188" for this suite. 05/04/23 12:48:53.929
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:48:53.937
May  4 12:48:53.937: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename webhook 05/04/23 12:48:53.938
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:48:53.96
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:48:53.969
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/04/23 12:48:53.994
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/04/23 12:48:54.494
STEP: Deploying the webhook pod 05/04/23 12:48:54.505
STEP: Wait for the deployment to be ready 05/04/23 12:48:54.52
May  4 12:48:54.529: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/04/23 12:48:56.545
STEP: Verifying the service has paired with the endpoint 05/04/23 12:48:56.559
May  4 12:48:57.560: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
STEP: Listing all of the created validation webhooks 05/04/23 12:48:57.641
STEP: Creating a configMap that does not comply to the validation webhook rules 05/04/23 12:48:57.674
STEP: Deleting the collection of validation webhooks 05/04/23 12:48:57.704
STEP: Creating a configMap that does not comply to the validation webhook rules 05/04/23 12:48:57.763
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  4 12:48:57.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9678" for this suite. 05/04/23 12:48:57.783
STEP: Destroying namespace "webhook-9678-markers" for this suite. 05/04/23 12:48:57.793
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","completed":225,"skipped":4219,"failed":0}
------------------------------
• [3.937 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:48:53.937
    May  4 12:48:53.937: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename webhook 05/04/23 12:48:53.938
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:48:53.96
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:48:53.969
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/04/23 12:48:53.994
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/04/23 12:48:54.494
    STEP: Deploying the webhook pod 05/04/23 12:48:54.505
    STEP: Wait for the deployment to be ready 05/04/23 12:48:54.52
    May  4 12:48:54.529: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/04/23 12:48:56.545
    STEP: Verifying the service has paired with the endpoint 05/04/23 12:48:56.559
    May  4 12:48:57.560: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:581
    STEP: Listing all of the created validation webhooks 05/04/23 12:48:57.641
    STEP: Creating a configMap that does not comply to the validation webhook rules 05/04/23 12:48:57.674
    STEP: Deleting the collection of validation webhooks 05/04/23 12:48:57.704
    STEP: Creating a configMap that does not comply to the validation webhook rules 05/04/23 12:48:57.763
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  4 12:48:57.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9678" for this suite. 05/04/23 12:48:57.783
    STEP: Destroying namespace "webhook-9678-markers" for this suite. 05/04/23 12:48:57.793
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:48:57.874
May  4 12:48:57.876: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename events 05/04/23 12:48:57.878
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:48:57.9
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:48:57.904
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 05/04/23 12:48:57.908
STEP: listing all events in all namespaces 05/04/23 12:48:57.92
STEP: patching the test event 05/04/23 12:48:57.927
STEP: fetching the test event 05/04/23 12:48:57.936
STEP: updating the test event 05/04/23 12:48:57.941
STEP: getting the test event 05/04/23 12:48:57.954
STEP: deleting the test event 05/04/23 12:48:57.964
STEP: listing all events in all namespaces 05/04/23 12:48:57.979
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
May  4 12:48:57.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4245" for this suite. 05/04/23 12:48:57.993
{"msg":"PASSED [sig-instrumentation] Events should manage the lifecycle of an event [Conformance]","completed":226,"skipped":4219,"failed":0}
------------------------------
• [0.127 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:48:57.874
    May  4 12:48:57.876: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename events 05/04/23 12:48:57.878
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:48:57.9
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:48:57.904
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 05/04/23 12:48:57.908
    STEP: listing all events in all namespaces 05/04/23 12:48:57.92
    STEP: patching the test event 05/04/23 12:48:57.927
    STEP: fetching the test event 05/04/23 12:48:57.936
    STEP: updating the test event 05/04/23 12:48:57.941
    STEP: getting the test event 05/04/23 12:48:57.954
    STEP: deleting the test event 05/04/23 12:48:57.964
    STEP: listing all events in all namespaces 05/04/23 12:48:57.979
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    May  4 12:48:57.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-4245" for this suite. 05/04/23 12:48:57.993
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:48:58.001
May  4 12:48:58.002: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename pod-network-test 05/04/23 12:48:58.002
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:48:58.018
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:48:58.021
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-2559 05/04/23 12:48:58.024
STEP: creating a selector 05/04/23 12:48:58.024
STEP: Creating the service pods in kubernetes 05/04/23 12:48:58.024
May  4 12:48:58.024: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May  4 12:48:58.131: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2559" to be "running and ready"
May  4 12:48:58.143: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.11098ms
May  4 12:48:58.143: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May  4 12:49:00.151: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.019813939s
May  4 12:49:00.151: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  4 12:49:02.149: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.018096048s
May  4 12:49:02.149: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  4 12:49:04.150: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.019188364s
May  4 12:49:04.151: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  4 12:49:06.152: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.020461044s
May  4 12:49:06.152: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  4 12:49:08.149: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.017948857s
May  4 12:49:08.149: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  4 12:49:10.151: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.019564147s
May  4 12:49:10.151: INFO: The phase of Pod netserver-0 is Running (Ready = true)
May  4 12:49:10.151: INFO: Pod "netserver-0" satisfied condition "running and ready"
May  4 12:49:10.155: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2559" to be "running and ready"
May  4 12:49:10.160: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.45369ms
May  4 12:49:10.160: INFO: The phase of Pod netserver-1 is Running (Ready = true)
May  4 12:49:10.160: INFO: Pod "netserver-1" satisfied condition "running and ready"
May  4 12:49:10.165: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2559" to be "running and ready"
May  4 12:49:10.170: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.249724ms
May  4 12:49:10.170: INFO: The phase of Pod netserver-2 is Running (Ready = true)
May  4 12:49:10.170: INFO: Pod "netserver-2" satisfied condition "running and ready"
May  4 12:49:10.175: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-2559" to be "running and ready"
May  4 12:49:10.179: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 4.185651ms
May  4 12:49:10.179: INFO: The phase of Pod netserver-3 is Running (Ready = true)
May  4 12:49:10.179: INFO: Pod "netserver-3" satisfied condition "running and ready"
May  4 12:49:10.187: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-2559" to be "running and ready"
May  4 12:49:10.194: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=false. Elapsed: 6.672442ms
May  4 12:49:10.194: INFO: The phase of Pod netserver-4 is Running (Ready = false)
May  4 12:49:12.200: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=false. Elapsed: 2.012564802s
May  4 12:49:12.200: INFO: The phase of Pod netserver-4 is Running (Ready = false)
May  4 12:49:14.199: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=false. Elapsed: 4.011967347s
May  4 12:49:14.199: INFO: The phase of Pod netserver-4 is Running (Ready = false)
May  4 12:49:16.199: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=false. Elapsed: 6.011952886s
May  4 12:49:16.199: INFO: The phase of Pod netserver-4 is Running (Ready = false)
May  4 12:49:18.199: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=false. Elapsed: 8.011546074s
May  4 12:49:18.199: INFO: The phase of Pod netserver-4 is Running (Ready = false)
May  4 12:49:20.199: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 10.011310651s
May  4 12:49:20.199: INFO: The phase of Pod netserver-4 is Running (Ready = true)
May  4 12:49:20.199: INFO: Pod "netserver-4" satisfied condition "running and ready"
STEP: Creating test pods 05/04/23 12:49:20.202
May  4 12:49:20.221: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2559" to be "running"
May  4 12:49:20.227: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.221806ms
May  4 12:49:22.234: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013145419s
May  4 12:49:22.234: INFO: Pod "test-container-pod" satisfied condition "running"
May  4 12:49:22.239: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-2559" to be "running"
May  4 12:49:22.243: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.809051ms
May  4 12:49:22.243: INFO: Pod "host-test-container-pod" satisfied condition "running"
May  4 12:49:22.246: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
May  4 12:49:22.246: INFO: Going to poll 10.20.11.252 on port 8081 at least 0 times, with a maximum of 55 tries before failing
May  4 12:49:22.250: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.20.11.252 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2559 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  4 12:49:22.250: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
May  4 12:49:22.251: INFO: ExecWithOptions: Clientset creation
May  4 12:49:22.251: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-2559/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.20.11.252+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May  4 12:49:23.333: INFO: Found all 1 expected endpoints: [netserver-0]
May  4 12:49:23.334: INFO: Going to poll 10.20.142.161 on port 8081 at least 0 times, with a maximum of 55 tries before failing
May  4 12:49:23.338: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.20.142.161 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2559 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  4 12:49:23.338: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
May  4 12:49:23.338: INFO: ExecWithOptions: Clientset creation
May  4 12:49:23.338: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-2559/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.20.142.161+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May  4 12:49:24.427: INFO: Found all 1 expected endpoints: [netserver-1]
May  4 12:49:24.427: INFO: Going to poll 10.20.83.46 on port 8081 at least 0 times, with a maximum of 55 tries before failing
May  4 12:49:24.431: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.20.83.46 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2559 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  4 12:49:24.431: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
May  4 12:49:24.432: INFO: ExecWithOptions: Clientset creation
May  4 12:49:24.432: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-2559/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.20.83.46+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May  4 12:49:25.500: INFO: Found all 1 expected endpoints: [netserver-2]
May  4 12:49:25.500: INFO: Going to poll 10.20.92.171 on port 8081 at least 0 times, with a maximum of 55 tries before failing
May  4 12:49:25.505: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.20.92.171 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2559 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  4 12:49:25.505: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
May  4 12:49:25.505: INFO: ExecWithOptions: Clientset creation
May  4 12:49:25.505: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-2559/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.20.92.171+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May  4 12:49:26.580: INFO: Found all 1 expected endpoints: [netserver-3]
May  4 12:49:26.580: INFO: Going to poll 10.20.1.176 on port 8081 at least 0 times, with a maximum of 55 tries before failing
May  4 12:49:26.584: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.20.1.176 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2559 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  4 12:49:26.584: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
May  4 12:49:26.585: INFO: ExecWithOptions: Clientset creation
May  4 12:49:26.585: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-2559/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.20.1.176+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May  4 12:49:27.664: INFO: Found all 1 expected endpoints: [netserver-4]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
May  4 12:49:27.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2559" for this suite. 05/04/23 12:49:27.672
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","completed":227,"skipped":4225,"failed":0}
------------------------------
• [SLOW TEST] [29.678 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:48:58.001
    May  4 12:48:58.002: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename pod-network-test 05/04/23 12:48:58.002
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:48:58.018
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:48:58.021
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-2559 05/04/23 12:48:58.024
    STEP: creating a selector 05/04/23 12:48:58.024
    STEP: Creating the service pods in kubernetes 05/04/23 12:48:58.024
    May  4 12:48:58.024: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    May  4 12:48:58.131: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2559" to be "running and ready"
    May  4 12:48:58.143: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.11098ms
    May  4 12:48:58.143: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    May  4 12:49:00.151: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.019813939s
    May  4 12:49:00.151: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  4 12:49:02.149: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.018096048s
    May  4 12:49:02.149: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  4 12:49:04.150: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.019188364s
    May  4 12:49:04.151: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  4 12:49:06.152: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.020461044s
    May  4 12:49:06.152: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  4 12:49:08.149: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.017948857s
    May  4 12:49:08.149: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  4 12:49:10.151: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.019564147s
    May  4 12:49:10.151: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    May  4 12:49:10.151: INFO: Pod "netserver-0" satisfied condition "running and ready"
    May  4 12:49:10.155: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2559" to be "running and ready"
    May  4 12:49:10.160: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.45369ms
    May  4 12:49:10.160: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    May  4 12:49:10.160: INFO: Pod "netserver-1" satisfied condition "running and ready"
    May  4 12:49:10.165: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2559" to be "running and ready"
    May  4 12:49:10.170: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.249724ms
    May  4 12:49:10.170: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    May  4 12:49:10.170: INFO: Pod "netserver-2" satisfied condition "running and ready"
    May  4 12:49:10.175: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-2559" to be "running and ready"
    May  4 12:49:10.179: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 4.185651ms
    May  4 12:49:10.179: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    May  4 12:49:10.179: INFO: Pod "netserver-3" satisfied condition "running and ready"
    May  4 12:49:10.187: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-2559" to be "running and ready"
    May  4 12:49:10.194: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=false. Elapsed: 6.672442ms
    May  4 12:49:10.194: INFO: The phase of Pod netserver-4 is Running (Ready = false)
    May  4 12:49:12.200: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=false. Elapsed: 2.012564802s
    May  4 12:49:12.200: INFO: The phase of Pod netserver-4 is Running (Ready = false)
    May  4 12:49:14.199: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=false. Elapsed: 4.011967347s
    May  4 12:49:14.199: INFO: The phase of Pod netserver-4 is Running (Ready = false)
    May  4 12:49:16.199: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=false. Elapsed: 6.011952886s
    May  4 12:49:16.199: INFO: The phase of Pod netserver-4 is Running (Ready = false)
    May  4 12:49:18.199: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=false. Elapsed: 8.011546074s
    May  4 12:49:18.199: INFO: The phase of Pod netserver-4 is Running (Ready = false)
    May  4 12:49:20.199: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 10.011310651s
    May  4 12:49:20.199: INFO: The phase of Pod netserver-4 is Running (Ready = true)
    May  4 12:49:20.199: INFO: Pod "netserver-4" satisfied condition "running and ready"
    STEP: Creating test pods 05/04/23 12:49:20.202
    May  4 12:49:20.221: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2559" to be "running"
    May  4 12:49:20.227: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.221806ms
    May  4 12:49:22.234: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013145419s
    May  4 12:49:22.234: INFO: Pod "test-container-pod" satisfied condition "running"
    May  4 12:49:22.239: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-2559" to be "running"
    May  4 12:49:22.243: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.809051ms
    May  4 12:49:22.243: INFO: Pod "host-test-container-pod" satisfied condition "running"
    May  4 12:49:22.246: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
    May  4 12:49:22.246: INFO: Going to poll 10.20.11.252 on port 8081 at least 0 times, with a maximum of 55 tries before failing
    May  4 12:49:22.250: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.20.11.252 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2559 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  4 12:49:22.250: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    May  4 12:49:22.251: INFO: ExecWithOptions: Clientset creation
    May  4 12:49:22.251: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-2559/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.20.11.252+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May  4 12:49:23.333: INFO: Found all 1 expected endpoints: [netserver-0]
    May  4 12:49:23.334: INFO: Going to poll 10.20.142.161 on port 8081 at least 0 times, with a maximum of 55 tries before failing
    May  4 12:49:23.338: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.20.142.161 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2559 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  4 12:49:23.338: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    May  4 12:49:23.338: INFO: ExecWithOptions: Clientset creation
    May  4 12:49:23.338: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-2559/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.20.142.161+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May  4 12:49:24.427: INFO: Found all 1 expected endpoints: [netserver-1]
    May  4 12:49:24.427: INFO: Going to poll 10.20.83.46 on port 8081 at least 0 times, with a maximum of 55 tries before failing
    May  4 12:49:24.431: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.20.83.46 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2559 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  4 12:49:24.431: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    May  4 12:49:24.432: INFO: ExecWithOptions: Clientset creation
    May  4 12:49:24.432: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-2559/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.20.83.46+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May  4 12:49:25.500: INFO: Found all 1 expected endpoints: [netserver-2]
    May  4 12:49:25.500: INFO: Going to poll 10.20.92.171 on port 8081 at least 0 times, with a maximum of 55 tries before failing
    May  4 12:49:25.505: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.20.92.171 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2559 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  4 12:49:25.505: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    May  4 12:49:25.505: INFO: ExecWithOptions: Clientset creation
    May  4 12:49:25.505: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-2559/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.20.92.171+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May  4 12:49:26.580: INFO: Found all 1 expected endpoints: [netserver-3]
    May  4 12:49:26.580: INFO: Going to poll 10.20.1.176 on port 8081 at least 0 times, with a maximum of 55 tries before failing
    May  4 12:49:26.584: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.20.1.176 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2559 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  4 12:49:26.584: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    May  4 12:49:26.585: INFO: ExecWithOptions: Clientset creation
    May  4 12:49:26.585: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-2559/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.20.1.176+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May  4 12:49:27.664: INFO: Found all 1 expected endpoints: [netserver-4]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    May  4 12:49:27.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-2559" for this suite. 05/04/23 12:49:27.672
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:49:27.681
May  4 12:49:27.681: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename configmap 05/04/23 12:49:27.682
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:49:27.705
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:49:27.709
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
STEP: Creating configMap with name configmap-test-volume-map-bfd22f4a-2f27-45d1-85aa-17dde353560f 05/04/23 12:49:27.711
STEP: Creating a pod to test consume configMaps 05/04/23 12:49:27.717
May  4 12:49:27.733: INFO: Waiting up to 5m0s for pod "pod-configmaps-b612ee85-30ba-47a1-9be1-3dbbe77405a3" in namespace "configmap-9714" to be "Succeeded or Failed"
May  4 12:49:27.740: INFO: Pod "pod-configmaps-b612ee85-30ba-47a1-9be1-3dbbe77405a3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.997288ms
May  4 12:49:29.747: INFO: Pod "pod-configmaps-b612ee85-30ba-47a1-9be1-3dbbe77405a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013980611s
May  4 12:49:31.747: INFO: Pod "pod-configmaps-b612ee85-30ba-47a1-9be1-3dbbe77405a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013468587s
STEP: Saw pod success 05/04/23 12:49:31.747
May  4 12:49:31.747: INFO: Pod "pod-configmaps-b612ee85-30ba-47a1-9be1-3dbbe77405a3" satisfied condition "Succeeded or Failed"
May  4 12:49:31.751: INFO: Trying to get logs from node ip-10-0-1-232.us-west-2.compute.internal pod pod-configmaps-b612ee85-30ba-47a1-9be1-3dbbe77405a3 container agnhost-container: <nil>
STEP: delete the pod 05/04/23 12:49:31.768
May  4 12:49:31.789: INFO: Waiting for pod pod-configmaps-b612ee85-30ba-47a1-9be1-3dbbe77405a3 to disappear
May  4 12:49:31.792: INFO: Pod pod-configmaps-b612ee85-30ba-47a1-9be1-3dbbe77405a3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
May  4 12:49:31.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9714" for this suite. 05/04/23 12:49:31.801
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":228,"skipped":4235,"failed":0}
------------------------------
• [4.128 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:49:27.681
    May  4 12:49:27.681: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename configmap 05/04/23 12:49:27.682
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:49:27.705
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:49:27.709
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:98
    STEP: Creating configMap with name configmap-test-volume-map-bfd22f4a-2f27-45d1-85aa-17dde353560f 05/04/23 12:49:27.711
    STEP: Creating a pod to test consume configMaps 05/04/23 12:49:27.717
    May  4 12:49:27.733: INFO: Waiting up to 5m0s for pod "pod-configmaps-b612ee85-30ba-47a1-9be1-3dbbe77405a3" in namespace "configmap-9714" to be "Succeeded or Failed"
    May  4 12:49:27.740: INFO: Pod "pod-configmaps-b612ee85-30ba-47a1-9be1-3dbbe77405a3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.997288ms
    May  4 12:49:29.747: INFO: Pod "pod-configmaps-b612ee85-30ba-47a1-9be1-3dbbe77405a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013980611s
    May  4 12:49:31.747: INFO: Pod "pod-configmaps-b612ee85-30ba-47a1-9be1-3dbbe77405a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013468587s
    STEP: Saw pod success 05/04/23 12:49:31.747
    May  4 12:49:31.747: INFO: Pod "pod-configmaps-b612ee85-30ba-47a1-9be1-3dbbe77405a3" satisfied condition "Succeeded or Failed"
    May  4 12:49:31.751: INFO: Trying to get logs from node ip-10-0-1-232.us-west-2.compute.internal pod pod-configmaps-b612ee85-30ba-47a1-9be1-3dbbe77405a3 container agnhost-container: <nil>
    STEP: delete the pod 05/04/23 12:49:31.768
    May  4 12:49:31.789: INFO: Waiting for pod pod-configmaps-b612ee85-30ba-47a1-9be1-3dbbe77405a3 to disappear
    May  4 12:49:31.792: INFO: Pod pod-configmaps-b612ee85-30ba-47a1-9be1-3dbbe77405a3 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    May  4 12:49:31.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9714" for this suite. 05/04/23 12:49:31.801
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:49:31.812
May  4 12:49:31.812: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename daemonsets 05/04/23 12:49:31.813
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:49:31.828
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:49:31.831
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
May  4 12:49:31.872: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 05/04/23 12:49:31.879
May  4 12:49:31.892: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:31.893: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:31.893: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:31.898: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  4 12:49:31.898: INFO: Node ip-10-0-1-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
May  4 12:49:32.909: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:32.909: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:32.909: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:32.916: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May  4 12:49:32.916: INFO: Node ip-10-0-1-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
May  4 12:49:33.907: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:33.907: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:33.907: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:33.912: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
May  4 12:49:33.912: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
STEP: Update daemon pods image. 05/04/23 12:49:33.935
STEP: Check that daemon pods images are updated. 05/04/23 12:49:34.03
May  4 12:49:34.037: INFO: Wrong image for pod: daemon-set-7znrd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  4 12:49:34.037: INFO: Wrong image for pod: daemon-set-cw5zs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  4 12:49:34.037: INFO: Wrong image for pod: daemon-set-qq6mg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  4 12:49:34.037: INFO: Wrong image for pod: daemon-set-wlrdj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  4 12:49:34.046: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:34.046: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:34.046: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:35.054: INFO: Wrong image for pod: daemon-set-7znrd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  4 12:49:35.054: INFO: Wrong image for pod: daemon-set-cw5zs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  4 12:49:35.054: INFO: Wrong image for pod: daemon-set-qq6mg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  4 12:49:35.054: INFO: Wrong image for pod: daemon-set-wlrdj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  4 12:49:35.072: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:35.073: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:35.073: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:36.052: INFO: Wrong image for pod: daemon-set-7znrd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  4 12:49:36.052: INFO: Pod daemon-set-ckkfc is not available
May  4 12:49:36.052: INFO: Wrong image for pod: daemon-set-cw5zs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  4 12:49:36.052: INFO: Wrong image for pod: daemon-set-qq6mg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  4 12:49:36.052: INFO: Wrong image for pod: daemon-set-wlrdj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  4 12:49:36.061: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:36.061: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:36.061: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:37.055: INFO: Wrong image for pod: daemon-set-7znrd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  4 12:49:37.055: INFO: Pod daemon-set-ckkfc is not available
May  4 12:49:37.055: INFO: Wrong image for pod: daemon-set-cw5zs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  4 12:49:37.055: INFO: Wrong image for pod: daemon-set-qq6mg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  4 12:49:37.055: INFO: Wrong image for pod: daemon-set-wlrdj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  4 12:49:37.063: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:37.063: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:37.063: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:38.053: INFO: Wrong image for pod: daemon-set-cw5zs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  4 12:49:38.053: INFO: Wrong image for pod: daemon-set-qq6mg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  4 12:49:38.053: INFO: Wrong image for pod: daemon-set-wlrdj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  4 12:49:38.067: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:38.067: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:38.067: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:39.052: INFO: Wrong image for pod: daemon-set-cw5zs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  4 12:49:39.052: INFO: Wrong image for pod: daemon-set-qq6mg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  4 12:49:39.052: INFO: Wrong image for pod: daemon-set-wlrdj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  4 12:49:39.061: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:39.061: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:39.061: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:40.052: INFO: Wrong image for pod: daemon-set-cw5zs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  4 12:49:40.052: INFO: Pod daemon-set-pfntp is not available
May  4 12:49:40.052: INFO: Wrong image for pod: daemon-set-qq6mg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  4 12:49:40.052: INFO: Wrong image for pod: daemon-set-wlrdj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  4 12:49:40.060: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:40.060: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:40.060: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:41.062: INFO: Wrong image for pod: daemon-set-qq6mg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  4 12:49:41.062: INFO: Pod daemon-set-vccq2 is not available
May  4 12:49:41.062: INFO: Wrong image for pod: daemon-set-wlrdj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  4 12:49:41.069: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:41.069: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:41.069: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:42.053: INFO: Wrong image for pod: daemon-set-qq6mg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  4 12:49:42.053: INFO: Pod daemon-set-vccq2 is not available
May  4 12:49:42.053: INFO: Wrong image for pod: daemon-set-wlrdj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  4 12:49:42.066: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:42.066: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:42.066: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:43.054: INFO: Wrong image for pod: daemon-set-wlrdj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  4 12:49:43.060: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:43.061: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:43.061: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:44.054: INFO: Wrong image for pod: daemon-set-wlrdj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  4 12:49:44.061: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:44.061: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:44.061: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:45.053: INFO: Pod daemon-set-c82jb is not available
May  4 12:49:45.053: INFO: Wrong image for pod: daemon-set-wlrdj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  4 12:49:45.062: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:45.062: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:45.062: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:46.073: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:46.073: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:46.073: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:47.053: INFO: Pod daemon-set-4dqqx is not available
May  4 12:49:47.061: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:47.061: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:47.061: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster. 05/04/23 12:49:47.061
May  4 12:49:47.076: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:47.076: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:47.076: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:47.085: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
May  4 12:49:47.085: INFO: Node ip-10-0-1-224.us-west-2.compute.internal is running 0 daemon pod, expected 1
May  4 12:49:48.113: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:48.113: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:48.113: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 12:49:48.126: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
May  4 12:49:48.126: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 05/04/23 12:49:48.17
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6164, will wait for the garbage collector to delete the pods 05/04/23 12:49:48.17
May  4 12:49:48.237: INFO: Deleting DaemonSet.extensions daemon-set took: 10.711099ms
May  4 12:49:48.437: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.183355ms
May  4 12:49:50.642: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  4 12:49:50.642: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May  4 12:49:50.646: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"32368"},"items":null}

May  4 12:49:50.649: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"32368"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
May  4 12:49:50.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6164" for this suite. 05/04/23 12:49:50.687
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","completed":229,"skipped":4282,"failed":0}
------------------------------
• [SLOW TEST] [18.887 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:49:31.812
    May  4 12:49:31.812: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename daemonsets 05/04/23 12:49:31.813
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:49:31.828
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:49:31.831
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:373
    May  4 12:49:31.872: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 05/04/23 12:49:31.879
    May  4 12:49:31.892: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:31.893: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:31.893: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:31.898: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  4 12:49:31.898: INFO: Node ip-10-0-1-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
    May  4 12:49:32.909: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:32.909: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:32.909: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:32.916: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May  4 12:49:32.916: INFO: Node ip-10-0-1-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
    May  4 12:49:33.907: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:33.907: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:33.907: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:33.912: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    May  4 12:49:33.912: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    STEP: Update daemon pods image. 05/04/23 12:49:33.935
    STEP: Check that daemon pods images are updated. 05/04/23 12:49:34.03
    May  4 12:49:34.037: INFO: Wrong image for pod: daemon-set-7znrd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  4 12:49:34.037: INFO: Wrong image for pod: daemon-set-cw5zs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  4 12:49:34.037: INFO: Wrong image for pod: daemon-set-qq6mg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  4 12:49:34.037: INFO: Wrong image for pod: daemon-set-wlrdj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  4 12:49:34.046: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:34.046: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:34.046: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:35.054: INFO: Wrong image for pod: daemon-set-7znrd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  4 12:49:35.054: INFO: Wrong image for pod: daemon-set-cw5zs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  4 12:49:35.054: INFO: Wrong image for pod: daemon-set-qq6mg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  4 12:49:35.054: INFO: Wrong image for pod: daemon-set-wlrdj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  4 12:49:35.072: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:35.073: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:35.073: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:36.052: INFO: Wrong image for pod: daemon-set-7znrd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  4 12:49:36.052: INFO: Pod daemon-set-ckkfc is not available
    May  4 12:49:36.052: INFO: Wrong image for pod: daemon-set-cw5zs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  4 12:49:36.052: INFO: Wrong image for pod: daemon-set-qq6mg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  4 12:49:36.052: INFO: Wrong image for pod: daemon-set-wlrdj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  4 12:49:36.061: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:36.061: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:36.061: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:37.055: INFO: Wrong image for pod: daemon-set-7znrd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  4 12:49:37.055: INFO: Pod daemon-set-ckkfc is not available
    May  4 12:49:37.055: INFO: Wrong image for pod: daemon-set-cw5zs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  4 12:49:37.055: INFO: Wrong image for pod: daemon-set-qq6mg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  4 12:49:37.055: INFO: Wrong image for pod: daemon-set-wlrdj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  4 12:49:37.063: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:37.063: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:37.063: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:38.053: INFO: Wrong image for pod: daemon-set-cw5zs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  4 12:49:38.053: INFO: Wrong image for pod: daemon-set-qq6mg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  4 12:49:38.053: INFO: Wrong image for pod: daemon-set-wlrdj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  4 12:49:38.067: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:38.067: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:38.067: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:39.052: INFO: Wrong image for pod: daemon-set-cw5zs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  4 12:49:39.052: INFO: Wrong image for pod: daemon-set-qq6mg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  4 12:49:39.052: INFO: Wrong image for pod: daemon-set-wlrdj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  4 12:49:39.061: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:39.061: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:39.061: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:40.052: INFO: Wrong image for pod: daemon-set-cw5zs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  4 12:49:40.052: INFO: Pod daemon-set-pfntp is not available
    May  4 12:49:40.052: INFO: Wrong image for pod: daemon-set-qq6mg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  4 12:49:40.052: INFO: Wrong image for pod: daemon-set-wlrdj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  4 12:49:40.060: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:40.060: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:40.060: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:41.062: INFO: Wrong image for pod: daemon-set-qq6mg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  4 12:49:41.062: INFO: Pod daemon-set-vccq2 is not available
    May  4 12:49:41.062: INFO: Wrong image for pod: daemon-set-wlrdj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  4 12:49:41.069: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:41.069: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:41.069: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:42.053: INFO: Wrong image for pod: daemon-set-qq6mg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  4 12:49:42.053: INFO: Pod daemon-set-vccq2 is not available
    May  4 12:49:42.053: INFO: Wrong image for pod: daemon-set-wlrdj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  4 12:49:42.066: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:42.066: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:42.066: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:43.054: INFO: Wrong image for pod: daemon-set-wlrdj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  4 12:49:43.060: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:43.061: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:43.061: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:44.054: INFO: Wrong image for pod: daemon-set-wlrdj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  4 12:49:44.061: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:44.061: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:44.061: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:45.053: INFO: Pod daemon-set-c82jb is not available
    May  4 12:49:45.053: INFO: Wrong image for pod: daemon-set-wlrdj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  4 12:49:45.062: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:45.062: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:45.062: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:46.073: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:46.073: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:46.073: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:47.053: INFO: Pod daemon-set-4dqqx is not available
    May  4 12:49:47.061: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:47.061: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:47.061: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    STEP: Check that daemon pods are still running on every node of the cluster. 05/04/23 12:49:47.061
    May  4 12:49:47.076: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:47.076: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:47.076: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:47.085: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    May  4 12:49:47.085: INFO: Node ip-10-0-1-224.us-west-2.compute.internal is running 0 daemon pod, expected 1
    May  4 12:49:48.113: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:48.113: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:48.113: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 12:49:48.126: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    May  4 12:49:48.126: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 05/04/23 12:49:48.17
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6164, will wait for the garbage collector to delete the pods 05/04/23 12:49:48.17
    May  4 12:49:48.237: INFO: Deleting DaemonSet.extensions daemon-set took: 10.711099ms
    May  4 12:49:48.437: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.183355ms
    May  4 12:49:50.642: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  4 12:49:50.642: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    May  4 12:49:50.646: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"32368"},"items":null}

    May  4 12:49:50.649: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"32368"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    May  4 12:49:50.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-6164" for this suite. 05/04/23 12:49:50.687
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:49:50.7
May  4 12:49:50.700: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename gc 05/04/23 12:49:50.701
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:49:50.721
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:49:50.725
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 05/04/23 12:49:50.728
STEP: Wait for the Deployment to create new ReplicaSet 05/04/23 12:49:50.737
STEP: delete the deployment 05/04/23 12:49:51.263
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 05/04/23 12:49:51.276
STEP: Gathering metrics 05/04/23 12:49:51.806
W0504 12:49:51.827975      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
May  4 12:49:51.828: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
May  4 12:49:51.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5552" for this suite. 05/04/23 12:49:51.837
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","completed":230,"skipped":4292,"failed":0}
------------------------------
• [1.144 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:49:50.7
    May  4 12:49:50.700: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename gc 05/04/23 12:49:50.701
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:49:50.721
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:49:50.725
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 05/04/23 12:49:50.728
    STEP: Wait for the Deployment to create new ReplicaSet 05/04/23 12:49:50.737
    STEP: delete the deployment 05/04/23 12:49:51.263
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 05/04/23 12:49:51.276
    STEP: Gathering metrics 05/04/23 12:49:51.806
    W0504 12:49:51.827975      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    May  4 12:49:51.828: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    May  4 12:49:51.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-5552" for this suite. 05/04/23 12:49:51.837
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:49:51.844
May  4 12:49:51.844: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename svcaccounts 05/04/23 12:49:51.845
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:49:51.864
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:49:51.87
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
STEP: creating a ServiceAccount 05/04/23 12:49:51.873
STEP: watching for the ServiceAccount to be added 05/04/23 12:49:51.884
STEP: patching the ServiceAccount 05/04/23 12:49:51.886
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 05/04/23 12:49:51.899
STEP: deleting the ServiceAccount 05/04/23 12:49:51.908
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
May  4 12:49:51.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7694" for this suite. 05/04/23 12:49:51.943
{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","completed":231,"skipped":4294,"failed":0}
------------------------------
• [0.108 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:49:51.844
    May  4 12:49:51.844: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename svcaccounts 05/04/23 12:49:51.845
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:49:51.864
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:49:51.87
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:646
    STEP: creating a ServiceAccount 05/04/23 12:49:51.873
    STEP: watching for the ServiceAccount to be added 05/04/23 12:49:51.884
    STEP: patching the ServiceAccount 05/04/23 12:49:51.886
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 05/04/23 12:49:51.899
    STEP: deleting the ServiceAccount 05/04/23 12:49:51.908
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    May  4 12:49:51.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-7694" for this suite. 05/04/23 12:49:51.943
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:49:51.953
May  4 12:49:51.953: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename svcaccounts 05/04/23 12:49:51.955
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:49:51.979
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:49:51.986
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
May  4 12:49:52.030: INFO: Waiting up to 5m0s for pod "pod-service-account-e1e8b357-7eb4-4357-9ce8-88eae3b0d202" in namespace "svcaccounts-7817" to be "running"
May  4 12:49:52.035: INFO: Pod "pod-service-account-e1e8b357-7eb4-4357-9ce8-88eae3b0d202": Phase="Pending", Reason="", readiness=false. Elapsed: 4.476349ms
May  4 12:49:54.040: INFO: Pod "pod-service-account-e1e8b357-7eb4-4357-9ce8-88eae3b0d202": Phase="Running", Reason="", readiness=true. Elapsed: 2.009525954s
May  4 12:49:54.040: INFO: Pod "pod-service-account-e1e8b357-7eb4-4357-9ce8-88eae3b0d202" satisfied condition "running"
STEP: reading a file in the container 05/04/23 12:49:54.04
May  4 12:49:54.040: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7817 pod-service-account-e1e8b357-7eb4-4357-9ce8-88eae3b0d202 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 05/04/23 12:49:54.224
May  4 12:49:54.225: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7817 pod-service-account-e1e8b357-7eb4-4357-9ce8-88eae3b0d202 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 05/04/23 12:49:54.393
May  4 12:49:54.393: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7817 pod-service-account-e1e8b357-7eb4-4357-9ce8-88eae3b0d202 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
May  4 12:49:54.575: INFO: Got root ca configmap in namespace "svcaccounts-7817"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
May  4 12:49:54.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7817" for this suite. 05/04/23 12:49:54.584
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","completed":232,"skipped":4296,"failed":0}
------------------------------
• [2.638 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:49:51.953
    May  4 12:49:51.953: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename svcaccounts 05/04/23 12:49:51.955
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:49:51.979
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:49:51.986
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:75
    May  4 12:49:52.030: INFO: Waiting up to 5m0s for pod "pod-service-account-e1e8b357-7eb4-4357-9ce8-88eae3b0d202" in namespace "svcaccounts-7817" to be "running"
    May  4 12:49:52.035: INFO: Pod "pod-service-account-e1e8b357-7eb4-4357-9ce8-88eae3b0d202": Phase="Pending", Reason="", readiness=false. Elapsed: 4.476349ms
    May  4 12:49:54.040: INFO: Pod "pod-service-account-e1e8b357-7eb4-4357-9ce8-88eae3b0d202": Phase="Running", Reason="", readiness=true. Elapsed: 2.009525954s
    May  4 12:49:54.040: INFO: Pod "pod-service-account-e1e8b357-7eb4-4357-9ce8-88eae3b0d202" satisfied condition "running"
    STEP: reading a file in the container 05/04/23 12:49:54.04
    May  4 12:49:54.040: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7817 pod-service-account-e1e8b357-7eb4-4357-9ce8-88eae3b0d202 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 05/04/23 12:49:54.224
    May  4 12:49:54.225: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7817 pod-service-account-e1e8b357-7eb4-4357-9ce8-88eae3b0d202 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 05/04/23 12:49:54.393
    May  4 12:49:54.393: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7817 pod-service-account-e1e8b357-7eb4-4357-9ce8-88eae3b0d202 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    May  4 12:49:54.575: INFO: Got root ca configmap in namespace "svcaccounts-7817"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    May  4 12:49:54.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-7817" for this suite. 05/04/23 12:49:54.584
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:49:54.592
May  4 12:49:54.592: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename container-probe 05/04/23 12:49:54.593
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:49:54.611
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:49:54.614
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
STEP: Creating pod liveness-3483b002-62ad-44a9-9f26-a10ee7f1ec94 in namespace container-probe-7399 05/04/23 12:49:54.617
May  4 12:49:54.626: INFO: Waiting up to 5m0s for pod "liveness-3483b002-62ad-44a9-9f26-a10ee7f1ec94" in namespace "container-probe-7399" to be "not pending"
May  4 12:49:54.632: INFO: Pod "liveness-3483b002-62ad-44a9-9f26-a10ee7f1ec94": Phase="Pending", Reason="", readiness=false. Elapsed: 5.810727ms
May  4 12:49:56.638: INFO: Pod "liveness-3483b002-62ad-44a9-9f26-a10ee7f1ec94": Phase="Running", Reason="", readiness=true. Elapsed: 2.011960265s
May  4 12:49:56.638: INFO: Pod "liveness-3483b002-62ad-44a9-9f26-a10ee7f1ec94" satisfied condition "not pending"
May  4 12:49:56.638: INFO: Started pod liveness-3483b002-62ad-44a9-9f26-a10ee7f1ec94 in namespace container-probe-7399
STEP: checking the pod's current state and verifying that restartCount is present 05/04/23 12:49:56.638
May  4 12:49:56.643: INFO: Initial restart count of pod liveness-3483b002-62ad-44a9-9f26-a10ee7f1ec94 is 0
STEP: deleting the pod 05/04/23 12:53:57.33
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
May  4 12:53:57.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7399" for this suite. 05/04/23 12:53:57.364
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","completed":233,"skipped":4343,"failed":0}
------------------------------
• [SLOW TEST] [242.781 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:49:54.592
    May  4 12:49:54.592: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename container-probe 05/04/23 12:49:54.593
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:49:54.611
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:49:54.614
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:180
    STEP: Creating pod liveness-3483b002-62ad-44a9-9f26-a10ee7f1ec94 in namespace container-probe-7399 05/04/23 12:49:54.617
    May  4 12:49:54.626: INFO: Waiting up to 5m0s for pod "liveness-3483b002-62ad-44a9-9f26-a10ee7f1ec94" in namespace "container-probe-7399" to be "not pending"
    May  4 12:49:54.632: INFO: Pod "liveness-3483b002-62ad-44a9-9f26-a10ee7f1ec94": Phase="Pending", Reason="", readiness=false. Elapsed: 5.810727ms
    May  4 12:49:56.638: INFO: Pod "liveness-3483b002-62ad-44a9-9f26-a10ee7f1ec94": Phase="Running", Reason="", readiness=true. Elapsed: 2.011960265s
    May  4 12:49:56.638: INFO: Pod "liveness-3483b002-62ad-44a9-9f26-a10ee7f1ec94" satisfied condition "not pending"
    May  4 12:49:56.638: INFO: Started pod liveness-3483b002-62ad-44a9-9f26-a10ee7f1ec94 in namespace container-probe-7399
    STEP: checking the pod's current state and verifying that restartCount is present 05/04/23 12:49:56.638
    May  4 12:49:56.643: INFO: Initial restart count of pod liveness-3483b002-62ad-44a9-9f26-a10ee7f1ec94 is 0
    STEP: deleting the pod 05/04/23 12:53:57.33
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    May  4 12:53:57.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-7399" for this suite. 05/04/23 12:53:57.364
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:53:57.375
May  4 12:53:57.375: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename projected 05/04/23 12:53:57.376
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:53:57.402
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:53:57.408
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
STEP: Creating secret with name s-test-opt-del-a824c73b-d79b-4fb4-8800-4a7d919d86de 05/04/23 12:53:57.418
STEP: Creating secret with name s-test-opt-upd-6dd81950-bdef-4f93-80fb-ba8018517490 05/04/23 12:53:57.423
STEP: Creating the pod 05/04/23 12:53:57.428
May  4 12:53:57.447: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e824c5ed-2907-4d6b-9da3-5376c34196c8" in namespace "projected-4777" to be "running and ready"
May  4 12:53:57.458: INFO: Pod "pod-projected-secrets-e824c5ed-2907-4d6b-9da3-5376c34196c8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.817293ms
May  4 12:53:57.458: INFO: The phase of Pod pod-projected-secrets-e824c5ed-2907-4d6b-9da3-5376c34196c8 is Pending, waiting for it to be Running (with Ready = true)
May  4 12:53:59.462: INFO: Pod "pod-projected-secrets-e824c5ed-2907-4d6b-9da3-5376c34196c8": Phase="Running", Reason="", readiness=true. Elapsed: 2.014984923s
May  4 12:53:59.462: INFO: The phase of Pod pod-projected-secrets-e824c5ed-2907-4d6b-9da3-5376c34196c8 is Running (Ready = true)
May  4 12:53:59.462: INFO: Pod "pod-projected-secrets-e824c5ed-2907-4d6b-9da3-5376c34196c8" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-a824c73b-d79b-4fb4-8800-4a7d919d86de 05/04/23 12:53:59.501
STEP: Updating secret s-test-opt-upd-6dd81950-bdef-4f93-80fb-ba8018517490 05/04/23 12:53:59.508
STEP: Creating secret with name s-test-opt-create-fbaead74-89ca-48cc-a3df-20b93618dc40 05/04/23 12:53:59.515
STEP: waiting to observe update in volume 05/04/23 12:53:59.52
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
May  4 12:54:01.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4777" for this suite. 05/04/23 12:54:01.558
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":234,"skipped":4348,"failed":0}
------------------------------
• [4.191 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:53:57.375
    May  4 12:53:57.375: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename projected 05/04/23 12:53:57.376
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:53:57.402
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:53:57.408
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:214
    STEP: Creating secret with name s-test-opt-del-a824c73b-d79b-4fb4-8800-4a7d919d86de 05/04/23 12:53:57.418
    STEP: Creating secret with name s-test-opt-upd-6dd81950-bdef-4f93-80fb-ba8018517490 05/04/23 12:53:57.423
    STEP: Creating the pod 05/04/23 12:53:57.428
    May  4 12:53:57.447: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e824c5ed-2907-4d6b-9da3-5376c34196c8" in namespace "projected-4777" to be "running and ready"
    May  4 12:53:57.458: INFO: Pod "pod-projected-secrets-e824c5ed-2907-4d6b-9da3-5376c34196c8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.817293ms
    May  4 12:53:57.458: INFO: The phase of Pod pod-projected-secrets-e824c5ed-2907-4d6b-9da3-5376c34196c8 is Pending, waiting for it to be Running (with Ready = true)
    May  4 12:53:59.462: INFO: Pod "pod-projected-secrets-e824c5ed-2907-4d6b-9da3-5376c34196c8": Phase="Running", Reason="", readiness=true. Elapsed: 2.014984923s
    May  4 12:53:59.462: INFO: The phase of Pod pod-projected-secrets-e824c5ed-2907-4d6b-9da3-5376c34196c8 is Running (Ready = true)
    May  4 12:53:59.462: INFO: Pod "pod-projected-secrets-e824c5ed-2907-4d6b-9da3-5376c34196c8" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-a824c73b-d79b-4fb4-8800-4a7d919d86de 05/04/23 12:53:59.501
    STEP: Updating secret s-test-opt-upd-6dd81950-bdef-4f93-80fb-ba8018517490 05/04/23 12:53:59.508
    STEP: Creating secret with name s-test-opt-create-fbaead74-89ca-48cc-a3df-20b93618dc40 05/04/23 12:53:59.515
    STEP: waiting to observe update in volume 05/04/23 12:53:59.52
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    May  4 12:54:01.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4777" for this suite. 05/04/23 12:54:01.558
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:54:01.568
May  4 12:54:01.568: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename pod-network-test 05/04/23 12:54:01.569
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:54:01.592
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:54:01.595
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-2368 05/04/23 12:54:01.599
STEP: creating a selector 05/04/23 12:54:01.599
STEP: Creating the service pods in kubernetes 05/04/23 12:54:01.599
May  4 12:54:01.599: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May  4 12:54:01.671: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2368" to be "running and ready"
May  4 12:54:01.688: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 16.920667ms
May  4 12:54:01.688: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May  4 12:54:03.693: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.022639546s
May  4 12:54:03.693: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  4 12:54:05.694: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.023215434s
May  4 12:54:05.694: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  4 12:54:07.693: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.022081784s
May  4 12:54:07.693: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  4 12:54:09.695: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.024494512s
May  4 12:54:09.695: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  4 12:54:11.693: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.022574237s
May  4 12:54:11.693: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  4 12:54:13.692: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.021363734s
May  4 12:54:13.692: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  4 12:54:15.693: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.022564434s
May  4 12:54:15.693: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  4 12:54:17.692: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.021642775s
May  4 12:54:17.692: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  4 12:54:19.693: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.022046835s
May  4 12:54:19.693: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  4 12:54:21.698: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.027637244s
May  4 12:54:21.698: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  4 12:54:23.693: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.022435929s
May  4 12:54:23.693: INFO: The phase of Pod netserver-0 is Running (Ready = true)
May  4 12:54:23.693: INFO: Pod "netserver-0" satisfied condition "running and ready"
May  4 12:54:23.697: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2368" to be "running and ready"
May  4 12:54:23.700: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.376358ms
May  4 12:54:23.700: INFO: The phase of Pod netserver-1 is Running (Ready = true)
May  4 12:54:23.700: INFO: Pod "netserver-1" satisfied condition "running and ready"
May  4 12:54:23.703: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2368" to be "running and ready"
May  4 12:54:23.706: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.843685ms
May  4 12:54:23.706: INFO: The phase of Pod netserver-2 is Running (Ready = true)
May  4 12:54:23.706: INFO: Pod "netserver-2" satisfied condition "running and ready"
May  4 12:54:23.709: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-2368" to be "running and ready"
May  4 12:54:23.712: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 2.738922ms
May  4 12:54:23.712: INFO: The phase of Pod netserver-3 is Running (Ready = true)
May  4 12:54:23.712: INFO: Pod "netserver-3" satisfied condition "running and ready"
May  4 12:54:23.715: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-2368" to be "running and ready"
May  4 12:54:23.718: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 3.229552ms
May  4 12:54:23.718: INFO: The phase of Pod netserver-4 is Running (Ready = true)
May  4 12:54:23.718: INFO: Pod "netserver-4" satisfied condition "running and ready"
STEP: Creating test pods 05/04/23 12:54:23.721
May  4 12:54:23.726: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2368" to be "running"
May  4 12:54:23.729: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.310742ms
May  4 12:54:25.734: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008341802s
May  4 12:54:25.734: INFO: Pod "test-container-pod" satisfied condition "running"
May  4 12:54:25.738: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
May  4 12:54:25.738: INFO: Breadth first check of 10.20.11.255 on host 10.0.1.189...
May  4 12:54:25.743: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.142.172:9080/dial?request=hostname&protocol=udp&host=10.20.11.255&port=8081&tries=1'] Namespace:pod-network-test-2368 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  4 12:54:25.743: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
May  4 12:54:25.744: INFO: ExecWithOptions: Clientset creation
May  4 12:54:25.744: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-2368/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.20.142.172%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.20.11.255%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May  4 12:54:25.831: INFO: Waiting for responses: map[]
May  4 12:54:25.831: INFO: reached 10.20.11.255 after 0/1 tries
May  4 12:54:25.831: INFO: Breadth first check of 10.20.142.179 on host 10.0.1.216...
May  4 12:54:25.837: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.142.172:9080/dial?request=hostname&protocol=udp&host=10.20.142.179&port=8081&tries=1'] Namespace:pod-network-test-2368 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  4 12:54:25.837: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
May  4 12:54:25.837: INFO: ExecWithOptions: Clientset creation
May  4 12:54:25.837: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-2368/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.20.142.172%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.20.142.179%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May  4 12:54:25.921: INFO: Waiting for responses: map[]
May  4 12:54:25.921: INFO: reached 10.20.142.179 after 0/1 tries
May  4 12:54:25.921: INFO: Breadth first check of 10.20.83.6 on host 10.0.1.224...
May  4 12:54:25.926: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.142.172:9080/dial?request=hostname&protocol=udp&host=10.20.83.6&port=8081&tries=1'] Namespace:pod-network-test-2368 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  4 12:54:25.927: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
May  4 12:54:25.927: INFO: ExecWithOptions: Clientset creation
May  4 12:54:25.927: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-2368/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.20.142.172%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.20.83.6%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May  4 12:54:26.012: INFO: Waiting for responses: map[]
May  4 12:54:26.012: INFO: reached 10.20.83.6 after 0/1 tries
May  4 12:54:26.012: INFO: Breadth first check of 10.20.92.159 on host 10.0.1.232...
May  4 12:54:26.020: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.142.172:9080/dial?request=hostname&protocol=udp&host=10.20.92.159&port=8081&tries=1'] Namespace:pod-network-test-2368 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  4 12:54:26.021: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
May  4 12:54:26.021: INFO: ExecWithOptions: Clientset creation
May  4 12:54:26.021: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-2368/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.20.142.172%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.20.92.159%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May  4 12:54:26.115: INFO: Waiting for responses: map[]
May  4 12:54:26.115: INFO: reached 10.20.92.159 after 0/1 tries
May  4 12:54:26.115: INFO: Breadth first check of 10.20.1.179 on host 10.0.1.253...
May  4 12:54:26.119: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.142.172:9080/dial?request=hostname&protocol=udp&host=10.20.1.179&port=8081&tries=1'] Namespace:pod-network-test-2368 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  4 12:54:26.119: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
May  4 12:54:26.119: INFO: ExecWithOptions: Clientset creation
May  4 12:54:26.119: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-2368/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.20.142.172%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.20.1.179%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May  4 12:54:26.213: INFO: Waiting for responses: map[]
May  4 12:54:26.213: INFO: reached 10.20.1.179 after 0/1 tries
May  4 12:54:26.213: INFO: Going to retry 0 out of 5 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
May  4 12:54:26.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2368" for this suite. 05/04/23 12:54:26.222
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","completed":235,"skipped":4367,"failed":0}
------------------------------
• [SLOW TEST] [24.662 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:54:01.568
    May  4 12:54:01.568: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename pod-network-test 05/04/23 12:54:01.569
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:54:01.592
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:54:01.595
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-2368 05/04/23 12:54:01.599
    STEP: creating a selector 05/04/23 12:54:01.599
    STEP: Creating the service pods in kubernetes 05/04/23 12:54:01.599
    May  4 12:54:01.599: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    May  4 12:54:01.671: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2368" to be "running and ready"
    May  4 12:54:01.688: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 16.920667ms
    May  4 12:54:01.688: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    May  4 12:54:03.693: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.022639546s
    May  4 12:54:03.693: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  4 12:54:05.694: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.023215434s
    May  4 12:54:05.694: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  4 12:54:07.693: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.022081784s
    May  4 12:54:07.693: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  4 12:54:09.695: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.024494512s
    May  4 12:54:09.695: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  4 12:54:11.693: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.022574237s
    May  4 12:54:11.693: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  4 12:54:13.692: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.021363734s
    May  4 12:54:13.692: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  4 12:54:15.693: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.022564434s
    May  4 12:54:15.693: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  4 12:54:17.692: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.021642775s
    May  4 12:54:17.692: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  4 12:54:19.693: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.022046835s
    May  4 12:54:19.693: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  4 12:54:21.698: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.027637244s
    May  4 12:54:21.698: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  4 12:54:23.693: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.022435929s
    May  4 12:54:23.693: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    May  4 12:54:23.693: INFO: Pod "netserver-0" satisfied condition "running and ready"
    May  4 12:54:23.697: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2368" to be "running and ready"
    May  4 12:54:23.700: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.376358ms
    May  4 12:54:23.700: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    May  4 12:54:23.700: INFO: Pod "netserver-1" satisfied condition "running and ready"
    May  4 12:54:23.703: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2368" to be "running and ready"
    May  4 12:54:23.706: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.843685ms
    May  4 12:54:23.706: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    May  4 12:54:23.706: INFO: Pod "netserver-2" satisfied condition "running and ready"
    May  4 12:54:23.709: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-2368" to be "running and ready"
    May  4 12:54:23.712: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 2.738922ms
    May  4 12:54:23.712: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    May  4 12:54:23.712: INFO: Pod "netserver-3" satisfied condition "running and ready"
    May  4 12:54:23.715: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-2368" to be "running and ready"
    May  4 12:54:23.718: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 3.229552ms
    May  4 12:54:23.718: INFO: The phase of Pod netserver-4 is Running (Ready = true)
    May  4 12:54:23.718: INFO: Pod "netserver-4" satisfied condition "running and ready"
    STEP: Creating test pods 05/04/23 12:54:23.721
    May  4 12:54:23.726: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2368" to be "running"
    May  4 12:54:23.729: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.310742ms
    May  4 12:54:25.734: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008341802s
    May  4 12:54:25.734: INFO: Pod "test-container-pod" satisfied condition "running"
    May  4 12:54:25.738: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
    May  4 12:54:25.738: INFO: Breadth first check of 10.20.11.255 on host 10.0.1.189...
    May  4 12:54:25.743: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.142.172:9080/dial?request=hostname&protocol=udp&host=10.20.11.255&port=8081&tries=1'] Namespace:pod-network-test-2368 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  4 12:54:25.743: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    May  4 12:54:25.744: INFO: ExecWithOptions: Clientset creation
    May  4 12:54:25.744: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-2368/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.20.142.172%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.20.11.255%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May  4 12:54:25.831: INFO: Waiting for responses: map[]
    May  4 12:54:25.831: INFO: reached 10.20.11.255 after 0/1 tries
    May  4 12:54:25.831: INFO: Breadth first check of 10.20.142.179 on host 10.0.1.216...
    May  4 12:54:25.837: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.142.172:9080/dial?request=hostname&protocol=udp&host=10.20.142.179&port=8081&tries=1'] Namespace:pod-network-test-2368 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  4 12:54:25.837: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    May  4 12:54:25.837: INFO: ExecWithOptions: Clientset creation
    May  4 12:54:25.837: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-2368/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.20.142.172%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.20.142.179%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May  4 12:54:25.921: INFO: Waiting for responses: map[]
    May  4 12:54:25.921: INFO: reached 10.20.142.179 after 0/1 tries
    May  4 12:54:25.921: INFO: Breadth first check of 10.20.83.6 on host 10.0.1.224...
    May  4 12:54:25.926: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.142.172:9080/dial?request=hostname&protocol=udp&host=10.20.83.6&port=8081&tries=1'] Namespace:pod-network-test-2368 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  4 12:54:25.927: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    May  4 12:54:25.927: INFO: ExecWithOptions: Clientset creation
    May  4 12:54:25.927: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-2368/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.20.142.172%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.20.83.6%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May  4 12:54:26.012: INFO: Waiting for responses: map[]
    May  4 12:54:26.012: INFO: reached 10.20.83.6 after 0/1 tries
    May  4 12:54:26.012: INFO: Breadth first check of 10.20.92.159 on host 10.0.1.232...
    May  4 12:54:26.020: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.142.172:9080/dial?request=hostname&protocol=udp&host=10.20.92.159&port=8081&tries=1'] Namespace:pod-network-test-2368 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  4 12:54:26.021: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    May  4 12:54:26.021: INFO: ExecWithOptions: Clientset creation
    May  4 12:54:26.021: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-2368/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.20.142.172%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.20.92.159%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May  4 12:54:26.115: INFO: Waiting for responses: map[]
    May  4 12:54:26.115: INFO: reached 10.20.92.159 after 0/1 tries
    May  4 12:54:26.115: INFO: Breadth first check of 10.20.1.179 on host 10.0.1.253...
    May  4 12:54:26.119: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.142.172:9080/dial?request=hostname&protocol=udp&host=10.20.1.179&port=8081&tries=1'] Namespace:pod-network-test-2368 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  4 12:54:26.119: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    May  4 12:54:26.119: INFO: ExecWithOptions: Clientset creation
    May  4 12:54:26.119: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-2368/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.20.142.172%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.20.1.179%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May  4 12:54:26.213: INFO: Waiting for responses: map[]
    May  4 12:54:26.213: INFO: reached 10.20.1.179 after 0/1 tries
    May  4 12:54:26.213: INFO: Going to retry 0 out of 5 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    May  4 12:54:26.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-2368" for this suite. 05/04/23 12:54:26.222
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:54:26.23
May  4 12:54:26.230: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename secrets 05/04/23 12:54:26.231
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:54:26.249
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:54:26.253
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
STEP: Creating secret with name secret-test-457cde87-82ba-450a-b020-e3b032df6dbc 05/04/23 12:54:26.255
STEP: Creating a pod to test consume secrets 05/04/23 12:54:26.273
May  4 12:54:26.284: INFO: Waiting up to 5m0s for pod "pod-secrets-eadadd8c-5f1a-4af7-aa70-5b11584eea91" in namespace "secrets-1705" to be "Succeeded or Failed"
May  4 12:54:26.287: INFO: Pod "pod-secrets-eadadd8c-5f1a-4af7-aa70-5b11584eea91": Phase="Pending", Reason="", readiness=false. Elapsed: 3.385079ms
May  4 12:54:28.292: INFO: Pod "pod-secrets-eadadd8c-5f1a-4af7-aa70-5b11584eea91": Phase="Running", Reason="", readiness=false. Elapsed: 2.00773247s
May  4 12:54:30.295: INFO: Pod "pod-secrets-eadadd8c-5f1a-4af7-aa70-5b11584eea91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011137065s
STEP: Saw pod success 05/04/23 12:54:30.295
May  4 12:54:30.295: INFO: Pod "pod-secrets-eadadd8c-5f1a-4af7-aa70-5b11584eea91" satisfied condition "Succeeded or Failed"
May  4 12:54:30.300: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-secrets-eadadd8c-5f1a-4af7-aa70-5b11584eea91 container secret-volume-test: <nil>
STEP: delete the pod 05/04/23 12:54:30.317
May  4 12:54:30.342: INFO: Waiting for pod pod-secrets-eadadd8c-5f1a-4af7-aa70-5b11584eea91 to disappear
May  4 12:54:30.350: INFO: Pod pod-secrets-eadadd8c-5f1a-4af7-aa70-5b11584eea91 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
May  4 12:54:30.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1705" for this suite. 05/04/23 12:54:30.358
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":236,"skipped":4371,"failed":0}
------------------------------
• [4.139 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:54:26.23
    May  4 12:54:26.230: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename secrets 05/04/23 12:54:26.231
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:54:26.249
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:54:26.253
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:56
    STEP: Creating secret with name secret-test-457cde87-82ba-450a-b020-e3b032df6dbc 05/04/23 12:54:26.255
    STEP: Creating a pod to test consume secrets 05/04/23 12:54:26.273
    May  4 12:54:26.284: INFO: Waiting up to 5m0s for pod "pod-secrets-eadadd8c-5f1a-4af7-aa70-5b11584eea91" in namespace "secrets-1705" to be "Succeeded or Failed"
    May  4 12:54:26.287: INFO: Pod "pod-secrets-eadadd8c-5f1a-4af7-aa70-5b11584eea91": Phase="Pending", Reason="", readiness=false. Elapsed: 3.385079ms
    May  4 12:54:28.292: INFO: Pod "pod-secrets-eadadd8c-5f1a-4af7-aa70-5b11584eea91": Phase="Running", Reason="", readiness=false. Elapsed: 2.00773247s
    May  4 12:54:30.295: INFO: Pod "pod-secrets-eadadd8c-5f1a-4af7-aa70-5b11584eea91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011137065s
    STEP: Saw pod success 05/04/23 12:54:30.295
    May  4 12:54:30.295: INFO: Pod "pod-secrets-eadadd8c-5f1a-4af7-aa70-5b11584eea91" satisfied condition "Succeeded or Failed"
    May  4 12:54:30.300: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-secrets-eadadd8c-5f1a-4af7-aa70-5b11584eea91 container secret-volume-test: <nil>
    STEP: delete the pod 05/04/23 12:54:30.317
    May  4 12:54:30.342: INFO: Waiting for pod pod-secrets-eadadd8c-5f1a-4af7-aa70-5b11584eea91 to disappear
    May  4 12:54:30.350: INFO: Pod pod-secrets-eadadd8c-5f1a-4af7-aa70-5b11584eea91 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    May  4 12:54:30.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-1705" for this suite. 05/04/23 12:54:30.358
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:54:30.371
May  4 12:54:30.371: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename emptydir 05/04/23 12:54:30.372
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:54:30.402
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:54:30.408
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
STEP: Creating a pod to test emptydir 0777 on tmpfs 05/04/23 12:54:30.412
May  4 12:54:30.423: INFO: Waiting up to 5m0s for pod "pod-ae510665-43c1-42a2-9ae5-c50c5a54b849" in namespace "emptydir-3584" to be "Succeeded or Failed"
May  4 12:54:30.431: INFO: Pod "pod-ae510665-43c1-42a2-9ae5-c50c5a54b849": Phase="Pending", Reason="", readiness=false. Elapsed: 8.321733ms
May  4 12:54:32.435: INFO: Pod "pod-ae510665-43c1-42a2-9ae5-c50c5a54b849": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012512615s
May  4 12:54:34.437: INFO: Pod "pod-ae510665-43c1-42a2-9ae5-c50c5a54b849": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014372723s
STEP: Saw pod success 05/04/23 12:54:34.437
May  4 12:54:34.437: INFO: Pod "pod-ae510665-43c1-42a2-9ae5-c50c5a54b849" satisfied condition "Succeeded or Failed"
May  4 12:54:34.442: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-ae510665-43c1-42a2-9ae5-c50c5a54b849 container test-container: <nil>
STEP: delete the pod 05/04/23 12:54:34.449
May  4 12:54:34.469: INFO: Waiting for pod pod-ae510665-43c1-42a2-9ae5-c50c5a54b849 to disappear
May  4 12:54:34.475: INFO: Pod pod-ae510665-43c1-42a2-9ae5-c50c5a54b849 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May  4 12:54:34.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3584" for this suite. 05/04/23 12:54:34.483
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":237,"skipped":4386,"failed":0}
------------------------------
• [4.119 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:54:30.371
    May  4 12:54:30.371: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename emptydir 05/04/23 12:54:30.372
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:54:30.402
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:54:30.408
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:116
    STEP: Creating a pod to test emptydir 0777 on tmpfs 05/04/23 12:54:30.412
    May  4 12:54:30.423: INFO: Waiting up to 5m0s for pod "pod-ae510665-43c1-42a2-9ae5-c50c5a54b849" in namespace "emptydir-3584" to be "Succeeded or Failed"
    May  4 12:54:30.431: INFO: Pod "pod-ae510665-43c1-42a2-9ae5-c50c5a54b849": Phase="Pending", Reason="", readiness=false. Elapsed: 8.321733ms
    May  4 12:54:32.435: INFO: Pod "pod-ae510665-43c1-42a2-9ae5-c50c5a54b849": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012512615s
    May  4 12:54:34.437: INFO: Pod "pod-ae510665-43c1-42a2-9ae5-c50c5a54b849": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014372723s
    STEP: Saw pod success 05/04/23 12:54:34.437
    May  4 12:54:34.437: INFO: Pod "pod-ae510665-43c1-42a2-9ae5-c50c5a54b849" satisfied condition "Succeeded or Failed"
    May  4 12:54:34.442: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-ae510665-43c1-42a2-9ae5-c50c5a54b849 container test-container: <nil>
    STEP: delete the pod 05/04/23 12:54:34.449
    May  4 12:54:34.469: INFO: Waiting for pod pod-ae510665-43c1-42a2-9ae5-c50c5a54b849 to disappear
    May  4 12:54:34.475: INFO: Pod pod-ae510665-43c1-42a2-9ae5-c50c5a54b849 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May  4 12:54:34.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3584" for this suite. 05/04/23 12:54:34.483
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:54:34.491
May  4 12:54:34.491: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename services 05/04/23 12:54:34.492
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:54:34.513
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:54:34.516
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
STEP: creating service multi-endpoint-test in namespace services-9076 05/04/23 12:54:34.519
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9076 to expose endpoints map[] 05/04/23 12:54:34.539
May  4 12:54:34.547: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
May  4 12:54:35.555: INFO: successfully validated that service multi-endpoint-test in namespace services-9076 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-9076 05/04/23 12:54:35.555
May  4 12:54:35.574: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-9076" to be "running and ready"
May  4 12:54:35.581: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.561178ms
May  4 12:54:35.581: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May  4 12:54:37.587: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.012617014s
May  4 12:54:37.587: INFO: The phase of Pod pod1 is Running (Ready = true)
May  4 12:54:37.587: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9076 to expose endpoints map[pod1:[100]] 05/04/23 12:54:37.591
May  4 12:54:37.603: INFO: successfully validated that service multi-endpoint-test in namespace services-9076 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-9076 05/04/23 12:54:37.603
May  4 12:54:37.610: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-9076" to be "running and ready"
May  4 12:54:37.616: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.985134ms
May  4 12:54:37.616: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May  4 12:54:39.622: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.011369122s
May  4 12:54:39.622: INFO: The phase of Pod pod2 is Running (Ready = true)
May  4 12:54:39.622: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9076 to expose endpoints map[pod1:[100] pod2:[101]] 05/04/23 12:54:39.628
May  4 12:54:39.645: INFO: successfully validated that service multi-endpoint-test in namespace services-9076 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 05/04/23 12:54:39.645
May  4 12:54:39.645: INFO: Creating new exec pod
May  4 12:54:39.651: INFO: Waiting up to 5m0s for pod "execpodg6z9m" in namespace "services-9076" to be "running"
May  4 12:54:39.664: INFO: Pod "execpodg6z9m": Phase="Pending", Reason="", readiness=false. Elapsed: 12.992109ms
May  4 12:54:41.669: INFO: Pod "execpodg6z9m": Phase="Running", Reason="", readiness=true. Elapsed: 2.017683246s
May  4 12:54:41.669: INFO: Pod "execpodg6z9m" satisfied condition "running"
May  4 12:54:42.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-9076 exec execpodg6z9m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
May  4 12:54:42.877: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
May  4 12:54:42.877: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  4 12:54:42.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-9076 exec execpodg6z9m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.21.67.25 80'
May  4 12:54:43.053: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.21.67.25 80\nConnection to 10.21.67.25 80 port [tcp/http] succeeded!\n"
May  4 12:54:43.053: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  4 12:54:43.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-9076 exec execpodg6z9m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
May  4 12:54:43.269: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
May  4 12:54:43.269: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  4 12:54:43.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-9076 exec execpodg6z9m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.21.67.25 81'
May  4 12:54:43.436: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.21.67.25 81\nConnection to 10.21.67.25 81 port [tcp/*] succeeded!\n"
May  4 12:54:43.436: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-9076 05/04/23 12:54:43.436
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9076 to expose endpoints map[pod2:[101]] 05/04/23 12:54:43.464
May  4 12:54:43.483: INFO: successfully validated that service multi-endpoint-test in namespace services-9076 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-9076 05/04/23 12:54:43.483
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9076 to expose endpoints map[] 05/04/23 12:54:43.519
May  4 12:54:44.552: INFO: successfully validated that service multi-endpoint-test in namespace services-9076 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  4 12:54:44.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9076" for this suite. 05/04/23 12:54:44.604
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","completed":238,"skipped":4387,"failed":0}
------------------------------
• [SLOW TEST] [10.130 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:54:34.491
    May  4 12:54:34.491: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename services 05/04/23 12:54:34.492
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:54:34.513
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:54:34.516
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:852
    STEP: creating service multi-endpoint-test in namespace services-9076 05/04/23 12:54:34.519
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9076 to expose endpoints map[] 05/04/23 12:54:34.539
    May  4 12:54:34.547: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    May  4 12:54:35.555: INFO: successfully validated that service multi-endpoint-test in namespace services-9076 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-9076 05/04/23 12:54:35.555
    May  4 12:54:35.574: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-9076" to be "running and ready"
    May  4 12:54:35.581: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.561178ms
    May  4 12:54:35.581: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    May  4 12:54:37.587: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.012617014s
    May  4 12:54:37.587: INFO: The phase of Pod pod1 is Running (Ready = true)
    May  4 12:54:37.587: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9076 to expose endpoints map[pod1:[100]] 05/04/23 12:54:37.591
    May  4 12:54:37.603: INFO: successfully validated that service multi-endpoint-test in namespace services-9076 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-9076 05/04/23 12:54:37.603
    May  4 12:54:37.610: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-9076" to be "running and ready"
    May  4 12:54:37.616: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.985134ms
    May  4 12:54:37.616: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    May  4 12:54:39.622: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.011369122s
    May  4 12:54:39.622: INFO: The phase of Pod pod2 is Running (Ready = true)
    May  4 12:54:39.622: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9076 to expose endpoints map[pod1:[100] pod2:[101]] 05/04/23 12:54:39.628
    May  4 12:54:39.645: INFO: successfully validated that service multi-endpoint-test in namespace services-9076 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 05/04/23 12:54:39.645
    May  4 12:54:39.645: INFO: Creating new exec pod
    May  4 12:54:39.651: INFO: Waiting up to 5m0s for pod "execpodg6z9m" in namespace "services-9076" to be "running"
    May  4 12:54:39.664: INFO: Pod "execpodg6z9m": Phase="Pending", Reason="", readiness=false. Elapsed: 12.992109ms
    May  4 12:54:41.669: INFO: Pod "execpodg6z9m": Phase="Running", Reason="", readiness=true. Elapsed: 2.017683246s
    May  4 12:54:41.669: INFO: Pod "execpodg6z9m" satisfied condition "running"
    May  4 12:54:42.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-9076 exec execpodg6z9m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
    May  4 12:54:42.877: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    May  4 12:54:42.877: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  4 12:54:42.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-9076 exec execpodg6z9m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.21.67.25 80'
    May  4 12:54:43.053: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.21.67.25 80\nConnection to 10.21.67.25 80 port [tcp/http] succeeded!\n"
    May  4 12:54:43.053: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  4 12:54:43.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-9076 exec execpodg6z9m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
    May  4 12:54:43.269: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    May  4 12:54:43.269: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  4 12:54:43.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-9076 exec execpodg6z9m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.21.67.25 81'
    May  4 12:54:43.436: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.21.67.25 81\nConnection to 10.21.67.25 81 port [tcp/*] succeeded!\n"
    May  4 12:54:43.436: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-9076 05/04/23 12:54:43.436
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9076 to expose endpoints map[pod2:[101]] 05/04/23 12:54:43.464
    May  4 12:54:43.483: INFO: successfully validated that service multi-endpoint-test in namespace services-9076 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-9076 05/04/23 12:54:43.483
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9076 to expose endpoints map[] 05/04/23 12:54:43.519
    May  4 12:54:44.552: INFO: successfully validated that service multi-endpoint-test in namespace services-9076 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  4 12:54:44.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9076" for this suite. 05/04/23 12:54:44.604
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:54:44.621
May  4 12:54:44.621: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename crd-publish-openapi 05/04/23 12:54:44.622
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:54:44.643
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:54:44.646
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
STEP: set up a multi version CRD 05/04/23 12:54:44.65
May  4 12:54:44.651: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: mark a version not serverd 05/04/23 12:54:57.169
STEP: check the unserved version gets removed 05/04/23 12:54:57.19
STEP: check the other version is not changed 05/04/23 12:55:03.523
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  4 12:55:13.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1313" for this suite. 05/04/23 12:55:13.846
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","completed":239,"skipped":4390,"failed":0}
------------------------------
• [SLOW TEST] [29.243 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:54:44.621
    May  4 12:54:44.621: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename crd-publish-openapi 05/04/23 12:54:44.622
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:54:44.643
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:54:44.646
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:441
    STEP: set up a multi version CRD 05/04/23 12:54:44.65
    May  4 12:54:44.651: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: mark a version not serverd 05/04/23 12:54:57.169
    STEP: check the unserved version gets removed 05/04/23 12:54:57.19
    STEP: check the other version is not changed 05/04/23 12:55:03.523
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  4 12:55:13.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-1313" for this suite. 05/04/23 12:55:13.846
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:55:13.865
May  4 12:55:13.865: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename replication-controller 05/04/23 12:55:13.866
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:55:13.9
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:55:13.905
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
STEP: Creating replication controller my-hostname-basic-6b71cdc7-0c32-4a94-92c5-44e6e8654ce4 05/04/23 12:55:13.909
May  4 12:55:13.931: INFO: Pod name my-hostname-basic-6b71cdc7-0c32-4a94-92c5-44e6e8654ce4: Found 1 pods out of 1
May  4 12:55:13.931: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-6b71cdc7-0c32-4a94-92c5-44e6e8654ce4" are running
May  4 12:55:13.931: INFO: Waiting up to 5m0s for pod "my-hostname-basic-6b71cdc7-0c32-4a94-92c5-44e6e8654ce4-2jx6z" in namespace "replication-controller-9640" to be "running"
May  4 12:55:13.955: INFO: Pod "my-hostname-basic-6b71cdc7-0c32-4a94-92c5-44e6e8654ce4-2jx6z": Phase="Pending", Reason="", readiness=false. Elapsed: 24.060484ms
May  4 12:55:15.961: INFO: Pod "my-hostname-basic-6b71cdc7-0c32-4a94-92c5-44e6e8654ce4-2jx6z": Phase="Running", Reason="", readiness=true. Elapsed: 2.029844606s
May  4 12:55:15.961: INFO: Pod "my-hostname-basic-6b71cdc7-0c32-4a94-92c5-44e6e8654ce4-2jx6z" satisfied condition "running"
May  4 12:55:15.961: INFO: Pod "my-hostname-basic-6b71cdc7-0c32-4a94-92c5-44e6e8654ce4-2jx6z" is running (conditions: [])
May  4 12:55:15.961: INFO: Trying to dial the pod
May  4 12:55:20.978: INFO: Controller my-hostname-basic-6b71cdc7-0c32-4a94-92c5-44e6e8654ce4: Got expected result from replica 1 [my-hostname-basic-6b71cdc7-0c32-4a94-92c5-44e6e8654ce4-2jx6z]: "my-hostname-basic-6b71cdc7-0c32-4a94-92c5-44e6e8654ce4-2jx6z", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
May  4 12:55:20.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9640" for this suite. 05/04/23 12:55:20.985
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","completed":240,"skipped":4397,"failed":0}
------------------------------
• [SLOW TEST] [7.129 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:55:13.865
    May  4 12:55:13.865: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename replication-controller 05/04/23 12:55:13.866
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:55:13.9
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:55:13.905
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:66
    STEP: Creating replication controller my-hostname-basic-6b71cdc7-0c32-4a94-92c5-44e6e8654ce4 05/04/23 12:55:13.909
    May  4 12:55:13.931: INFO: Pod name my-hostname-basic-6b71cdc7-0c32-4a94-92c5-44e6e8654ce4: Found 1 pods out of 1
    May  4 12:55:13.931: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-6b71cdc7-0c32-4a94-92c5-44e6e8654ce4" are running
    May  4 12:55:13.931: INFO: Waiting up to 5m0s for pod "my-hostname-basic-6b71cdc7-0c32-4a94-92c5-44e6e8654ce4-2jx6z" in namespace "replication-controller-9640" to be "running"
    May  4 12:55:13.955: INFO: Pod "my-hostname-basic-6b71cdc7-0c32-4a94-92c5-44e6e8654ce4-2jx6z": Phase="Pending", Reason="", readiness=false. Elapsed: 24.060484ms
    May  4 12:55:15.961: INFO: Pod "my-hostname-basic-6b71cdc7-0c32-4a94-92c5-44e6e8654ce4-2jx6z": Phase="Running", Reason="", readiness=true. Elapsed: 2.029844606s
    May  4 12:55:15.961: INFO: Pod "my-hostname-basic-6b71cdc7-0c32-4a94-92c5-44e6e8654ce4-2jx6z" satisfied condition "running"
    May  4 12:55:15.961: INFO: Pod "my-hostname-basic-6b71cdc7-0c32-4a94-92c5-44e6e8654ce4-2jx6z" is running (conditions: [])
    May  4 12:55:15.961: INFO: Trying to dial the pod
    May  4 12:55:20.978: INFO: Controller my-hostname-basic-6b71cdc7-0c32-4a94-92c5-44e6e8654ce4: Got expected result from replica 1 [my-hostname-basic-6b71cdc7-0c32-4a94-92c5-44e6e8654ce4-2jx6z]: "my-hostname-basic-6b71cdc7-0c32-4a94-92c5-44e6e8654ce4-2jx6z", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    May  4 12:55:20.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-9640" for this suite. 05/04/23 12:55:20.985
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:55:20.995
May  4 12:55:20.995: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename services 05/04/23 12:55:20.995
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:55:21.018
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:55:21.024
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
STEP: creating an Endpoint 05/04/23 12:55:21.03
STEP: waiting for available Endpoint 05/04/23 12:55:21.037
STEP: listing all Endpoints 05/04/23 12:55:21.039
STEP: updating the Endpoint 05/04/23 12:55:21.044
STEP: fetching the Endpoint 05/04/23 12:55:21.051
STEP: patching the Endpoint 05/04/23 12:55:21.056
STEP: fetching the Endpoint 05/04/23 12:55:21.069
STEP: deleting the Endpoint by Collection 05/04/23 12:55:21.073
STEP: waiting for Endpoint deletion 05/04/23 12:55:21.083
STEP: fetching the Endpoint 05/04/23 12:55:21.085
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  4 12:55:21.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1289" for this suite. 05/04/23 12:55:21.096
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","completed":241,"skipped":4410,"failed":0}
------------------------------
• [0.111 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:55:20.995
    May  4 12:55:20.995: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename services 05/04/23 12:55:20.995
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:55:21.018
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:55:21.024
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3231
    STEP: creating an Endpoint 05/04/23 12:55:21.03
    STEP: waiting for available Endpoint 05/04/23 12:55:21.037
    STEP: listing all Endpoints 05/04/23 12:55:21.039
    STEP: updating the Endpoint 05/04/23 12:55:21.044
    STEP: fetching the Endpoint 05/04/23 12:55:21.051
    STEP: patching the Endpoint 05/04/23 12:55:21.056
    STEP: fetching the Endpoint 05/04/23 12:55:21.069
    STEP: deleting the Endpoint by Collection 05/04/23 12:55:21.073
    STEP: waiting for Endpoint deletion 05/04/23 12:55:21.083
    STEP: fetching the Endpoint 05/04/23 12:55:21.085
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  4 12:55:21.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1289" for this suite. 05/04/23 12:55:21.096
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:55:21.106
May  4 12:55:21.106: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename runtimeclass 05/04/23 12:55:21.107
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:55:21.129
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:55:21.132
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-6629-delete-me 05/04/23 12:55:21.141
STEP: Waiting for the RuntimeClass to disappear 05/04/23 12:55:21.149
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
May  4 12:55:21.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-6629" for this suite. 05/04/23 12:55:21.169
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","completed":242,"skipped":4412,"failed":0}
------------------------------
• [0.080 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:55:21.106
    May  4 12:55:21.106: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename runtimeclass 05/04/23 12:55:21.107
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:55:21.129
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:55:21.132
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-6629-delete-me 05/04/23 12:55:21.141
    STEP: Waiting for the RuntimeClass to disappear 05/04/23 12:55:21.149
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    May  4 12:55:21.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-6629" for this suite. 05/04/23 12:55:21.169
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:55:21.187
May  4 12:55:21.187: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename pods 05/04/23 12:55:21.188
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:55:21.209
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:55:21.212
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
STEP: creating the pod 05/04/23 12:55:21.215
STEP: setting up watch 05/04/23 12:55:21.215
STEP: submitting the pod to kubernetes 05/04/23 12:55:21.32
STEP: verifying the pod is in kubernetes 05/04/23 12:55:21.329
STEP: verifying pod creation was observed 05/04/23 12:55:21.333
May  4 12:55:21.333: INFO: Waiting up to 5m0s for pod "pod-submit-remove-1b5c0975-521e-4f2e-ae70-e38f27f1ab87" in namespace "pods-2072" to be "running"
May  4 12:55:21.336: INFO: Pod "pod-submit-remove-1b5c0975-521e-4f2e-ae70-e38f27f1ab87": Phase="Pending", Reason="", readiness=false. Elapsed: 3.231479ms
May  4 12:55:23.342: INFO: Pod "pod-submit-remove-1b5c0975-521e-4f2e-ae70-e38f27f1ab87": Phase="Running", Reason="", readiness=true. Elapsed: 2.009016548s
May  4 12:55:23.342: INFO: Pod "pod-submit-remove-1b5c0975-521e-4f2e-ae70-e38f27f1ab87" satisfied condition "running"
STEP: deleting the pod gracefully 05/04/23 12:55:23.348
STEP: verifying pod deletion was observed 05/04/23 12:55:23.372
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
May  4 12:55:25.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2072" for this suite. 05/04/23 12:55:25.307
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","completed":243,"skipped":4433,"failed":0}
------------------------------
• [4.130 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:55:21.187
    May  4 12:55:21.187: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename pods 05/04/23 12:55:21.188
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:55:21.209
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:55:21.212
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:225
    STEP: creating the pod 05/04/23 12:55:21.215
    STEP: setting up watch 05/04/23 12:55:21.215
    STEP: submitting the pod to kubernetes 05/04/23 12:55:21.32
    STEP: verifying the pod is in kubernetes 05/04/23 12:55:21.329
    STEP: verifying pod creation was observed 05/04/23 12:55:21.333
    May  4 12:55:21.333: INFO: Waiting up to 5m0s for pod "pod-submit-remove-1b5c0975-521e-4f2e-ae70-e38f27f1ab87" in namespace "pods-2072" to be "running"
    May  4 12:55:21.336: INFO: Pod "pod-submit-remove-1b5c0975-521e-4f2e-ae70-e38f27f1ab87": Phase="Pending", Reason="", readiness=false. Elapsed: 3.231479ms
    May  4 12:55:23.342: INFO: Pod "pod-submit-remove-1b5c0975-521e-4f2e-ae70-e38f27f1ab87": Phase="Running", Reason="", readiness=true. Elapsed: 2.009016548s
    May  4 12:55:23.342: INFO: Pod "pod-submit-remove-1b5c0975-521e-4f2e-ae70-e38f27f1ab87" satisfied condition "running"
    STEP: deleting the pod gracefully 05/04/23 12:55:23.348
    STEP: verifying pod deletion was observed 05/04/23 12:55:23.372
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    May  4 12:55:25.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-2072" for this suite. 05/04/23 12:55:25.307
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:55:25.323
May  4 12:55:25.323: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename taint-single-pod 05/04/23 12:55:25.324
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:55:25.348
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:55:25.352
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
May  4 12:55:25.356: INFO: Waiting up to 1m0s for all nodes to be ready
May  4 12:56:25.404: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
May  4 12:56:25.409: INFO: Starting informer...
STEP: Starting pod... 05/04/23 12:56:25.409
May  4 12:56:25.633: INFO: Pod is running on ip-10-0-1-224.us-west-2.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node 05/04/23 12:56:25.633
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/04/23 12:56:25.649
STEP: Waiting short time to make sure Pod is queued for deletion 05/04/23 12:56:25.653
May  4 12:56:25.653: INFO: Pod wasn't evicted. Proceeding
May  4 12:56:25.653: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/04/23 12:56:25.675
STEP: Waiting some time to make sure that toleration time passed. 05/04/23 12:56:25.689
May  4 12:57:40.690: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
May  4 12:57:40.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-9368" for this suite. 05/04/23 12:57:40.698
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","completed":244,"skipped":4529,"failed":0}
------------------------------
• [SLOW TEST] [135.382 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:55:25.323
    May  4 12:55:25.323: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename taint-single-pod 05/04/23 12:55:25.324
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:55:25.348
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:55:25.352
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:166
    May  4 12:55:25.356: INFO: Waiting up to 1m0s for all nodes to be ready
    May  4 12:56:25.404: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:289
    May  4 12:56:25.409: INFO: Starting informer...
    STEP: Starting pod... 05/04/23 12:56:25.409
    May  4 12:56:25.633: INFO: Pod is running on ip-10-0-1-224.us-west-2.compute.internal. Tainting Node
    STEP: Trying to apply a taint on the Node 05/04/23 12:56:25.633
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/04/23 12:56:25.649
    STEP: Waiting short time to make sure Pod is queued for deletion 05/04/23 12:56:25.653
    May  4 12:56:25.653: INFO: Pod wasn't evicted. Proceeding
    May  4 12:56:25.653: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/04/23 12:56:25.675
    STEP: Waiting some time to make sure that toleration time passed. 05/04/23 12:56:25.689
    May  4 12:57:40.690: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:187
    May  4 12:57:40.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-single-pod-9368" for this suite. 05/04/23 12:57:40.698
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:57:40.706
May  4 12:57:40.706: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename projected 05/04/23 12:57:40.707
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:57:40.724
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:57:40.727
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
STEP: Creating projection with secret that has name projected-secret-test-map-d9b38525-8cfb-4265-86c9-e50c43393ee1 05/04/23 12:57:40.73
STEP: Creating a pod to test consume secrets 05/04/23 12:57:40.737
May  4 12:57:40.749: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d848572c-7f29-4ee3-a2ae-13fca2698c08" in namespace "projected-660" to be "Succeeded or Failed"
May  4 12:57:40.755: INFO: Pod "pod-projected-secrets-d848572c-7f29-4ee3-a2ae-13fca2698c08": Phase="Pending", Reason="", readiness=false. Elapsed: 6.132693ms
May  4 12:57:42.761: INFO: Pod "pod-projected-secrets-d848572c-7f29-4ee3-a2ae-13fca2698c08": Phase="Running", Reason="", readiness=false. Elapsed: 2.012755864s
May  4 12:57:44.761: INFO: Pod "pod-projected-secrets-d848572c-7f29-4ee3-a2ae-13fca2698c08": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01235203s
STEP: Saw pod success 05/04/23 12:57:44.761
May  4 12:57:44.761: INFO: Pod "pod-projected-secrets-d848572c-7f29-4ee3-a2ae-13fca2698c08" satisfied condition "Succeeded or Failed"
May  4 12:57:44.769: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-projected-secrets-d848572c-7f29-4ee3-a2ae-13fca2698c08 container projected-secret-volume-test: <nil>
STEP: delete the pod 05/04/23 12:57:44.785
May  4 12:57:44.800: INFO: Waiting for pod pod-projected-secrets-d848572c-7f29-4ee3-a2ae-13fca2698c08 to disappear
May  4 12:57:44.804: INFO: Pod pod-projected-secrets-d848572c-7f29-4ee3-a2ae-13fca2698c08 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
May  4 12:57:44.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-660" for this suite. 05/04/23 12:57:44.813
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":245,"skipped":4543,"failed":0}
------------------------------
• [4.115 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:57:40.706
    May  4 12:57:40.706: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename projected 05/04/23 12:57:40.707
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:57:40.724
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:57:40.727
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:77
    STEP: Creating projection with secret that has name projected-secret-test-map-d9b38525-8cfb-4265-86c9-e50c43393ee1 05/04/23 12:57:40.73
    STEP: Creating a pod to test consume secrets 05/04/23 12:57:40.737
    May  4 12:57:40.749: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d848572c-7f29-4ee3-a2ae-13fca2698c08" in namespace "projected-660" to be "Succeeded or Failed"
    May  4 12:57:40.755: INFO: Pod "pod-projected-secrets-d848572c-7f29-4ee3-a2ae-13fca2698c08": Phase="Pending", Reason="", readiness=false. Elapsed: 6.132693ms
    May  4 12:57:42.761: INFO: Pod "pod-projected-secrets-d848572c-7f29-4ee3-a2ae-13fca2698c08": Phase="Running", Reason="", readiness=false. Elapsed: 2.012755864s
    May  4 12:57:44.761: INFO: Pod "pod-projected-secrets-d848572c-7f29-4ee3-a2ae-13fca2698c08": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01235203s
    STEP: Saw pod success 05/04/23 12:57:44.761
    May  4 12:57:44.761: INFO: Pod "pod-projected-secrets-d848572c-7f29-4ee3-a2ae-13fca2698c08" satisfied condition "Succeeded or Failed"
    May  4 12:57:44.769: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-projected-secrets-d848572c-7f29-4ee3-a2ae-13fca2698c08 container projected-secret-volume-test: <nil>
    STEP: delete the pod 05/04/23 12:57:44.785
    May  4 12:57:44.800: INFO: Waiting for pod pod-projected-secrets-d848572c-7f29-4ee3-a2ae-13fca2698c08 to disappear
    May  4 12:57:44.804: INFO: Pod pod-projected-secrets-d848572c-7f29-4ee3-a2ae-13fca2698c08 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    May  4 12:57:44.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-660" for this suite. 05/04/23 12:57:44.813
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:57:44.822
May  4 12:57:44.822: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename projected 05/04/23 12:57:44.823
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:57:44.844
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:57:44.847
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
STEP: Creating projection with secret that has name projected-secret-test-2d3a2ea0-1f85-4c51-99eb-68482816dec6 05/04/23 12:57:44.85
STEP: Creating a pod to test consume secrets 05/04/23 12:57:44.86
May  4 12:57:44.873: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-93a46201-b45b-4fdf-9a92-bb595f06377a" in namespace "projected-3757" to be "Succeeded or Failed"
May  4 12:57:44.876: INFO: Pod "pod-projected-secrets-93a46201-b45b-4fdf-9a92-bb595f06377a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.854694ms
May  4 12:57:46.882: INFO: Pod "pod-projected-secrets-93a46201-b45b-4fdf-9a92-bb595f06377a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009571772s
May  4 12:57:48.882: INFO: Pod "pod-projected-secrets-93a46201-b45b-4fdf-9a92-bb595f06377a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009403384s
STEP: Saw pod success 05/04/23 12:57:48.882
May  4 12:57:48.882: INFO: Pod "pod-projected-secrets-93a46201-b45b-4fdf-9a92-bb595f06377a" satisfied condition "Succeeded or Failed"
May  4 12:57:48.888: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-projected-secrets-93a46201-b45b-4fdf-9a92-bb595f06377a container projected-secret-volume-test: <nil>
STEP: delete the pod 05/04/23 12:57:48.896
May  4 12:57:48.923: INFO: Waiting for pod pod-projected-secrets-93a46201-b45b-4fdf-9a92-bb595f06377a to disappear
May  4 12:57:48.927: INFO: Pod pod-projected-secrets-93a46201-b45b-4fdf-9a92-bb595f06377a no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
May  4 12:57:48.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3757" for this suite. 05/04/23 12:57:48.941
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":246,"skipped":4548,"failed":0}
------------------------------
• [4.128 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:57:44.822
    May  4 12:57:44.822: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename projected 05/04/23 12:57:44.823
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:57:44.844
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:57:44.847
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:55
    STEP: Creating projection with secret that has name projected-secret-test-2d3a2ea0-1f85-4c51-99eb-68482816dec6 05/04/23 12:57:44.85
    STEP: Creating a pod to test consume secrets 05/04/23 12:57:44.86
    May  4 12:57:44.873: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-93a46201-b45b-4fdf-9a92-bb595f06377a" in namespace "projected-3757" to be "Succeeded or Failed"
    May  4 12:57:44.876: INFO: Pod "pod-projected-secrets-93a46201-b45b-4fdf-9a92-bb595f06377a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.854694ms
    May  4 12:57:46.882: INFO: Pod "pod-projected-secrets-93a46201-b45b-4fdf-9a92-bb595f06377a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009571772s
    May  4 12:57:48.882: INFO: Pod "pod-projected-secrets-93a46201-b45b-4fdf-9a92-bb595f06377a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009403384s
    STEP: Saw pod success 05/04/23 12:57:48.882
    May  4 12:57:48.882: INFO: Pod "pod-projected-secrets-93a46201-b45b-4fdf-9a92-bb595f06377a" satisfied condition "Succeeded or Failed"
    May  4 12:57:48.888: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-projected-secrets-93a46201-b45b-4fdf-9a92-bb595f06377a container projected-secret-volume-test: <nil>
    STEP: delete the pod 05/04/23 12:57:48.896
    May  4 12:57:48.923: INFO: Waiting for pod pod-projected-secrets-93a46201-b45b-4fdf-9a92-bb595f06377a to disappear
    May  4 12:57:48.927: INFO: Pod pod-projected-secrets-93a46201-b45b-4fdf-9a92-bb595f06377a no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    May  4 12:57:48.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3757" for this suite. 05/04/23 12:57:48.941
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:57:48.95
May  4 12:57:48.950: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename secrets 05/04/23 12:57:48.951
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:57:48.981
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:57:48.986
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
STEP: Creating secret with name secret-test-bf36d8d6-6385-4cde-85ad-c3fadc6a1205 05/04/23 12:57:48.989
STEP: Creating a pod to test consume secrets 05/04/23 12:57:48.999
May  4 12:57:49.019: INFO: Waiting up to 5m0s for pod "pod-secrets-80ee0b90-f317-4f0e-bc2f-273b8933c744" in namespace "secrets-5144" to be "Succeeded or Failed"
May  4 12:57:49.026: INFO: Pod "pod-secrets-80ee0b90-f317-4f0e-bc2f-273b8933c744": Phase="Pending", Reason="", readiness=false. Elapsed: 7.59929ms
May  4 12:57:51.032: INFO: Pod "pod-secrets-80ee0b90-f317-4f0e-bc2f-273b8933c744": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013406404s
May  4 12:57:53.032: INFO: Pod "pod-secrets-80ee0b90-f317-4f0e-bc2f-273b8933c744": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012690634s
STEP: Saw pod success 05/04/23 12:57:53.032
May  4 12:57:53.032: INFO: Pod "pod-secrets-80ee0b90-f317-4f0e-bc2f-273b8933c744" satisfied condition "Succeeded or Failed"
May  4 12:57:53.036: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-secrets-80ee0b90-f317-4f0e-bc2f-273b8933c744 container secret-volume-test: <nil>
STEP: delete the pod 05/04/23 12:57:53.045
May  4 12:57:53.069: INFO: Waiting for pod pod-secrets-80ee0b90-f317-4f0e-bc2f-273b8933c744 to disappear
May  4 12:57:53.075: INFO: Pod pod-secrets-80ee0b90-f317-4f0e-bc2f-273b8933c744 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
May  4 12:57:53.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5144" for this suite. 05/04/23 12:57:53.086
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":247,"skipped":4556,"failed":0}
------------------------------
• [4.146 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:57:48.95
    May  4 12:57:48.950: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename secrets 05/04/23 12:57:48.951
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:57:48.981
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:57:48.986
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:67
    STEP: Creating secret with name secret-test-bf36d8d6-6385-4cde-85ad-c3fadc6a1205 05/04/23 12:57:48.989
    STEP: Creating a pod to test consume secrets 05/04/23 12:57:48.999
    May  4 12:57:49.019: INFO: Waiting up to 5m0s for pod "pod-secrets-80ee0b90-f317-4f0e-bc2f-273b8933c744" in namespace "secrets-5144" to be "Succeeded or Failed"
    May  4 12:57:49.026: INFO: Pod "pod-secrets-80ee0b90-f317-4f0e-bc2f-273b8933c744": Phase="Pending", Reason="", readiness=false. Elapsed: 7.59929ms
    May  4 12:57:51.032: INFO: Pod "pod-secrets-80ee0b90-f317-4f0e-bc2f-273b8933c744": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013406404s
    May  4 12:57:53.032: INFO: Pod "pod-secrets-80ee0b90-f317-4f0e-bc2f-273b8933c744": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012690634s
    STEP: Saw pod success 05/04/23 12:57:53.032
    May  4 12:57:53.032: INFO: Pod "pod-secrets-80ee0b90-f317-4f0e-bc2f-273b8933c744" satisfied condition "Succeeded or Failed"
    May  4 12:57:53.036: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-secrets-80ee0b90-f317-4f0e-bc2f-273b8933c744 container secret-volume-test: <nil>
    STEP: delete the pod 05/04/23 12:57:53.045
    May  4 12:57:53.069: INFO: Waiting for pod pod-secrets-80ee0b90-f317-4f0e-bc2f-273b8933c744 to disappear
    May  4 12:57:53.075: INFO: Pod pod-secrets-80ee0b90-f317-4f0e-bc2f-273b8933c744 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    May  4 12:57:53.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5144" for this suite. 05/04/23 12:57:53.086
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:57:53.098
May  4 12:57:53.098: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename podtemplate 05/04/23 12:57:53.099
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:57:53.125
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:57:53.132
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 05/04/23 12:57:53.136
STEP: Replace a pod template 05/04/23 12:57:53.147
May  4 12:57:53.164: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
May  4 12:57:53.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-3137" for this suite. 05/04/23 12:57:53.178
{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","completed":248,"skipped":4590,"failed":0}
------------------------------
• [0.108 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:57:53.098
    May  4 12:57:53.098: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename podtemplate 05/04/23 12:57:53.099
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:57:53.125
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:57:53.132
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 05/04/23 12:57:53.136
    STEP: Replace a pod template 05/04/23 12:57:53.147
    May  4 12:57:53.164: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    May  4 12:57:53.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-3137" for this suite. 05/04/23 12:57:53.178
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:57:53.207
May  4 12:57:53.207: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename container-lifecycle-hook 05/04/23 12:57:53.208
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:57:53.237
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:57:53.245
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 05/04/23 12:57:53.275
May  4 12:57:53.290: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-3266" to be "running and ready"
May  4 12:57:53.299: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 8.540025ms
May  4 12:57:53.299: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May  4 12:57:55.308: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.017137604s
May  4 12:57:55.308: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
May  4 12:57:55.308: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
STEP: create the pod with lifecycle hook 05/04/23 12:57:55.313
May  4 12:57:55.323: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-3266" to be "running and ready"
May  4 12:57:55.329: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.067059ms
May  4 12:57:55.329: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
May  4 12:57:57.347: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.023798543s
May  4 12:57:57.347: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
May  4 12:57:57.347: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 05/04/23 12:57:57.358
May  4 12:57:57.388: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May  4 12:57:57.398: INFO: Pod pod-with-prestop-exec-hook still exists
May  4 12:57:59.398: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May  4 12:57:59.403: INFO: Pod pod-with-prestop-exec-hook still exists
May  4 12:58:01.399: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May  4 12:58:01.404: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 05/04/23 12:58:01.404
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
May  4 12:58:01.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3266" for this suite. 05/04/23 12:58:01.42
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","completed":249,"skipped":4621,"failed":0}
------------------------------
• [SLOW TEST] [8.222 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:114

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:57:53.207
    May  4 12:57:53.207: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename container-lifecycle-hook 05/04/23 12:57:53.208
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:57:53.237
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:57:53.245
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 05/04/23 12:57:53.275
    May  4 12:57:53.290: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-3266" to be "running and ready"
    May  4 12:57:53.299: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 8.540025ms
    May  4 12:57:53.299: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    May  4 12:57:55.308: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.017137604s
    May  4 12:57:55.308: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    May  4 12:57:55.308: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:114
    STEP: create the pod with lifecycle hook 05/04/23 12:57:55.313
    May  4 12:57:55.323: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-3266" to be "running and ready"
    May  4 12:57:55.329: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.067059ms
    May  4 12:57:55.329: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    May  4 12:57:57.347: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.023798543s
    May  4 12:57:57.347: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    May  4 12:57:57.347: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 05/04/23 12:57:57.358
    May  4 12:57:57.388: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    May  4 12:57:57.398: INFO: Pod pod-with-prestop-exec-hook still exists
    May  4 12:57:59.398: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    May  4 12:57:59.403: INFO: Pod pod-with-prestop-exec-hook still exists
    May  4 12:58:01.399: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    May  4 12:58:01.404: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 05/04/23 12:58:01.404
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    May  4 12:58:01.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-3266" for this suite. 05/04/23 12:58:01.42
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:58:01.431
May  4 12:58:01.431: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename pods 05/04/23 12:58:01.433
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:58:01.454
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:58:01.457
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
STEP: creating pod 05/04/23 12:58:01.464
May  4 12:58:01.480: INFO: Waiting up to 5m0s for pod "pod-hostip-dc68709b-2c3f-467c-87b5-3aef833c6385" in namespace "pods-8132" to be "running and ready"
May  4 12:58:01.484: INFO: Pod "pod-hostip-dc68709b-2c3f-467c-87b5-3aef833c6385": Phase="Pending", Reason="", readiness=false. Elapsed: 4.259897ms
May  4 12:58:01.484: INFO: The phase of Pod pod-hostip-dc68709b-2c3f-467c-87b5-3aef833c6385 is Pending, waiting for it to be Running (with Ready = true)
May  4 12:58:03.489: INFO: Pod "pod-hostip-dc68709b-2c3f-467c-87b5-3aef833c6385": Phase="Running", Reason="", readiness=true. Elapsed: 2.009015898s
May  4 12:58:03.489: INFO: The phase of Pod pod-hostip-dc68709b-2c3f-467c-87b5-3aef833c6385 is Running (Ready = true)
May  4 12:58:03.489: INFO: Pod "pod-hostip-dc68709b-2c3f-467c-87b5-3aef833c6385" satisfied condition "running and ready"
May  4 12:58:03.500: INFO: Pod pod-hostip-dc68709b-2c3f-467c-87b5-3aef833c6385 has hostIP: 10.0.1.224
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
May  4 12:58:03.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8132" for this suite. 05/04/23 12:58:03.512
{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","completed":250,"skipped":4647,"failed":0}
------------------------------
• [2.096 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:58:01.431
    May  4 12:58:01.431: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename pods 05/04/23 12:58:01.433
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:58:01.454
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:58:01.457
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:203
    STEP: creating pod 05/04/23 12:58:01.464
    May  4 12:58:01.480: INFO: Waiting up to 5m0s for pod "pod-hostip-dc68709b-2c3f-467c-87b5-3aef833c6385" in namespace "pods-8132" to be "running and ready"
    May  4 12:58:01.484: INFO: Pod "pod-hostip-dc68709b-2c3f-467c-87b5-3aef833c6385": Phase="Pending", Reason="", readiness=false. Elapsed: 4.259897ms
    May  4 12:58:01.484: INFO: The phase of Pod pod-hostip-dc68709b-2c3f-467c-87b5-3aef833c6385 is Pending, waiting for it to be Running (with Ready = true)
    May  4 12:58:03.489: INFO: Pod "pod-hostip-dc68709b-2c3f-467c-87b5-3aef833c6385": Phase="Running", Reason="", readiness=true. Elapsed: 2.009015898s
    May  4 12:58:03.489: INFO: The phase of Pod pod-hostip-dc68709b-2c3f-467c-87b5-3aef833c6385 is Running (Ready = true)
    May  4 12:58:03.489: INFO: Pod "pod-hostip-dc68709b-2c3f-467c-87b5-3aef833c6385" satisfied condition "running and ready"
    May  4 12:58:03.500: INFO: Pod pod-hostip-dc68709b-2c3f-467c-87b5-3aef833c6385 has hostIP: 10.0.1.224
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    May  4 12:58:03.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8132" for this suite. 05/04/23 12:58:03.512
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:58:03.528
May  4 12:58:03.528: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename kubelet-test 05/04/23 12:58:03.528
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:58:03.551
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:58:03.556
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
May  4 12:58:03.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6209" for this suite. 05/04/23 12:58:03.634
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","completed":251,"skipped":4657,"failed":0}
------------------------------
• [0.117 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:58:03.528
    May  4 12:58:03.528: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename kubelet-test 05/04/23 12:58:03.528
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:58:03.551
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:58:03.556
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    May  4 12:58:03.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-6209" for this suite. 05/04/23 12:58:03.634
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:58:03.645
May  4 12:58:03.645: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename sched-preemption 05/04/23 12:58:03.646
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:58:03.671
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:58:03.677
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
May  4 12:58:03.698: INFO: Waiting up to 1m0s for all nodes to be ready
May  4 12:59:03.768: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
STEP: Create pods that use 4/5 of node resources. 05/04/23 12:59:03.773
May  4 12:59:03.814: INFO: Created pod: pod0-0-sched-preemption-low-priority
May  4 12:59:03.829: INFO: Created pod: pod0-1-sched-preemption-medium-priority
May  4 12:59:03.893: INFO: Created pod: pod1-0-sched-preemption-medium-priority
May  4 12:59:03.909: INFO: Created pod: pod1-1-sched-preemption-medium-priority
May  4 12:59:03.975: INFO: Created pod: pod2-0-sched-preemption-medium-priority
May  4 12:59:04.004: INFO: Created pod: pod2-1-sched-preemption-medium-priority
May  4 12:59:04.062: INFO: Created pod: pod3-0-sched-preemption-medium-priority
May  4 12:59:04.090: INFO: Created pod: pod3-1-sched-preemption-medium-priority
May  4 12:59:04.204: INFO: Created pod: pod4-0-sched-preemption-medium-priority
May  4 12:59:04.247: INFO: Created pod: pod4-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 05/04/23 12:59:04.247
May  4 12:59:04.247: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-4119" to be "running"
May  4 12:59:04.262: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 14.994846ms
May  4 12:59:06.267: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.020073324s
May  4 12:59:06.267: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
May  4 12:59:06.267: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-4119" to be "running"
May  4 12:59:06.271: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.778609ms
May  4 12:59:06.271: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
May  4 12:59:06.271: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-4119" to be "running"
May  4 12:59:06.276: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.573216ms
May  4 12:59:06.276: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
May  4 12:59:06.276: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-4119" to be "running"
May  4 12:59:06.283: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.046656ms
May  4 12:59:06.283: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
May  4 12:59:06.283: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-4119" to be "running"
May  4 12:59:06.287: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.737496ms
May  4 12:59:06.287: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
May  4 12:59:06.288: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-4119" to be "running"
May  4 12:59:06.291: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.79452ms
May  4 12:59:06.291: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
May  4 12:59:06.291: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-4119" to be "running"
May  4 12:59:06.295: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.049235ms
May  4 12:59:06.295: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
May  4 12:59:06.295: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-4119" to be "running"
May  4 12:59:06.300: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.112323ms
May  4 12:59:06.300: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
May  4 12:59:06.300: INFO: Waiting up to 5m0s for pod "pod4-0-sched-preemption-medium-priority" in namespace "sched-preemption-4119" to be "running"
May  4 12:59:06.304: INFO: Pod "pod4-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.499303ms
May  4 12:59:06.304: INFO: Pod "pod4-0-sched-preemption-medium-priority" satisfied condition "running"
May  4 12:59:06.304: INFO: Waiting up to 5m0s for pod "pod4-1-sched-preemption-medium-priority" in namespace "sched-preemption-4119" to be "running"
May  4 12:59:06.310: INFO: Pod "pod4-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.373137ms
May  4 12:59:06.310: INFO: Pod "pod4-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 05/04/23 12:59:06.31
May  4 12:59:06.324: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
May  4 12:59:06.328: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.855276ms
May  4 12:59:08.333: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00945591s
May  4 12:59:10.333: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.009037881s
May  4 12:59:10.333: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
May  4 12:59:10.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4119" for this suite. 05/04/23 12:59:10.412
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","completed":252,"skipped":4658,"failed":0}
------------------------------
• [SLOW TEST] [66.925 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:58:03.645
    May  4 12:58:03.645: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename sched-preemption 05/04/23 12:58:03.646
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:58:03.671
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:58:03.677
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    May  4 12:58:03.698: INFO: Waiting up to 1m0s for all nodes to be ready
    May  4 12:59:03.768: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:218
    STEP: Create pods that use 4/5 of node resources. 05/04/23 12:59:03.773
    May  4 12:59:03.814: INFO: Created pod: pod0-0-sched-preemption-low-priority
    May  4 12:59:03.829: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    May  4 12:59:03.893: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    May  4 12:59:03.909: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    May  4 12:59:03.975: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    May  4 12:59:04.004: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    May  4 12:59:04.062: INFO: Created pod: pod3-0-sched-preemption-medium-priority
    May  4 12:59:04.090: INFO: Created pod: pod3-1-sched-preemption-medium-priority
    May  4 12:59:04.204: INFO: Created pod: pod4-0-sched-preemption-medium-priority
    May  4 12:59:04.247: INFO: Created pod: pod4-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 05/04/23 12:59:04.247
    May  4 12:59:04.247: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-4119" to be "running"
    May  4 12:59:04.262: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 14.994846ms
    May  4 12:59:06.267: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.020073324s
    May  4 12:59:06.267: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    May  4 12:59:06.267: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-4119" to be "running"
    May  4 12:59:06.271: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.778609ms
    May  4 12:59:06.271: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    May  4 12:59:06.271: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-4119" to be "running"
    May  4 12:59:06.276: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.573216ms
    May  4 12:59:06.276: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    May  4 12:59:06.276: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-4119" to be "running"
    May  4 12:59:06.283: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.046656ms
    May  4 12:59:06.283: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    May  4 12:59:06.283: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-4119" to be "running"
    May  4 12:59:06.287: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.737496ms
    May  4 12:59:06.287: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    May  4 12:59:06.288: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-4119" to be "running"
    May  4 12:59:06.291: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.79452ms
    May  4 12:59:06.291: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    May  4 12:59:06.291: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-4119" to be "running"
    May  4 12:59:06.295: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.049235ms
    May  4 12:59:06.295: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
    May  4 12:59:06.295: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-4119" to be "running"
    May  4 12:59:06.300: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.112323ms
    May  4 12:59:06.300: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
    May  4 12:59:06.300: INFO: Waiting up to 5m0s for pod "pod4-0-sched-preemption-medium-priority" in namespace "sched-preemption-4119" to be "running"
    May  4 12:59:06.304: INFO: Pod "pod4-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.499303ms
    May  4 12:59:06.304: INFO: Pod "pod4-0-sched-preemption-medium-priority" satisfied condition "running"
    May  4 12:59:06.304: INFO: Waiting up to 5m0s for pod "pod4-1-sched-preemption-medium-priority" in namespace "sched-preemption-4119" to be "running"
    May  4 12:59:06.310: INFO: Pod "pod4-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.373137ms
    May  4 12:59:06.310: INFO: Pod "pod4-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 05/04/23 12:59:06.31
    May  4 12:59:06.324: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    May  4 12:59:06.328: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.855276ms
    May  4 12:59:08.333: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00945591s
    May  4 12:59:10.333: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.009037881s
    May  4 12:59:10.333: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    May  4 12:59:10.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-4119" for this suite. 05/04/23 12:59:10.412
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:59:10.573
May  4 12:59:10.573: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename services 05/04/23 12:59:10.574
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:59:10.614
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:59:10.622
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
STEP: creating service in namespace services-1616 05/04/23 12:59:10.638
STEP: creating service affinity-clusterip-transition in namespace services-1616 05/04/23 12:59:10.638
STEP: creating replication controller affinity-clusterip-transition in namespace services-1616 05/04/23 12:59:10.663
I0504 12:59:10.681238      21 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-1616, replica count: 3
I0504 12:59:13.732868      21 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  4 12:59:13.740: INFO: Creating new exec pod
May  4 12:59:13.753: INFO: Waiting up to 5m0s for pod "execpod-affinityvhxr6" in namespace "services-1616" to be "running"
May  4 12:59:13.757: INFO: Pod "execpod-affinityvhxr6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.432848ms
May  4 12:59:15.763: INFO: Pod "execpod-affinityvhxr6": Phase="Running", Reason="", readiness=true. Elapsed: 2.009725994s
May  4 12:59:15.763: INFO: Pod "execpod-affinityvhxr6" satisfied condition "running"
May  4 12:59:16.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-1616 exec execpod-affinityvhxr6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
May  4 12:59:16.937: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
May  4 12:59:16.937: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  4 12:59:16.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-1616 exec execpod-affinityvhxr6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.21.191.190 80'
May  4 12:59:17.081: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.21.191.190 80\nConnection to 10.21.191.190 80 port [tcp/http] succeeded!\n"
May  4 12:59:17.081: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  4 12:59:17.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-1616 exec execpod-affinityvhxr6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.21.191.190:80/ ; done'
May  4 12:59:17.324: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n"
May  4 12:59:17.324: INFO: stdout: "\naffinity-clusterip-transition-6cx6g\naffinity-clusterip-transition-csdhz\naffinity-clusterip-transition-hbh8k\naffinity-clusterip-transition-6cx6g\naffinity-clusterip-transition-csdhz\naffinity-clusterip-transition-hbh8k\naffinity-clusterip-transition-6cx6g\naffinity-clusterip-transition-csdhz\naffinity-clusterip-transition-hbh8k\naffinity-clusterip-transition-6cx6g\naffinity-clusterip-transition-csdhz\naffinity-clusterip-transition-hbh8k\naffinity-clusterip-transition-6cx6g\naffinity-clusterip-transition-csdhz\naffinity-clusterip-transition-hbh8k\naffinity-clusterip-transition-6cx6g"
May  4 12:59:17.324: INFO: Received response from host: affinity-clusterip-transition-6cx6g
May  4 12:59:17.324: INFO: Received response from host: affinity-clusterip-transition-csdhz
May  4 12:59:17.324: INFO: Received response from host: affinity-clusterip-transition-hbh8k
May  4 12:59:17.324: INFO: Received response from host: affinity-clusterip-transition-6cx6g
May  4 12:59:17.324: INFO: Received response from host: affinity-clusterip-transition-csdhz
May  4 12:59:17.324: INFO: Received response from host: affinity-clusterip-transition-hbh8k
May  4 12:59:17.324: INFO: Received response from host: affinity-clusterip-transition-6cx6g
May  4 12:59:17.324: INFO: Received response from host: affinity-clusterip-transition-csdhz
May  4 12:59:17.324: INFO: Received response from host: affinity-clusterip-transition-hbh8k
May  4 12:59:17.324: INFO: Received response from host: affinity-clusterip-transition-6cx6g
May  4 12:59:17.324: INFO: Received response from host: affinity-clusterip-transition-csdhz
May  4 12:59:17.324: INFO: Received response from host: affinity-clusterip-transition-hbh8k
May  4 12:59:17.324: INFO: Received response from host: affinity-clusterip-transition-6cx6g
May  4 12:59:17.324: INFO: Received response from host: affinity-clusterip-transition-csdhz
May  4 12:59:17.324: INFO: Received response from host: affinity-clusterip-transition-hbh8k
May  4 12:59:17.324: INFO: Received response from host: affinity-clusterip-transition-6cx6g
May  4 12:59:17.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-1616 exec execpod-affinityvhxr6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.21.191.190:80/ ; done'
May  4 12:59:17.574: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n"
May  4 12:59:17.575: INFO: stdout: "\naffinity-clusterip-transition-hbh8k\naffinity-clusterip-transition-hbh8k\naffinity-clusterip-transition-hbh8k\naffinity-clusterip-transition-hbh8k\naffinity-clusterip-transition-hbh8k\naffinity-clusterip-transition-hbh8k\naffinity-clusterip-transition-hbh8k\naffinity-clusterip-transition-hbh8k\naffinity-clusterip-transition-hbh8k\naffinity-clusterip-transition-hbh8k\naffinity-clusterip-transition-hbh8k\naffinity-clusterip-transition-hbh8k\naffinity-clusterip-transition-hbh8k\naffinity-clusterip-transition-hbh8k\naffinity-clusterip-transition-hbh8k\naffinity-clusterip-transition-hbh8k"
May  4 12:59:17.575: INFO: Received response from host: affinity-clusterip-transition-hbh8k
May  4 12:59:17.575: INFO: Received response from host: affinity-clusterip-transition-hbh8k
May  4 12:59:17.575: INFO: Received response from host: affinity-clusterip-transition-hbh8k
May  4 12:59:17.575: INFO: Received response from host: affinity-clusterip-transition-hbh8k
May  4 12:59:17.575: INFO: Received response from host: affinity-clusterip-transition-hbh8k
May  4 12:59:17.575: INFO: Received response from host: affinity-clusterip-transition-hbh8k
May  4 12:59:17.575: INFO: Received response from host: affinity-clusterip-transition-hbh8k
May  4 12:59:17.575: INFO: Received response from host: affinity-clusterip-transition-hbh8k
May  4 12:59:17.575: INFO: Received response from host: affinity-clusterip-transition-hbh8k
May  4 12:59:17.575: INFO: Received response from host: affinity-clusterip-transition-hbh8k
May  4 12:59:17.575: INFO: Received response from host: affinity-clusterip-transition-hbh8k
May  4 12:59:17.575: INFO: Received response from host: affinity-clusterip-transition-hbh8k
May  4 12:59:17.575: INFO: Received response from host: affinity-clusterip-transition-hbh8k
May  4 12:59:17.575: INFO: Received response from host: affinity-clusterip-transition-hbh8k
May  4 12:59:17.575: INFO: Received response from host: affinity-clusterip-transition-hbh8k
May  4 12:59:17.575: INFO: Received response from host: affinity-clusterip-transition-hbh8k
May  4 12:59:17.575: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-1616, will wait for the garbage collector to delete the pods 05/04/23 12:59:17.592
May  4 12:59:17.662: INFO: Deleting ReplicationController affinity-clusterip-transition took: 10.150348ms
May  4 12:59:17.763: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.327155ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  4 12:59:19.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1616" for this suite. 05/04/23 12:59:19.994
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","completed":253,"skipped":4713,"failed":0}
------------------------------
• [SLOW TEST] [9.434 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:59:10.573
    May  4 12:59:10.573: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename services 05/04/23 12:59:10.574
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:59:10.614
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:59:10.622
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2189
    STEP: creating service in namespace services-1616 05/04/23 12:59:10.638
    STEP: creating service affinity-clusterip-transition in namespace services-1616 05/04/23 12:59:10.638
    STEP: creating replication controller affinity-clusterip-transition in namespace services-1616 05/04/23 12:59:10.663
    I0504 12:59:10.681238      21 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-1616, replica count: 3
    I0504 12:59:13.732868      21 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  4 12:59:13.740: INFO: Creating new exec pod
    May  4 12:59:13.753: INFO: Waiting up to 5m0s for pod "execpod-affinityvhxr6" in namespace "services-1616" to be "running"
    May  4 12:59:13.757: INFO: Pod "execpod-affinityvhxr6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.432848ms
    May  4 12:59:15.763: INFO: Pod "execpod-affinityvhxr6": Phase="Running", Reason="", readiness=true. Elapsed: 2.009725994s
    May  4 12:59:15.763: INFO: Pod "execpod-affinityvhxr6" satisfied condition "running"
    May  4 12:59:16.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-1616 exec execpod-affinityvhxr6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
    May  4 12:59:16.937: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    May  4 12:59:16.937: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  4 12:59:16.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-1616 exec execpod-affinityvhxr6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.21.191.190 80'
    May  4 12:59:17.081: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.21.191.190 80\nConnection to 10.21.191.190 80 port [tcp/http] succeeded!\n"
    May  4 12:59:17.081: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  4 12:59:17.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-1616 exec execpod-affinityvhxr6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.21.191.190:80/ ; done'
    May  4 12:59:17.324: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n"
    May  4 12:59:17.324: INFO: stdout: "\naffinity-clusterip-transition-6cx6g\naffinity-clusterip-transition-csdhz\naffinity-clusterip-transition-hbh8k\naffinity-clusterip-transition-6cx6g\naffinity-clusterip-transition-csdhz\naffinity-clusterip-transition-hbh8k\naffinity-clusterip-transition-6cx6g\naffinity-clusterip-transition-csdhz\naffinity-clusterip-transition-hbh8k\naffinity-clusterip-transition-6cx6g\naffinity-clusterip-transition-csdhz\naffinity-clusterip-transition-hbh8k\naffinity-clusterip-transition-6cx6g\naffinity-clusterip-transition-csdhz\naffinity-clusterip-transition-hbh8k\naffinity-clusterip-transition-6cx6g"
    May  4 12:59:17.324: INFO: Received response from host: affinity-clusterip-transition-6cx6g
    May  4 12:59:17.324: INFO: Received response from host: affinity-clusterip-transition-csdhz
    May  4 12:59:17.324: INFO: Received response from host: affinity-clusterip-transition-hbh8k
    May  4 12:59:17.324: INFO: Received response from host: affinity-clusterip-transition-6cx6g
    May  4 12:59:17.324: INFO: Received response from host: affinity-clusterip-transition-csdhz
    May  4 12:59:17.324: INFO: Received response from host: affinity-clusterip-transition-hbh8k
    May  4 12:59:17.324: INFO: Received response from host: affinity-clusterip-transition-6cx6g
    May  4 12:59:17.324: INFO: Received response from host: affinity-clusterip-transition-csdhz
    May  4 12:59:17.324: INFO: Received response from host: affinity-clusterip-transition-hbh8k
    May  4 12:59:17.324: INFO: Received response from host: affinity-clusterip-transition-6cx6g
    May  4 12:59:17.324: INFO: Received response from host: affinity-clusterip-transition-csdhz
    May  4 12:59:17.324: INFO: Received response from host: affinity-clusterip-transition-hbh8k
    May  4 12:59:17.324: INFO: Received response from host: affinity-clusterip-transition-6cx6g
    May  4 12:59:17.324: INFO: Received response from host: affinity-clusterip-transition-csdhz
    May  4 12:59:17.324: INFO: Received response from host: affinity-clusterip-transition-hbh8k
    May  4 12:59:17.324: INFO: Received response from host: affinity-clusterip-transition-6cx6g
    May  4 12:59:17.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-1616 exec execpod-affinityvhxr6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.21.191.190:80/ ; done'
    May  4 12:59:17.574: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.191.190:80/\n"
    May  4 12:59:17.575: INFO: stdout: "\naffinity-clusterip-transition-hbh8k\naffinity-clusterip-transition-hbh8k\naffinity-clusterip-transition-hbh8k\naffinity-clusterip-transition-hbh8k\naffinity-clusterip-transition-hbh8k\naffinity-clusterip-transition-hbh8k\naffinity-clusterip-transition-hbh8k\naffinity-clusterip-transition-hbh8k\naffinity-clusterip-transition-hbh8k\naffinity-clusterip-transition-hbh8k\naffinity-clusterip-transition-hbh8k\naffinity-clusterip-transition-hbh8k\naffinity-clusterip-transition-hbh8k\naffinity-clusterip-transition-hbh8k\naffinity-clusterip-transition-hbh8k\naffinity-clusterip-transition-hbh8k"
    May  4 12:59:17.575: INFO: Received response from host: affinity-clusterip-transition-hbh8k
    May  4 12:59:17.575: INFO: Received response from host: affinity-clusterip-transition-hbh8k
    May  4 12:59:17.575: INFO: Received response from host: affinity-clusterip-transition-hbh8k
    May  4 12:59:17.575: INFO: Received response from host: affinity-clusterip-transition-hbh8k
    May  4 12:59:17.575: INFO: Received response from host: affinity-clusterip-transition-hbh8k
    May  4 12:59:17.575: INFO: Received response from host: affinity-clusterip-transition-hbh8k
    May  4 12:59:17.575: INFO: Received response from host: affinity-clusterip-transition-hbh8k
    May  4 12:59:17.575: INFO: Received response from host: affinity-clusterip-transition-hbh8k
    May  4 12:59:17.575: INFO: Received response from host: affinity-clusterip-transition-hbh8k
    May  4 12:59:17.575: INFO: Received response from host: affinity-clusterip-transition-hbh8k
    May  4 12:59:17.575: INFO: Received response from host: affinity-clusterip-transition-hbh8k
    May  4 12:59:17.575: INFO: Received response from host: affinity-clusterip-transition-hbh8k
    May  4 12:59:17.575: INFO: Received response from host: affinity-clusterip-transition-hbh8k
    May  4 12:59:17.575: INFO: Received response from host: affinity-clusterip-transition-hbh8k
    May  4 12:59:17.575: INFO: Received response from host: affinity-clusterip-transition-hbh8k
    May  4 12:59:17.575: INFO: Received response from host: affinity-clusterip-transition-hbh8k
    May  4 12:59:17.575: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-1616, will wait for the garbage collector to delete the pods 05/04/23 12:59:17.592
    May  4 12:59:17.662: INFO: Deleting ReplicationController affinity-clusterip-transition took: 10.150348ms
    May  4 12:59:17.763: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.327155ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  4 12:59:19.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1616" for this suite. 05/04/23 12:59:19.994
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:59:20.008
May  4 12:59:20.008: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename replication-controller 05/04/23 12:59:20.009
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:59:20.028
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:59:20.033
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
STEP: creating a ReplicationController 05/04/23 12:59:20.043
STEP: waiting for RC to be added 05/04/23 12:59:20.05
STEP: waiting for available Replicas 05/04/23 12:59:20.05
STEP: patching ReplicationController 05/04/23 12:59:21.832
STEP: waiting for RC to be modified 05/04/23 12:59:21.846
STEP: patching ReplicationController status 05/04/23 12:59:21.846
STEP: waiting for RC to be modified 05/04/23 12:59:21.855
STEP: waiting for available Replicas 05/04/23 12:59:21.856
STEP: fetching ReplicationController status 05/04/23 12:59:21.863
STEP: patching ReplicationController scale 05/04/23 12:59:21.868
STEP: waiting for RC to be modified 05/04/23 12:59:21.879
STEP: waiting for ReplicationController's scale to be the max amount 05/04/23 12:59:21.879
STEP: fetching ReplicationController; ensuring that it's patched 05/04/23 12:59:23.532
STEP: updating ReplicationController status 05/04/23 12:59:23.536
STEP: waiting for RC to be modified 05/04/23 12:59:23.544
STEP: listing all ReplicationControllers 05/04/23 12:59:23.544
STEP: checking that ReplicationController has expected values 05/04/23 12:59:23.553
STEP: deleting ReplicationControllers by collection 05/04/23 12:59:23.553
STEP: waiting for ReplicationController to have a DELETED watchEvent 05/04/23 12:59:23.568
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
May  4 12:59:23.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7500" for this suite. 05/04/23 12:59:23.652
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","completed":254,"skipped":4721,"failed":0}
------------------------------
• [3.658 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:59:20.008
    May  4 12:59:20.008: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename replication-controller 05/04/23 12:59:20.009
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:59:20.028
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:59:20.033
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:109
    STEP: creating a ReplicationController 05/04/23 12:59:20.043
    STEP: waiting for RC to be added 05/04/23 12:59:20.05
    STEP: waiting for available Replicas 05/04/23 12:59:20.05
    STEP: patching ReplicationController 05/04/23 12:59:21.832
    STEP: waiting for RC to be modified 05/04/23 12:59:21.846
    STEP: patching ReplicationController status 05/04/23 12:59:21.846
    STEP: waiting for RC to be modified 05/04/23 12:59:21.855
    STEP: waiting for available Replicas 05/04/23 12:59:21.856
    STEP: fetching ReplicationController status 05/04/23 12:59:21.863
    STEP: patching ReplicationController scale 05/04/23 12:59:21.868
    STEP: waiting for RC to be modified 05/04/23 12:59:21.879
    STEP: waiting for ReplicationController's scale to be the max amount 05/04/23 12:59:21.879
    STEP: fetching ReplicationController; ensuring that it's patched 05/04/23 12:59:23.532
    STEP: updating ReplicationController status 05/04/23 12:59:23.536
    STEP: waiting for RC to be modified 05/04/23 12:59:23.544
    STEP: listing all ReplicationControllers 05/04/23 12:59:23.544
    STEP: checking that ReplicationController has expected values 05/04/23 12:59:23.553
    STEP: deleting ReplicationControllers by collection 05/04/23 12:59:23.553
    STEP: waiting for ReplicationController to have a DELETED watchEvent 05/04/23 12:59:23.568
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    May  4 12:59:23.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-7500" for this suite. 05/04/23 12:59:23.652
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:59:23.667
May  4 12:59:23.667: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename prestop 05/04/23 12:59:23.668
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:59:23.699
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:59:23.705
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-3708 05/04/23 12:59:23.712
STEP: Waiting for pods to come up. 05/04/23 12:59:23.725
May  4 12:59:23.725: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-3708" to be "running"
May  4 12:59:23.731: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 6.699526ms
May  4 12:59:25.740: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.014716491s
May  4 12:59:25.740: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-3708 05/04/23 12:59:25.745
May  4 12:59:25.752: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-3708" to be "running"
May  4 12:59:25.756: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 3.620071ms
May  4 12:59:27.762: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.009651516s
May  4 12:59:27.762: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 05/04/23 12:59:27.762
May  4 12:59:32.783: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 05/04/23 12:59:32.783
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
May  4 12:59:32.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-3708" for this suite. 05/04/23 12:59:32.827
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","completed":255,"skipped":4739,"failed":0}
------------------------------
• [SLOW TEST] [9.171 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:59:23.667
    May  4 12:59:23.667: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename prestop 05/04/23 12:59:23.668
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:59:23.699
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:59:23.705
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-3708 05/04/23 12:59:23.712
    STEP: Waiting for pods to come up. 05/04/23 12:59:23.725
    May  4 12:59:23.725: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-3708" to be "running"
    May  4 12:59:23.731: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 6.699526ms
    May  4 12:59:25.740: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.014716491s
    May  4 12:59:25.740: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-3708 05/04/23 12:59:25.745
    May  4 12:59:25.752: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-3708" to be "running"
    May  4 12:59:25.756: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 3.620071ms
    May  4 12:59:27.762: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.009651516s
    May  4 12:59:27.762: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 05/04/23 12:59:27.762
    May  4 12:59:32.783: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 05/04/23 12:59:32.783
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/framework.go:187
    May  4 12:59:32.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "prestop-3708" for this suite. 05/04/23 12:59:32.827
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:59:32.839
May  4 12:59:32.839: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename emptydir 05/04/23 12:59:32.84
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:59:32.87
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:59:32.878
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
STEP: Creating a pod to test emptydir 0777 on node default medium 05/04/23 12:59:32.886
May  4 12:59:32.904: INFO: Waiting up to 5m0s for pod "pod-88136ae0-ef18-4144-a3f1-3103c2c15b87" in namespace "emptydir-9424" to be "Succeeded or Failed"
May  4 12:59:32.912: INFO: Pod "pod-88136ae0-ef18-4144-a3f1-3103c2c15b87": Phase="Pending", Reason="", readiness=false. Elapsed: 7.417845ms
May  4 12:59:34.917: INFO: Pod "pod-88136ae0-ef18-4144-a3f1-3103c2c15b87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013058526s
May  4 12:59:36.918: INFO: Pod "pod-88136ae0-ef18-4144-a3f1-3103c2c15b87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013677909s
STEP: Saw pod success 05/04/23 12:59:36.918
May  4 12:59:36.918: INFO: Pod "pod-88136ae0-ef18-4144-a3f1-3103c2c15b87" satisfied condition "Succeeded or Failed"
May  4 12:59:36.922: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-88136ae0-ef18-4144-a3f1-3103c2c15b87 container test-container: <nil>
STEP: delete the pod 05/04/23 12:59:36.941
May  4 12:59:36.961: INFO: Waiting for pod pod-88136ae0-ef18-4144-a3f1-3103c2c15b87 to disappear
May  4 12:59:36.964: INFO: Pod pod-88136ae0-ef18-4144-a3f1-3103c2c15b87 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May  4 12:59:36.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9424" for this suite. 05/04/23 12:59:36.973
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":256,"skipped":4753,"failed":0}
------------------------------
• [4.144 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:59:32.839
    May  4 12:59:32.839: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename emptydir 05/04/23 12:59:32.84
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:59:32.87
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:59:32.878
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:216
    STEP: Creating a pod to test emptydir 0777 on node default medium 05/04/23 12:59:32.886
    May  4 12:59:32.904: INFO: Waiting up to 5m0s for pod "pod-88136ae0-ef18-4144-a3f1-3103c2c15b87" in namespace "emptydir-9424" to be "Succeeded or Failed"
    May  4 12:59:32.912: INFO: Pod "pod-88136ae0-ef18-4144-a3f1-3103c2c15b87": Phase="Pending", Reason="", readiness=false. Elapsed: 7.417845ms
    May  4 12:59:34.917: INFO: Pod "pod-88136ae0-ef18-4144-a3f1-3103c2c15b87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013058526s
    May  4 12:59:36.918: INFO: Pod "pod-88136ae0-ef18-4144-a3f1-3103c2c15b87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013677909s
    STEP: Saw pod success 05/04/23 12:59:36.918
    May  4 12:59:36.918: INFO: Pod "pod-88136ae0-ef18-4144-a3f1-3103c2c15b87" satisfied condition "Succeeded or Failed"
    May  4 12:59:36.922: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-88136ae0-ef18-4144-a3f1-3103c2c15b87 container test-container: <nil>
    STEP: delete the pod 05/04/23 12:59:36.941
    May  4 12:59:36.961: INFO: Waiting for pod pod-88136ae0-ef18-4144-a3f1-3103c2c15b87 to disappear
    May  4 12:59:36.964: INFO: Pod pod-88136ae0-ef18-4144-a3f1-3103c2c15b87 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May  4 12:59:36.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9424" for this suite. 05/04/23 12:59:36.973
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 12:59:36.984
May  4 12:59:36.984: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename aggregator 05/04/23 12:59:36.985
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:59:37.006
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:59:37.013
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
May  4 12:59:37.015: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 05/04/23 12:59:37.016
May  4 12:59:37.525: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
May  4 12:59:39.596: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  4 12:59:41.601: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  4 12:59:43.602: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  4 12:59:45.601: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  4 12:59:47.601: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  4 12:59:49.602: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  4 12:59:51.602: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  4 12:59:53.601: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  4 12:59:55.601: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  4 12:59:57.603: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  4 12:59:59.605: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  4 13:00:01.954: INFO: Waited 340.192504ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 05/04/23 13:00:02.12
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 05/04/23 13:00:02.127
STEP: List APIServices 05/04/23 13:00:02.143
May  4 13:00:02.155: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
May  4 13:00:02.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-1977" for this suite. 05/04/23 13:00:02.532
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","completed":257,"skipped":4772,"failed":0}
------------------------------
• [SLOW TEST] [25.571 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 12:59:36.984
    May  4 12:59:36.984: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename aggregator 05/04/23 12:59:36.985
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 12:59:37.006
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 12:59:37.013
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    May  4 12:59:37.015: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 05/04/23 12:59:37.016
    May  4 12:59:37.525: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    May  4 12:59:39.596: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  4 12:59:41.601: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  4 12:59:43.602: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  4 12:59:45.601: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  4 12:59:47.601: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  4 12:59:49.602: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  4 12:59:51.602: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  4 12:59:53.601: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  4 12:59:55.601: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  4 12:59:57.603: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  4 12:59:59.605: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 12, 59, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  4 13:00:01.954: INFO: Waited 340.192504ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 05/04/23 13:00:02.12
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 05/04/23 13:00:02.127
    STEP: List APIServices 05/04/23 13:00:02.143
    May  4 13:00:02.155: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:187
    May  4 13:00:02.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "aggregator-1977" for this suite. 05/04/23 13:00:02.532
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:00:02.556
May  4 13:00:02.556: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename job 05/04/23 13:00:02.556
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:00:02.596
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:00:02.605
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
STEP: Creating a job 05/04/23 13:00:02.613
STEP: Ensure pods equal to paralellism count is attached to the job 05/04/23 13:00:02.625
STEP: patching /status 05/04/23 13:00:08.63
STEP: updating /status 05/04/23 13:00:08.639
STEP: get /status 05/04/23 13:00:08.687
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
May  4 13:00:08.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5022" for this suite. 05/04/23 13:00:08.705
{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","completed":258,"skipped":4785,"failed":0}
------------------------------
• [SLOW TEST] [6.158 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:00:02.556
    May  4 13:00:02.556: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename job 05/04/23 13:00:02.556
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:00:02.596
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:00:02.605
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:464
    STEP: Creating a job 05/04/23 13:00:02.613
    STEP: Ensure pods equal to paralellism count is attached to the job 05/04/23 13:00:02.625
    STEP: patching /status 05/04/23 13:00:08.63
    STEP: updating /status 05/04/23 13:00:08.639
    STEP: get /status 05/04/23 13:00:08.687
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    May  4 13:00:08.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-5022" for this suite. 05/04/23 13:00:08.705
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:00:08.716
May  4 13:00:08.716: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename subpath 05/04/23 13:00:08.717
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:00:08.736
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:00:08.739
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 05/04/23 13:00:08.745
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-n76l 05/04/23 13:00:08.759
STEP: Creating a pod to test atomic-volume-subpath 05/04/23 13:00:08.759
May  4 13:00:08.771: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-n76l" in namespace "subpath-5790" to be "Succeeded or Failed"
May  4 13:00:08.777: INFO: Pod "pod-subpath-test-secret-n76l": Phase="Pending", Reason="", readiness=false. Elapsed: 5.155216ms
May  4 13:00:10.782: INFO: Pod "pod-subpath-test-secret-n76l": Phase="Running", Reason="", readiness=true. Elapsed: 2.010352622s
May  4 13:00:12.783: INFO: Pod "pod-subpath-test-secret-n76l": Phase="Running", Reason="", readiness=true. Elapsed: 4.011691993s
May  4 13:00:14.783: INFO: Pod "pod-subpath-test-secret-n76l": Phase="Running", Reason="", readiness=true. Elapsed: 6.011918655s
May  4 13:00:16.785: INFO: Pod "pod-subpath-test-secret-n76l": Phase="Running", Reason="", readiness=true. Elapsed: 8.013436563s
May  4 13:00:18.782: INFO: Pod "pod-subpath-test-secret-n76l": Phase="Running", Reason="", readiness=true. Elapsed: 10.010242653s
May  4 13:00:20.782: INFO: Pod "pod-subpath-test-secret-n76l": Phase="Running", Reason="", readiness=true. Elapsed: 12.010481665s
May  4 13:00:22.782: INFO: Pod "pod-subpath-test-secret-n76l": Phase="Running", Reason="", readiness=true. Elapsed: 14.010770619s
May  4 13:00:24.781: INFO: Pod "pod-subpath-test-secret-n76l": Phase="Running", Reason="", readiness=true. Elapsed: 16.010070441s
May  4 13:00:26.782: INFO: Pod "pod-subpath-test-secret-n76l": Phase="Running", Reason="", readiness=true. Elapsed: 18.010832584s
May  4 13:00:28.781: INFO: Pod "pod-subpath-test-secret-n76l": Phase="Running", Reason="", readiness=true. Elapsed: 20.010086535s
May  4 13:00:30.783: INFO: Pod "pod-subpath-test-secret-n76l": Phase="Running", Reason="", readiness=false. Elapsed: 22.011389808s
May  4 13:00:32.782: INFO: Pod "pod-subpath-test-secret-n76l": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.010705558s
STEP: Saw pod success 05/04/23 13:00:32.782
May  4 13:00:32.782: INFO: Pod "pod-subpath-test-secret-n76l" satisfied condition "Succeeded or Failed"
May  4 13:00:32.792: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-subpath-test-secret-n76l container test-container-subpath-secret-n76l: <nil>
STEP: delete the pod 05/04/23 13:00:32.812
May  4 13:00:32.850: INFO: Waiting for pod pod-subpath-test-secret-n76l to disappear
May  4 13:00:32.859: INFO: Pod pod-subpath-test-secret-n76l no longer exists
STEP: Deleting pod pod-subpath-test-secret-n76l 05/04/23 13:00:32.859
May  4 13:00:32.859: INFO: Deleting pod "pod-subpath-test-secret-n76l" in namespace "subpath-5790"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
May  4 13:00:32.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5790" for this suite. 05/04/23 13:00:32.878
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","completed":259,"skipped":4831,"failed":0}
------------------------------
• [SLOW TEST] [24.178 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:00:08.716
    May  4 13:00:08.716: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename subpath 05/04/23 13:00:08.717
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:00:08.736
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:00:08.739
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 05/04/23 13:00:08.745
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-n76l 05/04/23 13:00:08.759
    STEP: Creating a pod to test atomic-volume-subpath 05/04/23 13:00:08.759
    May  4 13:00:08.771: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-n76l" in namespace "subpath-5790" to be "Succeeded or Failed"
    May  4 13:00:08.777: INFO: Pod "pod-subpath-test-secret-n76l": Phase="Pending", Reason="", readiness=false. Elapsed: 5.155216ms
    May  4 13:00:10.782: INFO: Pod "pod-subpath-test-secret-n76l": Phase="Running", Reason="", readiness=true. Elapsed: 2.010352622s
    May  4 13:00:12.783: INFO: Pod "pod-subpath-test-secret-n76l": Phase="Running", Reason="", readiness=true. Elapsed: 4.011691993s
    May  4 13:00:14.783: INFO: Pod "pod-subpath-test-secret-n76l": Phase="Running", Reason="", readiness=true. Elapsed: 6.011918655s
    May  4 13:00:16.785: INFO: Pod "pod-subpath-test-secret-n76l": Phase="Running", Reason="", readiness=true. Elapsed: 8.013436563s
    May  4 13:00:18.782: INFO: Pod "pod-subpath-test-secret-n76l": Phase="Running", Reason="", readiness=true. Elapsed: 10.010242653s
    May  4 13:00:20.782: INFO: Pod "pod-subpath-test-secret-n76l": Phase="Running", Reason="", readiness=true. Elapsed: 12.010481665s
    May  4 13:00:22.782: INFO: Pod "pod-subpath-test-secret-n76l": Phase="Running", Reason="", readiness=true. Elapsed: 14.010770619s
    May  4 13:00:24.781: INFO: Pod "pod-subpath-test-secret-n76l": Phase="Running", Reason="", readiness=true. Elapsed: 16.010070441s
    May  4 13:00:26.782: INFO: Pod "pod-subpath-test-secret-n76l": Phase="Running", Reason="", readiness=true. Elapsed: 18.010832584s
    May  4 13:00:28.781: INFO: Pod "pod-subpath-test-secret-n76l": Phase="Running", Reason="", readiness=true. Elapsed: 20.010086535s
    May  4 13:00:30.783: INFO: Pod "pod-subpath-test-secret-n76l": Phase="Running", Reason="", readiness=false. Elapsed: 22.011389808s
    May  4 13:00:32.782: INFO: Pod "pod-subpath-test-secret-n76l": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.010705558s
    STEP: Saw pod success 05/04/23 13:00:32.782
    May  4 13:00:32.782: INFO: Pod "pod-subpath-test-secret-n76l" satisfied condition "Succeeded or Failed"
    May  4 13:00:32.792: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-subpath-test-secret-n76l container test-container-subpath-secret-n76l: <nil>
    STEP: delete the pod 05/04/23 13:00:32.812
    May  4 13:00:32.850: INFO: Waiting for pod pod-subpath-test-secret-n76l to disappear
    May  4 13:00:32.859: INFO: Pod pod-subpath-test-secret-n76l no longer exists
    STEP: Deleting pod pod-subpath-test-secret-n76l 05/04/23 13:00:32.859
    May  4 13:00:32.859: INFO: Deleting pod "pod-subpath-test-secret-n76l" in namespace "subpath-5790"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    May  4 13:00:32.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-5790" for this suite. 05/04/23 13:00:32.878
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:00:32.895
May  4 13:00:32.895: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename projected 05/04/23 13:00:32.896
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:00:32.935
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:00:32.949
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
STEP: Creating configMap with name projected-configmap-test-volume-82a9fbff-a958-45ac-97b5-7a25c06838e3 05/04/23 13:00:32.954
STEP: Creating a pod to test consume configMaps 05/04/23 13:00:32.967
May  4 13:00:32.991: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f8fab90e-d281-48ec-82cb-c5c89349c568" in namespace "projected-2848" to be "Succeeded or Failed"
May  4 13:00:33.011: INFO: Pod "pod-projected-configmaps-f8fab90e-d281-48ec-82cb-c5c89349c568": Phase="Pending", Reason="", readiness=false. Elapsed: 19.637861ms
May  4 13:00:35.023: INFO: Pod "pod-projected-configmaps-f8fab90e-d281-48ec-82cb-c5c89349c568": Phase="Running", Reason="", readiness=true. Elapsed: 2.031908537s
May  4 13:00:37.018: INFO: Pod "pod-projected-configmaps-f8fab90e-d281-48ec-82cb-c5c89349c568": Phase="Running", Reason="", readiness=false. Elapsed: 4.026980725s
May  4 13:00:39.022: INFO: Pod "pod-projected-configmaps-f8fab90e-d281-48ec-82cb-c5c89349c568": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030135906s
STEP: Saw pod success 05/04/23 13:00:39.022
May  4 13:00:39.022: INFO: Pod "pod-projected-configmaps-f8fab90e-d281-48ec-82cb-c5c89349c568" satisfied condition "Succeeded or Failed"
May  4 13:00:39.032: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-projected-configmaps-f8fab90e-d281-48ec-82cb-c5c89349c568 container agnhost-container: <nil>
STEP: delete the pod 05/04/23 13:00:39.049
May  4 13:00:39.075: INFO: Waiting for pod pod-projected-configmaps-f8fab90e-d281-48ec-82cb-c5c89349c568 to disappear
May  4 13:00:39.089: INFO: Pod pod-projected-configmaps-f8fab90e-d281-48ec-82cb-c5c89349c568 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
May  4 13:00:39.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2848" for this suite. 05/04/23 13:00:39.112
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":260,"skipped":4838,"failed":0}
------------------------------
• [SLOW TEST] [6.230 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:00:32.895
    May  4 13:00:32.895: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename projected 05/04/23 13:00:32.896
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:00:32.935
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:00:32.949
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:56
    STEP: Creating configMap with name projected-configmap-test-volume-82a9fbff-a958-45ac-97b5-7a25c06838e3 05/04/23 13:00:32.954
    STEP: Creating a pod to test consume configMaps 05/04/23 13:00:32.967
    May  4 13:00:32.991: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f8fab90e-d281-48ec-82cb-c5c89349c568" in namespace "projected-2848" to be "Succeeded or Failed"
    May  4 13:00:33.011: INFO: Pod "pod-projected-configmaps-f8fab90e-d281-48ec-82cb-c5c89349c568": Phase="Pending", Reason="", readiness=false. Elapsed: 19.637861ms
    May  4 13:00:35.023: INFO: Pod "pod-projected-configmaps-f8fab90e-d281-48ec-82cb-c5c89349c568": Phase="Running", Reason="", readiness=true. Elapsed: 2.031908537s
    May  4 13:00:37.018: INFO: Pod "pod-projected-configmaps-f8fab90e-d281-48ec-82cb-c5c89349c568": Phase="Running", Reason="", readiness=false. Elapsed: 4.026980725s
    May  4 13:00:39.022: INFO: Pod "pod-projected-configmaps-f8fab90e-d281-48ec-82cb-c5c89349c568": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030135906s
    STEP: Saw pod success 05/04/23 13:00:39.022
    May  4 13:00:39.022: INFO: Pod "pod-projected-configmaps-f8fab90e-d281-48ec-82cb-c5c89349c568" satisfied condition "Succeeded or Failed"
    May  4 13:00:39.032: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-projected-configmaps-f8fab90e-d281-48ec-82cb-c5c89349c568 container agnhost-container: <nil>
    STEP: delete the pod 05/04/23 13:00:39.049
    May  4 13:00:39.075: INFO: Waiting for pod pod-projected-configmaps-f8fab90e-d281-48ec-82cb-c5c89349c568 to disappear
    May  4 13:00:39.089: INFO: Pod pod-projected-configmaps-f8fab90e-d281-48ec-82cb-c5c89349c568 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    May  4 13:00:39.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2848" for this suite. 05/04/23 13:00:39.112
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:00:39.127
May  4 13:00:39.127: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename kubectl 05/04/23 13:00:39.13
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:00:39.16
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:00:39.165
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
STEP: creating Agnhost RC 05/04/23 13:00:39.168
May  4 13:00:39.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-5235 create -f -'
May  4 13:00:40.703: INFO: stderr: ""
May  4 13:00:40.703: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 05/04/23 13:00:40.703
May  4 13:00:41.726: INFO: Selector matched 1 pods for map[app:agnhost]
May  4 13:00:41.726: INFO: Found 0 / 1
May  4 13:00:42.710: INFO: Selector matched 1 pods for map[app:agnhost]
May  4 13:00:42.711: INFO: Found 1 / 1
May  4 13:00:42.711: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 05/04/23 13:00:42.711
May  4 13:00:42.719: INFO: Selector matched 1 pods for map[app:agnhost]
May  4 13:00:42.719: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May  4 13:00:42.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-5235 patch pod agnhost-primary-5v8vg -p {"metadata":{"annotations":{"x":"y"}}}'
May  4 13:00:42.837: INFO: stderr: ""
May  4 13:00:42.837: INFO: stdout: "pod/agnhost-primary-5v8vg patched\n"
STEP: checking annotations 05/04/23 13:00:42.837
May  4 13:00:42.843: INFO: Selector matched 1 pods for map[app:agnhost]
May  4 13:00:42.843: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  4 13:00:42.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5235" for this suite. 05/04/23 13:00:42.859
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","completed":261,"skipped":4858,"failed":0}
------------------------------
• [3.741 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1644
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:00:39.127
    May  4 13:00:39.127: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename kubectl 05/04/23 13:00:39.13
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:00:39.16
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:00:39.165
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1650
    STEP: creating Agnhost RC 05/04/23 13:00:39.168
    May  4 13:00:39.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-5235 create -f -'
    May  4 13:00:40.703: INFO: stderr: ""
    May  4 13:00:40.703: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 05/04/23 13:00:40.703
    May  4 13:00:41.726: INFO: Selector matched 1 pods for map[app:agnhost]
    May  4 13:00:41.726: INFO: Found 0 / 1
    May  4 13:00:42.710: INFO: Selector matched 1 pods for map[app:agnhost]
    May  4 13:00:42.711: INFO: Found 1 / 1
    May  4 13:00:42.711: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 05/04/23 13:00:42.711
    May  4 13:00:42.719: INFO: Selector matched 1 pods for map[app:agnhost]
    May  4 13:00:42.719: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    May  4 13:00:42.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-5235 patch pod agnhost-primary-5v8vg -p {"metadata":{"annotations":{"x":"y"}}}'
    May  4 13:00:42.837: INFO: stderr: ""
    May  4 13:00:42.837: INFO: stdout: "pod/agnhost-primary-5v8vg patched\n"
    STEP: checking annotations 05/04/23 13:00:42.837
    May  4 13:00:42.843: INFO: Selector matched 1 pods for map[app:agnhost]
    May  4 13:00:42.843: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  4 13:00:42.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5235" for this suite. 05/04/23 13:00:42.859
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:00:42.884
May  4 13:00:42.884: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename svcaccounts 05/04/23 13:00:42.887
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:00:42.911
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:00:42.915
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
May  4 13:00:42.941: INFO: created pod pod-service-account-defaultsa
May  4 13:00:42.941: INFO: pod pod-service-account-defaultsa service account token volume mount: true
May  4 13:00:42.953: INFO: created pod pod-service-account-mountsa
May  4 13:00:42.953: INFO: pod pod-service-account-mountsa service account token volume mount: true
May  4 13:00:42.972: INFO: created pod pod-service-account-nomountsa
May  4 13:00:42.972: INFO: pod pod-service-account-nomountsa service account token volume mount: false
May  4 13:00:42.985: INFO: created pod pod-service-account-defaultsa-mountspec
May  4 13:00:42.985: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
May  4 13:00:42.998: INFO: created pod pod-service-account-mountsa-mountspec
May  4 13:00:42.998: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
May  4 13:00:43.011: INFO: created pod pod-service-account-nomountsa-mountspec
May  4 13:00:43.011: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
May  4 13:00:43.026: INFO: created pod pod-service-account-defaultsa-nomountspec
May  4 13:00:43.026: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
May  4 13:00:43.035: INFO: created pod pod-service-account-mountsa-nomountspec
May  4 13:00:43.035: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
May  4 13:00:43.047: INFO: created pod pod-service-account-nomountsa-nomountspec
May  4 13:00:43.047: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
May  4 13:00:43.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6112" for this suite. 05/04/23 13:00:43.068
{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","completed":262,"skipped":4942,"failed":0}
------------------------------
• [0.196 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:00:42.884
    May  4 13:00:42.884: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename svcaccounts 05/04/23 13:00:42.887
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:00:42.911
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:00:42.915
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:158
    May  4 13:00:42.941: INFO: created pod pod-service-account-defaultsa
    May  4 13:00:42.941: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    May  4 13:00:42.953: INFO: created pod pod-service-account-mountsa
    May  4 13:00:42.953: INFO: pod pod-service-account-mountsa service account token volume mount: true
    May  4 13:00:42.972: INFO: created pod pod-service-account-nomountsa
    May  4 13:00:42.972: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    May  4 13:00:42.985: INFO: created pod pod-service-account-defaultsa-mountspec
    May  4 13:00:42.985: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    May  4 13:00:42.998: INFO: created pod pod-service-account-mountsa-mountspec
    May  4 13:00:42.998: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    May  4 13:00:43.011: INFO: created pod pod-service-account-nomountsa-mountspec
    May  4 13:00:43.011: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    May  4 13:00:43.026: INFO: created pod pod-service-account-defaultsa-nomountspec
    May  4 13:00:43.026: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    May  4 13:00:43.035: INFO: created pod pod-service-account-mountsa-nomountspec
    May  4 13:00:43.035: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    May  4 13:00:43.047: INFO: created pod pod-service-account-nomountsa-nomountspec
    May  4 13:00:43.047: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    May  4 13:00:43.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-6112" for this suite. 05/04/23 13:00:43.068
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:00:43.082
May  4 13:00:43.082: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename tables 05/04/23 13:00:43.083
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:00:43.121
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:00:43.128
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
May  4 13:00:43.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-4299" for this suite. 05/04/23 13:00:43.153
{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","completed":263,"skipped":4952,"failed":0}
------------------------------
• [0.085 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:00:43.082
    May  4 13:00:43.082: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename tables 05/04/23 13:00:43.083
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:00:43.121
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:00:43.128
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:187
    May  4 13:00:43.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "tables-4299" for this suite. 05/04/23 13:00:43.153
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:00:43.167
May  4 13:00:43.167: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename kubectl 05/04/23 13:00:43.168
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:00:43.206
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:00:43.211
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1492
STEP: creating the pod 05/04/23 13:00:43.214
May  4 13:00:43.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3849 create -f -'
May  4 13:00:44.607: INFO: stderr: ""
May  4 13:00:44.607: INFO: stdout: "pod/pause created\n"
May  4 13:00:44.607: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
May  4 13:00:44.607: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-3849" to be "running and ready"
May  4 13:00:44.622: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 14.807147ms
May  4 13:00:44.622: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'ip-10-0-1-224.us-west-2.compute.internal' to be 'Running' but was 'Pending'
May  4 13:00:46.629: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022053524s
May  4 13:00:46.629: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'ip-10-0-1-224.us-west-2.compute.internal' to be 'Running' but was 'Pending'
May  4 13:00:48.628: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.021287878s
May  4 13:00:48.628: INFO: Pod "pause" satisfied condition "running and ready"
May  4 13:00:48.628: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
STEP: adding the label testing-label with value testing-label-value to a pod 05/04/23 13:00:48.628
May  4 13:00:48.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3849 label pods pause testing-label=testing-label-value'
May  4 13:00:48.745: INFO: stderr: ""
May  4 13:00:48.745: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 05/04/23 13:00:48.745
May  4 13:00:48.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3849 get pod pause -L testing-label'
May  4 13:00:48.860: INFO: stderr: ""
May  4 13:00:48.860: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod 05/04/23 13:00:48.86
May  4 13:00:48.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3849 label pods pause testing-label-'
May  4 13:00:48.993: INFO: stderr: ""
May  4 13:00:48.993: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 05/04/23 13:00:48.993
May  4 13:00:48.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3849 get pod pause -L testing-label'
May  4 13:00:49.114: INFO: stderr: ""
May  4 13:00:49.114: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1498
STEP: using delete to clean up resources 05/04/23 13:00:49.114
May  4 13:00:49.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3849 delete --grace-period=0 --force -f -'
May  4 13:00:49.234: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  4 13:00:49.234: INFO: stdout: "pod \"pause\" force deleted\n"
May  4 13:00:49.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3849 get rc,svc -l name=pause --no-headers'
May  4 13:00:49.306: INFO: stderr: "No resources found in kubectl-3849 namespace.\n"
May  4 13:00:49.307: INFO: stdout: ""
May  4 13:00:49.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3849 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May  4 13:00:49.379: INFO: stderr: ""
May  4 13:00:49.379: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  4 13:00:49.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3849" for this suite. 05/04/23 13:00:49.39
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","completed":264,"skipped":4956,"failed":0}
------------------------------
• [SLOW TEST] [6.232 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1490
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:00:43.167
    May  4 13:00:43.167: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename kubectl 05/04/23 13:00:43.168
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:00:43.206
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:00:43.211
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1492
    STEP: creating the pod 05/04/23 13:00:43.214
    May  4 13:00:43.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3849 create -f -'
    May  4 13:00:44.607: INFO: stderr: ""
    May  4 13:00:44.607: INFO: stdout: "pod/pause created\n"
    May  4 13:00:44.607: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    May  4 13:00:44.607: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-3849" to be "running and ready"
    May  4 13:00:44.622: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 14.807147ms
    May  4 13:00:44.622: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'ip-10-0-1-224.us-west-2.compute.internal' to be 'Running' but was 'Pending'
    May  4 13:00:46.629: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022053524s
    May  4 13:00:46.629: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'ip-10-0-1-224.us-west-2.compute.internal' to be 'Running' but was 'Pending'
    May  4 13:00:48.628: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.021287878s
    May  4 13:00:48.628: INFO: Pod "pause" satisfied condition "running and ready"
    May  4 13:00:48.628: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1507
    STEP: adding the label testing-label with value testing-label-value to a pod 05/04/23 13:00:48.628
    May  4 13:00:48.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3849 label pods pause testing-label=testing-label-value'
    May  4 13:00:48.745: INFO: stderr: ""
    May  4 13:00:48.745: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 05/04/23 13:00:48.745
    May  4 13:00:48.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3849 get pod pause -L testing-label'
    May  4 13:00:48.860: INFO: stderr: ""
    May  4 13:00:48.860: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 05/04/23 13:00:48.86
    May  4 13:00:48.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3849 label pods pause testing-label-'
    May  4 13:00:48.993: INFO: stderr: ""
    May  4 13:00:48.993: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 05/04/23 13:00:48.993
    May  4 13:00:48.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3849 get pod pause -L testing-label'
    May  4 13:00:49.114: INFO: stderr: ""
    May  4 13:00:49.114: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1498
    STEP: using delete to clean up resources 05/04/23 13:00:49.114
    May  4 13:00:49.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3849 delete --grace-period=0 --force -f -'
    May  4 13:00:49.234: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May  4 13:00:49.234: INFO: stdout: "pod \"pause\" force deleted\n"
    May  4 13:00:49.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3849 get rc,svc -l name=pause --no-headers'
    May  4 13:00:49.306: INFO: stderr: "No resources found in kubectl-3849 namespace.\n"
    May  4 13:00:49.307: INFO: stdout: ""
    May  4 13:00:49.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3849 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    May  4 13:00:49.379: INFO: stderr: ""
    May  4 13:00:49.379: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  4 13:00:49.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3849" for this suite. 05/04/23 13:00:49.39
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:00:49.4
May  4 13:00:49.400: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename configmap 05/04/23 13:00:49.401
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:00:49.424
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:00:49.427
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
STEP: Creating configMap with name configmap-test-volume-95027592-d440-4697-bd4d-4360fc3af215 05/04/23 13:00:49.43
STEP: Creating a pod to test consume configMaps 05/04/23 13:00:49.436
May  4 13:00:49.450: INFO: Waiting up to 5m0s for pod "pod-configmaps-e1380535-3d5e-4f94-a90c-79c9fd9d3089" in namespace "configmap-5349" to be "Succeeded or Failed"
May  4 13:00:49.455: INFO: Pod "pod-configmaps-e1380535-3d5e-4f94-a90c-79c9fd9d3089": Phase="Pending", Reason="", readiness=false. Elapsed: 5.014967ms
May  4 13:00:51.460: INFO: Pod "pod-configmaps-e1380535-3d5e-4f94-a90c-79c9fd9d3089": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010512334s
May  4 13:00:53.463: INFO: Pod "pod-configmaps-e1380535-3d5e-4f94-a90c-79c9fd9d3089": Phase="Running", Reason="", readiness=false. Elapsed: 4.013229974s
May  4 13:00:55.470: INFO: Pod "pod-configmaps-e1380535-3d5e-4f94-a90c-79c9fd9d3089": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020165757s
STEP: Saw pod success 05/04/23 13:00:55.47
May  4 13:00:55.471: INFO: Pod "pod-configmaps-e1380535-3d5e-4f94-a90c-79c9fd9d3089" satisfied condition "Succeeded or Failed"
May  4 13:00:55.483: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-configmaps-e1380535-3d5e-4f94-a90c-79c9fd9d3089 container agnhost-container: <nil>
STEP: delete the pod 05/04/23 13:00:55.506
May  4 13:00:55.544: INFO: Waiting for pod pod-configmaps-e1380535-3d5e-4f94-a90c-79c9fd9d3089 to disappear
May  4 13:00:55.549: INFO: Pod pod-configmaps-e1380535-3d5e-4f94-a90c-79c9fd9d3089 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
May  4 13:00:55.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5349" for this suite. 05/04/23 13:00:55.568
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":265,"skipped":4980,"failed":0}
------------------------------
• [SLOW TEST] [6.187 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:00:49.4
    May  4 13:00:49.400: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename configmap 05/04/23 13:00:49.401
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:00:49.424
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:00:49.427
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:73
    STEP: Creating configMap with name configmap-test-volume-95027592-d440-4697-bd4d-4360fc3af215 05/04/23 13:00:49.43
    STEP: Creating a pod to test consume configMaps 05/04/23 13:00:49.436
    May  4 13:00:49.450: INFO: Waiting up to 5m0s for pod "pod-configmaps-e1380535-3d5e-4f94-a90c-79c9fd9d3089" in namespace "configmap-5349" to be "Succeeded or Failed"
    May  4 13:00:49.455: INFO: Pod "pod-configmaps-e1380535-3d5e-4f94-a90c-79c9fd9d3089": Phase="Pending", Reason="", readiness=false. Elapsed: 5.014967ms
    May  4 13:00:51.460: INFO: Pod "pod-configmaps-e1380535-3d5e-4f94-a90c-79c9fd9d3089": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010512334s
    May  4 13:00:53.463: INFO: Pod "pod-configmaps-e1380535-3d5e-4f94-a90c-79c9fd9d3089": Phase="Running", Reason="", readiness=false. Elapsed: 4.013229974s
    May  4 13:00:55.470: INFO: Pod "pod-configmaps-e1380535-3d5e-4f94-a90c-79c9fd9d3089": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020165757s
    STEP: Saw pod success 05/04/23 13:00:55.47
    May  4 13:00:55.471: INFO: Pod "pod-configmaps-e1380535-3d5e-4f94-a90c-79c9fd9d3089" satisfied condition "Succeeded or Failed"
    May  4 13:00:55.483: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-configmaps-e1380535-3d5e-4f94-a90c-79c9fd9d3089 container agnhost-container: <nil>
    STEP: delete the pod 05/04/23 13:00:55.506
    May  4 13:00:55.544: INFO: Waiting for pod pod-configmaps-e1380535-3d5e-4f94-a90c-79c9fd9d3089 to disappear
    May  4 13:00:55.549: INFO: Pod pod-configmaps-e1380535-3d5e-4f94-a90c-79c9fd9d3089 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    May  4 13:00:55.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-5349" for this suite. 05/04/23 13:00:55.568
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:00:55.587
May  4 13:00:55.587: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename resourcequota 05/04/23 13:00:55.588
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:00:55.628
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:00:55.647
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
STEP: Discovering how many secrets are in namespace by default 05/04/23 13:00:55.657
STEP: Counting existing ResourceQuota 05/04/23 13:01:00.663
STEP: Creating a ResourceQuota 05/04/23 13:01:05.667
STEP: Ensuring resource quota status is calculated 05/04/23 13:01:05.676
STEP: Creating a Secret 05/04/23 13:01:07.683
STEP: Ensuring resource quota status captures secret creation 05/04/23 13:01:07.705
STEP: Deleting a secret 05/04/23 13:01:09.711
STEP: Ensuring resource quota status released usage 05/04/23 13:01:09.721
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
May  4 13:01:11.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7027" for this suite. 05/04/23 13:01:11.743
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","completed":266,"skipped":4989,"failed":0}
------------------------------
• [SLOW TEST] [16.165 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:00:55.587
    May  4 13:00:55.587: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename resourcequota 05/04/23 13:00:55.588
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:00:55.628
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:00:55.647
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:150
    STEP: Discovering how many secrets are in namespace by default 05/04/23 13:00:55.657
    STEP: Counting existing ResourceQuota 05/04/23 13:01:00.663
    STEP: Creating a ResourceQuota 05/04/23 13:01:05.667
    STEP: Ensuring resource quota status is calculated 05/04/23 13:01:05.676
    STEP: Creating a Secret 05/04/23 13:01:07.683
    STEP: Ensuring resource quota status captures secret creation 05/04/23 13:01:07.705
    STEP: Deleting a secret 05/04/23 13:01:09.711
    STEP: Ensuring resource quota status released usage 05/04/23 13:01:09.721
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    May  4 13:01:11.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-7027" for this suite. 05/04/23 13:01:11.743
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:01:11.752
May  4 13:01:11.752: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename configmap 05/04/23 13:01:11.754
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:01:11.78
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:01:11.785
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
STEP: creating a ConfigMap 05/04/23 13:01:11.789
STEP: fetching the ConfigMap 05/04/23 13:01:11.799
STEP: patching the ConfigMap 05/04/23 13:01:11.803
STEP: listing all ConfigMaps in all namespaces with a label selector 05/04/23 13:01:11.812
STEP: deleting the ConfigMap by collection with a label selector 05/04/23 13:01:11.857
STEP: listing all ConfigMaps in test namespace 05/04/23 13:01:11.87
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
May  4 13:01:11.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4146" for this suite. 05/04/23 13:01:11.883
{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","completed":267,"skipped":4998,"failed":0}
------------------------------
• [0.139 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:01:11.752
    May  4 13:01:11.752: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename configmap 05/04/23 13:01:11.754
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:01:11.78
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:01:11.785
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:168
    STEP: creating a ConfigMap 05/04/23 13:01:11.789
    STEP: fetching the ConfigMap 05/04/23 13:01:11.799
    STEP: patching the ConfigMap 05/04/23 13:01:11.803
    STEP: listing all ConfigMaps in all namespaces with a label selector 05/04/23 13:01:11.812
    STEP: deleting the ConfigMap by collection with a label selector 05/04/23 13:01:11.857
    STEP: listing all ConfigMaps in test namespace 05/04/23 13:01:11.87
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    May  4 13:01:11.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4146" for this suite. 05/04/23 13:01:11.883
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:01:11.892
May  4 13:01:11.892: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename kubelet-test 05/04/23 13:01:11.893
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:01:11.921
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:01:11.925
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 05/04/23 13:01:11.942
May  4 13:01:11.942: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesed5d4cd9-1a57-4893-a76c-893b06a7e72e" in namespace "kubelet-test-4002" to be "completed"
May  4 13:01:11.946: INFO: Pod "agnhost-host-aliasesed5d4cd9-1a57-4893-a76c-893b06a7e72e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.700822ms
May  4 13:01:13.951: INFO: Pod "agnhost-host-aliasesed5d4cd9-1a57-4893-a76c-893b06a7e72e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009309207s
May  4 13:01:15.951: INFO: Pod "agnhost-host-aliasesed5d4cd9-1a57-4893-a76c-893b06a7e72e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009199657s
May  4 13:01:15.951: INFO: Pod "agnhost-host-aliasesed5d4cd9-1a57-4893-a76c-893b06a7e72e" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
May  4 13:01:15.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4002" for this suite. 05/04/23 13:01:15.968
{"msg":"PASSED [sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]","completed":268,"skipped":5002,"failed":0}
------------------------------
• [4.086 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:01:11.892
    May  4 13:01:11.892: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename kubelet-test 05/04/23 13:01:11.893
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:01:11.921
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:01:11.925
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 05/04/23 13:01:11.942
    May  4 13:01:11.942: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesed5d4cd9-1a57-4893-a76c-893b06a7e72e" in namespace "kubelet-test-4002" to be "completed"
    May  4 13:01:11.946: INFO: Pod "agnhost-host-aliasesed5d4cd9-1a57-4893-a76c-893b06a7e72e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.700822ms
    May  4 13:01:13.951: INFO: Pod "agnhost-host-aliasesed5d4cd9-1a57-4893-a76c-893b06a7e72e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009309207s
    May  4 13:01:15.951: INFO: Pod "agnhost-host-aliasesed5d4cd9-1a57-4893-a76c-893b06a7e72e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009199657s
    May  4 13:01:15.951: INFO: Pod "agnhost-host-aliasesed5d4cd9-1a57-4893-a76c-893b06a7e72e" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    May  4 13:01:15.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-4002" for this suite. 05/04/23 13:01:15.968
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:01:15.978
May  4 13:01:15.978: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename services 05/04/23 13:01:15.979
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:01:16.002
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:01:16.011
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-4297 05/04/23 13:01:16.014
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 05/04/23 13:01:16.03
STEP: creating service externalsvc in namespace services-4297 05/04/23 13:01:16.03
STEP: creating replication controller externalsvc in namespace services-4297 05/04/23 13:01:16.054
I0504 13:01:16.071002      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-4297, replica count: 2
I0504 13:01:19.122469      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 05/04/23 13:01:19.127
May  4 13:01:19.152: INFO: Creating new exec pod
May  4 13:01:19.170: INFO: Waiting up to 5m0s for pod "execpodkw7jw" in namespace "services-4297" to be "running"
May  4 13:01:19.185: INFO: Pod "execpodkw7jw": Phase="Pending", Reason="", readiness=false. Elapsed: 14.733814ms
May  4 13:01:21.195: INFO: Pod "execpodkw7jw": Phase="Running", Reason="", readiness=true. Elapsed: 2.024999021s
May  4 13:01:21.195: INFO: Pod "execpodkw7jw" satisfied condition "running"
May  4 13:01:21.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-4297 exec execpodkw7jw -- /bin/sh -x -c nslookup clusterip-service.services-4297.svc.cluster.local'
May  4 13:01:21.455: INFO: stderr: "+ nslookup clusterip-service.services-4297.svc.cluster.local\n"
May  4 13:01:21.455: INFO: stdout: "Server:\t\t10.21.0.10\nAddress:\t10.21.0.10#53\n\nclusterip-service.services-4297.svc.cluster.local\tcanonical name = externalsvc.services-4297.svc.cluster.local.\nName:\texternalsvc.services-4297.svc.cluster.local\nAddress: 10.21.91.130\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4297, will wait for the garbage collector to delete the pods 05/04/23 13:01:21.455
May  4 13:01:21.524: INFO: Deleting ReplicationController externalsvc took: 14.099177ms
May  4 13:01:21.625: INFO: Terminating ReplicationController externalsvc pods took: 100.908659ms
May  4 13:01:24.005: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  4 13:01:24.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4297" for this suite. 05/04/23 13:01:24.068
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","completed":269,"skipped":5010,"failed":0}
------------------------------
• [SLOW TEST] [8.102 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:01:15.978
    May  4 13:01:15.978: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename services 05/04/23 13:01:15.979
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:01:16.002
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:01:16.011
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1481
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-4297 05/04/23 13:01:16.014
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 05/04/23 13:01:16.03
    STEP: creating service externalsvc in namespace services-4297 05/04/23 13:01:16.03
    STEP: creating replication controller externalsvc in namespace services-4297 05/04/23 13:01:16.054
    I0504 13:01:16.071002      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-4297, replica count: 2
    I0504 13:01:19.122469      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 05/04/23 13:01:19.127
    May  4 13:01:19.152: INFO: Creating new exec pod
    May  4 13:01:19.170: INFO: Waiting up to 5m0s for pod "execpodkw7jw" in namespace "services-4297" to be "running"
    May  4 13:01:19.185: INFO: Pod "execpodkw7jw": Phase="Pending", Reason="", readiness=false. Elapsed: 14.733814ms
    May  4 13:01:21.195: INFO: Pod "execpodkw7jw": Phase="Running", Reason="", readiness=true. Elapsed: 2.024999021s
    May  4 13:01:21.195: INFO: Pod "execpodkw7jw" satisfied condition "running"
    May  4 13:01:21.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-4297 exec execpodkw7jw -- /bin/sh -x -c nslookup clusterip-service.services-4297.svc.cluster.local'
    May  4 13:01:21.455: INFO: stderr: "+ nslookup clusterip-service.services-4297.svc.cluster.local\n"
    May  4 13:01:21.455: INFO: stdout: "Server:\t\t10.21.0.10\nAddress:\t10.21.0.10#53\n\nclusterip-service.services-4297.svc.cluster.local\tcanonical name = externalsvc.services-4297.svc.cluster.local.\nName:\texternalsvc.services-4297.svc.cluster.local\nAddress: 10.21.91.130\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-4297, will wait for the garbage collector to delete the pods 05/04/23 13:01:21.455
    May  4 13:01:21.524: INFO: Deleting ReplicationController externalsvc took: 14.099177ms
    May  4 13:01:21.625: INFO: Terminating ReplicationController externalsvc pods took: 100.908659ms
    May  4 13:01:24.005: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  4 13:01:24.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4297" for this suite. 05/04/23 13:01:24.068
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:01:24.084
May  4 13:01:24.085: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename svc-latency 05/04/23 13:01:24.086
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:01:24.127
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:01:24.134
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
May  4 13:01:24.138: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: creating replication controller svc-latency-rc in namespace svc-latency-4360 05/04/23 13:01:24.138
I0504 13:01:24.150082      21 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-4360, replica count: 1
I0504 13:01:25.201489      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0504 13:01:26.202440      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  4 13:01:26.334: INFO: Created: latency-svc-cjfqn
May  4 13:01:26.338: INFO: Got endpoints: latency-svc-cjfqn [35.474623ms]
May  4 13:01:26.394: INFO: Created: latency-svc-fg2sm
May  4 13:01:26.396: INFO: Got endpoints: latency-svc-fg2sm [57.474524ms]
May  4 13:01:26.434: INFO: Created: latency-svc-ngghq
May  4 13:01:26.444: INFO: Got endpoints: latency-svc-ngghq [104.185244ms]
May  4 13:01:26.462: INFO: Created: latency-svc-xvlws
May  4 13:01:26.471: INFO: Got endpoints: latency-svc-xvlws [132.076736ms]
May  4 13:01:26.498: INFO: Created: latency-svc-n9xzb
May  4 13:01:26.537: INFO: Got endpoints: latency-svc-n9xzb [194.438083ms]
May  4 13:01:26.554: INFO: Created: latency-svc-wfjfr
May  4 13:01:26.582: INFO: Got endpoints: latency-svc-wfjfr [241.005538ms]
May  4 13:01:26.609: INFO: Created: latency-svc-47shl
May  4 13:01:26.633: INFO: Created: latency-svc-nbqw2
May  4 13:01:26.636: INFO: Got endpoints: latency-svc-47shl [294.809014ms]
May  4 13:01:26.657: INFO: Got endpoints: latency-svc-nbqw2 [316.220494ms]
May  4 13:01:26.680: INFO: Created: latency-svc-bqhhg
May  4 13:01:26.705: INFO: Got endpoints: latency-svc-bqhhg [363.384715ms]
May  4 13:01:26.715: INFO: Created: latency-svc-8n45b
May  4 13:01:26.739: INFO: Got endpoints: latency-svc-8n45b [399.18223ms]
May  4 13:01:26.743: INFO: Created: latency-svc-5zx96
May  4 13:01:26.802: INFO: Created: latency-svc-mc79d
May  4 13:01:26.813: INFO: Got endpoints: latency-svc-5zx96 [472.431154ms]
May  4 13:01:26.828: INFO: Got endpoints: latency-svc-mc79d [488.164039ms]
May  4 13:01:26.846: INFO: Created: latency-svc-4z2br
May  4 13:01:26.855: INFO: Got endpoints: latency-svc-4z2br [512.910103ms]
May  4 13:01:26.880: INFO: Created: latency-svc-kdd7g
May  4 13:01:26.883: INFO: Got endpoints: latency-svc-kdd7g [542.238995ms]
May  4 13:01:26.917: INFO: Created: latency-svc-vhwp5
May  4 13:01:26.926: INFO: Got endpoints: latency-svc-vhwp5 [583.829386ms]
May  4 13:01:26.945: INFO: Created: latency-svc-hlvn5
May  4 13:01:26.962: INFO: Got endpoints: latency-svc-hlvn5 [620.366816ms]
May  4 13:01:26.992: INFO: Created: latency-svc-hwtr8
May  4 13:01:26.992: INFO: Got endpoints: latency-svc-hwtr8 [595.448963ms]
May  4 13:01:27.021: INFO: Created: latency-svc-pndqm
May  4 13:01:27.031: INFO: Got endpoints: latency-svc-pndqm [586.933189ms]
May  4 13:01:27.051: INFO: Created: latency-svc-d5bwr
May  4 13:01:27.063: INFO: Got endpoints: latency-svc-d5bwr [591.456946ms]
May  4 13:01:27.068: INFO: Created: latency-svc-wl929
May  4 13:01:27.087: INFO: Got endpoints: latency-svc-wl929 [550.169376ms]
May  4 13:01:27.108: INFO: Created: latency-svc-kljsk
May  4 13:01:27.109: INFO: Got endpoints: latency-svc-kljsk [526.734619ms]
May  4 13:01:27.136: INFO: Created: latency-svc-t5n9x
May  4 13:01:27.146: INFO: Got endpoints: latency-svc-t5n9x [509.541897ms]
May  4 13:01:27.165: INFO: Created: latency-svc-vgxgt
May  4 13:01:27.176: INFO: Created: latency-svc-q7g7t
May  4 13:01:27.177: INFO: Got endpoints: latency-svc-vgxgt [519.750886ms]
May  4 13:01:27.185: INFO: Got endpoints: latency-svc-q7g7t [479.632885ms]
May  4 13:01:27.194: INFO: Created: latency-svc-9jf2p
May  4 13:01:27.201: INFO: Got endpoints: latency-svc-9jf2p [461.876728ms]
May  4 13:01:27.210: INFO: Created: latency-svc-k6w6v
May  4 13:01:27.220: INFO: Got endpoints: latency-svc-k6w6v [406.876039ms]
May  4 13:01:27.229: INFO: Created: latency-svc-twd9t
May  4 13:01:27.274: INFO: Got endpoints: latency-svc-twd9t [446.307147ms]
May  4 13:01:27.287: INFO: Created: latency-svc-vv59v
May  4 13:01:27.293: INFO: Got endpoints: latency-svc-vv59v [438.446654ms]
May  4 13:01:27.306: INFO: Created: latency-svc-str8j
May  4 13:01:27.319: INFO: Got endpoints: latency-svc-str8j [436.124144ms]
May  4 13:01:27.326: INFO: Created: latency-svc-wnrfm
May  4 13:01:27.377: INFO: Got endpoints: latency-svc-wnrfm [451.014749ms]
May  4 13:01:27.385: INFO: Created: latency-svc-wb656
May  4 13:01:27.390: INFO: Got endpoints: latency-svc-wb656 [427.897746ms]
May  4 13:01:27.400: INFO: Created: latency-svc-mtfdc
May  4 13:01:27.422: INFO: Got endpoints: latency-svc-mtfdc [430.677983ms]
May  4 13:01:27.429: INFO: Created: latency-svc-ncs78
May  4 13:01:27.446: INFO: Got endpoints: latency-svc-ncs78 [414.826233ms]
May  4 13:01:27.453: INFO: Created: latency-svc-qp94n
May  4 13:01:27.462: INFO: Got endpoints: latency-svc-qp94n [398.97382ms]
May  4 13:01:27.464: INFO: Created: latency-svc-6n7gz
May  4 13:01:27.475: INFO: Got endpoints: latency-svc-6n7gz [388.473552ms]
May  4 13:01:27.486: INFO: Created: latency-svc-5cbdl
May  4 13:01:27.498: INFO: Got endpoints: latency-svc-5cbdl [389.079787ms]
May  4 13:01:27.504: INFO: Created: latency-svc-lz82p
May  4 13:01:27.514: INFO: Got endpoints: latency-svc-lz82p [367.442044ms]
May  4 13:01:27.516: INFO: Created: latency-svc-z6rhb
May  4 13:01:27.523: INFO: Got endpoints: latency-svc-z6rhb [345.663512ms]
May  4 13:01:27.533: INFO: Created: latency-svc-qd8d8
May  4 13:01:27.544: INFO: Got endpoints: latency-svc-qd8d8 [358.5729ms]
May  4 13:01:27.547: INFO: Created: latency-svc-5xwjw
May  4 13:01:27.558: INFO: Got endpoints: latency-svc-5xwjw [356.508269ms]
May  4 13:01:27.568: INFO: Created: latency-svc-mmw42
May  4 13:01:27.585: INFO: Got endpoints: latency-svc-mmw42 [364.536157ms]
May  4 13:01:27.589: INFO: Created: latency-svc-d4kbp
May  4 13:01:27.595: INFO: Got endpoints: latency-svc-d4kbp [320.735012ms]
May  4 13:01:27.604: INFO: Created: latency-svc-cs92b
May  4 13:01:27.613: INFO: Got endpoints: latency-svc-cs92b [319.537034ms]
May  4 13:01:27.621: INFO: Created: latency-svc-7vkzk
May  4 13:01:27.636: INFO: Got endpoints: latency-svc-7vkzk [316.633159ms]
May  4 13:01:27.641: INFO: Created: latency-svc-rxjn8
May  4 13:01:27.651: INFO: Got endpoints: latency-svc-rxjn8 [274.070714ms]
May  4 13:01:27.658: INFO: Created: latency-svc-hdxwl
May  4 13:01:27.664: INFO: Got endpoints: latency-svc-hdxwl [273.680122ms]
May  4 13:01:27.671: INFO: Created: latency-svc-rnk2f
May  4 13:01:27.680: INFO: Got endpoints: latency-svc-rnk2f [257.90091ms]
May  4 13:01:27.687: INFO: Created: latency-svc-bhsjm
May  4 13:01:27.693: INFO: Got endpoints: latency-svc-bhsjm [246.936061ms]
May  4 13:01:27.697: INFO: Created: latency-svc-857hc
May  4 13:01:27.706: INFO: Got endpoints: latency-svc-857hc [244.132714ms]
May  4 13:01:27.712: INFO: Created: latency-svc-qd44v
May  4 13:01:27.729: INFO: Created: latency-svc-qcdmh
May  4 13:01:27.731: INFO: Got endpoints: latency-svc-qd44v [255.096917ms]
May  4 13:01:27.751: INFO: Got endpoints: latency-svc-qcdmh [252.919838ms]
May  4 13:01:27.758: INFO: Created: latency-svc-dpsj5
May  4 13:01:27.761: INFO: Got endpoints: latency-svc-dpsj5 [247.480537ms]
May  4 13:01:27.771: INFO: Created: latency-svc-hvrzm
May  4 13:01:27.779: INFO: Got endpoints: latency-svc-hvrzm [255.628241ms]
May  4 13:01:27.789: INFO: Created: latency-svc-7n549
May  4 13:01:27.795: INFO: Got endpoints: latency-svc-7n549 [251.115477ms]
May  4 13:01:27.830: INFO: Created: latency-svc-2g5m9
May  4 13:01:27.830: INFO: Created: latency-svc-5bbs8
May  4 13:01:27.833: INFO: Got endpoints: latency-svc-2g5m9 [38.683931ms]
May  4 13:01:27.837: INFO: Got endpoints: latency-svc-5bbs8 [278.695817ms]
May  4 13:01:27.856: INFO: Created: latency-svc-kjjkh
May  4 13:01:27.860: INFO: Got endpoints: latency-svc-kjjkh [265.232269ms]
May  4 13:01:27.870: INFO: Created: latency-svc-5tv8h
May  4 13:01:27.883: INFO: Got endpoints: latency-svc-5tv8h [269.933972ms]
May  4 13:01:27.886: INFO: Created: latency-svc-jhb5q
May  4 13:01:27.893: INFO: Got endpoints: latency-svc-jhb5q [256.89034ms]
May  4 13:01:27.897: INFO: Created: latency-svc-s7t6d
May  4 13:01:27.905: INFO: Got endpoints: latency-svc-s7t6d [253.82376ms]
May  4 13:01:27.912: INFO: Created: latency-svc-zbs8p
May  4 13:01:27.955: INFO: Got endpoints: latency-svc-zbs8p [290.991343ms]
May  4 13:01:27.961: INFO: Created: latency-svc-lq2vh
May  4 13:01:27.968: INFO: Got endpoints: latency-svc-lq2vh [282.772466ms]
May  4 13:01:27.975: INFO: Created: latency-svc-4rvmc
May  4 13:01:27.985: INFO: Created: latency-svc-b7hcm
May  4 13:01:28.024: INFO: Got endpoints: latency-svc-4rvmc [331.471804ms]
May  4 13:01:28.030: INFO: Created: latency-svc-g4qkt
May  4 13:01:28.039: INFO: Got endpoints: latency-svc-b7hcm [332.883796ms]
May  4 13:01:28.041: INFO: Created: latency-svc-fp47x
May  4 13:01:28.055: INFO: Created: latency-svc-m5ld5
May  4 13:01:28.066: INFO: Created: latency-svc-nx28l
May  4 13:01:28.077: INFO: Created: latency-svc-4j97g
May  4 13:01:28.092: INFO: Got endpoints: latency-svc-g4qkt [361.090618ms]
May  4 13:01:28.097: INFO: Created: latency-svc-2qdjh
May  4 13:01:28.102: INFO: Created: latency-svc-rnstk
May  4 13:01:28.115: INFO: Created: latency-svc-q6rdt
May  4 13:01:28.142: INFO: Got endpoints: latency-svc-fp47x [391.49013ms]
May  4 13:01:28.147: INFO: Created: latency-svc-qz5zc
May  4 13:01:28.156: INFO: Created: latency-svc-cjr4m
May  4 13:01:28.167: INFO: Created: latency-svc-vtjk5
May  4 13:01:28.178: INFO: Created: latency-svc-bcnkm
May  4 13:01:28.198: INFO: Got endpoints: latency-svc-m5ld5 [436.506889ms]
May  4 13:01:28.198: INFO: Created: latency-svc-g8857
May  4 13:01:28.208: INFO: Created: latency-svc-vxxwr
May  4 13:01:28.218: INFO: Created: latency-svc-brsf2
May  4 13:01:28.230: INFO: Created: latency-svc-vl6wc
May  4 13:01:28.245: INFO: Got endpoints: latency-svc-nx28l [465.323112ms]
May  4 13:01:28.245: INFO: Created: latency-svc-b44fl
May  4 13:01:28.255: INFO: Created: latency-svc-mjbxs
May  4 13:01:28.268: INFO: Created: latency-svc-ggng5
May  4 13:01:28.291: INFO: Got endpoints: latency-svc-4j97g [705.738882ms]
May  4 13:01:28.309: INFO: Created: latency-svc-v9jl9
May  4 13:01:28.339: INFO: Got endpoints: latency-svc-2qdjh [504.863596ms]
May  4 13:01:28.354: INFO: Created: latency-svc-cf8r7
May  4 13:01:28.389: INFO: Got endpoints: latency-svc-rnstk [551.784785ms]
May  4 13:01:28.403: INFO: Created: latency-svc-vpqmz
May  4 13:01:28.440: INFO: Got endpoints: latency-svc-q6rdt [579.551659ms]
May  4 13:01:28.455: INFO: Created: latency-svc-m6c6k
May  4 13:01:28.491: INFO: Got endpoints: latency-svc-qz5zc [607.636708ms]
May  4 13:01:28.505: INFO: Created: latency-svc-26mck
May  4 13:01:28.540: INFO: Got endpoints: latency-svc-cjr4m [647.219279ms]
May  4 13:01:28.555: INFO: Created: latency-svc-kj9wk
May  4 13:01:28.589: INFO: Got endpoints: latency-svc-vtjk5 [684.83574ms]
May  4 13:01:28.607: INFO: Created: latency-svc-6vl8p
May  4 13:01:28.640: INFO: Got endpoints: latency-svc-bcnkm [685.508582ms]
May  4 13:01:28.654: INFO: Created: latency-svc-7nr7d
May  4 13:01:28.690: INFO: Got endpoints: latency-svc-g8857 [722.414964ms]
May  4 13:01:28.704: INFO: Created: latency-svc-wg8sx
May  4 13:01:28.739: INFO: Got endpoints: latency-svc-vxxwr [714.971144ms]
May  4 13:01:28.754: INFO: Created: latency-svc-8sp4m
May  4 13:01:28.792: INFO: Got endpoints: latency-svc-brsf2 [753.139198ms]
May  4 13:01:28.808: INFO: Created: latency-svc-98k99
May  4 13:01:28.839: INFO: Got endpoints: latency-svc-vl6wc [747.396209ms]
May  4 13:01:28.855: INFO: Created: latency-svc-wndsj
May  4 13:01:28.893: INFO: Got endpoints: latency-svc-b44fl [750.499557ms]
May  4 13:01:28.910: INFO: Created: latency-svc-8nfp9
May  4 13:01:28.940: INFO: Got endpoints: latency-svc-mjbxs [742.629939ms]
May  4 13:01:28.964: INFO: Created: latency-svc-mb6nv
May  4 13:01:28.997: INFO: Got endpoints: latency-svc-ggng5 [752.766969ms]
May  4 13:01:29.017: INFO: Created: latency-svc-9bjkm
May  4 13:01:29.044: INFO: Got endpoints: latency-svc-v9jl9 [753.632235ms]
May  4 13:01:29.095: INFO: Got endpoints: latency-svc-cf8r7 [756.209889ms]
May  4 13:01:29.100: INFO: Created: latency-svc-82ktq
May  4 13:01:29.119: INFO: Created: latency-svc-vjzkf
May  4 13:01:29.140: INFO: Got endpoints: latency-svc-vpqmz [750.999809ms]
May  4 13:01:29.198: INFO: Created: latency-svc-ncxm6
May  4 13:01:29.201: INFO: Got endpoints: latency-svc-m6c6k [760.423743ms]
May  4 13:01:29.221: INFO: Created: latency-svc-2n8b5
May  4 13:01:29.239: INFO: Got endpoints: latency-svc-26mck [748.133537ms]
May  4 13:01:29.259: INFO: Created: latency-svc-dljdl
May  4 13:01:29.294: INFO: Got endpoints: latency-svc-kj9wk [753.994382ms]
May  4 13:01:29.310: INFO: Created: latency-svc-kcbjq
May  4 13:01:29.340: INFO: Got endpoints: latency-svc-6vl8p [750.725465ms]
May  4 13:01:29.359: INFO: Created: latency-svc-77j6z
May  4 13:01:29.393: INFO: Got endpoints: latency-svc-7nr7d [752.30621ms]
May  4 13:01:29.415: INFO: Created: latency-svc-cp945
May  4 13:01:29.443: INFO: Got endpoints: latency-svc-wg8sx [752.023832ms]
May  4 13:01:29.463: INFO: Created: latency-svc-gb6s8
May  4 13:01:29.502: INFO: Got endpoints: latency-svc-8sp4m [762.335184ms]
May  4 13:01:29.531: INFO: Created: latency-svc-9rhg6
May  4 13:01:29.539: INFO: Got endpoints: latency-svc-98k99 [747.448365ms]
May  4 13:01:29.560: INFO: Created: latency-svc-nztps
May  4 13:01:29.591: INFO: Got endpoints: latency-svc-wndsj [751.961197ms]
May  4 13:01:29.607: INFO: Created: latency-svc-nlhld
May  4 13:01:29.640: INFO: Got endpoints: latency-svc-8nfp9 [747.001462ms]
May  4 13:01:29.656: INFO: Created: latency-svc-xzdc6
May  4 13:01:29.694: INFO: Got endpoints: latency-svc-mb6nv [752.934876ms]
May  4 13:01:29.717: INFO: Created: latency-svc-547p4
May  4 13:01:29.740: INFO: Got endpoints: latency-svc-9bjkm [742.583581ms]
May  4 13:01:29.761: INFO: Created: latency-svc-bvvsq
May  4 13:01:29.791: INFO: Got endpoints: latency-svc-82ktq [746.214636ms]
May  4 13:01:29.824: INFO: Created: latency-svc-vhzrg
May  4 13:01:29.842: INFO: Got endpoints: latency-svc-vjzkf [747.208872ms]
May  4 13:01:29.867: INFO: Created: latency-svc-bltcb
May  4 13:01:29.893: INFO: Got endpoints: latency-svc-ncxm6 [752.959629ms]
May  4 13:01:29.911: INFO: Created: latency-svc-5kcth
May  4 13:01:29.939: INFO: Got endpoints: latency-svc-2n8b5 [737.970462ms]
May  4 13:01:29.957: INFO: Created: latency-svc-wcg4r
May  4 13:01:29.988: INFO: Got endpoints: latency-svc-dljdl [748.346414ms]
May  4 13:01:30.005: INFO: Created: latency-svc-kkrlb
May  4 13:01:30.039: INFO: Got endpoints: latency-svc-kcbjq [744.033481ms]
May  4 13:01:30.058: INFO: Created: latency-svc-529j2
May  4 13:01:30.098: INFO: Got endpoints: latency-svc-77j6z [757.900881ms]
May  4 13:01:30.117: INFO: Created: latency-svc-qtgpv
May  4 13:01:30.140: INFO: Got endpoints: latency-svc-cp945 [747.339382ms]
May  4 13:01:30.155: INFO: Created: latency-svc-hqh2s
May  4 13:01:30.189: INFO: Got endpoints: latency-svc-gb6s8 [746.698334ms]
May  4 13:01:30.206: INFO: Created: latency-svc-grr7n
May  4 13:01:30.241: INFO: Got endpoints: latency-svc-9rhg6 [739.156359ms]
May  4 13:01:30.257: INFO: Created: latency-svc-9gwqw
May  4 13:01:30.290: INFO: Got endpoints: latency-svc-nztps [750.709052ms]
May  4 13:01:30.314: INFO: Created: latency-svc-w4nmr
May  4 13:01:30.342: INFO: Got endpoints: latency-svc-nlhld [751.026228ms]
May  4 13:01:30.359: INFO: Created: latency-svc-4v4zm
May  4 13:01:30.392: INFO: Got endpoints: latency-svc-xzdc6 [751.574594ms]
May  4 13:01:30.409: INFO: Created: latency-svc-jv8cx
May  4 13:01:30.441: INFO: Got endpoints: latency-svc-547p4 [747.459815ms]
May  4 13:01:30.461: INFO: Created: latency-svc-jx6gv
May  4 13:01:30.490: INFO: Got endpoints: latency-svc-bvvsq [749.741295ms]
May  4 13:01:30.506: INFO: Created: latency-svc-q8zt9
May  4 13:01:30.542: INFO: Got endpoints: latency-svc-vhzrg [751.179353ms]
May  4 13:01:30.560: INFO: Created: latency-svc-x5g92
May  4 13:01:30.591: INFO: Got endpoints: latency-svc-bltcb [748.873684ms]
May  4 13:01:30.605: INFO: Created: latency-svc-pthsv
May  4 13:01:30.643: INFO: Got endpoints: latency-svc-5kcth [750.174059ms]
May  4 13:01:30.701: INFO: Got endpoints: latency-svc-wcg4r [762.690704ms]
May  4 13:01:30.709: INFO: Created: latency-svc-rs9cb
May  4 13:01:30.718: INFO: Created: latency-svc-kzp76
May  4 13:01:30.743: INFO: Got endpoints: latency-svc-kkrlb [755.560677ms]
May  4 13:01:30.763: INFO: Created: latency-svc-7d6vt
May  4 13:01:30.791: INFO: Got endpoints: latency-svc-529j2 [752.767995ms]
May  4 13:01:30.807: INFO: Created: latency-svc-2vvsc
May  4 13:01:30.841: INFO: Got endpoints: latency-svc-qtgpv [742.417187ms]
May  4 13:01:30.856: INFO: Created: latency-svc-jwjd7
May  4 13:01:30.890: INFO: Got endpoints: latency-svc-hqh2s [750.055571ms]
May  4 13:01:30.905: INFO: Created: latency-svc-r4ttj
May  4 13:01:30.939: INFO: Got endpoints: latency-svc-grr7n [749.636328ms]
May  4 13:01:30.960: INFO: Created: latency-svc-x97m7
May  4 13:01:30.988: INFO: Got endpoints: latency-svc-9gwqw [747.57659ms]
May  4 13:01:31.003: INFO: Created: latency-svc-zsz6h
May  4 13:01:31.040: INFO: Got endpoints: latency-svc-w4nmr [749.51262ms]
May  4 13:01:31.062: INFO: Created: latency-svc-9j64b
May  4 13:01:31.093: INFO: Got endpoints: latency-svc-4v4zm [750.327687ms]
May  4 13:01:31.113: INFO: Created: latency-svc-xhmdt
May  4 13:01:31.141: INFO: Got endpoints: latency-svc-jv8cx [749.078682ms]
May  4 13:01:31.158: INFO: Created: latency-svc-cpsjh
May  4 13:01:31.192: INFO: Got endpoints: latency-svc-jx6gv [751.169729ms]
May  4 13:01:31.219: INFO: Created: latency-svc-hr7vs
May  4 13:01:31.242: INFO: Got endpoints: latency-svc-q8zt9 [751.840138ms]
May  4 13:01:31.260: INFO: Created: latency-svc-c62kk
May  4 13:01:31.291: INFO: Got endpoints: latency-svc-x5g92 [749.071379ms]
May  4 13:01:31.311: INFO: Created: latency-svc-bslwc
May  4 13:01:31.341: INFO: Got endpoints: latency-svc-pthsv [749.43214ms]
May  4 13:01:31.360: INFO: Created: latency-svc-22gpb
May  4 13:01:31.391: INFO: Got endpoints: latency-svc-rs9cb [747.612134ms]
May  4 13:01:31.407: INFO: Created: latency-svc-qjmnw
May  4 13:01:31.441: INFO: Got endpoints: latency-svc-kzp76 [739.028824ms]
May  4 13:01:31.456: INFO: Created: latency-svc-9lznq
May  4 13:01:31.494: INFO: Got endpoints: latency-svc-7d6vt [750.371569ms]
May  4 13:01:31.512: INFO: Created: latency-svc-rbrfk
May  4 13:01:31.541: INFO: Got endpoints: latency-svc-2vvsc [749.985115ms]
May  4 13:01:31.560: INFO: Created: latency-svc-vgv6s
May  4 13:01:31.591: INFO: Got endpoints: latency-svc-jwjd7 [750.594757ms]
May  4 13:01:31.607: INFO: Created: latency-svc-z9kp5
May  4 13:01:31.641: INFO: Got endpoints: latency-svc-r4ttj [750.859199ms]
May  4 13:01:31.666: INFO: Created: latency-svc-dqhwh
May  4 13:01:31.690: INFO: Got endpoints: latency-svc-x97m7 [750.41195ms]
May  4 13:01:31.711: INFO: Created: latency-svc-4wtlp
May  4 13:01:31.745: INFO: Got endpoints: latency-svc-zsz6h [756.418262ms]
May  4 13:01:31.764: INFO: Created: latency-svc-5sbwd
May  4 13:01:31.789: INFO: Got endpoints: latency-svc-9j64b [749.69456ms]
May  4 13:01:31.807: INFO: Created: latency-svc-k4r9s
May  4 13:01:31.851: INFO: Got endpoints: latency-svc-xhmdt [757.855217ms]
May  4 13:01:31.873: INFO: Created: latency-svc-cc5wv
May  4 13:01:31.891: INFO: Got endpoints: latency-svc-cpsjh [749.680903ms]
May  4 13:01:31.916: INFO: Created: latency-svc-2r5bk
May  4 13:01:31.942: INFO: Got endpoints: latency-svc-hr7vs [749.646704ms]
May  4 13:01:31.963: INFO: Created: latency-svc-zs4mh
May  4 13:01:31.994: INFO: Got endpoints: latency-svc-c62kk [752.27664ms]
May  4 13:01:32.028: INFO: Created: latency-svc-6rw7q
May  4 13:01:32.040: INFO: Got endpoints: latency-svc-bslwc [749.313035ms]
May  4 13:01:32.060: INFO: Created: latency-svc-x466g
May  4 13:01:32.093: INFO: Got endpoints: latency-svc-22gpb [751.748276ms]
May  4 13:01:32.113: INFO: Created: latency-svc-vf8ns
May  4 13:01:32.139: INFO: Got endpoints: latency-svc-qjmnw [748.039219ms]
May  4 13:01:32.158: INFO: Created: latency-svc-prxjs
May  4 13:01:32.194: INFO: Got endpoints: latency-svc-9lznq [753.820543ms]
May  4 13:01:32.218: INFO: Created: latency-svc-rthrh
May  4 13:01:32.245: INFO: Got endpoints: latency-svc-rbrfk [751.55958ms]
May  4 13:01:32.264: INFO: Created: latency-svc-kzwqv
May  4 13:01:32.301: INFO: Got endpoints: latency-svc-vgv6s [759.627302ms]
May  4 13:01:32.325: INFO: Created: latency-svc-thw2k
May  4 13:01:32.339: INFO: Got endpoints: latency-svc-z9kp5 [747.576868ms]
May  4 13:01:32.391: INFO: Got endpoints: latency-svc-dqhwh [750.17307ms]
May  4 13:01:32.396: INFO: Created: latency-svc-bczg8
May  4 13:01:32.413: INFO: Created: latency-svc-hzs4g
May  4 13:01:32.442: INFO: Got endpoints: latency-svc-4wtlp [752.120759ms]
May  4 13:01:32.477: INFO: Created: latency-svc-nf8n9
May  4 13:01:32.492: INFO: Got endpoints: latency-svc-5sbwd [747.170797ms]
May  4 13:01:32.516: INFO: Created: latency-svc-5r57f
May  4 13:01:32.539: INFO: Got endpoints: latency-svc-k4r9s [749.298903ms]
May  4 13:01:32.556: INFO: Created: latency-svc-gxnfz
May  4 13:01:32.593: INFO: Got endpoints: latency-svc-cc5wv [742.782212ms]
May  4 13:01:32.617: INFO: Created: latency-svc-qhsrq
May  4 13:01:32.645: INFO: Got endpoints: latency-svc-2r5bk [754.130241ms]
May  4 13:01:32.672: INFO: Created: latency-svc-8ggdh
May  4 13:01:32.696: INFO: Got endpoints: latency-svc-zs4mh [754.263514ms]
May  4 13:01:32.732: INFO: Created: latency-svc-pkg66
May  4 13:01:32.742: INFO: Got endpoints: latency-svc-6rw7q [747.656031ms]
May  4 13:01:32.762: INFO: Created: latency-svc-srtsc
May  4 13:01:32.790: INFO: Got endpoints: latency-svc-x466g [749.178311ms]
May  4 13:01:32.807: INFO: Created: latency-svc-8qhvs
May  4 13:01:32.844: INFO: Got endpoints: latency-svc-vf8ns [750.968486ms]
May  4 13:01:32.864: INFO: Created: latency-svc-lm2dx
May  4 13:01:32.893: INFO: Got endpoints: latency-svc-prxjs [754.406028ms]
May  4 13:01:32.909: INFO: Created: latency-svc-6lqnx
May  4 13:01:32.944: INFO: Got endpoints: latency-svc-rthrh [749.520625ms]
May  4 13:01:32.974: INFO: Created: latency-svc-tf7cp
May  4 13:01:32.991: INFO: Got endpoints: latency-svc-kzwqv [745.929516ms]
May  4 13:01:33.008: INFO: Created: latency-svc-tgf52
May  4 13:01:33.039: INFO: Got endpoints: latency-svc-thw2k [737.85198ms]
May  4 13:01:33.068: INFO: Created: latency-svc-4kl2w
May  4 13:01:33.093: INFO: Got endpoints: latency-svc-bczg8 [754.141123ms]
May  4 13:01:33.111: INFO: Created: latency-svc-z92zq
May  4 13:01:33.143: INFO: Got endpoints: latency-svc-hzs4g [751.849374ms]
May  4 13:01:33.163: INFO: Created: latency-svc-qh64n
May  4 13:01:33.191: INFO: Got endpoints: latency-svc-nf8n9 [749.513215ms]
May  4 13:01:33.211: INFO: Created: latency-svc-8w72x
May  4 13:01:33.239: INFO: Got endpoints: latency-svc-5r57f [747.080217ms]
May  4 13:01:33.264: INFO: Created: latency-svc-7m5jf
May  4 13:01:33.289: INFO: Got endpoints: latency-svc-gxnfz [750.572044ms]
May  4 13:01:33.305: INFO: Created: latency-svc-fgb2d
May  4 13:01:33.342: INFO: Got endpoints: latency-svc-qhsrq [747.731252ms]
May  4 13:01:33.359: INFO: Created: latency-svc-d56g5
May  4 13:01:33.391: INFO: Got endpoints: latency-svc-8ggdh [746.227618ms]
May  4 13:01:33.410: INFO: Created: latency-svc-n2c2b
May  4 13:01:33.442: INFO: Got endpoints: latency-svc-pkg66 [745.82935ms]
May  4 13:01:33.457: INFO: Created: latency-svc-2c5rn
May  4 13:01:33.490: INFO: Got endpoints: latency-svc-srtsc [747.950664ms]
May  4 13:01:33.509: INFO: Created: latency-svc-vt9z4
May  4 13:01:33.542: INFO: Got endpoints: latency-svc-8qhvs [752.160477ms]
May  4 13:01:33.560: INFO: Created: latency-svc-vnl6w
May  4 13:01:33.591: INFO: Got endpoints: latency-svc-lm2dx [746.914871ms]
May  4 13:01:33.618: INFO: Created: latency-svc-5hdlk
May  4 13:01:33.643: INFO: Got endpoints: latency-svc-6lqnx [749.755824ms]
May  4 13:01:33.669: INFO: Created: latency-svc-dhv98
May  4 13:01:33.691: INFO: Got endpoints: latency-svc-tf7cp [746.831476ms]
May  4 13:01:33.713: INFO: Created: latency-svc-t5rdg
May  4 13:01:33.741: INFO: Got endpoints: latency-svc-tgf52 [749.398221ms]
May  4 13:01:33.766: INFO: Created: latency-svc-gn4nm
May  4 13:01:33.790: INFO: Got endpoints: latency-svc-4kl2w [750.872811ms]
May  4 13:01:33.814: INFO: Created: latency-svc-mhz8v
May  4 13:01:33.844: INFO: Got endpoints: latency-svc-z92zq [750.412467ms]
May  4 13:01:33.864: INFO: Created: latency-svc-w6v6k
May  4 13:01:33.893: INFO: Got endpoints: latency-svc-qh64n [749.682805ms]
May  4 13:01:33.922: INFO: Created: latency-svc-r5498
May  4 13:01:33.942: INFO: Got endpoints: latency-svc-8w72x [750.100001ms]
May  4 13:01:33.959: INFO: Created: latency-svc-2x8pn
May  4 13:01:33.993: INFO: Got endpoints: latency-svc-7m5jf [752.997603ms]
May  4 13:01:34.011: INFO: Created: latency-svc-lcdk6
May  4 13:01:34.070: INFO: Got endpoints: latency-svc-fgb2d [780.151537ms]
May  4 13:01:34.099: INFO: Got endpoints: latency-svc-d56g5 [756.984912ms]
May  4 13:01:34.100: INFO: Created: latency-svc-pzdj7
May  4 13:01:34.115: INFO: Created: latency-svc-r7fcz
May  4 13:01:34.149: INFO: Got endpoints: latency-svc-n2c2b [758.037763ms]
May  4 13:01:34.207: INFO: Got endpoints: latency-svc-2c5rn [764.637452ms]
May  4 13:01:34.210: INFO: Created: latency-svc-cdcrb
May  4 13:01:34.242: INFO: Got endpoints: latency-svc-vt9z4 [752.021814ms]
May  4 13:01:34.293: INFO: Got endpoints: latency-svc-vnl6w [751.508233ms]
May  4 13:01:34.342: INFO: Got endpoints: latency-svc-5hdlk [751.354188ms]
May  4 13:01:34.392: INFO: Got endpoints: latency-svc-dhv98 [748.601425ms]
May  4 13:01:34.442: INFO: Got endpoints: latency-svc-t5rdg [750.823195ms]
May  4 13:01:34.490: INFO: Got endpoints: latency-svc-gn4nm [749.089946ms]
May  4 13:01:34.543: INFO: Got endpoints: latency-svc-mhz8v [753.118006ms]
May  4 13:01:34.589: INFO: Got endpoints: latency-svc-w6v6k [745.285741ms]
May  4 13:01:34.642: INFO: Got endpoints: latency-svc-r5498 [748.491581ms]
May  4 13:01:34.690: INFO: Got endpoints: latency-svc-2x8pn [748.60672ms]
May  4 13:01:34.744: INFO: Got endpoints: latency-svc-lcdk6 [751.559278ms]
May  4 13:01:34.792: INFO: Got endpoints: latency-svc-pzdj7 [721.918033ms]
May  4 13:01:34.841: INFO: Got endpoints: latency-svc-r7fcz [742.645304ms]
May  4 13:01:34.891: INFO: Got endpoints: latency-svc-cdcrb [741.603322ms]
May  4 13:01:34.891: INFO: Latencies: [38.683931ms 57.474524ms 104.185244ms 132.076736ms 194.438083ms 241.005538ms 244.132714ms 246.936061ms 247.480537ms 251.115477ms 252.919838ms 253.82376ms 255.096917ms 255.628241ms 256.89034ms 257.90091ms 265.232269ms 269.933972ms 273.680122ms 274.070714ms 278.695817ms 282.772466ms 290.991343ms 294.809014ms 316.220494ms 316.633159ms 319.537034ms 320.735012ms 331.471804ms 332.883796ms 345.663512ms 356.508269ms 358.5729ms 361.090618ms 363.384715ms 364.536157ms 367.442044ms 388.473552ms 389.079787ms 391.49013ms 398.97382ms 399.18223ms 406.876039ms 414.826233ms 427.897746ms 430.677983ms 436.124144ms 436.506889ms 438.446654ms 446.307147ms 451.014749ms 461.876728ms 465.323112ms 472.431154ms 479.632885ms 488.164039ms 504.863596ms 509.541897ms 512.910103ms 519.750886ms 526.734619ms 542.238995ms 550.169376ms 551.784785ms 579.551659ms 583.829386ms 586.933189ms 591.456946ms 595.448963ms 607.636708ms 620.366816ms 647.219279ms 684.83574ms 685.508582ms 705.738882ms 714.971144ms 721.918033ms 722.414964ms 737.85198ms 737.970462ms 739.028824ms 739.156359ms 741.603322ms 742.417187ms 742.583581ms 742.629939ms 742.645304ms 742.782212ms 744.033481ms 745.285741ms 745.82935ms 745.929516ms 746.214636ms 746.227618ms 746.698334ms 746.831476ms 746.914871ms 747.001462ms 747.080217ms 747.170797ms 747.208872ms 747.339382ms 747.396209ms 747.448365ms 747.459815ms 747.57659ms 747.576868ms 747.612134ms 747.656031ms 747.731252ms 747.950664ms 748.039219ms 748.133537ms 748.346414ms 748.491581ms 748.601425ms 748.60672ms 748.873684ms 749.071379ms 749.078682ms 749.089946ms 749.178311ms 749.298903ms 749.313035ms 749.398221ms 749.43214ms 749.51262ms 749.513215ms 749.520625ms 749.636328ms 749.646704ms 749.680903ms 749.682805ms 749.69456ms 749.741295ms 749.755824ms 749.985115ms 750.055571ms 750.100001ms 750.17307ms 750.174059ms 750.327687ms 750.371569ms 750.41195ms 750.412467ms 750.499557ms 750.572044ms 750.594757ms 750.709052ms 750.725465ms 750.823195ms 750.859199ms 750.872811ms 750.968486ms 750.999809ms 751.026228ms 751.169729ms 751.179353ms 751.354188ms 751.508233ms 751.559278ms 751.55958ms 751.574594ms 751.748276ms 751.840138ms 751.849374ms 751.961197ms 752.021814ms 752.023832ms 752.120759ms 752.160477ms 752.27664ms 752.30621ms 752.766969ms 752.767995ms 752.934876ms 752.959629ms 752.997603ms 753.118006ms 753.139198ms 753.632235ms 753.820543ms 753.994382ms 754.130241ms 754.141123ms 754.263514ms 754.406028ms 755.560677ms 756.209889ms 756.418262ms 756.984912ms 757.855217ms 757.900881ms 758.037763ms 759.627302ms 760.423743ms 762.335184ms 762.690704ms 764.637452ms 780.151537ms]
May  4 13:01:34.891: INFO: 50 %ile: 747.208872ms
May  4 13:01:34.891: INFO: 90 %ile: 753.632235ms
May  4 13:01:34.891: INFO: 99 %ile: 764.637452ms
May  4 13:01:34.891: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
May  4 13:01:34.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-4360" for this suite. 05/04/23 13:01:34.906
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","completed":270,"skipped":5038,"failed":0}
------------------------------
• [SLOW TEST] [10.833 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:01:24.084
    May  4 13:01:24.085: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename svc-latency 05/04/23 13:01:24.086
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:01:24.127
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:01:24.134
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    May  4 13:01:24.138: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-4360 05/04/23 13:01:24.138
    I0504 13:01:24.150082      21 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-4360, replica count: 1
    I0504 13:01:25.201489      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0504 13:01:26.202440      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  4 13:01:26.334: INFO: Created: latency-svc-cjfqn
    May  4 13:01:26.338: INFO: Got endpoints: latency-svc-cjfqn [35.474623ms]
    May  4 13:01:26.394: INFO: Created: latency-svc-fg2sm
    May  4 13:01:26.396: INFO: Got endpoints: latency-svc-fg2sm [57.474524ms]
    May  4 13:01:26.434: INFO: Created: latency-svc-ngghq
    May  4 13:01:26.444: INFO: Got endpoints: latency-svc-ngghq [104.185244ms]
    May  4 13:01:26.462: INFO: Created: latency-svc-xvlws
    May  4 13:01:26.471: INFO: Got endpoints: latency-svc-xvlws [132.076736ms]
    May  4 13:01:26.498: INFO: Created: latency-svc-n9xzb
    May  4 13:01:26.537: INFO: Got endpoints: latency-svc-n9xzb [194.438083ms]
    May  4 13:01:26.554: INFO: Created: latency-svc-wfjfr
    May  4 13:01:26.582: INFO: Got endpoints: latency-svc-wfjfr [241.005538ms]
    May  4 13:01:26.609: INFO: Created: latency-svc-47shl
    May  4 13:01:26.633: INFO: Created: latency-svc-nbqw2
    May  4 13:01:26.636: INFO: Got endpoints: latency-svc-47shl [294.809014ms]
    May  4 13:01:26.657: INFO: Got endpoints: latency-svc-nbqw2 [316.220494ms]
    May  4 13:01:26.680: INFO: Created: latency-svc-bqhhg
    May  4 13:01:26.705: INFO: Got endpoints: latency-svc-bqhhg [363.384715ms]
    May  4 13:01:26.715: INFO: Created: latency-svc-8n45b
    May  4 13:01:26.739: INFO: Got endpoints: latency-svc-8n45b [399.18223ms]
    May  4 13:01:26.743: INFO: Created: latency-svc-5zx96
    May  4 13:01:26.802: INFO: Created: latency-svc-mc79d
    May  4 13:01:26.813: INFO: Got endpoints: latency-svc-5zx96 [472.431154ms]
    May  4 13:01:26.828: INFO: Got endpoints: latency-svc-mc79d [488.164039ms]
    May  4 13:01:26.846: INFO: Created: latency-svc-4z2br
    May  4 13:01:26.855: INFO: Got endpoints: latency-svc-4z2br [512.910103ms]
    May  4 13:01:26.880: INFO: Created: latency-svc-kdd7g
    May  4 13:01:26.883: INFO: Got endpoints: latency-svc-kdd7g [542.238995ms]
    May  4 13:01:26.917: INFO: Created: latency-svc-vhwp5
    May  4 13:01:26.926: INFO: Got endpoints: latency-svc-vhwp5 [583.829386ms]
    May  4 13:01:26.945: INFO: Created: latency-svc-hlvn5
    May  4 13:01:26.962: INFO: Got endpoints: latency-svc-hlvn5 [620.366816ms]
    May  4 13:01:26.992: INFO: Created: latency-svc-hwtr8
    May  4 13:01:26.992: INFO: Got endpoints: latency-svc-hwtr8 [595.448963ms]
    May  4 13:01:27.021: INFO: Created: latency-svc-pndqm
    May  4 13:01:27.031: INFO: Got endpoints: latency-svc-pndqm [586.933189ms]
    May  4 13:01:27.051: INFO: Created: latency-svc-d5bwr
    May  4 13:01:27.063: INFO: Got endpoints: latency-svc-d5bwr [591.456946ms]
    May  4 13:01:27.068: INFO: Created: latency-svc-wl929
    May  4 13:01:27.087: INFO: Got endpoints: latency-svc-wl929 [550.169376ms]
    May  4 13:01:27.108: INFO: Created: latency-svc-kljsk
    May  4 13:01:27.109: INFO: Got endpoints: latency-svc-kljsk [526.734619ms]
    May  4 13:01:27.136: INFO: Created: latency-svc-t5n9x
    May  4 13:01:27.146: INFO: Got endpoints: latency-svc-t5n9x [509.541897ms]
    May  4 13:01:27.165: INFO: Created: latency-svc-vgxgt
    May  4 13:01:27.176: INFO: Created: latency-svc-q7g7t
    May  4 13:01:27.177: INFO: Got endpoints: latency-svc-vgxgt [519.750886ms]
    May  4 13:01:27.185: INFO: Got endpoints: latency-svc-q7g7t [479.632885ms]
    May  4 13:01:27.194: INFO: Created: latency-svc-9jf2p
    May  4 13:01:27.201: INFO: Got endpoints: latency-svc-9jf2p [461.876728ms]
    May  4 13:01:27.210: INFO: Created: latency-svc-k6w6v
    May  4 13:01:27.220: INFO: Got endpoints: latency-svc-k6w6v [406.876039ms]
    May  4 13:01:27.229: INFO: Created: latency-svc-twd9t
    May  4 13:01:27.274: INFO: Got endpoints: latency-svc-twd9t [446.307147ms]
    May  4 13:01:27.287: INFO: Created: latency-svc-vv59v
    May  4 13:01:27.293: INFO: Got endpoints: latency-svc-vv59v [438.446654ms]
    May  4 13:01:27.306: INFO: Created: latency-svc-str8j
    May  4 13:01:27.319: INFO: Got endpoints: latency-svc-str8j [436.124144ms]
    May  4 13:01:27.326: INFO: Created: latency-svc-wnrfm
    May  4 13:01:27.377: INFO: Got endpoints: latency-svc-wnrfm [451.014749ms]
    May  4 13:01:27.385: INFO: Created: latency-svc-wb656
    May  4 13:01:27.390: INFO: Got endpoints: latency-svc-wb656 [427.897746ms]
    May  4 13:01:27.400: INFO: Created: latency-svc-mtfdc
    May  4 13:01:27.422: INFO: Got endpoints: latency-svc-mtfdc [430.677983ms]
    May  4 13:01:27.429: INFO: Created: latency-svc-ncs78
    May  4 13:01:27.446: INFO: Got endpoints: latency-svc-ncs78 [414.826233ms]
    May  4 13:01:27.453: INFO: Created: latency-svc-qp94n
    May  4 13:01:27.462: INFO: Got endpoints: latency-svc-qp94n [398.97382ms]
    May  4 13:01:27.464: INFO: Created: latency-svc-6n7gz
    May  4 13:01:27.475: INFO: Got endpoints: latency-svc-6n7gz [388.473552ms]
    May  4 13:01:27.486: INFO: Created: latency-svc-5cbdl
    May  4 13:01:27.498: INFO: Got endpoints: latency-svc-5cbdl [389.079787ms]
    May  4 13:01:27.504: INFO: Created: latency-svc-lz82p
    May  4 13:01:27.514: INFO: Got endpoints: latency-svc-lz82p [367.442044ms]
    May  4 13:01:27.516: INFO: Created: latency-svc-z6rhb
    May  4 13:01:27.523: INFO: Got endpoints: latency-svc-z6rhb [345.663512ms]
    May  4 13:01:27.533: INFO: Created: latency-svc-qd8d8
    May  4 13:01:27.544: INFO: Got endpoints: latency-svc-qd8d8 [358.5729ms]
    May  4 13:01:27.547: INFO: Created: latency-svc-5xwjw
    May  4 13:01:27.558: INFO: Got endpoints: latency-svc-5xwjw [356.508269ms]
    May  4 13:01:27.568: INFO: Created: latency-svc-mmw42
    May  4 13:01:27.585: INFO: Got endpoints: latency-svc-mmw42 [364.536157ms]
    May  4 13:01:27.589: INFO: Created: latency-svc-d4kbp
    May  4 13:01:27.595: INFO: Got endpoints: latency-svc-d4kbp [320.735012ms]
    May  4 13:01:27.604: INFO: Created: latency-svc-cs92b
    May  4 13:01:27.613: INFO: Got endpoints: latency-svc-cs92b [319.537034ms]
    May  4 13:01:27.621: INFO: Created: latency-svc-7vkzk
    May  4 13:01:27.636: INFO: Got endpoints: latency-svc-7vkzk [316.633159ms]
    May  4 13:01:27.641: INFO: Created: latency-svc-rxjn8
    May  4 13:01:27.651: INFO: Got endpoints: latency-svc-rxjn8 [274.070714ms]
    May  4 13:01:27.658: INFO: Created: latency-svc-hdxwl
    May  4 13:01:27.664: INFO: Got endpoints: latency-svc-hdxwl [273.680122ms]
    May  4 13:01:27.671: INFO: Created: latency-svc-rnk2f
    May  4 13:01:27.680: INFO: Got endpoints: latency-svc-rnk2f [257.90091ms]
    May  4 13:01:27.687: INFO: Created: latency-svc-bhsjm
    May  4 13:01:27.693: INFO: Got endpoints: latency-svc-bhsjm [246.936061ms]
    May  4 13:01:27.697: INFO: Created: latency-svc-857hc
    May  4 13:01:27.706: INFO: Got endpoints: latency-svc-857hc [244.132714ms]
    May  4 13:01:27.712: INFO: Created: latency-svc-qd44v
    May  4 13:01:27.729: INFO: Created: latency-svc-qcdmh
    May  4 13:01:27.731: INFO: Got endpoints: latency-svc-qd44v [255.096917ms]
    May  4 13:01:27.751: INFO: Got endpoints: latency-svc-qcdmh [252.919838ms]
    May  4 13:01:27.758: INFO: Created: latency-svc-dpsj5
    May  4 13:01:27.761: INFO: Got endpoints: latency-svc-dpsj5 [247.480537ms]
    May  4 13:01:27.771: INFO: Created: latency-svc-hvrzm
    May  4 13:01:27.779: INFO: Got endpoints: latency-svc-hvrzm [255.628241ms]
    May  4 13:01:27.789: INFO: Created: latency-svc-7n549
    May  4 13:01:27.795: INFO: Got endpoints: latency-svc-7n549 [251.115477ms]
    May  4 13:01:27.830: INFO: Created: latency-svc-2g5m9
    May  4 13:01:27.830: INFO: Created: latency-svc-5bbs8
    May  4 13:01:27.833: INFO: Got endpoints: latency-svc-2g5m9 [38.683931ms]
    May  4 13:01:27.837: INFO: Got endpoints: latency-svc-5bbs8 [278.695817ms]
    May  4 13:01:27.856: INFO: Created: latency-svc-kjjkh
    May  4 13:01:27.860: INFO: Got endpoints: latency-svc-kjjkh [265.232269ms]
    May  4 13:01:27.870: INFO: Created: latency-svc-5tv8h
    May  4 13:01:27.883: INFO: Got endpoints: latency-svc-5tv8h [269.933972ms]
    May  4 13:01:27.886: INFO: Created: latency-svc-jhb5q
    May  4 13:01:27.893: INFO: Got endpoints: latency-svc-jhb5q [256.89034ms]
    May  4 13:01:27.897: INFO: Created: latency-svc-s7t6d
    May  4 13:01:27.905: INFO: Got endpoints: latency-svc-s7t6d [253.82376ms]
    May  4 13:01:27.912: INFO: Created: latency-svc-zbs8p
    May  4 13:01:27.955: INFO: Got endpoints: latency-svc-zbs8p [290.991343ms]
    May  4 13:01:27.961: INFO: Created: latency-svc-lq2vh
    May  4 13:01:27.968: INFO: Got endpoints: latency-svc-lq2vh [282.772466ms]
    May  4 13:01:27.975: INFO: Created: latency-svc-4rvmc
    May  4 13:01:27.985: INFO: Created: latency-svc-b7hcm
    May  4 13:01:28.024: INFO: Got endpoints: latency-svc-4rvmc [331.471804ms]
    May  4 13:01:28.030: INFO: Created: latency-svc-g4qkt
    May  4 13:01:28.039: INFO: Got endpoints: latency-svc-b7hcm [332.883796ms]
    May  4 13:01:28.041: INFO: Created: latency-svc-fp47x
    May  4 13:01:28.055: INFO: Created: latency-svc-m5ld5
    May  4 13:01:28.066: INFO: Created: latency-svc-nx28l
    May  4 13:01:28.077: INFO: Created: latency-svc-4j97g
    May  4 13:01:28.092: INFO: Got endpoints: latency-svc-g4qkt [361.090618ms]
    May  4 13:01:28.097: INFO: Created: latency-svc-2qdjh
    May  4 13:01:28.102: INFO: Created: latency-svc-rnstk
    May  4 13:01:28.115: INFO: Created: latency-svc-q6rdt
    May  4 13:01:28.142: INFO: Got endpoints: latency-svc-fp47x [391.49013ms]
    May  4 13:01:28.147: INFO: Created: latency-svc-qz5zc
    May  4 13:01:28.156: INFO: Created: latency-svc-cjr4m
    May  4 13:01:28.167: INFO: Created: latency-svc-vtjk5
    May  4 13:01:28.178: INFO: Created: latency-svc-bcnkm
    May  4 13:01:28.198: INFO: Got endpoints: latency-svc-m5ld5 [436.506889ms]
    May  4 13:01:28.198: INFO: Created: latency-svc-g8857
    May  4 13:01:28.208: INFO: Created: latency-svc-vxxwr
    May  4 13:01:28.218: INFO: Created: latency-svc-brsf2
    May  4 13:01:28.230: INFO: Created: latency-svc-vl6wc
    May  4 13:01:28.245: INFO: Got endpoints: latency-svc-nx28l [465.323112ms]
    May  4 13:01:28.245: INFO: Created: latency-svc-b44fl
    May  4 13:01:28.255: INFO: Created: latency-svc-mjbxs
    May  4 13:01:28.268: INFO: Created: latency-svc-ggng5
    May  4 13:01:28.291: INFO: Got endpoints: latency-svc-4j97g [705.738882ms]
    May  4 13:01:28.309: INFO: Created: latency-svc-v9jl9
    May  4 13:01:28.339: INFO: Got endpoints: latency-svc-2qdjh [504.863596ms]
    May  4 13:01:28.354: INFO: Created: latency-svc-cf8r7
    May  4 13:01:28.389: INFO: Got endpoints: latency-svc-rnstk [551.784785ms]
    May  4 13:01:28.403: INFO: Created: latency-svc-vpqmz
    May  4 13:01:28.440: INFO: Got endpoints: latency-svc-q6rdt [579.551659ms]
    May  4 13:01:28.455: INFO: Created: latency-svc-m6c6k
    May  4 13:01:28.491: INFO: Got endpoints: latency-svc-qz5zc [607.636708ms]
    May  4 13:01:28.505: INFO: Created: latency-svc-26mck
    May  4 13:01:28.540: INFO: Got endpoints: latency-svc-cjr4m [647.219279ms]
    May  4 13:01:28.555: INFO: Created: latency-svc-kj9wk
    May  4 13:01:28.589: INFO: Got endpoints: latency-svc-vtjk5 [684.83574ms]
    May  4 13:01:28.607: INFO: Created: latency-svc-6vl8p
    May  4 13:01:28.640: INFO: Got endpoints: latency-svc-bcnkm [685.508582ms]
    May  4 13:01:28.654: INFO: Created: latency-svc-7nr7d
    May  4 13:01:28.690: INFO: Got endpoints: latency-svc-g8857 [722.414964ms]
    May  4 13:01:28.704: INFO: Created: latency-svc-wg8sx
    May  4 13:01:28.739: INFO: Got endpoints: latency-svc-vxxwr [714.971144ms]
    May  4 13:01:28.754: INFO: Created: latency-svc-8sp4m
    May  4 13:01:28.792: INFO: Got endpoints: latency-svc-brsf2 [753.139198ms]
    May  4 13:01:28.808: INFO: Created: latency-svc-98k99
    May  4 13:01:28.839: INFO: Got endpoints: latency-svc-vl6wc [747.396209ms]
    May  4 13:01:28.855: INFO: Created: latency-svc-wndsj
    May  4 13:01:28.893: INFO: Got endpoints: latency-svc-b44fl [750.499557ms]
    May  4 13:01:28.910: INFO: Created: latency-svc-8nfp9
    May  4 13:01:28.940: INFO: Got endpoints: latency-svc-mjbxs [742.629939ms]
    May  4 13:01:28.964: INFO: Created: latency-svc-mb6nv
    May  4 13:01:28.997: INFO: Got endpoints: latency-svc-ggng5 [752.766969ms]
    May  4 13:01:29.017: INFO: Created: latency-svc-9bjkm
    May  4 13:01:29.044: INFO: Got endpoints: latency-svc-v9jl9 [753.632235ms]
    May  4 13:01:29.095: INFO: Got endpoints: latency-svc-cf8r7 [756.209889ms]
    May  4 13:01:29.100: INFO: Created: latency-svc-82ktq
    May  4 13:01:29.119: INFO: Created: latency-svc-vjzkf
    May  4 13:01:29.140: INFO: Got endpoints: latency-svc-vpqmz [750.999809ms]
    May  4 13:01:29.198: INFO: Created: latency-svc-ncxm6
    May  4 13:01:29.201: INFO: Got endpoints: latency-svc-m6c6k [760.423743ms]
    May  4 13:01:29.221: INFO: Created: latency-svc-2n8b5
    May  4 13:01:29.239: INFO: Got endpoints: latency-svc-26mck [748.133537ms]
    May  4 13:01:29.259: INFO: Created: latency-svc-dljdl
    May  4 13:01:29.294: INFO: Got endpoints: latency-svc-kj9wk [753.994382ms]
    May  4 13:01:29.310: INFO: Created: latency-svc-kcbjq
    May  4 13:01:29.340: INFO: Got endpoints: latency-svc-6vl8p [750.725465ms]
    May  4 13:01:29.359: INFO: Created: latency-svc-77j6z
    May  4 13:01:29.393: INFO: Got endpoints: latency-svc-7nr7d [752.30621ms]
    May  4 13:01:29.415: INFO: Created: latency-svc-cp945
    May  4 13:01:29.443: INFO: Got endpoints: latency-svc-wg8sx [752.023832ms]
    May  4 13:01:29.463: INFO: Created: latency-svc-gb6s8
    May  4 13:01:29.502: INFO: Got endpoints: latency-svc-8sp4m [762.335184ms]
    May  4 13:01:29.531: INFO: Created: latency-svc-9rhg6
    May  4 13:01:29.539: INFO: Got endpoints: latency-svc-98k99 [747.448365ms]
    May  4 13:01:29.560: INFO: Created: latency-svc-nztps
    May  4 13:01:29.591: INFO: Got endpoints: latency-svc-wndsj [751.961197ms]
    May  4 13:01:29.607: INFO: Created: latency-svc-nlhld
    May  4 13:01:29.640: INFO: Got endpoints: latency-svc-8nfp9 [747.001462ms]
    May  4 13:01:29.656: INFO: Created: latency-svc-xzdc6
    May  4 13:01:29.694: INFO: Got endpoints: latency-svc-mb6nv [752.934876ms]
    May  4 13:01:29.717: INFO: Created: latency-svc-547p4
    May  4 13:01:29.740: INFO: Got endpoints: latency-svc-9bjkm [742.583581ms]
    May  4 13:01:29.761: INFO: Created: latency-svc-bvvsq
    May  4 13:01:29.791: INFO: Got endpoints: latency-svc-82ktq [746.214636ms]
    May  4 13:01:29.824: INFO: Created: latency-svc-vhzrg
    May  4 13:01:29.842: INFO: Got endpoints: latency-svc-vjzkf [747.208872ms]
    May  4 13:01:29.867: INFO: Created: latency-svc-bltcb
    May  4 13:01:29.893: INFO: Got endpoints: latency-svc-ncxm6 [752.959629ms]
    May  4 13:01:29.911: INFO: Created: latency-svc-5kcth
    May  4 13:01:29.939: INFO: Got endpoints: latency-svc-2n8b5 [737.970462ms]
    May  4 13:01:29.957: INFO: Created: latency-svc-wcg4r
    May  4 13:01:29.988: INFO: Got endpoints: latency-svc-dljdl [748.346414ms]
    May  4 13:01:30.005: INFO: Created: latency-svc-kkrlb
    May  4 13:01:30.039: INFO: Got endpoints: latency-svc-kcbjq [744.033481ms]
    May  4 13:01:30.058: INFO: Created: latency-svc-529j2
    May  4 13:01:30.098: INFO: Got endpoints: latency-svc-77j6z [757.900881ms]
    May  4 13:01:30.117: INFO: Created: latency-svc-qtgpv
    May  4 13:01:30.140: INFO: Got endpoints: latency-svc-cp945 [747.339382ms]
    May  4 13:01:30.155: INFO: Created: latency-svc-hqh2s
    May  4 13:01:30.189: INFO: Got endpoints: latency-svc-gb6s8 [746.698334ms]
    May  4 13:01:30.206: INFO: Created: latency-svc-grr7n
    May  4 13:01:30.241: INFO: Got endpoints: latency-svc-9rhg6 [739.156359ms]
    May  4 13:01:30.257: INFO: Created: latency-svc-9gwqw
    May  4 13:01:30.290: INFO: Got endpoints: latency-svc-nztps [750.709052ms]
    May  4 13:01:30.314: INFO: Created: latency-svc-w4nmr
    May  4 13:01:30.342: INFO: Got endpoints: latency-svc-nlhld [751.026228ms]
    May  4 13:01:30.359: INFO: Created: latency-svc-4v4zm
    May  4 13:01:30.392: INFO: Got endpoints: latency-svc-xzdc6 [751.574594ms]
    May  4 13:01:30.409: INFO: Created: latency-svc-jv8cx
    May  4 13:01:30.441: INFO: Got endpoints: latency-svc-547p4 [747.459815ms]
    May  4 13:01:30.461: INFO: Created: latency-svc-jx6gv
    May  4 13:01:30.490: INFO: Got endpoints: latency-svc-bvvsq [749.741295ms]
    May  4 13:01:30.506: INFO: Created: latency-svc-q8zt9
    May  4 13:01:30.542: INFO: Got endpoints: latency-svc-vhzrg [751.179353ms]
    May  4 13:01:30.560: INFO: Created: latency-svc-x5g92
    May  4 13:01:30.591: INFO: Got endpoints: latency-svc-bltcb [748.873684ms]
    May  4 13:01:30.605: INFO: Created: latency-svc-pthsv
    May  4 13:01:30.643: INFO: Got endpoints: latency-svc-5kcth [750.174059ms]
    May  4 13:01:30.701: INFO: Got endpoints: latency-svc-wcg4r [762.690704ms]
    May  4 13:01:30.709: INFO: Created: latency-svc-rs9cb
    May  4 13:01:30.718: INFO: Created: latency-svc-kzp76
    May  4 13:01:30.743: INFO: Got endpoints: latency-svc-kkrlb [755.560677ms]
    May  4 13:01:30.763: INFO: Created: latency-svc-7d6vt
    May  4 13:01:30.791: INFO: Got endpoints: latency-svc-529j2 [752.767995ms]
    May  4 13:01:30.807: INFO: Created: latency-svc-2vvsc
    May  4 13:01:30.841: INFO: Got endpoints: latency-svc-qtgpv [742.417187ms]
    May  4 13:01:30.856: INFO: Created: latency-svc-jwjd7
    May  4 13:01:30.890: INFO: Got endpoints: latency-svc-hqh2s [750.055571ms]
    May  4 13:01:30.905: INFO: Created: latency-svc-r4ttj
    May  4 13:01:30.939: INFO: Got endpoints: latency-svc-grr7n [749.636328ms]
    May  4 13:01:30.960: INFO: Created: latency-svc-x97m7
    May  4 13:01:30.988: INFO: Got endpoints: latency-svc-9gwqw [747.57659ms]
    May  4 13:01:31.003: INFO: Created: latency-svc-zsz6h
    May  4 13:01:31.040: INFO: Got endpoints: latency-svc-w4nmr [749.51262ms]
    May  4 13:01:31.062: INFO: Created: latency-svc-9j64b
    May  4 13:01:31.093: INFO: Got endpoints: latency-svc-4v4zm [750.327687ms]
    May  4 13:01:31.113: INFO: Created: latency-svc-xhmdt
    May  4 13:01:31.141: INFO: Got endpoints: latency-svc-jv8cx [749.078682ms]
    May  4 13:01:31.158: INFO: Created: latency-svc-cpsjh
    May  4 13:01:31.192: INFO: Got endpoints: latency-svc-jx6gv [751.169729ms]
    May  4 13:01:31.219: INFO: Created: latency-svc-hr7vs
    May  4 13:01:31.242: INFO: Got endpoints: latency-svc-q8zt9 [751.840138ms]
    May  4 13:01:31.260: INFO: Created: latency-svc-c62kk
    May  4 13:01:31.291: INFO: Got endpoints: latency-svc-x5g92 [749.071379ms]
    May  4 13:01:31.311: INFO: Created: latency-svc-bslwc
    May  4 13:01:31.341: INFO: Got endpoints: latency-svc-pthsv [749.43214ms]
    May  4 13:01:31.360: INFO: Created: latency-svc-22gpb
    May  4 13:01:31.391: INFO: Got endpoints: latency-svc-rs9cb [747.612134ms]
    May  4 13:01:31.407: INFO: Created: latency-svc-qjmnw
    May  4 13:01:31.441: INFO: Got endpoints: latency-svc-kzp76 [739.028824ms]
    May  4 13:01:31.456: INFO: Created: latency-svc-9lznq
    May  4 13:01:31.494: INFO: Got endpoints: latency-svc-7d6vt [750.371569ms]
    May  4 13:01:31.512: INFO: Created: latency-svc-rbrfk
    May  4 13:01:31.541: INFO: Got endpoints: latency-svc-2vvsc [749.985115ms]
    May  4 13:01:31.560: INFO: Created: latency-svc-vgv6s
    May  4 13:01:31.591: INFO: Got endpoints: latency-svc-jwjd7 [750.594757ms]
    May  4 13:01:31.607: INFO: Created: latency-svc-z9kp5
    May  4 13:01:31.641: INFO: Got endpoints: latency-svc-r4ttj [750.859199ms]
    May  4 13:01:31.666: INFO: Created: latency-svc-dqhwh
    May  4 13:01:31.690: INFO: Got endpoints: latency-svc-x97m7 [750.41195ms]
    May  4 13:01:31.711: INFO: Created: latency-svc-4wtlp
    May  4 13:01:31.745: INFO: Got endpoints: latency-svc-zsz6h [756.418262ms]
    May  4 13:01:31.764: INFO: Created: latency-svc-5sbwd
    May  4 13:01:31.789: INFO: Got endpoints: latency-svc-9j64b [749.69456ms]
    May  4 13:01:31.807: INFO: Created: latency-svc-k4r9s
    May  4 13:01:31.851: INFO: Got endpoints: latency-svc-xhmdt [757.855217ms]
    May  4 13:01:31.873: INFO: Created: latency-svc-cc5wv
    May  4 13:01:31.891: INFO: Got endpoints: latency-svc-cpsjh [749.680903ms]
    May  4 13:01:31.916: INFO: Created: latency-svc-2r5bk
    May  4 13:01:31.942: INFO: Got endpoints: latency-svc-hr7vs [749.646704ms]
    May  4 13:01:31.963: INFO: Created: latency-svc-zs4mh
    May  4 13:01:31.994: INFO: Got endpoints: latency-svc-c62kk [752.27664ms]
    May  4 13:01:32.028: INFO: Created: latency-svc-6rw7q
    May  4 13:01:32.040: INFO: Got endpoints: latency-svc-bslwc [749.313035ms]
    May  4 13:01:32.060: INFO: Created: latency-svc-x466g
    May  4 13:01:32.093: INFO: Got endpoints: latency-svc-22gpb [751.748276ms]
    May  4 13:01:32.113: INFO: Created: latency-svc-vf8ns
    May  4 13:01:32.139: INFO: Got endpoints: latency-svc-qjmnw [748.039219ms]
    May  4 13:01:32.158: INFO: Created: latency-svc-prxjs
    May  4 13:01:32.194: INFO: Got endpoints: latency-svc-9lznq [753.820543ms]
    May  4 13:01:32.218: INFO: Created: latency-svc-rthrh
    May  4 13:01:32.245: INFO: Got endpoints: latency-svc-rbrfk [751.55958ms]
    May  4 13:01:32.264: INFO: Created: latency-svc-kzwqv
    May  4 13:01:32.301: INFO: Got endpoints: latency-svc-vgv6s [759.627302ms]
    May  4 13:01:32.325: INFO: Created: latency-svc-thw2k
    May  4 13:01:32.339: INFO: Got endpoints: latency-svc-z9kp5 [747.576868ms]
    May  4 13:01:32.391: INFO: Got endpoints: latency-svc-dqhwh [750.17307ms]
    May  4 13:01:32.396: INFO: Created: latency-svc-bczg8
    May  4 13:01:32.413: INFO: Created: latency-svc-hzs4g
    May  4 13:01:32.442: INFO: Got endpoints: latency-svc-4wtlp [752.120759ms]
    May  4 13:01:32.477: INFO: Created: latency-svc-nf8n9
    May  4 13:01:32.492: INFO: Got endpoints: latency-svc-5sbwd [747.170797ms]
    May  4 13:01:32.516: INFO: Created: latency-svc-5r57f
    May  4 13:01:32.539: INFO: Got endpoints: latency-svc-k4r9s [749.298903ms]
    May  4 13:01:32.556: INFO: Created: latency-svc-gxnfz
    May  4 13:01:32.593: INFO: Got endpoints: latency-svc-cc5wv [742.782212ms]
    May  4 13:01:32.617: INFO: Created: latency-svc-qhsrq
    May  4 13:01:32.645: INFO: Got endpoints: latency-svc-2r5bk [754.130241ms]
    May  4 13:01:32.672: INFO: Created: latency-svc-8ggdh
    May  4 13:01:32.696: INFO: Got endpoints: latency-svc-zs4mh [754.263514ms]
    May  4 13:01:32.732: INFO: Created: latency-svc-pkg66
    May  4 13:01:32.742: INFO: Got endpoints: latency-svc-6rw7q [747.656031ms]
    May  4 13:01:32.762: INFO: Created: latency-svc-srtsc
    May  4 13:01:32.790: INFO: Got endpoints: latency-svc-x466g [749.178311ms]
    May  4 13:01:32.807: INFO: Created: latency-svc-8qhvs
    May  4 13:01:32.844: INFO: Got endpoints: latency-svc-vf8ns [750.968486ms]
    May  4 13:01:32.864: INFO: Created: latency-svc-lm2dx
    May  4 13:01:32.893: INFO: Got endpoints: latency-svc-prxjs [754.406028ms]
    May  4 13:01:32.909: INFO: Created: latency-svc-6lqnx
    May  4 13:01:32.944: INFO: Got endpoints: latency-svc-rthrh [749.520625ms]
    May  4 13:01:32.974: INFO: Created: latency-svc-tf7cp
    May  4 13:01:32.991: INFO: Got endpoints: latency-svc-kzwqv [745.929516ms]
    May  4 13:01:33.008: INFO: Created: latency-svc-tgf52
    May  4 13:01:33.039: INFO: Got endpoints: latency-svc-thw2k [737.85198ms]
    May  4 13:01:33.068: INFO: Created: latency-svc-4kl2w
    May  4 13:01:33.093: INFO: Got endpoints: latency-svc-bczg8 [754.141123ms]
    May  4 13:01:33.111: INFO: Created: latency-svc-z92zq
    May  4 13:01:33.143: INFO: Got endpoints: latency-svc-hzs4g [751.849374ms]
    May  4 13:01:33.163: INFO: Created: latency-svc-qh64n
    May  4 13:01:33.191: INFO: Got endpoints: latency-svc-nf8n9 [749.513215ms]
    May  4 13:01:33.211: INFO: Created: latency-svc-8w72x
    May  4 13:01:33.239: INFO: Got endpoints: latency-svc-5r57f [747.080217ms]
    May  4 13:01:33.264: INFO: Created: latency-svc-7m5jf
    May  4 13:01:33.289: INFO: Got endpoints: latency-svc-gxnfz [750.572044ms]
    May  4 13:01:33.305: INFO: Created: latency-svc-fgb2d
    May  4 13:01:33.342: INFO: Got endpoints: latency-svc-qhsrq [747.731252ms]
    May  4 13:01:33.359: INFO: Created: latency-svc-d56g5
    May  4 13:01:33.391: INFO: Got endpoints: latency-svc-8ggdh [746.227618ms]
    May  4 13:01:33.410: INFO: Created: latency-svc-n2c2b
    May  4 13:01:33.442: INFO: Got endpoints: latency-svc-pkg66 [745.82935ms]
    May  4 13:01:33.457: INFO: Created: latency-svc-2c5rn
    May  4 13:01:33.490: INFO: Got endpoints: latency-svc-srtsc [747.950664ms]
    May  4 13:01:33.509: INFO: Created: latency-svc-vt9z4
    May  4 13:01:33.542: INFO: Got endpoints: latency-svc-8qhvs [752.160477ms]
    May  4 13:01:33.560: INFO: Created: latency-svc-vnl6w
    May  4 13:01:33.591: INFO: Got endpoints: latency-svc-lm2dx [746.914871ms]
    May  4 13:01:33.618: INFO: Created: latency-svc-5hdlk
    May  4 13:01:33.643: INFO: Got endpoints: latency-svc-6lqnx [749.755824ms]
    May  4 13:01:33.669: INFO: Created: latency-svc-dhv98
    May  4 13:01:33.691: INFO: Got endpoints: latency-svc-tf7cp [746.831476ms]
    May  4 13:01:33.713: INFO: Created: latency-svc-t5rdg
    May  4 13:01:33.741: INFO: Got endpoints: latency-svc-tgf52 [749.398221ms]
    May  4 13:01:33.766: INFO: Created: latency-svc-gn4nm
    May  4 13:01:33.790: INFO: Got endpoints: latency-svc-4kl2w [750.872811ms]
    May  4 13:01:33.814: INFO: Created: latency-svc-mhz8v
    May  4 13:01:33.844: INFO: Got endpoints: latency-svc-z92zq [750.412467ms]
    May  4 13:01:33.864: INFO: Created: latency-svc-w6v6k
    May  4 13:01:33.893: INFO: Got endpoints: latency-svc-qh64n [749.682805ms]
    May  4 13:01:33.922: INFO: Created: latency-svc-r5498
    May  4 13:01:33.942: INFO: Got endpoints: latency-svc-8w72x [750.100001ms]
    May  4 13:01:33.959: INFO: Created: latency-svc-2x8pn
    May  4 13:01:33.993: INFO: Got endpoints: latency-svc-7m5jf [752.997603ms]
    May  4 13:01:34.011: INFO: Created: latency-svc-lcdk6
    May  4 13:01:34.070: INFO: Got endpoints: latency-svc-fgb2d [780.151537ms]
    May  4 13:01:34.099: INFO: Got endpoints: latency-svc-d56g5 [756.984912ms]
    May  4 13:01:34.100: INFO: Created: latency-svc-pzdj7
    May  4 13:01:34.115: INFO: Created: latency-svc-r7fcz
    May  4 13:01:34.149: INFO: Got endpoints: latency-svc-n2c2b [758.037763ms]
    May  4 13:01:34.207: INFO: Got endpoints: latency-svc-2c5rn [764.637452ms]
    May  4 13:01:34.210: INFO: Created: latency-svc-cdcrb
    May  4 13:01:34.242: INFO: Got endpoints: latency-svc-vt9z4 [752.021814ms]
    May  4 13:01:34.293: INFO: Got endpoints: latency-svc-vnl6w [751.508233ms]
    May  4 13:01:34.342: INFO: Got endpoints: latency-svc-5hdlk [751.354188ms]
    May  4 13:01:34.392: INFO: Got endpoints: latency-svc-dhv98 [748.601425ms]
    May  4 13:01:34.442: INFO: Got endpoints: latency-svc-t5rdg [750.823195ms]
    May  4 13:01:34.490: INFO: Got endpoints: latency-svc-gn4nm [749.089946ms]
    May  4 13:01:34.543: INFO: Got endpoints: latency-svc-mhz8v [753.118006ms]
    May  4 13:01:34.589: INFO: Got endpoints: latency-svc-w6v6k [745.285741ms]
    May  4 13:01:34.642: INFO: Got endpoints: latency-svc-r5498 [748.491581ms]
    May  4 13:01:34.690: INFO: Got endpoints: latency-svc-2x8pn [748.60672ms]
    May  4 13:01:34.744: INFO: Got endpoints: latency-svc-lcdk6 [751.559278ms]
    May  4 13:01:34.792: INFO: Got endpoints: latency-svc-pzdj7 [721.918033ms]
    May  4 13:01:34.841: INFO: Got endpoints: latency-svc-r7fcz [742.645304ms]
    May  4 13:01:34.891: INFO: Got endpoints: latency-svc-cdcrb [741.603322ms]
    May  4 13:01:34.891: INFO: Latencies: [38.683931ms 57.474524ms 104.185244ms 132.076736ms 194.438083ms 241.005538ms 244.132714ms 246.936061ms 247.480537ms 251.115477ms 252.919838ms 253.82376ms 255.096917ms 255.628241ms 256.89034ms 257.90091ms 265.232269ms 269.933972ms 273.680122ms 274.070714ms 278.695817ms 282.772466ms 290.991343ms 294.809014ms 316.220494ms 316.633159ms 319.537034ms 320.735012ms 331.471804ms 332.883796ms 345.663512ms 356.508269ms 358.5729ms 361.090618ms 363.384715ms 364.536157ms 367.442044ms 388.473552ms 389.079787ms 391.49013ms 398.97382ms 399.18223ms 406.876039ms 414.826233ms 427.897746ms 430.677983ms 436.124144ms 436.506889ms 438.446654ms 446.307147ms 451.014749ms 461.876728ms 465.323112ms 472.431154ms 479.632885ms 488.164039ms 504.863596ms 509.541897ms 512.910103ms 519.750886ms 526.734619ms 542.238995ms 550.169376ms 551.784785ms 579.551659ms 583.829386ms 586.933189ms 591.456946ms 595.448963ms 607.636708ms 620.366816ms 647.219279ms 684.83574ms 685.508582ms 705.738882ms 714.971144ms 721.918033ms 722.414964ms 737.85198ms 737.970462ms 739.028824ms 739.156359ms 741.603322ms 742.417187ms 742.583581ms 742.629939ms 742.645304ms 742.782212ms 744.033481ms 745.285741ms 745.82935ms 745.929516ms 746.214636ms 746.227618ms 746.698334ms 746.831476ms 746.914871ms 747.001462ms 747.080217ms 747.170797ms 747.208872ms 747.339382ms 747.396209ms 747.448365ms 747.459815ms 747.57659ms 747.576868ms 747.612134ms 747.656031ms 747.731252ms 747.950664ms 748.039219ms 748.133537ms 748.346414ms 748.491581ms 748.601425ms 748.60672ms 748.873684ms 749.071379ms 749.078682ms 749.089946ms 749.178311ms 749.298903ms 749.313035ms 749.398221ms 749.43214ms 749.51262ms 749.513215ms 749.520625ms 749.636328ms 749.646704ms 749.680903ms 749.682805ms 749.69456ms 749.741295ms 749.755824ms 749.985115ms 750.055571ms 750.100001ms 750.17307ms 750.174059ms 750.327687ms 750.371569ms 750.41195ms 750.412467ms 750.499557ms 750.572044ms 750.594757ms 750.709052ms 750.725465ms 750.823195ms 750.859199ms 750.872811ms 750.968486ms 750.999809ms 751.026228ms 751.169729ms 751.179353ms 751.354188ms 751.508233ms 751.559278ms 751.55958ms 751.574594ms 751.748276ms 751.840138ms 751.849374ms 751.961197ms 752.021814ms 752.023832ms 752.120759ms 752.160477ms 752.27664ms 752.30621ms 752.766969ms 752.767995ms 752.934876ms 752.959629ms 752.997603ms 753.118006ms 753.139198ms 753.632235ms 753.820543ms 753.994382ms 754.130241ms 754.141123ms 754.263514ms 754.406028ms 755.560677ms 756.209889ms 756.418262ms 756.984912ms 757.855217ms 757.900881ms 758.037763ms 759.627302ms 760.423743ms 762.335184ms 762.690704ms 764.637452ms 780.151537ms]
    May  4 13:01:34.891: INFO: 50 %ile: 747.208872ms
    May  4 13:01:34.891: INFO: 90 %ile: 753.632235ms
    May  4 13:01:34.891: INFO: 99 %ile: 764.637452ms
    May  4 13:01:34.891: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:187
    May  4 13:01:34.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svc-latency-4360" for this suite. 05/04/23 13:01:34.906
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:01:34.918
May  4 13:01:34.918: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename runtimeclass 05/04/23 13:01:34.919
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:01:34.941
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:01:34.948
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
May  4 13:01:34.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-8580" for this suite. 05/04/23 13:01:34.978
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","completed":271,"skipped":5043,"failed":0}
------------------------------
• [0.069 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:01:34.918
    May  4 13:01:34.918: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename runtimeclass 05/04/23 13:01:34.919
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:01:34.941
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:01:34.948
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    May  4 13:01:34.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-8580" for this suite. 05/04/23 13:01:34.978
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:01:34.989
May  4 13:01:34.989: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename namespaces 05/04/23 13:01:34.99
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:01:35.018
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:01:35.021
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
STEP: Creating a test namespace 05/04/23 13:01:35.025
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:01:35.053
STEP: Creating a service in the namespace 05/04/23 13:01:35.058
STEP: Deleting the namespace 05/04/23 13:01:35.078
STEP: Waiting for the namespace to be removed. 05/04/23 13:01:35.09
STEP: Recreating the namespace 05/04/23 13:01:42.096
STEP: Verifying there is no service in the namespace 05/04/23 13:01:42.131
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
May  4 13:01:42.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6401" for this suite. 05/04/23 13:01:42.157
STEP: Destroying namespace "nsdeletetest-5623" for this suite. 05/04/23 13:01:42.168
May  4 13:01:42.177: INFO: Namespace nsdeletetest-5623 was already deleted
STEP: Destroying namespace "nsdeletetest-3125" for this suite. 05/04/23 13:01:42.177
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","completed":272,"skipped":5071,"failed":0}
------------------------------
• [SLOW TEST] [7.209 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:01:34.989
    May  4 13:01:34.989: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename namespaces 05/04/23 13:01:34.99
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:01:35.018
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:01:35.021
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:250
    STEP: Creating a test namespace 05/04/23 13:01:35.025
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:01:35.053
    STEP: Creating a service in the namespace 05/04/23 13:01:35.058
    STEP: Deleting the namespace 05/04/23 13:01:35.078
    STEP: Waiting for the namespace to be removed. 05/04/23 13:01:35.09
    STEP: Recreating the namespace 05/04/23 13:01:42.096
    STEP: Verifying there is no service in the namespace 05/04/23 13:01:42.131
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    May  4 13:01:42.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-6401" for this suite. 05/04/23 13:01:42.157
    STEP: Destroying namespace "nsdeletetest-5623" for this suite. 05/04/23 13:01:42.168
    May  4 13:01:42.177: INFO: Namespace nsdeletetest-5623 was already deleted
    STEP: Destroying namespace "nsdeletetest-3125" for this suite. 05/04/23 13:01:42.177
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:01:42.198
May  4 13:01:42.198: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename namespaces 05/04/23 13:01:42.199
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:01:42.221
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:01:42.225
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
STEP: Read namespace status 05/04/23 13:01:42.228
May  4 13:01:42.233: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 05/04/23 13:01:42.233
May  4 13:01:42.243: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 05/04/23 13:01:42.243
May  4 13:01:42.258: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
May  4 13:01:42.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-202" for this suite. 05/04/23 13:01:42.27
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]","completed":273,"skipped":5074,"failed":0}
------------------------------
• [0.091 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:01:42.198
    May  4 13:01:42.198: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename namespaces 05/04/23 13:01:42.199
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:01:42.221
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:01:42.225
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:298
    STEP: Read namespace status 05/04/23 13:01:42.228
    May  4 13:01:42.233: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 05/04/23 13:01:42.233
    May  4 13:01:42.243: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 05/04/23 13:01:42.243
    May  4 13:01:42.258: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    May  4 13:01:42.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-202" for this suite. 05/04/23 13:01:42.27
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:01:42.291
May  4 13:01:42.291: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename crd-publish-openapi 05/04/23 13:01:42.292
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:01:42.321
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:01:42.332
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 05/04/23 13:01:42.335
May  4 13:01:42.336: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 05/04/23 13:02:03.626
May  4 13:02:03.626: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
May  4 13:02:09.730: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  4 13:02:32.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1301" for this suite. 05/04/23 13:02:32.623
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","completed":274,"skipped":5097,"failed":0}
------------------------------
• [SLOW TEST] [50.338 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:01:42.291
    May  4 13:01:42.291: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename crd-publish-openapi 05/04/23 13:01:42.292
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:01:42.321
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:01:42.332
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:308
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 05/04/23 13:01:42.335
    May  4 13:01:42.336: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 05/04/23 13:02:03.626
    May  4 13:02:03.626: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    May  4 13:02:09.730: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  4 13:02:32.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-1301" for this suite. 05/04/23 13:02:32.623
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:02:32.63
May  4 13:02:32.630: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename kubelet-test 05/04/23 13:02:32.631
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:02:32.655
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:02:32.659
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
May  4 13:02:36.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9557" for this suite. 05/04/23 13:02:36.691
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","completed":275,"skipped":5129,"failed":0}
------------------------------
• [4.069 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:02:32.63
    May  4 13:02:32.630: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename kubelet-test 05/04/23 13:02:32.631
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:02:32.655
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:02:32.659
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    May  4 13:02:36.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-9557" for this suite. 05/04/23 13:02:36.691
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:02:36.7
May  4 13:02:36.700: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename webhook 05/04/23 13:02:36.701
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:02:36.726
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:02:36.729
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/04/23 13:02:36.762
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/04/23 13:02:37.022
STEP: Deploying the webhook pod 05/04/23 13:02:37.033
STEP: Wait for the deployment to be ready 05/04/23 13:02:37.049
May  4 13:02:37.059: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 05/04/23 13:02:39.074
STEP: Verifying the service has paired with the endpoint 05/04/23 13:02:39.091
May  4 13:02:40.091: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 05/04/23 13:02:40.095
STEP: create a namespace for the webhook 05/04/23 13:02:40.111
STEP: create a configmap should be unconditionally rejected by the webhook 05/04/23 13:02:40.119
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  4 13:02:40.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5617" for this suite. 05/04/23 13:02:40.162
STEP: Destroying namespace "webhook-5617-markers" for this suite. 05/04/23 13:02:40.171
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","completed":276,"skipped":5142,"failed":0}
------------------------------
• [3.556 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:02:36.7
    May  4 13:02:36.700: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename webhook 05/04/23 13:02:36.701
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:02:36.726
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:02:36.729
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/04/23 13:02:36.762
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/04/23 13:02:37.022
    STEP: Deploying the webhook pod 05/04/23 13:02:37.033
    STEP: Wait for the deployment to be ready 05/04/23 13:02:37.049
    May  4 13:02:37.059: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 05/04/23 13:02:39.074
    STEP: Verifying the service has paired with the endpoint 05/04/23 13:02:39.091
    May  4 13:02:40.091: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:238
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 05/04/23 13:02:40.095
    STEP: create a namespace for the webhook 05/04/23 13:02:40.111
    STEP: create a configmap should be unconditionally rejected by the webhook 05/04/23 13:02:40.119
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  4 13:02:40.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5617" for this suite. 05/04/23 13:02:40.162
    STEP: Destroying namespace "webhook-5617-markers" for this suite. 05/04/23 13:02:40.171
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:02:40.257
May  4 13:02:40.257: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename container-lifecycle-hook 05/04/23 13:02:40.262
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:02:40.295
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:02:40.298
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 05/04/23 13:02:40.316
May  4 13:02:40.336: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4573" to be "running and ready"
May  4 13:02:40.343: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.394794ms
May  4 13:02:40.343: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May  4 13:02:42.347: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.010722718s
May  4 13:02:42.347: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
May  4 13:02:42.347: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
STEP: create the pod with lifecycle hook 05/04/23 13:02:42.351
May  4 13:02:42.357: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-4573" to be "running and ready"
May  4 13:02:42.361: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.130977ms
May  4 13:02:42.361: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
May  4 13:02:44.367: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.009894241s
May  4 13:02:44.367: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
May  4 13:02:44.367: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 05/04/23 13:02:44.371
STEP: delete the pod with lifecycle hook 05/04/23 13:02:44.404
May  4 13:02:44.414: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  4 13:02:44.419: INFO: Pod pod-with-poststart-exec-hook still exists
May  4 13:02:46.421: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  4 13:02:46.425: INFO: Pod pod-with-poststart-exec-hook still exists
May  4 13:02:48.420: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  4 13:02:48.429: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
May  4 13:02:48.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4573" for this suite. 05/04/23 13:02:48.44
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","completed":277,"skipped":5143,"failed":0}
------------------------------
• [SLOW TEST] [8.191 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:02:40.257
    May  4 13:02:40.257: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename container-lifecycle-hook 05/04/23 13:02:40.262
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:02:40.295
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:02:40.298
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 05/04/23 13:02:40.316
    May  4 13:02:40.336: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4573" to be "running and ready"
    May  4 13:02:40.343: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.394794ms
    May  4 13:02:40.343: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    May  4 13:02:42.347: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.010722718s
    May  4 13:02:42.347: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    May  4 13:02:42.347: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:97
    STEP: create the pod with lifecycle hook 05/04/23 13:02:42.351
    May  4 13:02:42.357: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-4573" to be "running and ready"
    May  4 13:02:42.361: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.130977ms
    May  4 13:02:42.361: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    May  4 13:02:44.367: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.009894241s
    May  4 13:02:44.367: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    May  4 13:02:44.367: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 05/04/23 13:02:44.371
    STEP: delete the pod with lifecycle hook 05/04/23 13:02:44.404
    May  4 13:02:44.414: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    May  4 13:02:44.419: INFO: Pod pod-with-poststart-exec-hook still exists
    May  4 13:02:46.421: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    May  4 13:02:46.425: INFO: Pod pod-with-poststart-exec-hook still exists
    May  4 13:02:48.420: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    May  4 13:02:48.429: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    May  4 13:02:48.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-4573" for this suite. 05/04/23 13:02:48.44
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:02:48.448
May  4 13:02:48.449: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename disruption 05/04/23 13:02:48.449
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:02:48.472
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:02:48.48
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:02:48.483
May  4 13:02:48.483: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename disruption-2 05/04/23 13:02:48.484
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:02:48.52
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:02:48.53
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
STEP: Waiting for the pdb to be processed 05/04/23 13:02:48.54
STEP: Waiting for the pdb to be processed 05/04/23 13:02:50.559
STEP: Waiting for the pdb to be processed 05/04/23 13:02:52.577
STEP: listing a collection of PDBs across all namespaces 05/04/23 13:02:54.59
STEP: listing a collection of PDBs in namespace disruption-3665 05/04/23 13:02:54.594
STEP: deleting a collection of PDBs 05/04/23 13:02:54.598
STEP: Waiting for the PDB collection to be deleted 05/04/23 13:02:54.612
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
May  4 13:02:54.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-427" for this suite. 05/04/23 13:02:54.624
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
May  4 13:02:54.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-3665" for this suite. 05/04/23 13:02:54.65
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","completed":278,"skipped":5143,"failed":0}
------------------------------
• [SLOW TEST] [6.208 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:77
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:02:48.448
    May  4 13:02:48.449: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename disruption 05/04/23 13:02:48.449
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:02:48.472
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:02:48.48
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:02:48.483
    May  4 13:02:48.483: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename disruption-2 05/04/23 13:02:48.484
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:02:48.52
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:02:48.53
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:86
    STEP: Waiting for the pdb to be processed 05/04/23 13:02:48.54
    STEP: Waiting for the pdb to be processed 05/04/23 13:02:50.559
    STEP: Waiting for the pdb to be processed 05/04/23 13:02:52.577
    STEP: listing a collection of PDBs across all namespaces 05/04/23 13:02:54.59
    STEP: listing a collection of PDBs in namespace disruption-3665 05/04/23 13:02:54.594
    STEP: deleting a collection of PDBs 05/04/23 13:02:54.598
    STEP: Waiting for the PDB collection to be deleted 05/04/23 13:02:54.612
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:187
    May  4 13:02:54.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2-427" for this suite. 05/04/23 13:02:54.624
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    May  4 13:02:54.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-3665" for this suite. 05/04/23 13:02:54.65
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:02:54.658
May  4 13:02:54.658: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename job 05/04/23 13:02:54.659
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:02:54.677
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:02:54.687
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
STEP: Creating a job 05/04/23 13:02:54.693
STEP: Ensuring active pods == parallelism 05/04/23 13:02:54.702
STEP: Orphaning one of the Job's Pods 05/04/23 13:02:56.706
May  4 13:02:57.224: INFO: Successfully updated pod "adopt-release-hpmwp"
STEP: Checking that the Job readopts the Pod 05/04/23 13:02:57.225
May  4 13:02:57.226: INFO: Waiting up to 15m0s for pod "adopt-release-hpmwp" in namespace "job-6080" to be "adopted"
May  4 13:02:57.237: INFO: Pod "adopt-release-hpmwp": Phase="Running", Reason="", readiness=true. Elapsed: 11.829022ms
May  4 13:02:59.242: INFO: Pod "adopt-release-hpmwp": Phase="Running", Reason="", readiness=true. Elapsed: 2.016669125s
May  4 13:02:59.242: INFO: Pod "adopt-release-hpmwp" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 05/04/23 13:02:59.242
May  4 13:02:59.764: INFO: Successfully updated pod "adopt-release-hpmwp"
STEP: Checking that the Job releases the Pod 05/04/23 13:02:59.764
May  4 13:02:59.764: INFO: Waiting up to 15m0s for pod "adopt-release-hpmwp" in namespace "job-6080" to be "released"
May  4 13:02:59.773: INFO: Pod "adopt-release-hpmwp": Phase="Running", Reason="", readiness=true. Elapsed: 8.597038ms
May  4 13:03:01.778: INFO: Pod "adopt-release-hpmwp": Phase="Running", Reason="", readiness=true. Elapsed: 2.013808768s
May  4 13:03:01.778: INFO: Pod "adopt-release-hpmwp" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
May  4 13:03:01.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6080" for this suite. 05/04/23 13:03:01.786
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","completed":279,"skipped":5166,"failed":0}
------------------------------
• [SLOW TEST] [7.136 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:02:54.658
    May  4 13:02:54.658: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename job 05/04/23 13:02:54.659
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:02:54.677
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:02:54.687
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:335
    STEP: Creating a job 05/04/23 13:02:54.693
    STEP: Ensuring active pods == parallelism 05/04/23 13:02:54.702
    STEP: Orphaning one of the Job's Pods 05/04/23 13:02:56.706
    May  4 13:02:57.224: INFO: Successfully updated pod "adopt-release-hpmwp"
    STEP: Checking that the Job readopts the Pod 05/04/23 13:02:57.225
    May  4 13:02:57.226: INFO: Waiting up to 15m0s for pod "adopt-release-hpmwp" in namespace "job-6080" to be "adopted"
    May  4 13:02:57.237: INFO: Pod "adopt-release-hpmwp": Phase="Running", Reason="", readiness=true. Elapsed: 11.829022ms
    May  4 13:02:59.242: INFO: Pod "adopt-release-hpmwp": Phase="Running", Reason="", readiness=true. Elapsed: 2.016669125s
    May  4 13:02:59.242: INFO: Pod "adopt-release-hpmwp" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 05/04/23 13:02:59.242
    May  4 13:02:59.764: INFO: Successfully updated pod "adopt-release-hpmwp"
    STEP: Checking that the Job releases the Pod 05/04/23 13:02:59.764
    May  4 13:02:59.764: INFO: Waiting up to 15m0s for pod "adopt-release-hpmwp" in namespace "job-6080" to be "released"
    May  4 13:02:59.773: INFO: Pod "adopt-release-hpmwp": Phase="Running", Reason="", readiness=true. Elapsed: 8.597038ms
    May  4 13:03:01.778: INFO: Pod "adopt-release-hpmwp": Phase="Running", Reason="", readiness=true. Elapsed: 2.013808768s
    May  4 13:03:01.778: INFO: Pod "adopt-release-hpmwp" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    May  4 13:03:01.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-6080" for this suite. 05/04/23 13:03:01.786
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:03:01.796
May  4 13:03:01.796: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename emptydir 05/04/23 13:03:01.8
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:03:01.817
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:03:01.822
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
STEP: Creating a pod to test emptydir 0777 on tmpfs 05/04/23 13:03:01.827
May  4 13:03:01.837: INFO: Waiting up to 5m0s for pod "pod-cf373677-1503-4c29-9e50-6adbd87b6226" in namespace "emptydir-219" to be "Succeeded or Failed"
May  4 13:03:01.844: INFO: Pod "pod-cf373677-1503-4c29-9e50-6adbd87b6226": Phase="Pending", Reason="", readiness=false. Elapsed: 7.061815ms
May  4 13:03:03.849: INFO: Pod "pod-cf373677-1503-4c29-9e50-6adbd87b6226": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011726569s
May  4 13:03:05.852: INFO: Pod "pod-cf373677-1503-4c29-9e50-6adbd87b6226": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015289627s
STEP: Saw pod success 05/04/23 13:03:05.852
May  4 13:03:05.852: INFO: Pod "pod-cf373677-1503-4c29-9e50-6adbd87b6226" satisfied condition "Succeeded or Failed"
May  4 13:03:05.856: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-cf373677-1503-4c29-9e50-6adbd87b6226 container test-container: <nil>
STEP: delete the pod 05/04/23 13:03:05.878
May  4 13:03:05.906: INFO: Waiting for pod pod-cf373677-1503-4c29-9e50-6adbd87b6226 to disappear
May  4 13:03:05.911: INFO: Pod pod-cf373677-1503-4c29-9e50-6adbd87b6226 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May  4 13:03:05.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-219" for this suite. 05/04/23 13:03:05.926
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":280,"skipped":5185,"failed":0}
------------------------------
• [4.138 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:03:01.796
    May  4 13:03:01.796: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename emptydir 05/04/23 13:03:01.8
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:03:01.817
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:03:01.822
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:146
    STEP: Creating a pod to test emptydir 0777 on tmpfs 05/04/23 13:03:01.827
    May  4 13:03:01.837: INFO: Waiting up to 5m0s for pod "pod-cf373677-1503-4c29-9e50-6adbd87b6226" in namespace "emptydir-219" to be "Succeeded or Failed"
    May  4 13:03:01.844: INFO: Pod "pod-cf373677-1503-4c29-9e50-6adbd87b6226": Phase="Pending", Reason="", readiness=false. Elapsed: 7.061815ms
    May  4 13:03:03.849: INFO: Pod "pod-cf373677-1503-4c29-9e50-6adbd87b6226": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011726569s
    May  4 13:03:05.852: INFO: Pod "pod-cf373677-1503-4c29-9e50-6adbd87b6226": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015289627s
    STEP: Saw pod success 05/04/23 13:03:05.852
    May  4 13:03:05.852: INFO: Pod "pod-cf373677-1503-4c29-9e50-6adbd87b6226" satisfied condition "Succeeded or Failed"
    May  4 13:03:05.856: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-cf373677-1503-4c29-9e50-6adbd87b6226 container test-container: <nil>
    STEP: delete the pod 05/04/23 13:03:05.878
    May  4 13:03:05.906: INFO: Waiting for pod pod-cf373677-1503-4c29-9e50-6adbd87b6226 to disappear
    May  4 13:03:05.911: INFO: Pod pod-cf373677-1503-4c29-9e50-6adbd87b6226 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May  4 13:03:05.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-219" for this suite. 05/04/23 13:03:05.926
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:03:05.935
May  4 13:03:05.935: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename downward-api 05/04/23 13:03:05.936
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:03:05.954
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:03:05.957
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
STEP: Creating a pod to test downward api env vars 05/04/23 13:03:05.964
May  4 13:03:05.975: INFO: Waiting up to 5m0s for pod "downward-api-54d2817b-5d7f-4fc3-9f21-a482d2931e9b" in namespace "downward-api-3109" to be "Succeeded or Failed"
May  4 13:03:05.979: INFO: Pod "downward-api-54d2817b-5d7f-4fc3-9f21-a482d2931e9b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.837818ms
May  4 13:03:07.984: INFO: Pod "downward-api-54d2817b-5d7f-4fc3-9f21-a482d2931e9b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008363738s
May  4 13:03:09.986: INFO: Pod "downward-api-54d2817b-5d7f-4fc3-9f21-a482d2931e9b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010569878s
STEP: Saw pod success 05/04/23 13:03:09.986
May  4 13:03:09.986: INFO: Pod "downward-api-54d2817b-5d7f-4fc3-9f21-a482d2931e9b" satisfied condition "Succeeded or Failed"
May  4 13:03:09.991: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod downward-api-54d2817b-5d7f-4fc3-9f21-a482d2931e9b container dapi-container: <nil>
STEP: delete the pod 05/04/23 13:03:10.006
May  4 13:03:10.026: INFO: Waiting for pod downward-api-54d2817b-5d7f-4fc3-9f21-a482d2931e9b to disappear
May  4 13:03:10.031: INFO: Pod downward-api-54d2817b-5d7f-4fc3-9f21-a482d2931e9b no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
May  4 13:03:10.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3109" for this suite. 05/04/23 13:03:10.039
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","completed":281,"skipped":5206,"failed":0}
------------------------------
• [4.114 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:03:05.935
    May  4 13:03:05.935: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename downward-api 05/04/23 13:03:05.936
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:03:05.954
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:03:05.957
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:216
    STEP: Creating a pod to test downward api env vars 05/04/23 13:03:05.964
    May  4 13:03:05.975: INFO: Waiting up to 5m0s for pod "downward-api-54d2817b-5d7f-4fc3-9f21-a482d2931e9b" in namespace "downward-api-3109" to be "Succeeded or Failed"
    May  4 13:03:05.979: INFO: Pod "downward-api-54d2817b-5d7f-4fc3-9f21-a482d2931e9b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.837818ms
    May  4 13:03:07.984: INFO: Pod "downward-api-54d2817b-5d7f-4fc3-9f21-a482d2931e9b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008363738s
    May  4 13:03:09.986: INFO: Pod "downward-api-54d2817b-5d7f-4fc3-9f21-a482d2931e9b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010569878s
    STEP: Saw pod success 05/04/23 13:03:09.986
    May  4 13:03:09.986: INFO: Pod "downward-api-54d2817b-5d7f-4fc3-9f21-a482d2931e9b" satisfied condition "Succeeded or Failed"
    May  4 13:03:09.991: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod downward-api-54d2817b-5d7f-4fc3-9f21-a482d2931e9b container dapi-container: <nil>
    STEP: delete the pod 05/04/23 13:03:10.006
    May  4 13:03:10.026: INFO: Waiting for pod downward-api-54d2817b-5d7f-4fc3-9f21-a482d2931e9b to disappear
    May  4 13:03:10.031: INFO: Pod downward-api-54d2817b-5d7f-4fc3-9f21-a482d2931e9b no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    May  4 13:03:10.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-3109" for this suite. 05/04/23 13:03:10.039
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:03:10.049
May  4 13:03:10.049: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename security-context 05/04/23 13:03:10.05
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:03:10.076
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:03:10.079
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 05/04/23 13:03:10.084
May  4 13:03:10.100: INFO: Waiting up to 5m0s for pod "security-context-f6695325-fbc0-4eae-9eec-6f182e177e75" in namespace "security-context-2119" to be "Succeeded or Failed"
May  4 13:03:10.109: INFO: Pod "security-context-f6695325-fbc0-4eae-9eec-6f182e177e75": Phase="Pending", Reason="", readiness=false. Elapsed: 9.305173ms
May  4 13:03:12.114: INFO: Pod "security-context-f6695325-fbc0-4eae-9eec-6f182e177e75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01384503s
May  4 13:03:14.117: INFO: Pod "security-context-f6695325-fbc0-4eae-9eec-6f182e177e75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016655443s
STEP: Saw pod success 05/04/23 13:03:14.117
May  4 13:03:14.117: INFO: Pod "security-context-f6695325-fbc0-4eae-9eec-6f182e177e75" satisfied condition "Succeeded or Failed"
May  4 13:03:14.124: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod security-context-f6695325-fbc0-4eae-9eec-6f182e177e75 container test-container: <nil>
STEP: delete the pod 05/04/23 13:03:14.139
May  4 13:03:14.155: INFO: Waiting for pod security-context-f6695325-fbc0-4eae-9eec-6f182e177e75 to disappear
May  4 13:03:14.162: INFO: Pod security-context-f6695325-fbc0-4eae-9eec-6f182e177e75 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
May  4 13:03:14.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-2119" for this suite. 05/04/23 13:03:14.172
{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":282,"skipped":5209,"failed":0}
------------------------------
• [4.130 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:03:10.049
    May  4 13:03:10.049: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename security-context 05/04/23 13:03:10.05
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:03:10.076
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:03:10.079
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:132
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 05/04/23 13:03:10.084
    May  4 13:03:10.100: INFO: Waiting up to 5m0s for pod "security-context-f6695325-fbc0-4eae-9eec-6f182e177e75" in namespace "security-context-2119" to be "Succeeded or Failed"
    May  4 13:03:10.109: INFO: Pod "security-context-f6695325-fbc0-4eae-9eec-6f182e177e75": Phase="Pending", Reason="", readiness=false. Elapsed: 9.305173ms
    May  4 13:03:12.114: INFO: Pod "security-context-f6695325-fbc0-4eae-9eec-6f182e177e75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01384503s
    May  4 13:03:14.117: INFO: Pod "security-context-f6695325-fbc0-4eae-9eec-6f182e177e75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016655443s
    STEP: Saw pod success 05/04/23 13:03:14.117
    May  4 13:03:14.117: INFO: Pod "security-context-f6695325-fbc0-4eae-9eec-6f182e177e75" satisfied condition "Succeeded or Failed"
    May  4 13:03:14.124: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod security-context-f6695325-fbc0-4eae-9eec-6f182e177e75 container test-container: <nil>
    STEP: delete the pod 05/04/23 13:03:14.139
    May  4 13:03:14.155: INFO: Waiting for pod security-context-f6695325-fbc0-4eae-9eec-6f182e177e75 to disappear
    May  4 13:03:14.162: INFO: Pod security-context-f6695325-fbc0-4eae-9eec-6f182e177e75 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    May  4 13:03:14.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-2119" for this suite. 05/04/23 13:03:14.172
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:03:14.18
May  4 13:03:14.180: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename secrets 05/04/23 13:03:14.181
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:03:14.206
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:03:14.214
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
STEP: Creating secret with name secret-test-af6a9ed8-7929-4851-98c5-430004f6a6c3 05/04/23 13:03:14.26
STEP: Creating a pod to test consume secrets 05/04/23 13:03:14.277
May  4 13:03:14.293: INFO: Waiting up to 5m0s for pod "pod-secrets-fdd0c989-038a-4377-882d-ae02cb9dee8e" in namespace "secrets-106" to be "Succeeded or Failed"
May  4 13:03:14.301: INFO: Pod "pod-secrets-fdd0c989-038a-4377-882d-ae02cb9dee8e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.032872ms
May  4 13:03:16.311: INFO: Pod "pod-secrets-fdd0c989-038a-4377-882d-ae02cb9dee8e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017890808s
May  4 13:03:18.311: INFO: Pod "pod-secrets-fdd0c989-038a-4377-882d-ae02cb9dee8e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018379537s
STEP: Saw pod success 05/04/23 13:03:18.311
May  4 13:03:18.311: INFO: Pod "pod-secrets-fdd0c989-038a-4377-882d-ae02cb9dee8e" satisfied condition "Succeeded or Failed"
May  4 13:03:18.317: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-secrets-fdd0c989-038a-4377-882d-ae02cb9dee8e container secret-volume-test: <nil>
STEP: delete the pod 05/04/23 13:03:18.332
May  4 13:03:18.369: INFO: Waiting for pod pod-secrets-fdd0c989-038a-4377-882d-ae02cb9dee8e to disappear
May  4 13:03:18.373: INFO: Pod pod-secrets-fdd0c989-038a-4377-882d-ae02cb9dee8e no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
May  4 13:03:18.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-106" for this suite. 05/04/23 13:03:18.395
STEP: Destroying namespace "secret-namespace-8270" for this suite. 05/04/23 13:03:18.41
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","completed":283,"skipped":5211,"failed":0}
------------------------------
• [4.246 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:03:14.18
    May  4 13:03:14.180: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename secrets 05/04/23 13:03:14.181
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:03:14.206
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:03:14.214
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:98
    STEP: Creating secret with name secret-test-af6a9ed8-7929-4851-98c5-430004f6a6c3 05/04/23 13:03:14.26
    STEP: Creating a pod to test consume secrets 05/04/23 13:03:14.277
    May  4 13:03:14.293: INFO: Waiting up to 5m0s for pod "pod-secrets-fdd0c989-038a-4377-882d-ae02cb9dee8e" in namespace "secrets-106" to be "Succeeded or Failed"
    May  4 13:03:14.301: INFO: Pod "pod-secrets-fdd0c989-038a-4377-882d-ae02cb9dee8e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.032872ms
    May  4 13:03:16.311: INFO: Pod "pod-secrets-fdd0c989-038a-4377-882d-ae02cb9dee8e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017890808s
    May  4 13:03:18.311: INFO: Pod "pod-secrets-fdd0c989-038a-4377-882d-ae02cb9dee8e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018379537s
    STEP: Saw pod success 05/04/23 13:03:18.311
    May  4 13:03:18.311: INFO: Pod "pod-secrets-fdd0c989-038a-4377-882d-ae02cb9dee8e" satisfied condition "Succeeded or Failed"
    May  4 13:03:18.317: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-secrets-fdd0c989-038a-4377-882d-ae02cb9dee8e container secret-volume-test: <nil>
    STEP: delete the pod 05/04/23 13:03:18.332
    May  4 13:03:18.369: INFO: Waiting for pod pod-secrets-fdd0c989-038a-4377-882d-ae02cb9dee8e to disappear
    May  4 13:03:18.373: INFO: Pod pod-secrets-fdd0c989-038a-4377-882d-ae02cb9dee8e no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    May  4 13:03:18.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-106" for this suite. 05/04/23 13:03:18.395
    STEP: Destroying namespace "secret-namespace-8270" for this suite. 05/04/23 13:03:18.41
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:03:18.428
May  4 13:03:18.429: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename sched-pred 05/04/23 13:03:18.43
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:03:18.466
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:03:18.47
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
May  4 13:03:18.475: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May  4 13:03:18.493: INFO: Waiting for terminating namespaces to be deleted...
May  4 13:03:18.503: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-189.us-west-2.compute.internal before test
May  4 13:03:18.590: INFO: calico-node-r8wl6 from kube-system started at 2023-05-04 11:14:50 +0000 UTC (1 container statuses recorded)
May  4 13:03:18.590: INFO: 	Container calico-node ready: true, restart count 0
May  4 13:03:18.590: INFO: calico-typha-85dbcd447f-6tg8g from kube-system started at 2023-05-04 11:15:25 +0000 UTC (1 container statuses recorded)
May  4 13:03:18.590: INFO: 	Container calico-typha ready: true, restart count 0
May  4 13:03:18.590: INFO: metrics-server-57bbf8548c-qkgc6 from kube-system started at 2023-05-04 11:16:29 +0000 UTC (1 container statuses recorded)
May  4 13:03:18.590: INFO: 	Container metrics-server ready: true, restart count 0
May  4 13:03:18.590: INFO: kube-state-metrics-5bf549bd69-djfll from pf9-monitoring started at 2023-05-04 12:56:25 +0000 UTC (1 container statuses recorded)
May  4 13:03:18.590: INFO: 	Container kube-state-metrics ready: true, restart count 0
May  4 13:03:18.590: INFO: node-exporter-tsxz4 from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
May  4 13:03:18.590: INFO: 	Container node-exporter ready: true, restart count 0
May  4 13:03:18.590: INFO: prometheus-operator-85498c86bb-gdnsq from pf9-operators started at 2023-05-04 11:16:25 +0000 UTC (1 container statuses recorded)
May  4 13:03:18.590: INFO: 	Container prometheus-operator ready: true, restart count 0
May  4 13:03:18.590: INFO: sonobuoy-e2e-job-a8c15777d8b94166 from sonobuoy started at 2023-05-04 11:44:14 +0000 UTC (2 container statuses recorded)
May  4 13:03:18.590: INFO: 	Container e2e ready: true, restart count 0
May  4 13:03:18.590: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  4 13:03:18.590: INFO: sonobuoy-systemd-logs-daemon-set-d34856018c814171-f2qjv from sonobuoy started at 2023-05-04 11:44:14 +0000 UTC (2 container statuses recorded)
May  4 13:03:18.590: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  4 13:03:18.590: INFO: 	Container systemd-logs ready: true, restart count 0
May  4 13:03:18.591: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-216.us-west-2.compute.internal before test
May  4 13:03:18.637: INFO: calico-node-ll8r2 from kube-system started at 2023-05-04 11:15:13 +0000 UTC (1 container statuses recorded)
May  4 13:03:18.637: INFO: 	Container calico-node ready: true, restart count 0
May  4 13:03:18.637: INFO: calico-typha-85dbcd447f-qw96l from kube-system started at 2023-05-04 11:15:29 +0000 UTC (1 container statuses recorded)
May  4 13:03:18.637: INFO: 	Container calico-typha ready: true, restart count 0
May  4 13:03:18.637: INFO: coredns-c7944df6b-z54gp from kube-system started at 2023-05-04 12:56:25 +0000 UTC (1 container statuses recorded)
May  4 13:03:18.637: INFO: 	Container coredns ready: true, restart count 0
May  4 13:03:18.637: INFO: grafana-84c9c8d6bd-rsbn8 from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (2 container statuses recorded)
May  4 13:03:18.637: INFO: 	Container grafana ready: true, restart count 0
May  4 13:03:18.637: INFO: 	Container proxy ready: true, restart count 0
May  4 13:03:18.637: INFO: node-exporter-pnw8x from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
May  4 13:03:18.637: INFO: 	Container node-exporter ready: true, restart count 0
May  4 13:03:18.637: INFO: sonobuoy from sonobuoy started at 2023-05-04 11:44:11 +0000 UTC (1 container statuses recorded)
May  4 13:03:18.637: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May  4 13:03:18.637: INFO: sonobuoy-systemd-logs-daemon-set-d34856018c814171-44cxg from sonobuoy started at 2023-05-04 11:44:14 +0000 UTC (2 container statuses recorded)
May  4 13:03:18.637: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  4 13:03:18.637: INFO: 	Container systemd-logs ready: true, restart count 0
May  4 13:03:18.637: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-224.us-west-2.compute.internal before test
May  4 13:03:18.687: INFO: adopt-release-bvsmp from job-6080 started at 2023-05-04 13:03:00 +0000 UTC (1 container statuses recorded)
May  4 13:03:18.687: INFO: 	Container c ready: true, restart count 0
May  4 13:03:18.687: INFO: adopt-release-hpmwp from job-6080 started at 2023-05-04 13:02:54 +0000 UTC (1 container statuses recorded)
May  4 13:03:18.687: INFO: 	Container c ready: true, restart count 0
May  4 13:03:18.687: INFO: adopt-release-l44qq from job-6080 started at 2023-05-04 13:02:54 +0000 UTC (1 container statuses recorded)
May  4 13:03:18.687: INFO: 	Container c ready: true, restart count 0
May  4 13:03:18.687: INFO: calico-node-bb7nh from kube-system started at 2023-05-04 11:14:53 +0000 UTC (1 container statuses recorded)
May  4 13:03:18.687: INFO: 	Container calico-node ready: true, restart count 0
May  4 13:03:18.687: INFO: node-exporter-qn2kc from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
May  4 13:03:18.687: INFO: 	Container node-exporter ready: true, restart count 0
May  4 13:03:18.687: INFO: sonobuoy-systemd-logs-daemon-set-d34856018c814171-lss27 from sonobuoy started at 2023-05-04 11:44:14 +0000 UTC (2 container statuses recorded)
May  4 13:03:18.687: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  4 13:03:18.687: INFO: 	Container systemd-logs ready: true, restart count 0
May  4 13:03:18.687: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-232.us-west-2.compute.internal before test
May  4 13:03:18.779: INFO: calico-kube-controllers-654d9ff976-ddwnz from kube-system started at 2023-05-04 11:15:10 +0000 UTC (1 container statuses recorded)
May  4 13:03:18.779: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May  4 13:03:18.779: INFO: calico-node-48gv4 from kube-system started at 2023-05-04 11:14:51 +0000 UTC (1 container statuses recorded)
May  4 13:03:18.779: INFO: 	Container calico-node ready: true, restart count 0
May  4 13:03:18.779: INFO: calico-typha-85dbcd447f-2s7df from kube-system started at 2023-05-04 11:15:10 +0000 UTC (1 container statuses recorded)
May  4 13:03:18.779: INFO: 	Container calico-typha ready: true, restart count 0
May  4 13:03:18.779: INFO: calico-typha-autoscaler-795696b9bd-qhx26 from kube-system started at 2023-05-04 11:15:10 +0000 UTC (1 container statuses recorded)
May  4 13:03:18.779: INFO: 	Container autoscaler ready: true, restart count 0
May  4 13:03:18.779: INFO: kube-dns-autoscaler-558cdf6846-nt4qj from kube-system started at 2023-05-04 12:56:25 +0000 UTC (1 container statuses recorded)
May  4 13:03:18.779: INFO: 	Container autoscaler ready: true, restart count 0
May  4 13:03:18.779: INFO: kube-state-metrics-857f6bcbbb-krrw9 from kube-system started at 2023-05-04 11:15:10 +0000 UTC (1 container statuses recorded)
May  4 13:03:18.779: INFO: 	Container kube-state-metrics ready: true, restart count 0
May  4 13:03:18.779: INFO: dashboard-metrics-scraper-7564894f4b-nvnb7 from kubernetes-dashboard started at 2023-05-04 12:56:25 +0000 UTC (1 container statuses recorded)
May  4 13:03:18.779: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
May  4 13:03:18.779: INFO: alertmanager-sysalert-0 from pf9-monitoring started at 2023-05-04 11:16:29 +0000 UTC (2 container statuses recorded)
May  4 13:03:18.779: INFO: 	Container alertmanager ready: true, restart count 1
May  4 13:03:18.779: INFO: 	Container config-reloader ready: true, restart count 0
May  4 13:03:18.779: INFO: node-exporter-49wcf from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
May  4 13:03:18.779: INFO: 	Container node-exporter ready: true, restart count 0
May  4 13:03:18.779: INFO: sonobuoy-systemd-logs-daemon-set-d34856018c814171-thbzc from sonobuoy started at 2023-05-04 11:44:14 +0000 UTC (2 container statuses recorded)
May  4 13:03:18.779: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  4 13:03:18.779: INFO: 	Container systemd-logs ready: true, restart count 0
May  4 13:03:18.779: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-253.us-west-2.compute.internal before test
May  4 13:03:18.833: INFO: calico-node-69h7w from kube-system started at 2023-05-04 11:14:49 +0000 UTC (1 container statuses recorded)
May  4 13:03:18.833: INFO: 	Container calico-node ready: true, restart count 0
May  4 13:03:18.833: INFO: coredns-c7944df6b-zhglb from kube-system started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
May  4 13:03:18.833: INFO: 	Container coredns ready: true, restart count 0
May  4 13:03:18.833: INFO: node-exporter-q8qnn from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
May  4 13:03:18.833: INFO: 	Container node-exporter ready: true, restart count 0
May  4 13:03:18.833: INFO: prometheus-system-0 from pf9-monitoring started at 2023-05-04 11:16:30 +0000 UTC (2 container statuses recorded)
May  4 13:03:18.833: INFO: 	Container config-reloader ready: true, restart count 0
May  4 13:03:18.833: INFO: 	Container prometheus ready: true, restart count 0
May  4 13:03:18.833: INFO: monhelper-57744bf759-6nndz from pf9-operators started at 2023-05-04 11:16:25 +0000 UTC (1 container statuses recorded)
May  4 13:03:18.833: INFO: 	Container monhelper ready: true, restart count 0
May  4 13:03:18.833: INFO: sonobuoy-systemd-logs-daemon-set-d34856018c814171-rx5lv from sonobuoy started at 2023-05-04 11:44:14 +0000 UTC (2 container statuses recorded)
May  4 13:03:18.833: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  4 13:03:18.833: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
STEP: verifying the node has the label node ip-10-0-1-189.us-west-2.compute.internal 05/04/23 13:03:18.944
STEP: verifying the node has the label node ip-10-0-1-216.us-west-2.compute.internal 05/04/23 13:03:18.983
STEP: verifying the node has the label node ip-10-0-1-224.us-west-2.compute.internal 05/04/23 13:03:19.049
STEP: verifying the node has the label node ip-10-0-1-232.us-west-2.compute.internal 05/04/23 13:03:19.074
STEP: verifying the node has the label node ip-10-0-1-253.us-west-2.compute.internal 05/04/23 13:03:19.094
May  4 13:03:19.138: INFO: Pod adopt-release-bvsmp requesting resource cpu=0m on Node ip-10-0-1-224.us-west-2.compute.internal
May  4 13:03:19.138: INFO: Pod adopt-release-hpmwp requesting resource cpu=0m on Node ip-10-0-1-224.us-west-2.compute.internal
May  4 13:03:19.138: INFO: Pod adopt-release-l44qq requesting resource cpu=0m on Node ip-10-0-1-224.us-west-2.compute.internal
May  4 13:03:19.138: INFO: Pod calico-kube-controllers-654d9ff976-ddwnz requesting resource cpu=1m on Node ip-10-0-1-232.us-west-2.compute.internal
May  4 13:03:19.138: INFO: Pod calico-node-48gv4 requesting resource cpu=250m on Node ip-10-0-1-232.us-west-2.compute.internal
May  4 13:03:19.138: INFO: Pod calico-node-69h7w requesting resource cpu=250m on Node ip-10-0-1-253.us-west-2.compute.internal
May  4 13:03:19.138: INFO: Pod calico-node-bb7nh requesting resource cpu=250m on Node ip-10-0-1-224.us-west-2.compute.internal
May  4 13:03:19.138: INFO: Pod calico-node-ll8r2 requesting resource cpu=250m on Node ip-10-0-1-216.us-west-2.compute.internal
May  4 13:03:19.138: INFO: Pod calico-node-r8wl6 requesting resource cpu=250m on Node ip-10-0-1-189.us-west-2.compute.internal
May  4 13:03:19.138: INFO: Pod calico-typha-85dbcd447f-2s7df requesting resource cpu=10m on Node ip-10-0-1-232.us-west-2.compute.internal
May  4 13:03:19.138: INFO: Pod calico-typha-85dbcd447f-6tg8g requesting resource cpu=10m on Node ip-10-0-1-189.us-west-2.compute.internal
May  4 13:03:19.138: INFO: Pod calico-typha-85dbcd447f-qw96l requesting resource cpu=10m on Node ip-10-0-1-216.us-west-2.compute.internal
May  4 13:03:19.138: INFO: Pod calico-typha-autoscaler-795696b9bd-qhx26 requesting resource cpu=20m on Node ip-10-0-1-232.us-west-2.compute.internal
May  4 13:03:19.138: INFO: Pod coredns-c7944df6b-z54gp requesting resource cpu=100m on Node ip-10-0-1-216.us-west-2.compute.internal
May  4 13:03:19.138: INFO: Pod coredns-c7944df6b-zhglb requesting resource cpu=100m on Node ip-10-0-1-253.us-west-2.compute.internal
May  4 13:03:19.138: INFO: Pod kube-dns-autoscaler-558cdf6846-nt4qj requesting resource cpu=20m on Node ip-10-0-1-232.us-west-2.compute.internal
May  4 13:03:19.138: INFO: Pod kube-state-metrics-857f6bcbbb-krrw9 requesting resource cpu=0m on Node ip-10-0-1-232.us-west-2.compute.internal
May  4 13:03:19.138: INFO: Pod metrics-server-57bbf8548c-qkgc6 requesting resource cpu=100m on Node ip-10-0-1-189.us-west-2.compute.internal
May  4 13:03:19.138: INFO: Pod dashboard-metrics-scraper-7564894f4b-nvnb7 requesting resource cpu=0m on Node ip-10-0-1-232.us-west-2.compute.internal
May  4 13:03:19.138: INFO: Pod alertmanager-sysalert-0 requesting resource cpu=200m on Node ip-10-0-1-232.us-west-2.compute.internal
May  4 13:03:19.138: INFO: Pod grafana-84c9c8d6bd-rsbn8 requesting resource cpu=100m on Node ip-10-0-1-216.us-west-2.compute.internal
May  4 13:03:19.138: INFO: Pod kube-state-metrics-5bf549bd69-djfll requesting resource cpu=100m on Node ip-10-0-1-189.us-west-2.compute.internal
May  4 13:03:19.138: INFO: Pod node-exporter-49wcf requesting resource cpu=102m on Node ip-10-0-1-232.us-west-2.compute.internal
May  4 13:03:19.138: INFO: Pod node-exporter-pnw8x requesting resource cpu=102m on Node ip-10-0-1-216.us-west-2.compute.internal
May  4 13:03:19.138: INFO: Pod node-exporter-q8qnn requesting resource cpu=102m on Node ip-10-0-1-253.us-west-2.compute.internal
May  4 13:03:19.138: INFO: Pod node-exporter-qn2kc requesting resource cpu=102m on Node ip-10-0-1-224.us-west-2.compute.internal
May  4 13:03:19.138: INFO: Pod node-exporter-tsxz4 requesting resource cpu=102m on Node ip-10-0-1-189.us-west-2.compute.internal
May  4 13:03:19.138: INFO: Pod prometheus-system-0 requesting resource cpu=600m on Node ip-10-0-1-253.us-west-2.compute.internal
May  4 13:03:19.138: INFO: Pod monhelper-57744bf759-6nndz requesting resource cpu=50m on Node ip-10-0-1-253.us-west-2.compute.internal
May  4 13:03:19.138: INFO: Pod prometheus-operator-85498c86bb-gdnsq requesting resource cpu=100m on Node ip-10-0-1-189.us-west-2.compute.internal
May  4 13:03:19.138: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-10-0-1-216.us-west-2.compute.internal
May  4 13:03:19.138: INFO: Pod sonobuoy-e2e-job-a8c15777d8b94166 requesting resource cpu=0m on Node ip-10-0-1-189.us-west-2.compute.internal
May  4 13:03:19.138: INFO: Pod sonobuoy-systemd-logs-daemon-set-d34856018c814171-44cxg requesting resource cpu=0m on Node ip-10-0-1-216.us-west-2.compute.internal
May  4 13:03:19.138: INFO: Pod sonobuoy-systemd-logs-daemon-set-d34856018c814171-f2qjv requesting resource cpu=0m on Node ip-10-0-1-189.us-west-2.compute.internal
May  4 13:03:19.138: INFO: Pod sonobuoy-systemd-logs-daemon-set-d34856018c814171-lss27 requesting resource cpu=0m on Node ip-10-0-1-224.us-west-2.compute.internal
May  4 13:03:19.138: INFO: Pod sonobuoy-systemd-logs-daemon-set-d34856018c814171-rx5lv requesting resource cpu=0m on Node ip-10-0-1-253.us-west-2.compute.internal
May  4 13:03:19.138: INFO: Pod sonobuoy-systemd-logs-daemon-set-d34856018c814171-thbzc requesting resource cpu=0m on Node ip-10-0-1-232.us-west-2.compute.internal
STEP: Starting Pods to consume most of the cluster CPU. 05/04/23 13:03:19.138
May  4 13:03:19.138: INFO: Creating a pod which consumes cpu=2336m on Node ip-10-0-1-189.us-west-2.compute.internal
May  4 13:03:19.153: INFO: Creating a pod which consumes cpu=2406m on Node ip-10-0-1-216.us-west-2.compute.internal
May  4 13:03:19.177: INFO: Creating a pod which consumes cpu=2553m on Node ip-10-0-1-224.us-west-2.compute.internal
May  4 13:03:19.190: INFO: Creating a pod which consumes cpu=2377m on Node ip-10-0-1-232.us-west-2.compute.internal
May  4 13:03:19.208: INFO: Creating a pod which consumes cpu=2028m on Node ip-10-0-1-253.us-west-2.compute.internal
May  4 13:03:19.246: INFO: Waiting up to 5m0s for pod "filler-pod-393b471c-9025-41db-9e35-f7cd8d93a4f0" in namespace "sched-pred-2698" to be "running"
May  4 13:03:19.285: INFO: Pod "filler-pod-393b471c-9025-41db-9e35-f7cd8d93a4f0": Phase="Pending", Reason="", readiness=false. Elapsed: 38.508393ms
May  4 13:03:21.297: INFO: Pod "filler-pod-393b471c-9025-41db-9e35-f7cd8d93a4f0": Phase="Running", Reason="", readiness=true. Elapsed: 2.050477374s
May  4 13:03:21.297: INFO: Pod "filler-pod-393b471c-9025-41db-9e35-f7cd8d93a4f0" satisfied condition "running"
May  4 13:03:21.297: INFO: Waiting up to 5m0s for pod "filler-pod-f4c05e9c-2afb-4903-abcf-295114adc6dc" in namespace "sched-pred-2698" to be "running"
May  4 13:03:21.307: INFO: Pod "filler-pod-f4c05e9c-2afb-4903-abcf-295114adc6dc": Phase="Running", Reason="", readiness=true. Elapsed: 10.455919ms
May  4 13:03:21.307: INFO: Pod "filler-pod-f4c05e9c-2afb-4903-abcf-295114adc6dc" satisfied condition "running"
May  4 13:03:21.308: INFO: Waiting up to 5m0s for pod "filler-pod-81b7aaeb-c4e1-44d5-adb6-9c8b7b8204e2" in namespace "sched-pred-2698" to be "running"
May  4 13:03:21.314: INFO: Pod "filler-pod-81b7aaeb-c4e1-44d5-adb6-9c8b7b8204e2": Phase="Running", Reason="", readiness=true. Elapsed: 6.751564ms
May  4 13:03:21.314: INFO: Pod "filler-pod-81b7aaeb-c4e1-44d5-adb6-9c8b7b8204e2" satisfied condition "running"
May  4 13:03:21.315: INFO: Waiting up to 5m0s for pod "filler-pod-3307ff45-2d19-4983-a6f0-f9e1bf4221f2" in namespace "sched-pred-2698" to be "running"
May  4 13:03:21.325: INFO: Pod "filler-pod-3307ff45-2d19-4983-a6f0-f9e1bf4221f2": Phase="Running", Reason="", readiness=true. Elapsed: 10.200713ms
May  4 13:03:21.325: INFO: Pod "filler-pod-3307ff45-2d19-4983-a6f0-f9e1bf4221f2" satisfied condition "running"
May  4 13:03:21.325: INFO: Waiting up to 5m0s for pod "filler-pod-b4358935-f3ed-4fab-b6db-60332e8c5371" in namespace "sched-pred-2698" to be "running"
May  4 13:03:21.333: INFO: Pod "filler-pod-b4358935-f3ed-4fab-b6db-60332e8c5371": Phase="Running", Reason="", readiness=true. Elapsed: 7.542107ms
May  4 13:03:21.333: INFO: Pod "filler-pod-b4358935-f3ed-4fab-b6db-60332e8c5371" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 05/04/23 13:03:21.333
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3307ff45-2d19-4983-a6f0-f9e1bf4221f2.175bf26480d711c6], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2698/filler-pod-3307ff45-2d19-4983-a6f0-f9e1bf4221f2 to ip-10-0-1-232.us-west-2.compute.internal] 05/04/23 13:03:21.345
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3307ff45-2d19-4983-a6f0-f9e1bf4221f2.175bf264b9fbf1f4], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 05/04/23 13:03:21.345
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3307ff45-2d19-4983-a6f0-f9e1bf4221f2.175bf264bb4dbd73], Reason = [Created], Message = [Created container filler-pod-3307ff45-2d19-4983-a6f0-f9e1bf4221f2] 05/04/23 13:03:21.346
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3307ff45-2d19-4983-a6f0-f9e1bf4221f2.175bf264c1a62058], Reason = [Started], Message = [Started container filler-pod-3307ff45-2d19-4983-a6f0-f9e1bf4221f2] 05/04/23 13:03:21.346
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-393b471c-9025-41db-9e35-f7cd8d93a4f0.175bf2647cee0c20], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2698/filler-pod-393b471c-9025-41db-9e35-f7cd8d93a4f0 to ip-10-0-1-189.us-west-2.compute.internal] 05/04/23 13:03:21.346
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-393b471c-9025-41db-9e35-f7cd8d93a4f0.175bf264a841587c], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 05/04/23 13:03:21.346
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-393b471c-9025-41db-9e35-f7cd8d93a4f0.175bf264a981daa7], Reason = [Created], Message = [Created container filler-pod-393b471c-9025-41db-9e35-f7cd8d93a4f0] 05/04/23 13:03:21.346
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-393b471c-9025-41db-9e35-f7cd8d93a4f0.175bf264b0bb9d33], Reason = [Started], Message = [Started container filler-pod-393b471c-9025-41db-9e35-f7cd8d93a4f0] 05/04/23 13:03:21.346
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-81b7aaeb-c4e1-44d5-adb6-9c8b7b8204e2.175bf2647f328d4a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2698/filler-pod-81b7aaeb-c4e1-44d5-adb6-9c8b7b8204e2 to ip-10-0-1-224.us-west-2.compute.internal] 05/04/23 13:03:21.346
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-81b7aaeb-c4e1-44d5-adb6-9c8b7b8204e2.175bf264ac68bba1], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 05/04/23 13:03:21.346
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-81b7aaeb-c4e1-44d5-adb6-9c8b7b8204e2.175bf264ad87f2c1], Reason = [Created], Message = [Created container filler-pod-81b7aaeb-c4e1-44d5-adb6-9c8b7b8204e2] 05/04/23 13:03:21.346
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-81b7aaeb-c4e1-44d5-adb6-9c8b7b8204e2.175bf264b42261fe], Reason = [Started], Message = [Started container filler-pod-81b7aaeb-c4e1-44d5-adb6-9c8b7b8204e2] 05/04/23 13:03:21.346
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b4358935-f3ed-4fab-b6db-60332e8c5371.175bf264819f53e7], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2698/filler-pod-b4358935-f3ed-4fab-b6db-60332e8c5371 to ip-10-0-1-253.us-west-2.compute.internal] 05/04/23 13:03:21.346
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b4358935-f3ed-4fab-b6db-60332e8c5371.175bf264aefdbde8], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 05/04/23 13:03:21.346
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b4358935-f3ed-4fab-b6db-60332e8c5371.175bf264b019f7da], Reason = [Created], Message = [Created container filler-pod-b4358935-f3ed-4fab-b6db-60332e8c5371] 05/04/23 13:03:21.346
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b4358935-f3ed-4fab-b6db-60332e8c5371.175bf264b66cf410], Reason = [Started], Message = [Started container filler-pod-b4358935-f3ed-4fab-b6db-60332e8c5371] 05/04/23 13:03:21.347
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f4c05e9c-2afb-4903-abcf-295114adc6dc.175bf2647dd8c933], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2698/filler-pod-f4c05e9c-2afb-4903-abcf-295114adc6dc to ip-10-0-1-216.us-west-2.compute.internal] 05/04/23 13:03:21.347
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f4c05e9c-2afb-4903-abcf-295114adc6dc.175bf264ac2a198d], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 05/04/23 13:03:21.347
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f4c05e9c-2afb-4903-abcf-295114adc6dc.175bf264ae456ac6], Reason = [Created], Message = [Created container filler-pod-f4c05e9c-2afb-4903-abcf-295114adc6dc] 05/04/23 13:03:21.347
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f4c05e9c-2afb-4903-abcf-295114adc6dc.175bf264b624c16a], Reason = [Started], Message = [Started container filler-pod-f4c05e9c-2afb-4903-abcf-295114adc6dc] 05/04/23 13:03:21.347
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.175bf264ff998753], Reason = [FailedScheduling], Message = [0/8 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/master: true}, 5 Insufficient cpu. preemption: 0/8 nodes are available: 3 Preemption is not helpful for scheduling, 5 No preemption victims found for incoming pod.] 05/04/23 13:03:21.385
STEP: removing the label node off the node ip-10-0-1-189.us-west-2.compute.internal 05/04/23 13:03:22.373
STEP: verifying the node doesn't have the label node 05/04/23 13:03:22.407
STEP: removing the label node off the node ip-10-0-1-216.us-west-2.compute.internal 05/04/23 13:03:22.418
STEP: verifying the node doesn't have the label node 05/04/23 13:03:22.461
STEP: removing the label node off the node ip-10-0-1-224.us-west-2.compute.internal 05/04/23 13:03:22.476
STEP: verifying the node doesn't have the label node 05/04/23 13:03:22.511
STEP: removing the label node off the node ip-10-0-1-232.us-west-2.compute.internal 05/04/23 13:03:22.523
STEP: verifying the node doesn't have the label node 05/04/23 13:03:22.568
STEP: removing the label node off the node ip-10-0-1-253.us-west-2.compute.internal 05/04/23 13:03:22.58
STEP: verifying the node doesn't have the label node 05/04/23 13:03:22.599
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
May  4 13:03:22.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2698" for this suite. 05/04/23 13:03:22.63
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","completed":284,"skipped":5260,"failed":0}
------------------------------
• [4.224 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:03:18.428
    May  4 13:03:18.429: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename sched-pred 05/04/23 13:03:18.43
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:03:18.466
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:03:18.47
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    May  4 13:03:18.475: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    May  4 13:03:18.493: INFO: Waiting for terminating namespaces to be deleted...
    May  4 13:03:18.503: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-1-189.us-west-2.compute.internal before test
    May  4 13:03:18.590: INFO: calico-node-r8wl6 from kube-system started at 2023-05-04 11:14:50 +0000 UTC (1 container statuses recorded)
    May  4 13:03:18.590: INFO: 	Container calico-node ready: true, restart count 0
    May  4 13:03:18.590: INFO: calico-typha-85dbcd447f-6tg8g from kube-system started at 2023-05-04 11:15:25 +0000 UTC (1 container statuses recorded)
    May  4 13:03:18.590: INFO: 	Container calico-typha ready: true, restart count 0
    May  4 13:03:18.590: INFO: metrics-server-57bbf8548c-qkgc6 from kube-system started at 2023-05-04 11:16:29 +0000 UTC (1 container statuses recorded)
    May  4 13:03:18.590: INFO: 	Container metrics-server ready: true, restart count 0
    May  4 13:03:18.590: INFO: kube-state-metrics-5bf549bd69-djfll from pf9-monitoring started at 2023-05-04 12:56:25 +0000 UTC (1 container statuses recorded)
    May  4 13:03:18.590: INFO: 	Container kube-state-metrics ready: true, restart count 0
    May  4 13:03:18.590: INFO: node-exporter-tsxz4 from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
    May  4 13:03:18.590: INFO: 	Container node-exporter ready: true, restart count 0
    May  4 13:03:18.590: INFO: prometheus-operator-85498c86bb-gdnsq from pf9-operators started at 2023-05-04 11:16:25 +0000 UTC (1 container statuses recorded)
    May  4 13:03:18.590: INFO: 	Container prometheus-operator ready: true, restart count 0
    May  4 13:03:18.590: INFO: sonobuoy-e2e-job-a8c15777d8b94166 from sonobuoy started at 2023-05-04 11:44:14 +0000 UTC (2 container statuses recorded)
    May  4 13:03:18.590: INFO: 	Container e2e ready: true, restart count 0
    May  4 13:03:18.590: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  4 13:03:18.590: INFO: sonobuoy-systemd-logs-daemon-set-d34856018c814171-f2qjv from sonobuoy started at 2023-05-04 11:44:14 +0000 UTC (2 container statuses recorded)
    May  4 13:03:18.590: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  4 13:03:18.590: INFO: 	Container systemd-logs ready: true, restart count 0
    May  4 13:03:18.591: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-1-216.us-west-2.compute.internal before test
    May  4 13:03:18.637: INFO: calico-node-ll8r2 from kube-system started at 2023-05-04 11:15:13 +0000 UTC (1 container statuses recorded)
    May  4 13:03:18.637: INFO: 	Container calico-node ready: true, restart count 0
    May  4 13:03:18.637: INFO: calico-typha-85dbcd447f-qw96l from kube-system started at 2023-05-04 11:15:29 +0000 UTC (1 container statuses recorded)
    May  4 13:03:18.637: INFO: 	Container calico-typha ready: true, restart count 0
    May  4 13:03:18.637: INFO: coredns-c7944df6b-z54gp from kube-system started at 2023-05-04 12:56:25 +0000 UTC (1 container statuses recorded)
    May  4 13:03:18.637: INFO: 	Container coredns ready: true, restart count 0
    May  4 13:03:18.637: INFO: grafana-84c9c8d6bd-rsbn8 from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (2 container statuses recorded)
    May  4 13:03:18.637: INFO: 	Container grafana ready: true, restart count 0
    May  4 13:03:18.637: INFO: 	Container proxy ready: true, restart count 0
    May  4 13:03:18.637: INFO: node-exporter-pnw8x from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
    May  4 13:03:18.637: INFO: 	Container node-exporter ready: true, restart count 0
    May  4 13:03:18.637: INFO: sonobuoy from sonobuoy started at 2023-05-04 11:44:11 +0000 UTC (1 container statuses recorded)
    May  4 13:03:18.637: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    May  4 13:03:18.637: INFO: sonobuoy-systemd-logs-daemon-set-d34856018c814171-44cxg from sonobuoy started at 2023-05-04 11:44:14 +0000 UTC (2 container statuses recorded)
    May  4 13:03:18.637: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  4 13:03:18.637: INFO: 	Container systemd-logs ready: true, restart count 0
    May  4 13:03:18.637: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-1-224.us-west-2.compute.internal before test
    May  4 13:03:18.687: INFO: adopt-release-bvsmp from job-6080 started at 2023-05-04 13:03:00 +0000 UTC (1 container statuses recorded)
    May  4 13:03:18.687: INFO: 	Container c ready: true, restart count 0
    May  4 13:03:18.687: INFO: adopt-release-hpmwp from job-6080 started at 2023-05-04 13:02:54 +0000 UTC (1 container statuses recorded)
    May  4 13:03:18.687: INFO: 	Container c ready: true, restart count 0
    May  4 13:03:18.687: INFO: adopt-release-l44qq from job-6080 started at 2023-05-04 13:02:54 +0000 UTC (1 container statuses recorded)
    May  4 13:03:18.687: INFO: 	Container c ready: true, restart count 0
    May  4 13:03:18.687: INFO: calico-node-bb7nh from kube-system started at 2023-05-04 11:14:53 +0000 UTC (1 container statuses recorded)
    May  4 13:03:18.687: INFO: 	Container calico-node ready: true, restart count 0
    May  4 13:03:18.687: INFO: node-exporter-qn2kc from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
    May  4 13:03:18.687: INFO: 	Container node-exporter ready: true, restart count 0
    May  4 13:03:18.687: INFO: sonobuoy-systemd-logs-daemon-set-d34856018c814171-lss27 from sonobuoy started at 2023-05-04 11:44:14 +0000 UTC (2 container statuses recorded)
    May  4 13:03:18.687: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  4 13:03:18.687: INFO: 	Container systemd-logs ready: true, restart count 0
    May  4 13:03:18.687: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-1-232.us-west-2.compute.internal before test
    May  4 13:03:18.779: INFO: calico-kube-controllers-654d9ff976-ddwnz from kube-system started at 2023-05-04 11:15:10 +0000 UTC (1 container statuses recorded)
    May  4 13:03:18.779: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    May  4 13:03:18.779: INFO: calico-node-48gv4 from kube-system started at 2023-05-04 11:14:51 +0000 UTC (1 container statuses recorded)
    May  4 13:03:18.779: INFO: 	Container calico-node ready: true, restart count 0
    May  4 13:03:18.779: INFO: calico-typha-85dbcd447f-2s7df from kube-system started at 2023-05-04 11:15:10 +0000 UTC (1 container statuses recorded)
    May  4 13:03:18.779: INFO: 	Container calico-typha ready: true, restart count 0
    May  4 13:03:18.779: INFO: calico-typha-autoscaler-795696b9bd-qhx26 from kube-system started at 2023-05-04 11:15:10 +0000 UTC (1 container statuses recorded)
    May  4 13:03:18.779: INFO: 	Container autoscaler ready: true, restart count 0
    May  4 13:03:18.779: INFO: kube-dns-autoscaler-558cdf6846-nt4qj from kube-system started at 2023-05-04 12:56:25 +0000 UTC (1 container statuses recorded)
    May  4 13:03:18.779: INFO: 	Container autoscaler ready: true, restart count 0
    May  4 13:03:18.779: INFO: kube-state-metrics-857f6bcbbb-krrw9 from kube-system started at 2023-05-04 11:15:10 +0000 UTC (1 container statuses recorded)
    May  4 13:03:18.779: INFO: 	Container kube-state-metrics ready: true, restart count 0
    May  4 13:03:18.779: INFO: dashboard-metrics-scraper-7564894f4b-nvnb7 from kubernetes-dashboard started at 2023-05-04 12:56:25 +0000 UTC (1 container statuses recorded)
    May  4 13:03:18.779: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    May  4 13:03:18.779: INFO: alertmanager-sysalert-0 from pf9-monitoring started at 2023-05-04 11:16:29 +0000 UTC (2 container statuses recorded)
    May  4 13:03:18.779: INFO: 	Container alertmanager ready: true, restart count 1
    May  4 13:03:18.779: INFO: 	Container config-reloader ready: true, restart count 0
    May  4 13:03:18.779: INFO: node-exporter-49wcf from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
    May  4 13:03:18.779: INFO: 	Container node-exporter ready: true, restart count 0
    May  4 13:03:18.779: INFO: sonobuoy-systemd-logs-daemon-set-d34856018c814171-thbzc from sonobuoy started at 2023-05-04 11:44:14 +0000 UTC (2 container statuses recorded)
    May  4 13:03:18.779: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  4 13:03:18.779: INFO: 	Container systemd-logs ready: true, restart count 0
    May  4 13:03:18.779: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-1-253.us-west-2.compute.internal before test
    May  4 13:03:18.833: INFO: calico-node-69h7w from kube-system started at 2023-05-04 11:14:49 +0000 UTC (1 container statuses recorded)
    May  4 13:03:18.833: INFO: 	Container calico-node ready: true, restart count 0
    May  4 13:03:18.833: INFO: coredns-c7944df6b-zhglb from kube-system started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
    May  4 13:03:18.833: INFO: 	Container coredns ready: true, restart count 0
    May  4 13:03:18.833: INFO: node-exporter-q8qnn from pf9-monitoring started at 2023-05-04 11:16:28 +0000 UTC (1 container statuses recorded)
    May  4 13:03:18.833: INFO: 	Container node-exporter ready: true, restart count 0
    May  4 13:03:18.833: INFO: prometheus-system-0 from pf9-monitoring started at 2023-05-04 11:16:30 +0000 UTC (2 container statuses recorded)
    May  4 13:03:18.833: INFO: 	Container config-reloader ready: true, restart count 0
    May  4 13:03:18.833: INFO: 	Container prometheus ready: true, restart count 0
    May  4 13:03:18.833: INFO: monhelper-57744bf759-6nndz from pf9-operators started at 2023-05-04 11:16:25 +0000 UTC (1 container statuses recorded)
    May  4 13:03:18.833: INFO: 	Container monhelper ready: true, restart count 0
    May  4 13:03:18.833: INFO: sonobuoy-systemd-logs-daemon-set-d34856018c814171-rx5lv from sonobuoy started at 2023-05-04 11:44:14 +0000 UTC (2 container statuses recorded)
    May  4 13:03:18.833: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  4 13:03:18.833: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:326
    STEP: verifying the node has the label node ip-10-0-1-189.us-west-2.compute.internal 05/04/23 13:03:18.944
    STEP: verifying the node has the label node ip-10-0-1-216.us-west-2.compute.internal 05/04/23 13:03:18.983
    STEP: verifying the node has the label node ip-10-0-1-224.us-west-2.compute.internal 05/04/23 13:03:19.049
    STEP: verifying the node has the label node ip-10-0-1-232.us-west-2.compute.internal 05/04/23 13:03:19.074
    STEP: verifying the node has the label node ip-10-0-1-253.us-west-2.compute.internal 05/04/23 13:03:19.094
    May  4 13:03:19.138: INFO: Pod adopt-release-bvsmp requesting resource cpu=0m on Node ip-10-0-1-224.us-west-2.compute.internal
    May  4 13:03:19.138: INFO: Pod adopt-release-hpmwp requesting resource cpu=0m on Node ip-10-0-1-224.us-west-2.compute.internal
    May  4 13:03:19.138: INFO: Pod adopt-release-l44qq requesting resource cpu=0m on Node ip-10-0-1-224.us-west-2.compute.internal
    May  4 13:03:19.138: INFO: Pod calico-kube-controllers-654d9ff976-ddwnz requesting resource cpu=1m on Node ip-10-0-1-232.us-west-2.compute.internal
    May  4 13:03:19.138: INFO: Pod calico-node-48gv4 requesting resource cpu=250m on Node ip-10-0-1-232.us-west-2.compute.internal
    May  4 13:03:19.138: INFO: Pod calico-node-69h7w requesting resource cpu=250m on Node ip-10-0-1-253.us-west-2.compute.internal
    May  4 13:03:19.138: INFO: Pod calico-node-bb7nh requesting resource cpu=250m on Node ip-10-0-1-224.us-west-2.compute.internal
    May  4 13:03:19.138: INFO: Pod calico-node-ll8r2 requesting resource cpu=250m on Node ip-10-0-1-216.us-west-2.compute.internal
    May  4 13:03:19.138: INFO: Pod calico-node-r8wl6 requesting resource cpu=250m on Node ip-10-0-1-189.us-west-2.compute.internal
    May  4 13:03:19.138: INFO: Pod calico-typha-85dbcd447f-2s7df requesting resource cpu=10m on Node ip-10-0-1-232.us-west-2.compute.internal
    May  4 13:03:19.138: INFO: Pod calico-typha-85dbcd447f-6tg8g requesting resource cpu=10m on Node ip-10-0-1-189.us-west-2.compute.internal
    May  4 13:03:19.138: INFO: Pod calico-typha-85dbcd447f-qw96l requesting resource cpu=10m on Node ip-10-0-1-216.us-west-2.compute.internal
    May  4 13:03:19.138: INFO: Pod calico-typha-autoscaler-795696b9bd-qhx26 requesting resource cpu=20m on Node ip-10-0-1-232.us-west-2.compute.internal
    May  4 13:03:19.138: INFO: Pod coredns-c7944df6b-z54gp requesting resource cpu=100m on Node ip-10-0-1-216.us-west-2.compute.internal
    May  4 13:03:19.138: INFO: Pod coredns-c7944df6b-zhglb requesting resource cpu=100m on Node ip-10-0-1-253.us-west-2.compute.internal
    May  4 13:03:19.138: INFO: Pod kube-dns-autoscaler-558cdf6846-nt4qj requesting resource cpu=20m on Node ip-10-0-1-232.us-west-2.compute.internal
    May  4 13:03:19.138: INFO: Pod kube-state-metrics-857f6bcbbb-krrw9 requesting resource cpu=0m on Node ip-10-0-1-232.us-west-2.compute.internal
    May  4 13:03:19.138: INFO: Pod metrics-server-57bbf8548c-qkgc6 requesting resource cpu=100m on Node ip-10-0-1-189.us-west-2.compute.internal
    May  4 13:03:19.138: INFO: Pod dashboard-metrics-scraper-7564894f4b-nvnb7 requesting resource cpu=0m on Node ip-10-0-1-232.us-west-2.compute.internal
    May  4 13:03:19.138: INFO: Pod alertmanager-sysalert-0 requesting resource cpu=200m on Node ip-10-0-1-232.us-west-2.compute.internal
    May  4 13:03:19.138: INFO: Pod grafana-84c9c8d6bd-rsbn8 requesting resource cpu=100m on Node ip-10-0-1-216.us-west-2.compute.internal
    May  4 13:03:19.138: INFO: Pod kube-state-metrics-5bf549bd69-djfll requesting resource cpu=100m on Node ip-10-0-1-189.us-west-2.compute.internal
    May  4 13:03:19.138: INFO: Pod node-exporter-49wcf requesting resource cpu=102m on Node ip-10-0-1-232.us-west-2.compute.internal
    May  4 13:03:19.138: INFO: Pod node-exporter-pnw8x requesting resource cpu=102m on Node ip-10-0-1-216.us-west-2.compute.internal
    May  4 13:03:19.138: INFO: Pod node-exporter-q8qnn requesting resource cpu=102m on Node ip-10-0-1-253.us-west-2.compute.internal
    May  4 13:03:19.138: INFO: Pod node-exporter-qn2kc requesting resource cpu=102m on Node ip-10-0-1-224.us-west-2.compute.internal
    May  4 13:03:19.138: INFO: Pod node-exporter-tsxz4 requesting resource cpu=102m on Node ip-10-0-1-189.us-west-2.compute.internal
    May  4 13:03:19.138: INFO: Pod prometheus-system-0 requesting resource cpu=600m on Node ip-10-0-1-253.us-west-2.compute.internal
    May  4 13:03:19.138: INFO: Pod monhelper-57744bf759-6nndz requesting resource cpu=50m on Node ip-10-0-1-253.us-west-2.compute.internal
    May  4 13:03:19.138: INFO: Pod prometheus-operator-85498c86bb-gdnsq requesting resource cpu=100m on Node ip-10-0-1-189.us-west-2.compute.internal
    May  4 13:03:19.138: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-10-0-1-216.us-west-2.compute.internal
    May  4 13:03:19.138: INFO: Pod sonobuoy-e2e-job-a8c15777d8b94166 requesting resource cpu=0m on Node ip-10-0-1-189.us-west-2.compute.internal
    May  4 13:03:19.138: INFO: Pod sonobuoy-systemd-logs-daemon-set-d34856018c814171-44cxg requesting resource cpu=0m on Node ip-10-0-1-216.us-west-2.compute.internal
    May  4 13:03:19.138: INFO: Pod sonobuoy-systemd-logs-daemon-set-d34856018c814171-f2qjv requesting resource cpu=0m on Node ip-10-0-1-189.us-west-2.compute.internal
    May  4 13:03:19.138: INFO: Pod sonobuoy-systemd-logs-daemon-set-d34856018c814171-lss27 requesting resource cpu=0m on Node ip-10-0-1-224.us-west-2.compute.internal
    May  4 13:03:19.138: INFO: Pod sonobuoy-systemd-logs-daemon-set-d34856018c814171-rx5lv requesting resource cpu=0m on Node ip-10-0-1-253.us-west-2.compute.internal
    May  4 13:03:19.138: INFO: Pod sonobuoy-systemd-logs-daemon-set-d34856018c814171-thbzc requesting resource cpu=0m on Node ip-10-0-1-232.us-west-2.compute.internal
    STEP: Starting Pods to consume most of the cluster CPU. 05/04/23 13:03:19.138
    May  4 13:03:19.138: INFO: Creating a pod which consumes cpu=2336m on Node ip-10-0-1-189.us-west-2.compute.internal
    May  4 13:03:19.153: INFO: Creating a pod which consumes cpu=2406m on Node ip-10-0-1-216.us-west-2.compute.internal
    May  4 13:03:19.177: INFO: Creating a pod which consumes cpu=2553m on Node ip-10-0-1-224.us-west-2.compute.internal
    May  4 13:03:19.190: INFO: Creating a pod which consumes cpu=2377m on Node ip-10-0-1-232.us-west-2.compute.internal
    May  4 13:03:19.208: INFO: Creating a pod which consumes cpu=2028m on Node ip-10-0-1-253.us-west-2.compute.internal
    May  4 13:03:19.246: INFO: Waiting up to 5m0s for pod "filler-pod-393b471c-9025-41db-9e35-f7cd8d93a4f0" in namespace "sched-pred-2698" to be "running"
    May  4 13:03:19.285: INFO: Pod "filler-pod-393b471c-9025-41db-9e35-f7cd8d93a4f0": Phase="Pending", Reason="", readiness=false. Elapsed: 38.508393ms
    May  4 13:03:21.297: INFO: Pod "filler-pod-393b471c-9025-41db-9e35-f7cd8d93a4f0": Phase="Running", Reason="", readiness=true. Elapsed: 2.050477374s
    May  4 13:03:21.297: INFO: Pod "filler-pod-393b471c-9025-41db-9e35-f7cd8d93a4f0" satisfied condition "running"
    May  4 13:03:21.297: INFO: Waiting up to 5m0s for pod "filler-pod-f4c05e9c-2afb-4903-abcf-295114adc6dc" in namespace "sched-pred-2698" to be "running"
    May  4 13:03:21.307: INFO: Pod "filler-pod-f4c05e9c-2afb-4903-abcf-295114adc6dc": Phase="Running", Reason="", readiness=true. Elapsed: 10.455919ms
    May  4 13:03:21.307: INFO: Pod "filler-pod-f4c05e9c-2afb-4903-abcf-295114adc6dc" satisfied condition "running"
    May  4 13:03:21.308: INFO: Waiting up to 5m0s for pod "filler-pod-81b7aaeb-c4e1-44d5-adb6-9c8b7b8204e2" in namespace "sched-pred-2698" to be "running"
    May  4 13:03:21.314: INFO: Pod "filler-pod-81b7aaeb-c4e1-44d5-adb6-9c8b7b8204e2": Phase="Running", Reason="", readiness=true. Elapsed: 6.751564ms
    May  4 13:03:21.314: INFO: Pod "filler-pod-81b7aaeb-c4e1-44d5-adb6-9c8b7b8204e2" satisfied condition "running"
    May  4 13:03:21.315: INFO: Waiting up to 5m0s for pod "filler-pod-3307ff45-2d19-4983-a6f0-f9e1bf4221f2" in namespace "sched-pred-2698" to be "running"
    May  4 13:03:21.325: INFO: Pod "filler-pod-3307ff45-2d19-4983-a6f0-f9e1bf4221f2": Phase="Running", Reason="", readiness=true. Elapsed: 10.200713ms
    May  4 13:03:21.325: INFO: Pod "filler-pod-3307ff45-2d19-4983-a6f0-f9e1bf4221f2" satisfied condition "running"
    May  4 13:03:21.325: INFO: Waiting up to 5m0s for pod "filler-pod-b4358935-f3ed-4fab-b6db-60332e8c5371" in namespace "sched-pred-2698" to be "running"
    May  4 13:03:21.333: INFO: Pod "filler-pod-b4358935-f3ed-4fab-b6db-60332e8c5371": Phase="Running", Reason="", readiness=true. Elapsed: 7.542107ms
    May  4 13:03:21.333: INFO: Pod "filler-pod-b4358935-f3ed-4fab-b6db-60332e8c5371" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 05/04/23 13:03:21.333
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-3307ff45-2d19-4983-a6f0-f9e1bf4221f2.175bf26480d711c6], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2698/filler-pod-3307ff45-2d19-4983-a6f0-f9e1bf4221f2 to ip-10-0-1-232.us-west-2.compute.internal] 05/04/23 13:03:21.345
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-3307ff45-2d19-4983-a6f0-f9e1bf4221f2.175bf264b9fbf1f4], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 05/04/23 13:03:21.345
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-3307ff45-2d19-4983-a6f0-f9e1bf4221f2.175bf264bb4dbd73], Reason = [Created], Message = [Created container filler-pod-3307ff45-2d19-4983-a6f0-f9e1bf4221f2] 05/04/23 13:03:21.346
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-3307ff45-2d19-4983-a6f0-f9e1bf4221f2.175bf264c1a62058], Reason = [Started], Message = [Started container filler-pod-3307ff45-2d19-4983-a6f0-f9e1bf4221f2] 05/04/23 13:03:21.346
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-393b471c-9025-41db-9e35-f7cd8d93a4f0.175bf2647cee0c20], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2698/filler-pod-393b471c-9025-41db-9e35-f7cd8d93a4f0 to ip-10-0-1-189.us-west-2.compute.internal] 05/04/23 13:03:21.346
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-393b471c-9025-41db-9e35-f7cd8d93a4f0.175bf264a841587c], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 05/04/23 13:03:21.346
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-393b471c-9025-41db-9e35-f7cd8d93a4f0.175bf264a981daa7], Reason = [Created], Message = [Created container filler-pod-393b471c-9025-41db-9e35-f7cd8d93a4f0] 05/04/23 13:03:21.346
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-393b471c-9025-41db-9e35-f7cd8d93a4f0.175bf264b0bb9d33], Reason = [Started], Message = [Started container filler-pod-393b471c-9025-41db-9e35-f7cd8d93a4f0] 05/04/23 13:03:21.346
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-81b7aaeb-c4e1-44d5-adb6-9c8b7b8204e2.175bf2647f328d4a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2698/filler-pod-81b7aaeb-c4e1-44d5-adb6-9c8b7b8204e2 to ip-10-0-1-224.us-west-2.compute.internal] 05/04/23 13:03:21.346
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-81b7aaeb-c4e1-44d5-adb6-9c8b7b8204e2.175bf264ac68bba1], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 05/04/23 13:03:21.346
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-81b7aaeb-c4e1-44d5-adb6-9c8b7b8204e2.175bf264ad87f2c1], Reason = [Created], Message = [Created container filler-pod-81b7aaeb-c4e1-44d5-adb6-9c8b7b8204e2] 05/04/23 13:03:21.346
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-81b7aaeb-c4e1-44d5-adb6-9c8b7b8204e2.175bf264b42261fe], Reason = [Started], Message = [Started container filler-pod-81b7aaeb-c4e1-44d5-adb6-9c8b7b8204e2] 05/04/23 13:03:21.346
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b4358935-f3ed-4fab-b6db-60332e8c5371.175bf264819f53e7], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2698/filler-pod-b4358935-f3ed-4fab-b6db-60332e8c5371 to ip-10-0-1-253.us-west-2.compute.internal] 05/04/23 13:03:21.346
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b4358935-f3ed-4fab-b6db-60332e8c5371.175bf264aefdbde8], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 05/04/23 13:03:21.346
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b4358935-f3ed-4fab-b6db-60332e8c5371.175bf264b019f7da], Reason = [Created], Message = [Created container filler-pod-b4358935-f3ed-4fab-b6db-60332e8c5371] 05/04/23 13:03:21.346
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b4358935-f3ed-4fab-b6db-60332e8c5371.175bf264b66cf410], Reason = [Started], Message = [Started container filler-pod-b4358935-f3ed-4fab-b6db-60332e8c5371] 05/04/23 13:03:21.347
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f4c05e9c-2afb-4903-abcf-295114adc6dc.175bf2647dd8c933], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2698/filler-pod-f4c05e9c-2afb-4903-abcf-295114adc6dc to ip-10-0-1-216.us-west-2.compute.internal] 05/04/23 13:03:21.347
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f4c05e9c-2afb-4903-abcf-295114adc6dc.175bf264ac2a198d], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 05/04/23 13:03:21.347
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f4c05e9c-2afb-4903-abcf-295114adc6dc.175bf264ae456ac6], Reason = [Created], Message = [Created container filler-pod-f4c05e9c-2afb-4903-abcf-295114adc6dc] 05/04/23 13:03:21.347
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f4c05e9c-2afb-4903-abcf-295114adc6dc.175bf264b624c16a], Reason = [Started], Message = [Started container filler-pod-f4c05e9c-2afb-4903-abcf-295114adc6dc] 05/04/23 13:03:21.347
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.175bf264ff998753], Reason = [FailedScheduling], Message = [0/8 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/master: true}, 5 Insufficient cpu. preemption: 0/8 nodes are available: 3 Preemption is not helpful for scheduling, 5 No preemption victims found for incoming pod.] 05/04/23 13:03:21.385
    STEP: removing the label node off the node ip-10-0-1-189.us-west-2.compute.internal 05/04/23 13:03:22.373
    STEP: verifying the node doesn't have the label node 05/04/23 13:03:22.407
    STEP: removing the label node off the node ip-10-0-1-216.us-west-2.compute.internal 05/04/23 13:03:22.418
    STEP: verifying the node doesn't have the label node 05/04/23 13:03:22.461
    STEP: removing the label node off the node ip-10-0-1-224.us-west-2.compute.internal 05/04/23 13:03:22.476
    STEP: verifying the node doesn't have the label node 05/04/23 13:03:22.511
    STEP: removing the label node off the node ip-10-0-1-232.us-west-2.compute.internal 05/04/23 13:03:22.523
    STEP: verifying the node doesn't have the label node 05/04/23 13:03:22.568
    STEP: removing the label node off the node ip-10-0-1-253.us-west-2.compute.internal 05/04/23 13:03:22.58
    STEP: verifying the node doesn't have the label node 05/04/23 13:03:22.599
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    May  4 13:03:22.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-2698" for this suite. 05/04/23 13:03:22.63
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:03:22.654
May  4 13:03:22.654: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename gc 05/04/23 13:03:22.655
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:03:22.693
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:03:22.701
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 05/04/23 13:03:22.721
STEP: delete the rc 05/04/23 13:03:27.737
STEP: wait for the rc to be deleted 05/04/23 13:03:27.749
May  4 13:03:28.786: INFO: 80 pods remaining
May  4 13:03:28.786: INFO: 80 pods has nil DeletionTimestamp
May  4 13:03:28.786: INFO: 
May  4 13:03:29.779: INFO: 72 pods remaining
May  4 13:03:29.779: INFO: 72 pods has nil DeletionTimestamp
May  4 13:03:29.779: INFO: 
May  4 13:03:30.812: INFO: 60 pods remaining
May  4 13:03:30.812: INFO: 60 pods has nil DeletionTimestamp
May  4 13:03:30.812: INFO: 
May  4 13:03:31.780: INFO: 40 pods remaining
May  4 13:03:31.780: INFO: 40 pods has nil DeletionTimestamp
May  4 13:03:31.780: INFO: 
May  4 13:03:32.766: INFO: 32 pods remaining
May  4 13:03:32.766: INFO: 32 pods has nil DeletionTimestamp
May  4 13:03:32.766: INFO: 
May  4 13:03:33.769: INFO: 20 pods remaining
May  4 13:03:33.769: INFO: 20 pods has nil DeletionTimestamp
May  4 13:03:33.769: INFO: 
STEP: Gathering metrics 05/04/23 13:03:34.761
W0504 13:03:34.783325      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
May  4 13:03:34.783: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
May  4 13:03:34.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7955" for this suite. 05/04/23 13:03:34.79
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","completed":285,"skipped":5283,"failed":0}
------------------------------
• [SLOW TEST] [12.143 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:03:22.654
    May  4 13:03:22.654: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename gc 05/04/23 13:03:22.655
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:03:22.693
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:03:22.701
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 05/04/23 13:03:22.721
    STEP: delete the rc 05/04/23 13:03:27.737
    STEP: wait for the rc to be deleted 05/04/23 13:03:27.749
    May  4 13:03:28.786: INFO: 80 pods remaining
    May  4 13:03:28.786: INFO: 80 pods has nil DeletionTimestamp
    May  4 13:03:28.786: INFO: 
    May  4 13:03:29.779: INFO: 72 pods remaining
    May  4 13:03:29.779: INFO: 72 pods has nil DeletionTimestamp
    May  4 13:03:29.779: INFO: 
    May  4 13:03:30.812: INFO: 60 pods remaining
    May  4 13:03:30.812: INFO: 60 pods has nil DeletionTimestamp
    May  4 13:03:30.812: INFO: 
    May  4 13:03:31.780: INFO: 40 pods remaining
    May  4 13:03:31.780: INFO: 40 pods has nil DeletionTimestamp
    May  4 13:03:31.780: INFO: 
    May  4 13:03:32.766: INFO: 32 pods remaining
    May  4 13:03:32.766: INFO: 32 pods has nil DeletionTimestamp
    May  4 13:03:32.766: INFO: 
    May  4 13:03:33.769: INFO: 20 pods remaining
    May  4 13:03:33.769: INFO: 20 pods has nil DeletionTimestamp
    May  4 13:03:33.769: INFO: 
    STEP: Gathering metrics 05/04/23 13:03:34.761
    W0504 13:03:34.783325      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    May  4 13:03:34.783: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    May  4 13:03:34.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-7955" for this suite. 05/04/23 13:03:34.79
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:03:34.798
May  4 13:03:34.798: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename init-container 05/04/23 13:03:34.798
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:03:34.814
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:03:34.817
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
STEP: creating the pod 05/04/23 13:03:34.819
May  4 13:03:34.819: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
May  4 13:03:41.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9347" for this suite. 05/04/23 13:03:41.837
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","completed":286,"skipped":5286,"failed":0}
------------------------------
• [SLOW TEST] [7.045 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:03:34.798
    May  4 13:03:34.798: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename init-container 05/04/23 13:03:34.798
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:03:34.814
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:03:34.817
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:457
    STEP: creating the pod 05/04/23 13:03:34.819
    May  4 13:03:34.819: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    May  4 13:03:41.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-9347" for this suite. 05/04/23 13:03:41.837
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:03:41.843
May  4 13:03:41.843: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename resourcequota 05/04/23 13:03:41.844
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:03:41.863
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:03:41.866
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
STEP: Counting existing ResourceQuota 05/04/23 13:03:41.869
STEP: Creating a ResourceQuota 05/04/23 13:03:46.874
STEP: Ensuring resource quota status is calculated 05/04/23 13:03:46.882
STEP: Creating a Pod that fits quota 05/04/23 13:03:48.887
STEP: Ensuring ResourceQuota status captures the pod usage 05/04/23 13:03:48.905
STEP: Not allowing a pod to be created that exceeds remaining quota 05/04/23 13:03:50.909
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 05/04/23 13:03:50.912
STEP: Ensuring a pod cannot update its resource requirements 05/04/23 13:03:50.915
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 05/04/23 13:03:50.921
STEP: Deleting the pod 05/04/23 13:03:52.926
STEP: Ensuring resource quota status released the pod usage 05/04/23 13:03:52.945
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
May  4 13:03:54.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6559" for this suite. 05/04/23 13:03:54.963
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","completed":287,"skipped":5293,"failed":0}
------------------------------
• [SLOW TEST] [13.128 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:03:41.843
    May  4 13:03:41.843: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename resourcequota 05/04/23 13:03:41.844
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:03:41.863
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:03:41.866
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:220
    STEP: Counting existing ResourceQuota 05/04/23 13:03:41.869
    STEP: Creating a ResourceQuota 05/04/23 13:03:46.874
    STEP: Ensuring resource quota status is calculated 05/04/23 13:03:46.882
    STEP: Creating a Pod that fits quota 05/04/23 13:03:48.887
    STEP: Ensuring ResourceQuota status captures the pod usage 05/04/23 13:03:48.905
    STEP: Not allowing a pod to be created that exceeds remaining quota 05/04/23 13:03:50.909
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 05/04/23 13:03:50.912
    STEP: Ensuring a pod cannot update its resource requirements 05/04/23 13:03:50.915
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 05/04/23 13:03:50.921
    STEP: Deleting the pod 05/04/23 13:03:52.926
    STEP: Ensuring resource quota status released the pod usage 05/04/23 13:03:52.945
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    May  4 13:03:54.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-6559" for this suite. 05/04/23 13:03:54.963
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:03:54.972
May  4 13:03:54.973: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename projected 05/04/23 13:03:54.974
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:03:54.994
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:03:54.998
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
STEP: Creating a pod to test downward API volume plugin 05/04/23 13:03:55.003
May  4 13:03:55.014: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d1edd9d4-d1c2-4304-a0a1-d86b36bccb68" in namespace "projected-3942" to be "Succeeded or Failed"
May  4 13:03:55.018: INFO: Pod "downwardapi-volume-d1edd9d4-d1c2-4304-a0a1-d86b36bccb68": Phase="Pending", Reason="", readiness=false. Elapsed: 4.137912ms
May  4 13:03:57.024: INFO: Pod "downwardapi-volume-d1edd9d4-d1c2-4304-a0a1-d86b36bccb68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009984793s
May  4 13:03:59.023: INFO: Pod "downwardapi-volume-d1edd9d4-d1c2-4304-a0a1-d86b36bccb68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008755346s
STEP: Saw pod success 05/04/23 13:03:59.023
May  4 13:03:59.023: INFO: Pod "downwardapi-volume-d1edd9d4-d1c2-4304-a0a1-d86b36bccb68" satisfied condition "Succeeded or Failed"
May  4 13:03:59.027: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod downwardapi-volume-d1edd9d4-d1c2-4304-a0a1-d86b36bccb68 container client-container: <nil>
STEP: delete the pod 05/04/23 13:03:59.035
May  4 13:03:59.053: INFO: Waiting for pod downwardapi-volume-d1edd9d4-d1c2-4304-a0a1-d86b36bccb68 to disappear
May  4 13:03:59.057: INFO: Pod downwardapi-volume-d1edd9d4-d1c2-4304-a0a1-d86b36bccb68 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
May  4 13:03:59.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3942" for this suite. 05/04/23 13:03:59.068
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","completed":288,"skipped":5299,"failed":0}
------------------------------
• [4.104 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:03:54.972
    May  4 13:03:54.973: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename projected 05/04/23 13:03:54.974
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:03:54.994
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:03:54.998
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:234
    STEP: Creating a pod to test downward API volume plugin 05/04/23 13:03:55.003
    May  4 13:03:55.014: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d1edd9d4-d1c2-4304-a0a1-d86b36bccb68" in namespace "projected-3942" to be "Succeeded or Failed"
    May  4 13:03:55.018: INFO: Pod "downwardapi-volume-d1edd9d4-d1c2-4304-a0a1-d86b36bccb68": Phase="Pending", Reason="", readiness=false. Elapsed: 4.137912ms
    May  4 13:03:57.024: INFO: Pod "downwardapi-volume-d1edd9d4-d1c2-4304-a0a1-d86b36bccb68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009984793s
    May  4 13:03:59.023: INFO: Pod "downwardapi-volume-d1edd9d4-d1c2-4304-a0a1-d86b36bccb68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008755346s
    STEP: Saw pod success 05/04/23 13:03:59.023
    May  4 13:03:59.023: INFO: Pod "downwardapi-volume-d1edd9d4-d1c2-4304-a0a1-d86b36bccb68" satisfied condition "Succeeded or Failed"
    May  4 13:03:59.027: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod downwardapi-volume-d1edd9d4-d1c2-4304-a0a1-d86b36bccb68 container client-container: <nil>
    STEP: delete the pod 05/04/23 13:03:59.035
    May  4 13:03:59.053: INFO: Waiting for pod downwardapi-volume-d1edd9d4-d1c2-4304-a0a1-d86b36bccb68 to disappear
    May  4 13:03:59.057: INFO: Pod downwardapi-volume-d1edd9d4-d1c2-4304-a0a1-d86b36bccb68 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    May  4 13:03:59.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3942" for this suite. 05/04/23 13:03:59.068
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:03:59.088
May  4 13:03:59.088: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename deployment 05/04/23 13:03:59.089
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:03:59.114
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:03:59.118
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
May  4 13:03:59.124: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
May  4 13:03:59.140: INFO: Pod name sample-pod: Found 0 pods out of 1
May  4 13:04:04.155: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 05/04/23 13:04:04.155
May  4 13:04:04.155: INFO: Creating deployment "test-rolling-update-deployment"
May  4 13:04:04.170: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
May  4 13:04:04.195: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
May  4 13:04:06.203: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
May  4 13:04:06.208: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May  4 13:04:06.222: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-7549  ee91eac5-59b9-460f-b0fd-3b9cb4f899ff 41157 1 2023-05-04 13:04:04 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-05-04 13:04:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 13:04:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005d69fe8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-04 13:04:04 +0000 UTC,LastTransitionTime:2023-05-04 13:04:04 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-05-04 13:04:05 +0000 UTC,LastTransitionTime:2023-05-04 13:04:04 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May  4 13:04:06.232: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-7549  79721941-9821-4d12-b22d-7eb3ec343273 41147 1 2023-05-04 13:04:04 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment ee91eac5-59b9-460f-b0fd-3b9cb4f899ff 0xc005f05977 0xc005f05978}] [] [{kube-controller-manager Update apps/v1 2023-05-04 13:04:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee91eac5-59b9-460f-b0fd-3b9cb4f899ff\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 13:04:05 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005f05a88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May  4 13:04:06.233: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
May  4 13:04:06.233: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-7549  aaccd6b3-7ea5-4afa-9662-1500bcaf96db 41156 2 2023-05-04 13:03:59 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment ee91eac5-59b9-460f-b0fd-3b9cb4f899ff 0xc005f05847 0xc005f05848}] [] [{e2e.test Update apps/v1 2023-05-04 13:03:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 13:04:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee91eac5-59b9-460f-b0fd-3b9cb4f899ff\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-04 13:04:05 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005f05908 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  4 13:04:06.237: INFO: Pod "test-rolling-update-deployment-78f575d8ff-9szb7" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-9szb7 test-rolling-update-deployment-78f575d8ff- deployment-7549  7b56b8ff-c762-4116-b1fb-cb522345fc91 41146 0 2023-05-04 13:04:04 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:9658b5410885b584969bb5c1202e2640dc84443f25ff9dff40d6245e467e2037 cni.projectcalico.org/podIP:10.20.83.1/32 cni.projectcalico.org/podIPs:10.20.83.1/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 79721941-9821-4d12-b22d-7eb3ec343273 0xc005f05f27 0xc005f05f28}] [] [{calico Update v1 2023-05-04 13:04:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-04 13:04:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79721941-9821-4d12-b22d-7eb3ec343273\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-04 13:04:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.83.1\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4xg57,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4xg57,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-224.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:04:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:04:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:04:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:04:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.224,PodIP:10.20.83.1,StartTime:2023-05-04 13:04:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-04 13:04:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://3e3233524eb064dec1921374068925bf5f0ce57ec39fd0cde00b70805ae400d2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.83.1,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
May  4 13:04:06.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7549" for this suite. 05/04/23 13:04:06.25
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","completed":289,"skipped":5399,"failed":0}
------------------------------
• [SLOW TEST] [7.173 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:03:59.088
    May  4 13:03:59.088: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename deployment 05/04/23 13:03:59.089
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:03:59.114
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:03:59.118
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    May  4 13:03:59.124: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    May  4 13:03:59.140: INFO: Pod name sample-pod: Found 0 pods out of 1
    May  4 13:04:04.155: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 05/04/23 13:04:04.155
    May  4 13:04:04.155: INFO: Creating deployment "test-rolling-update-deployment"
    May  4 13:04:04.170: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    May  4 13:04:04.195: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    May  4 13:04:06.203: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    May  4 13:04:06.208: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May  4 13:04:06.222: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-7549  ee91eac5-59b9-460f-b0fd-3b9cb4f899ff 41157 1 2023-05-04 13:04:04 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-05-04 13:04:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 13:04:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005d69fe8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-04 13:04:04 +0000 UTC,LastTransitionTime:2023-05-04 13:04:04 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-05-04 13:04:05 +0000 UTC,LastTransitionTime:2023-05-04 13:04:04 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    May  4 13:04:06.232: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-7549  79721941-9821-4d12-b22d-7eb3ec343273 41147 1 2023-05-04 13:04:04 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment ee91eac5-59b9-460f-b0fd-3b9cb4f899ff 0xc005f05977 0xc005f05978}] [] [{kube-controller-manager Update apps/v1 2023-05-04 13:04:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee91eac5-59b9-460f-b0fd-3b9cb4f899ff\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 13:04:05 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005f05a88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    May  4 13:04:06.233: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    May  4 13:04:06.233: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-7549  aaccd6b3-7ea5-4afa-9662-1500bcaf96db 41156 2 2023-05-04 13:03:59 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment ee91eac5-59b9-460f-b0fd-3b9cb4f899ff 0xc005f05847 0xc005f05848}] [] [{e2e.test Update apps/v1 2023-05-04 13:03:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 13:04:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee91eac5-59b9-460f-b0fd-3b9cb4f899ff\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-04 13:04:05 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005f05908 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    May  4 13:04:06.237: INFO: Pod "test-rolling-update-deployment-78f575d8ff-9szb7" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-9szb7 test-rolling-update-deployment-78f575d8ff- deployment-7549  7b56b8ff-c762-4116-b1fb-cb522345fc91 41146 0 2023-05-04 13:04:04 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:9658b5410885b584969bb5c1202e2640dc84443f25ff9dff40d6245e467e2037 cni.projectcalico.org/podIP:10.20.83.1/32 cni.projectcalico.org/podIPs:10.20.83.1/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 79721941-9821-4d12-b22d-7eb3ec343273 0xc005f05f27 0xc005f05f28}] [] [{calico Update v1 2023-05-04 13:04:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-04 13:04:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79721941-9821-4d12-b22d-7eb3ec343273\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-04 13:04:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.83.1\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4xg57,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4xg57,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-224.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:04:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:04:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:04:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:04:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.224,PodIP:10.20.83.1,StartTime:2023-05-04 13:04:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-04 13:04:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://3e3233524eb064dec1921374068925bf5f0ce57ec39fd0cde00b70805ae400d2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.83.1,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    May  4 13:04:06.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-7549" for this suite. 05/04/23 13:04:06.25
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:04:06.262
May  4 13:04:06.263: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename projected 05/04/23 13:04:06.263
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:04:06.282
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:04:06.291
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
STEP: Creating configMap with name configmap-projected-all-test-volume-e2f81fbc-9a65-446e-abb3-763966f33ce8 05/04/23 13:04:06.295
STEP: Creating secret with name secret-projected-all-test-volume-fd91c62f-d9e7-475a-a79d-7d72f049458f 05/04/23 13:04:06.3
STEP: Creating a pod to test Check all projections for projected volume plugin 05/04/23 13:04:06.308
May  4 13:04:06.326: INFO: Waiting up to 5m0s for pod "projected-volume-6b297b88-674b-4cfb-ab35-49dcfd15516b" in namespace "projected-2176" to be "Succeeded or Failed"
May  4 13:04:06.330: INFO: Pod "projected-volume-6b297b88-674b-4cfb-ab35-49dcfd15516b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.078848ms
May  4 13:04:08.335: INFO: Pod "projected-volume-6b297b88-674b-4cfb-ab35-49dcfd15516b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009016209s
May  4 13:04:10.337: INFO: Pod "projected-volume-6b297b88-674b-4cfb-ab35-49dcfd15516b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010887337s
STEP: Saw pod success 05/04/23 13:04:10.337
May  4 13:04:10.337: INFO: Pod "projected-volume-6b297b88-674b-4cfb-ab35-49dcfd15516b" satisfied condition "Succeeded or Failed"
May  4 13:04:10.342: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod projected-volume-6b297b88-674b-4cfb-ab35-49dcfd15516b container projected-all-volume-test: <nil>
STEP: delete the pod 05/04/23 13:04:10.353
May  4 13:04:10.392: INFO: Waiting for pod projected-volume-6b297b88-674b-4cfb-ab35-49dcfd15516b to disappear
May  4 13:04:10.396: INFO: Pod projected-volume-6b297b88-674b-4cfb-ab35-49dcfd15516b no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
May  4 13:04:10.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2176" for this suite. 05/04/23 13:04:10.404
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","completed":290,"skipped":5405,"failed":0}
------------------------------
• [4.153 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:04:06.262
    May  4 13:04:06.263: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename projected 05/04/23 13:04:06.263
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:04:06.282
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:04:06.291
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:43
    STEP: Creating configMap with name configmap-projected-all-test-volume-e2f81fbc-9a65-446e-abb3-763966f33ce8 05/04/23 13:04:06.295
    STEP: Creating secret with name secret-projected-all-test-volume-fd91c62f-d9e7-475a-a79d-7d72f049458f 05/04/23 13:04:06.3
    STEP: Creating a pod to test Check all projections for projected volume plugin 05/04/23 13:04:06.308
    May  4 13:04:06.326: INFO: Waiting up to 5m0s for pod "projected-volume-6b297b88-674b-4cfb-ab35-49dcfd15516b" in namespace "projected-2176" to be "Succeeded or Failed"
    May  4 13:04:06.330: INFO: Pod "projected-volume-6b297b88-674b-4cfb-ab35-49dcfd15516b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.078848ms
    May  4 13:04:08.335: INFO: Pod "projected-volume-6b297b88-674b-4cfb-ab35-49dcfd15516b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009016209s
    May  4 13:04:10.337: INFO: Pod "projected-volume-6b297b88-674b-4cfb-ab35-49dcfd15516b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010887337s
    STEP: Saw pod success 05/04/23 13:04:10.337
    May  4 13:04:10.337: INFO: Pod "projected-volume-6b297b88-674b-4cfb-ab35-49dcfd15516b" satisfied condition "Succeeded or Failed"
    May  4 13:04:10.342: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod projected-volume-6b297b88-674b-4cfb-ab35-49dcfd15516b container projected-all-volume-test: <nil>
    STEP: delete the pod 05/04/23 13:04:10.353
    May  4 13:04:10.392: INFO: Waiting for pod projected-volume-6b297b88-674b-4cfb-ab35-49dcfd15516b to disappear
    May  4 13:04:10.396: INFO: Pod projected-volume-6b297b88-674b-4cfb-ab35-49dcfd15516b no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:187
    May  4 13:04:10.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2176" for this suite. 05/04/23 13:04:10.404
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:04:10.417
May  4 13:04:10.417: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename watch 05/04/23 13:04:10.419
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:04:10.443
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:04:10.45
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 05/04/23 13:04:10.458
STEP: modifying the configmap once 05/04/23 13:04:10.463
STEP: modifying the configmap a second time 05/04/23 13:04:10.474
STEP: deleting the configmap 05/04/23 13:04:10.498
STEP: creating a watch on configmaps from the resource version returned by the first update 05/04/23 13:04:10.511
STEP: Expecting to observe notifications for all changes to the configmap after the first update 05/04/23 13:04:10.513
May  4 13:04:10.519: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1781  2cbbbd62-205b-4cc6-bafb-b70fa2e805d5 41201 0 2023-05-04 13:04:10 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-04 13:04:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May  4 13:04:10.519: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1781  2cbbbd62-205b-4cc6-bafb-b70fa2e805d5 41202 0 2023-05-04 13:04:10 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-04 13:04:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
May  4 13:04:10.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1781" for this suite. 05/04/23 13:04:10.534
{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","completed":291,"skipped":5418,"failed":0}
------------------------------
• [0.129 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:04:10.417
    May  4 13:04:10.417: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename watch 05/04/23 13:04:10.419
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:04:10.443
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:04:10.45
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 05/04/23 13:04:10.458
    STEP: modifying the configmap once 05/04/23 13:04:10.463
    STEP: modifying the configmap a second time 05/04/23 13:04:10.474
    STEP: deleting the configmap 05/04/23 13:04:10.498
    STEP: creating a watch on configmaps from the resource version returned by the first update 05/04/23 13:04:10.511
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 05/04/23 13:04:10.513
    May  4 13:04:10.519: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1781  2cbbbd62-205b-4cc6-bafb-b70fa2e805d5 41201 0 2023-05-04 13:04:10 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-04 13:04:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    May  4 13:04:10.519: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1781  2cbbbd62-205b-4cc6-bafb-b70fa2e805d5 41202 0 2023-05-04 13:04:10 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-04 13:04:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    May  4 13:04:10.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-1781" for this suite. 05/04/23 13:04:10.534
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:04:10.548
May  4 13:04:10.548: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename crd-publish-openapi 05/04/23 13:04:10.549
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:04:10.564
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:04:10.568
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 05/04/23 13:04:10.571
May  4 13:04:10.574: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
May  4 13:04:16.583: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  4 13:04:36.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8991" for this suite. 05/04/23 13:04:36.656
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","completed":292,"skipped":5447,"failed":0}
------------------------------
• [SLOW TEST] [26.121 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:04:10.548
    May  4 13:04:10.548: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename crd-publish-openapi 05/04/23 13:04:10.549
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:04:10.564
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:04:10.568
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:275
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 05/04/23 13:04:10.571
    May  4 13:04:10.574: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    May  4 13:04:16.583: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  4 13:04:36.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-8991" for this suite. 05/04/23 13:04:36.656
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:04:36.67
May  4 13:04:36.670: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename configmap 05/04/23 13:04:36.671
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:04:36.692
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:04:36.696
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
STEP: Creating configMap with name configmap-test-upd-c0059019-cf8c-433e-afff-fa4e321892e7 05/04/23 13:04:36.708
STEP: Creating the pod 05/04/23 13:04:36.716
May  4 13:04:36.730: INFO: Waiting up to 5m0s for pod "pod-configmaps-e0d6ab74-44eb-47c7-a680-ad8f790b130a" in namespace "configmap-1210" to be "running and ready"
May  4 13:04:36.736: INFO: Pod "pod-configmaps-e0d6ab74-44eb-47c7-a680-ad8f790b130a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.036049ms
May  4 13:04:36.736: INFO: The phase of Pod pod-configmaps-e0d6ab74-44eb-47c7-a680-ad8f790b130a is Pending, waiting for it to be Running (with Ready = true)
May  4 13:04:38.743: INFO: Pod "pod-configmaps-e0d6ab74-44eb-47c7-a680-ad8f790b130a": Phase="Running", Reason="", readiness=true. Elapsed: 2.01297109s
May  4 13:04:38.743: INFO: The phase of Pod pod-configmaps-e0d6ab74-44eb-47c7-a680-ad8f790b130a is Running (Ready = true)
May  4 13:04:38.743: INFO: Pod "pod-configmaps-e0d6ab74-44eb-47c7-a680-ad8f790b130a" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-c0059019-cf8c-433e-afff-fa4e321892e7 05/04/23 13:04:38.768
STEP: waiting to observe update in volume 05/04/23 13:04:38.779
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
May  4 13:04:40.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1210" for this suite. 05/04/23 13:04:40.813
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":293,"skipped":5454,"failed":0}
------------------------------
• [4.156 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:04:36.67
    May  4 13:04:36.670: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename configmap 05/04/23 13:04:36.671
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:04:36.692
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:04:36.696
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:123
    STEP: Creating configMap with name configmap-test-upd-c0059019-cf8c-433e-afff-fa4e321892e7 05/04/23 13:04:36.708
    STEP: Creating the pod 05/04/23 13:04:36.716
    May  4 13:04:36.730: INFO: Waiting up to 5m0s for pod "pod-configmaps-e0d6ab74-44eb-47c7-a680-ad8f790b130a" in namespace "configmap-1210" to be "running and ready"
    May  4 13:04:36.736: INFO: Pod "pod-configmaps-e0d6ab74-44eb-47c7-a680-ad8f790b130a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.036049ms
    May  4 13:04:36.736: INFO: The phase of Pod pod-configmaps-e0d6ab74-44eb-47c7-a680-ad8f790b130a is Pending, waiting for it to be Running (with Ready = true)
    May  4 13:04:38.743: INFO: Pod "pod-configmaps-e0d6ab74-44eb-47c7-a680-ad8f790b130a": Phase="Running", Reason="", readiness=true. Elapsed: 2.01297109s
    May  4 13:04:38.743: INFO: The phase of Pod pod-configmaps-e0d6ab74-44eb-47c7-a680-ad8f790b130a is Running (Ready = true)
    May  4 13:04:38.743: INFO: Pod "pod-configmaps-e0d6ab74-44eb-47c7-a680-ad8f790b130a" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-c0059019-cf8c-433e-afff-fa4e321892e7 05/04/23 13:04:38.768
    STEP: waiting to observe update in volume 05/04/23 13:04:38.779
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    May  4 13:04:40.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1210" for this suite. 05/04/23 13:04:40.813
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:04:40.826
May  4 13:04:40.826: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename daemonsets 05/04/23 13:04:40.827
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:04:40.875
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:04:40.878
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
STEP: Creating simple DaemonSet "daemon-set" 05/04/23 13:04:40.977
STEP: Check that daemon pods launch on every node of the cluster. 05/04/23 13:04:40.988
May  4 13:04:41.002: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 13:04:41.002: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 13:04:41.003: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 13:04:41.008: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  4 13:04:41.008: INFO: Node ip-10-0-1-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
May  4 13:04:42.016: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 13:04:42.016: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 13:04:42.016: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 13:04:42.022: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  4 13:04:42.022: INFO: Node ip-10-0-1-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
May  4 13:04:43.018: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 13:04:43.018: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 13:04:43.018: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 13:04:43.031: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
May  4 13:04:43.031: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
STEP: Getting /status 05/04/23 13:04:43.037
May  4 13:04:43.043: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 05/04/23 13:04:43.043
May  4 13:04:43.064: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 05/04/23 13:04:43.064
May  4 13:04:43.071: INFO: Observed &DaemonSet event: ADDED
May  4 13:04:43.071: INFO: Observed &DaemonSet event: MODIFIED
May  4 13:04:43.071: INFO: Observed &DaemonSet event: MODIFIED
May  4 13:04:43.071: INFO: Observed &DaemonSet event: MODIFIED
May  4 13:04:43.071: INFO: Observed &DaemonSet event: MODIFIED
May  4 13:04:43.072: INFO: Observed &DaemonSet event: MODIFIED
May  4 13:04:43.072: INFO: Observed &DaemonSet event: MODIFIED
May  4 13:04:43.072: INFO: Observed &DaemonSet event: MODIFIED
May  4 13:04:43.072: INFO: Found daemon set daemon-set in namespace daemonsets-9785 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May  4 13:04:43.072: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 05/04/23 13:04:43.072
STEP: watching for the daemon set status to be patched 05/04/23 13:04:43.085
May  4 13:04:43.087: INFO: Observed &DaemonSet event: ADDED
May  4 13:04:43.087: INFO: Observed &DaemonSet event: MODIFIED
May  4 13:04:43.088: INFO: Observed &DaemonSet event: MODIFIED
May  4 13:04:43.088: INFO: Observed &DaemonSet event: MODIFIED
May  4 13:04:43.088: INFO: Observed &DaemonSet event: MODIFIED
May  4 13:04:43.088: INFO: Observed &DaemonSet event: MODIFIED
May  4 13:04:43.088: INFO: Observed &DaemonSet event: MODIFIED
May  4 13:04:43.088: INFO: Observed &DaemonSet event: MODIFIED
May  4 13:04:43.088: INFO: Observed daemon set daemon-set in namespace daemonsets-9785 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May  4 13:04:43.089: INFO: Observed &DaemonSet event: MODIFIED
May  4 13:04:43.089: INFO: Found daemon set daemon-set in namespace daemonsets-9785 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
May  4 13:04:43.089: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 05/04/23 13:04:43.098
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9785, will wait for the garbage collector to delete the pods 05/04/23 13:04:43.098
May  4 13:04:43.167: INFO: Deleting DaemonSet.extensions daemon-set took: 9.018443ms
May  4 13:04:43.267: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.116126ms
May  4 13:04:46.072: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  4 13:04:46.072: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May  4 13:04:46.078: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"41494"},"items":null}

May  4 13:04:46.087: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"41494"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
May  4 13:04:46.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9785" for this suite. 05/04/23 13:04:46.146
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","completed":294,"skipped":5457,"failed":0}
------------------------------
• [SLOW TEST] [5.329 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:04:40.826
    May  4 13:04:40.826: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename daemonsets 05/04/23 13:04:40.827
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:04:40.875
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:04:40.878
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:861
    STEP: Creating simple DaemonSet "daemon-set" 05/04/23 13:04:40.977
    STEP: Check that daemon pods launch on every node of the cluster. 05/04/23 13:04:40.988
    May  4 13:04:41.002: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 13:04:41.002: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 13:04:41.003: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 13:04:41.008: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  4 13:04:41.008: INFO: Node ip-10-0-1-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
    May  4 13:04:42.016: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 13:04:42.016: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 13:04:42.016: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 13:04:42.022: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  4 13:04:42.022: INFO: Node ip-10-0-1-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
    May  4 13:04:43.018: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 13:04:43.018: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 13:04:43.018: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 13:04:43.031: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    May  4 13:04:43.031: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    STEP: Getting /status 05/04/23 13:04:43.037
    May  4 13:04:43.043: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 05/04/23 13:04:43.043
    May  4 13:04:43.064: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 05/04/23 13:04:43.064
    May  4 13:04:43.071: INFO: Observed &DaemonSet event: ADDED
    May  4 13:04:43.071: INFO: Observed &DaemonSet event: MODIFIED
    May  4 13:04:43.071: INFO: Observed &DaemonSet event: MODIFIED
    May  4 13:04:43.071: INFO: Observed &DaemonSet event: MODIFIED
    May  4 13:04:43.071: INFO: Observed &DaemonSet event: MODIFIED
    May  4 13:04:43.072: INFO: Observed &DaemonSet event: MODIFIED
    May  4 13:04:43.072: INFO: Observed &DaemonSet event: MODIFIED
    May  4 13:04:43.072: INFO: Observed &DaemonSet event: MODIFIED
    May  4 13:04:43.072: INFO: Found daemon set daemon-set in namespace daemonsets-9785 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    May  4 13:04:43.072: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 05/04/23 13:04:43.072
    STEP: watching for the daemon set status to be patched 05/04/23 13:04:43.085
    May  4 13:04:43.087: INFO: Observed &DaemonSet event: ADDED
    May  4 13:04:43.087: INFO: Observed &DaemonSet event: MODIFIED
    May  4 13:04:43.088: INFO: Observed &DaemonSet event: MODIFIED
    May  4 13:04:43.088: INFO: Observed &DaemonSet event: MODIFIED
    May  4 13:04:43.088: INFO: Observed &DaemonSet event: MODIFIED
    May  4 13:04:43.088: INFO: Observed &DaemonSet event: MODIFIED
    May  4 13:04:43.088: INFO: Observed &DaemonSet event: MODIFIED
    May  4 13:04:43.088: INFO: Observed &DaemonSet event: MODIFIED
    May  4 13:04:43.088: INFO: Observed daemon set daemon-set in namespace daemonsets-9785 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    May  4 13:04:43.089: INFO: Observed &DaemonSet event: MODIFIED
    May  4 13:04:43.089: INFO: Found daemon set daemon-set in namespace daemonsets-9785 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    May  4 13:04:43.089: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 05/04/23 13:04:43.098
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9785, will wait for the garbage collector to delete the pods 05/04/23 13:04:43.098
    May  4 13:04:43.167: INFO: Deleting DaemonSet.extensions daemon-set took: 9.018443ms
    May  4 13:04:43.267: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.116126ms
    May  4 13:04:46.072: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  4 13:04:46.072: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    May  4 13:04:46.078: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"41494"},"items":null}

    May  4 13:04:46.087: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"41494"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    May  4 13:04:46.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-9785" for this suite. 05/04/23 13:04:46.146
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:04:46.156
May  4 13:04:46.156: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename init-container 05/04/23 13:04:46.157
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:04:46.19
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:04:46.195
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
STEP: creating the pod 05/04/23 13:04:46.199
May  4 13:04:46.199: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
May  4 13:04:52.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7365" for this suite. 05/04/23 13:04:52.018
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","completed":295,"skipped":5458,"failed":0}
------------------------------
• [SLOW TEST] [5.874 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:04:46.156
    May  4 13:04:46.156: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename init-container 05/04/23 13:04:46.157
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:04:46.19
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:04:46.195
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:176
    STEP: creating the pod 05/04/23 13:04:46.199
    May  4 13:04:46.199: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    May  4 13:04:52.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-7365" for this suite. 05/04/23 13:04:52.018
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:04:52.031
May  4 13:04:52.031: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename custom-resource-definition 05/04/23 13:04:52.032
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:04:52.06
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:04:52.068
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
May  4 13:04:52.074: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  4 13:04:55.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1739" for this suite. 05/04/23 13:04:55.398
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","completed":296,"skipped":5476,"failed":0}
------------------------------
• [3.378 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:04:52.031
    May  4 13:04:52.031: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename custom-resource-definition 05/04/23 13:04:52.032
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:04:52.06
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:04:52.068
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    May  4 13:04:52.074: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  4 13:04:55.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-1739" for this suite. 05/04/23 13:04:55.398
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:04:55.41
May  4 13:04:55.410: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename kubectl 05/04/23 13:04:55.41
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:04:55.435
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:04:55.45
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
STEP: creating a replication controller 05/04/23 13:04:55.457
May  4 13:04:55.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3634 create -f -'
May  4 13:04:56.865: INFO: stderr: ""
May  4 13:04:56.866: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 05/04/23 13:04:56.866
May  4 13:04:56.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3634 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May  4 13:04:57.008: INFO: stderr: ""
May  4 13:04:57.008: INFO: stdout: "update-demo-nautilus-b5492 update-demo-nautilus-w5tsp "
May  4 13:04:57.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3634 get pods update-demo-nautilus-b5492 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  4 13:04:57.148: INFO: stderr: ""
May  4 13:04:57.148: INFO: stdout: ""
May  4 13:04:57.148: INFO: update-demo-nautilus-b5492 is created but not running
May  4 13:05:02.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3634 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May  4 13:05:02.364: INFO: stderr: ""
May  4 13:05:02.364: INFO: stdout: "update-demo-nautilus-b5492 update-demo-nautilus-w5tsp "
May  4 13:05:02.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3634 get pods update-demo-nautilus-b5492 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  4 13:05:02.509: INFO: stderr: ""
May  4 13:05:02.509: INFO: stdout: "true"
May  4 13:05:02.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3634 get pods update-demo-nautilus-b5492 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May  4 13:05:02.609: INFO: stderr: ""
May  4 13:05:02.609: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
May  4 13:05:02.609: INFO: validating pod update-demo-nautilus-b5492
May  4 13:05:02.616: INFO: got data: {
  "image": "nautilus.jpg"
}

May  4 13:05:02.618: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  4 13:05:02.618: INFO: update-demo-nautilus-b5492 is verified up and running
May  4 13:05:02.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3634 get pods update-demo-nautilus-w5tsp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  4 13:05:02.737: INFO: stderr: ""
May  4 13:05:02.737: INFO: stdout: "true"
May  4 13:05:02.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3634 get pods update-demo-nautilus-w5tsp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May  4 13:05:02.851: INFO: stderr: ""
May  4 13:05:02.851: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
May  4 13:05:02.851: INFO: validating pod update-demo-nautilus-w5tsp
May  4 13:05:02.857: INFO: got data: {
  "image": "nautilus.jpg"
}

May  4 13:05:02.857: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  4 13:05:02.857: INFO: update-demo-nautilus-w5tsp is verified up and running
STEP: scaling down the replication controller 05/04/23 13:05:02.857
May  4 13:05:02.859: INFO: scanned /root for discovery docs: <nil>
May  4 13:05:02.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3634 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
May  4 13:05:04.001: INFO: stderr: ""
May  4 13:05:04.001: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 05/04/23 13:05:04.001
May  4 13:05:04.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3634 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May  4 13:05:04.124: INFO: stderr: ""
May  4 13:05:04.124: INFO: stdout: "update-demo-nautilus-b5492 "
May  4 13:05:04.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3634 get pods update-demo-nautilus-b5492 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  4 13:05:04.258: INFO: stderr: ""
May  4 13:05:04.258: INFO: stdout: "true"
May  4 13:05:04.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3634 get pods update-demo-nautilus-b5492 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May  4 13:05:04.373: INFO: stderr: ""
May  4 13:05:04.373: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
May  4 13:05:04.373: INFO: validating pod update-demo-nautilus-b5492
May  4 13:05:04.378: INFO: got data: {
  "image": "nautilus.jpg"
}

May  4 13:05:04.378: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  4 13:05:04.378: INFO: update-demo-nautilus-b5492 is verified up and running
STEP: scaling up the replication controller 05/04/23 13:05:04.378
May  4 13:05:04.379: INFO: scanned /root for discovery docs: <nil>
May  4 13:05:04.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3634 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
May  4 13:05:05.548: INFO: stderr: ""
May  4 13:05:05.548: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 05/04/23 13:05:05.548
May  4 13:05:05.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3634 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May  4 13:05:05.640: INFO: stderr: ""
May  4 13:05:05.640: INFO: stdout: "update-demo-nautilus-6f9qr update-demo-nautilus-b5492 "
May  4 13:05:05.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3634 get pods update-demo-nautilus-6f9qr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  4 13:05:05.776: INFO: stderr: ""
May  4 13:05:05.776: INFO: stdout: ""
May  4 13:05:05.776: INFO: update-demo-nautilus-6f9qr is created but not running
May  4 13:05:10.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3634 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May  4 13:05:10.898: INFO: stderr: ""
May  4 13:05:10.898: INFO: stdout: "update-demo-nautilus-6f9qr update-demo-nautilus-b5492 "
May  4 13:05:10.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3634 get pods update-demo-nautilus-6f9qr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  4 13:05:10.996: INFO: stderr: ""
May  4 13:05:10.996: INFO: stdout: "true"
May  4 13:05:10.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3634 get pods update-demo-nautilus-6f9qr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May  4 13:05:11.111: INFO: stderr: ""
May  4 13:05:11.111: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
May  4 13:05:11.111: INFO: validating pod update-demo-nautilus-6f9qr
May  4 13:05:11.119: INFO: got data: {
  "image": "nautilus.jpg"
}

May  4 13:05:11.119: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  4 13:05:11.119: INFO: update-demo-nautilus-6f9qr is verified up and running
May  4 13:05:11.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3634 get pods update-demo-nautilus-b5492 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  4 13:05:11.232: INFO: stderr: ""
May  4 13:05:11.232: INFO: stdout: "true"
May  4 13:05:11.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3634 get pods update-demo-nautilus-b5492 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May  4 13:05:11.331: INFO: stderr: ""
May  4 13:05:11.331: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
May  4 13:05:11.331: INFO: validating pod update-demo-nautilus-b5492
May  4 13:05:11.338: INFO: got data: {
  "image": "nautilus.jpg"
}

May  4 13:05:11.338: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  4 13:05:11.338: INFO: update-demo-nautilus-b5492 is verified up and running
STEP: using delete to clean up resources 05/04/23 13:05:11.338
May  4 13:05:11.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3634 delete --grace-period=0 --force -f -'
May  4 13:05:11.429: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  4 13:05:11.429: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May  4 13:05:11.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3634 get rc,svc -l name=update-demo --no-headers'
May  4 13:05:11.519: INFO: stderr: "No resources found in kubectl-3634 namespace.\n"
May  4 13:05:11.519: INFO: stdout: ""
May  4 13:05:11.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3634 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May  4 13:05:11.603: INFO: stderr: ""
May  4 13:05:11.603: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  4 13:05:11.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3634" for this suite. 05/04/23 13:05:11.614
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","completed":297,"skipped":5501,"failed":0}
------------------------------
• [SLOW TEST] [16.215 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:350

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:04:55.41
    May  4 13:04:55.410: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename kubectl 05/04/23 13:04:55.41
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:04:55.435
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:04:55.45
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:350
    STEP: creating a replication controller 05/04/23 13:04:55.457
    May  4 13:04:55.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3634 create -f -'
    May  4 13:04:56.865: INFO: stderr: ""
    May  4 13:04:56.866: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 05/04/23 13:04:56.866
    May  4 13:04:56.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3634 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May  4 13:04:57.008: INFO: stderr: ""
    May  4 13:04:57.008: INFO: stdout: "update-demo-nautilus-b5492 update-demo-nautilus-w5tsp "
    May  4 13:04:57.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3634 get pods update-demo-nautilus-b5492 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  4 13:04:57.148: INFO: stderr: ""
    May  4 13:04:57.148: INFO: stdout: ""
    May  4 13:04:57.148: INFO: update-demo-nautilus-b5492 is created but not running
    May  4 13:05:02.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3634 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May  4 13:05:02.364: INFO: stderr: ""
    May  4 13:05:02.364: INFO: stdout: "update-demo-nautilus-b5492 update-demo-nautilus-w5tsp "
    May  4 13:05:02.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3634 get pods update-demo-nautilus-b5492 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  4 13:05:02.509: INFO: stderr: ""
    May  4 13:05:02.509: INFO: stdout: "true"
    May  4 13:05:02.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3634 get pods update-demo-nautilus-b5492 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May  4 13:05:02.609: INFO: stderr: ""
    May  4 13:05:02.609: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    May  4 13:05:02.609: INFO: validating pod update-demo-nautilus-b5492
    May  4 13:05:02.616: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May  4 13:05:02.618: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May  4 13:05:02.618: INFO: update-demo-nautilus-b5492 is verified up and running
    May  4 13:05:02.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3634 get pods update-demo-nautilus-w5tsp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  4 13:05:02.737: INFO: stderr: ""
    May  4 13:05:02.737: INFO: stdout: "true"
    May  4 13:05:02.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3634 get pods update-demo-nautilus-w5tsp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May  4 13:05:02.851: INFO: stderr: ""
    May  4 13:05:02.851: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    May  4 13:05:02.851: INFO: validating pod update-demo-nautilus-w5tsp
    May  4 13:05:02.857: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May  4 13:05:02.857: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May  4 13:05:02.857: INFO: update-demo-nautilus-w5tsp is verified up and running
    STEP: scaling down the replication controller 05/04/23 13:05:02.857
    May  4 13:05:02.859: INFO: scanned /root for discovery docs: <nil>
    May  4 13:05:02.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3634 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    May  4 13:05:04.001: INFO: stderr: ""
    May  4 13:05:04.001: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 05/04/23 13:05:04.001
    May  4 13:05:04.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3634 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May  4 13:05:04.124: INFO: stderr: ""
    May  4 13:05:04.124: INFO: stdout: "update-demo-nautilus-b5492 "
    May  4 13:05:04.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3634 get pods update-demo-nautilus-b5492 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  4 13:05:04.258: INFO: stderr: ""
    May  4 13:05:04.258: INFO: stdout: "true"
    May  4 13:05:04.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3634 get pods update-demo-nautilus-b5492 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May  4 13:05:04.373: INFO: stderr: ""
    May  4 13:05:04.373: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    May  4 13:05:04.373: INFO: validating pod update-demo-nautilus-b5492
    May  4 13:05:04.378: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May  4 13:05:04.378: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May  4 13:05:04.378: INFO: update-demo-nautilus-b5492 is verified up and running
    STEP: scaling up the replication controller 05/04/23 13:05:04.378
    May  4 13:05:04.379: INFO: scanned /root for discovery docs: <nil>
    May  4 13:05:04.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3634 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    May  4 13:05:05.548: INFO: stderr: ""
    May  4 13:05:05.548: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 05/04/23 13:05:05.548
    May  4 13:05:05.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3634 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May  4 13:05:05.640: INFO: stderr: ""
    May  4 13:05:05.640: INFO: stdout: "update-demo-nautilus-6f9qr update-demo-nautilus-b5492 "
    May  4 13:05:05.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3634 get pods update-demo-nautilus-6f9qr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  4 13:05:05.776: INFO: stderr: ""
    May  4 13:05:05.776: INFO: stdout: ""
    May  4 13:05:05.776: INFO: update-demo-nautilus-6f9qr is created but not running
    May  4 13:05:10.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3634 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May  4 13:05:10.898: INFO: stderr: ""
    May  4 13:05:10.898: INFO: stdout: "update-demo-nautilus-6f9qr update-demo-nautilus-b5492 "
    May  4 13:05:10.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3634 get pods update-demo-nautilus-6f9qr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  4 13:05:10.996: INFO: stderr: ""
    May  4 13:05:10.996: INFO: stdout: "true"
    May  4 13:05:10.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3634 get pods update-demo-nautilus-6f9qr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May  4 13:05:11.111: INFO: stderr: ""
    May  4 13:05:11.111: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    May  4 13:05:11.111: INFO: validating pod update-demo-nautilus-6f9qr
    May  4 13:05:11.119: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May  4 13:05:11.119: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May  4 13:05:11.119: INFO: update-demo-nautilus-6f9qr is verified up and running
    May  4 13:05:11.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3634 get pods update-demo-nautilus-b5492 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  4 13:05:11.232: INFO: stderr: ""
    May  4 13:05:11.232: INFO: stdout: "true"
    May  4 13:05:11.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3634 get pods update-demo-nautilus-b5492 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May  4 13:05:11.331: INFO: stderr: ""
    May  4 13:05:11.331: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    May  4 13:05:11.331: INFO: validating pod update-demo-nautilus-b5492
    May  4 13:05:11.338: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May  4 13:05:11.338: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May  4 13:05:11.338: INFO: update-demo-nautilus-b5492 is verified up and running
    STEP: using delete to clean up resources 05/04/23 13:05:11.338
    May  4 13:05:11.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3634 delete --grace-period=0 --force -f -'
    May  4 13:05:11.429: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May  4 13:05:11.429: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    May  4 13:05:11.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3634 get rc,svc -l name=update-demo --no-headers'
    May  4 13:05:11.519: INFO: stderr: "No resources found in kubectl-3634 namespace.\n"
    May  4 13:05:11.519: INFO: stdout: ""
    May  4 13:05:11.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3634 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    May  4 13:05:11.603: INFO: stderr: ""
    May  4 13:05:11.603: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  4 13:05:11.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3634" for this suite. 05/04/23 13:05:11.614
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:05:11.626
May  4 13:05:11.626: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename statefulset 05/04/23 13:05:11.627
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:05:11.648
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:05:11.653
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-4801 05/04/23 13:05:11.661
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
STEP: Looking for a node to schedule stateful set and pod 05/04/23 13:05:11.67
STEP: Creating pod with conflicting port in namespace statefulset-4801 05/04/23 13:05:11.686
STEP: Waiting until pod test-pod will start running in namespace statefulset-4801 05/04/23 13:05:11.704
May  4 13:05:11.704: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-4801" to be "running"
May  4 13:05:11.711: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.321394ms
May  4 13:05:13.716: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0124241s
May  4 13:05:15.727: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.023676413s
May  4 13:05:15.728: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-4801 05/04/23 13:05:15.728
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-4801 05/04/23 13:05:15.736
May  4 13:05:15.773: INFO: Observed stateful pod in namespace: statefulset-4801, name: ss-0, uid: 095fce38-9af4-4c0b-a3f6-6cb5a48c4324, status phase: Pending. Waiting for statefulset controller to delete.
May  4 13:05:15.802: INFO: Observed stateful pod in namespace: statefulset-4801, name: ss-0, uid: 095fce38-9af4-4c0b-a3f6-6cb5a48c4324, status phase: Failed. Waiting for statefulset controller to delete.
May  4 13:05:15.813: INFO: Observed stateful pod in namespace: statefulset-4801, name: ss-0, uid: 095fce38-9af4-4c0b-a3f6-6cb5a48c4324, status phase: Failed. Waiting for statefulset controller to delete.
May  4 13:05:15.820: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-4801
STEP: Removing pod with conflicting port in namespace statefulset-4801 05/04/23 13:05:15.82
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-4801 and will be in running state 05/04/23 13:05:15.842
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
May  4 13:05:19.869: INFO: Deleting all statefulset in ns statefulset-4801
May  4 13:05:19.875: INFO: Scaling statefulset ss to 0
May  4 13:05:29.952: INFO: Waiting for statefulset status.replicas updated to 0
May  4 13:05:29.959: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
May  4 13:05:29.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4801" for this suite. 05/04/23 13:05:30.007
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","completed":298,"skipped":5527,"failed":0}
------------------------------
• [SLOW TEST] [18.392 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:737

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:05:11.626
    May  4 13:05:11.626: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename statefulset 05/04/23 13:05:11.627
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:05:11.648
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:05:11.653
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-4801 05/04/23 13:05:11.661
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:737
    STEP: Looking for a node to schedule stateful set and pod 05/04/23 13:05:11.67
    STEP: Creating pod with conflicting port in namespace statefulset-4801 05/04/23 13:05:11.686
    STEP: Waiting until pod test-pod will start running in namespace statefulset-4801 05/04/23 13:05:11.704
    May  4 13:05:11.704: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-4801" to be "running"
    May  4 13:05:11.711: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.321394ms
    May  4 13:05:13.716: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0124241s
    May  4 13:05:15.727: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.023676413s
    May  4 13:05:15.728: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-4801 05/04/23 13:05:15.728
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-4801 05/04/23 13:05:15.736
    May  4 13:05:15.773: INFO: Observed stateful pod in namespace: statefulset-4801, name: ss-0, uid: 095fce38-9af4-4c0b-a3f6-6cb5a48c4324, status phase: Pending. Waiting for statefulset controller to delete.
    May  4 13:05:15.802: INFO: Observed stateful pod in namespace: statefulset-4801, name: ss-0, uid: 095fce38-9af4-4c0b-a3f6-6cb5a48c4324, status phase: Failed. Waiting for statefulset controller to delete.
    May  4 13:05:15.813: INFO: Observed stateful pod in namespace: statefulset-4801, name: ss-0, uid: 095fce38-9af4-4c0b-a3f6-6cb5a48c4324, status phase: Failed. Waiting for statefulset controller to delete.
    May  4 13:05:15.820: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-4801
    STEP: Removing pod with conflicting port in namespace statefulset-4801 05/04/23 13:05:15.82
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-4801 and will be in running state 05/04/23 13:05:15.842
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    May  4 13:05:19.869: INFO: Deleting all statefulset in ns statefulset-4801
    May  4 13:05:19.875: INFO: Scaling statefulset ss to 0
    May  4 13:05:29.952: INFO: Waiting for statefulset status.replicas updated to 0
    May  4 13:05:29.959: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    May  4 13:05:29.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-4801" for this suite. 05/04/23 13:05:30.007
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:05:30.02
May  4 13:05:30.020: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename container-probe 05/04/23 13:05:30.021
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:05:30.055
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:05:30.06
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
STEP: Creating pod busybox-5135f6a1-472f-4081-a9ce-1ba4dc27bc95 in namespace container-probe-9051 05/04/23 13:05:30.065
May  4 13:05:30.094: INFO: Waiting up to 5m0s for pod "busybox-5135f6a1-472f-4081-a9ce-1ba4dc27bc95" in namespace "container-probe-9051" to be "not pending"
May  4 13:05:30.100: INFO: Pod "busybox-5135f6a1-472f-4081-a9ce-1ba4dc27bc95": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012802ms
May  4 13:05:32.105: INFO: Pod "busybox-5135f6a1-472f-4081-a9ce-1ba4dc27bc95": Phase="Running", Reason="", readiness=true. Elapsed: 2.011000587s
May  4 13:05:32.105: INFO: Pod "busybox-5135f6a1-472f-4081-a9ce-1ba4dc27bc95" satisfied condition "not pending"
May  4 13:05:32.105: INFO: Started pod busybox-5135f6a1-472f-4081-a9ce-1ba4dc27bc95 in namespace container-probe-9051
STEP: checking the pod's current state and verifying that restartCount is present 05/04/23 13:05:32.105
May  4 13:05:32.109: INFO: Initial restart count of pod busybox-5135f6a1-472f-4081-a9ce-1ba4dc27bc95 is 0
May  4 13:06:22.287: INFO: Restart count of pod container-probe-9051/busybox-5135f6a1-472f-4081-a9ce-1ba4dc27bc95 is now 1 (50.177642953s elapsed)
STEP: deleting the pod 05/04/23 13:06:22.287
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
May  4 13:06:22.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9051" for this suite. 05/04/23 13:06:22.349
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":299,"skipped":5562,"failed":0}
------------------------------
• [SLOW TEST] [52.346 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:05:30.02
    May  4 13:05:30.020: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename container-probe 05/04/23 13:05:30.021
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:05:30.055
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:05:30.06
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:131
    STEP: Creating pod busybox-5135f6a1-472f-4081-a9ce-1ba4dc27bc95 in namespace container-probe-9051 05/04/23 13:05:30.065
    May  4 13:05:30.094: INFO: Waiting up to 5m0s for pod "busybox-5135f6a1-472f-4081-a9ce-1ba4dc27bc95" in namespace "container-probe-9051" to be "not pending"
    May  4 13:05:30.100: INFO: Pod "busybox-5135f6a1-472f-4081-a9ce-1ba4dc27bc95": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012802ms
    May  4 13:05:32.105: INFO: Pod "busybox-5135f6a1-472f-4081-a9ce-1ba4dc27bc95": Phase="Running", Reason="", readiness=true. Elapsed: 2.011000587s
    May  4 13:05:32.105: INFO: Pod "busybox-5135f6a1-472f-4081-a9ce-1ba4dc27bc95" satisfied condition "not pending"
    May  4 13:05:32.105: INFO: Started pod busybox-5135f6a1-472f-4081-a9ce-1ba4dc27bc95 in namespace container-probe-9051
    STEP: checking the pod's current state and verifying that restartCount is present 05/04/23 13:05:32.105
    May  4 13:05:32.109: INFO: Initial restart count of pod busybox-5135f6a1-472f-4081-a9ce-1ba4dc27bc95 is 0
    May  4 13:06:22.287: INFO: Restart count of pod container-probe-9051/busybox-5135f6a1-472f-4081-a9ce-1ba4dc27bc95 is now 1 (50.177642953s elapsed)
    STEP: deleting the pod 05/04/23 13:06:22.287
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    May  4 13:06:22.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-9051" for this suite. 05/04/23 13:06:22.349
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:06:22.367
May  4 13:06:22.367: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename webhook 05/04/23 13:06:22.368
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:06:22.4
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:06:22.406
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/04/23 13:06:22.447
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/04/23 13:06:22.952
STEP: Deploying the webhook pod 05/04/23 13:06:22.962
STEP: Wait for the deployment to be ready 05/04/23 13:06:22.985
May  4 13:06:22.998: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 05/04/23 13:06:25.014
STEP: Verifying the service has paired with the endpoint 05/04/23 13:06:25.037
May  4 13:06:26.037: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
May  4 13:06:26.042: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Registering the custom resource webhook via the AdmissionRegistration API 05/04/23 13:06:26.554
STEP: Creating a custom resource that should be denied by the webhook 05/04/23 13:06:26.579
STEP: Creating a custom resource whose deletion would be denied by the webhook 05/04/23 13:06:28.672
STEP: Updating the custom resource with disallowed data should be denied 05/04/23 13:06:28.687
STEP: Deleting the custom resource should be denied 05/04/23 13:06:28.698
STEP: Remove the offending key and value from the custom resource data 05/04/23 13:06:28.707
STEP: Deleting the updated custom resource should be successful 05/04/23 13:06:28.72
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  4 13:06:29.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4104" for this suite. 05/04/23 13:06:29.266
STEP: Destroying namespace "webhook-4104-markers" for this suite. 05/04/23 13:06:29.277
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","completed":300,"skipped":5562,"failed":0}
------------------------------
• [SLOW TEST] [7.036 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:06:22.367
    May  4 13:06:22.367: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename webhook 05/04/23 13:06:22.368
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:06:22.4
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:06:22.406
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/04/23 13:06:22.447
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/04/23 13:06:22.952
    STEP: Deploying the webhook pod 05/04/23 13:06:22.962
    STEP: Wait for the deployment to be ready 05/04/23 13:06:22.985
    May  4 13:06:22.998: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 05/04/23 13:06:25.014
    STEP: Verifying the service has paired with the endpoint 05/04/23 13:06:25.037
    May  4 13:06:26.037: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:220
    May  4 13:06:26.042: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 05/04/23 13:06:26.554
    STEP: Creating a custom resource that should be denied by the webhook 05/04/23 13:06:26.579
    STEP: Creating a custom resource whose deletion would be denied by the webhook 05/04/23 13:06:28.672
    STEP: Updating the custom resource with disallowed data should be denied 05/04/23 13:06:28.687
    STEP: Deleting the custom resource should be denied 05/04/23 13:06:28.698
    STEP: Remove the offending key and value from the custom resource data 05/04/23 13:06:28.707
    STEP: Deleting the updated custom resource should be successful 05/04/23 13:06:28.72
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  4 13:06:29.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4104" for this suite. 05/04/23 13:06:29.266
    STEP: Destroying namespace "webhook-4104-markers" for this suite. 05/04/23 13:06:29.277
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:06:29.404
May  4 13:06:29.404: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename services 05/04/23 13:06:29.405
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:06:29.452
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:06:29.457
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
STEP: creating a service nodeport-service with the type=NodePort in namespace services-2373 05/04/23 13:06:29.461
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 05/04/23 13:06:29.537
STEP: creating service externalsvc in namespace services-2373 05/04/23 13:06:29.537
STEP: creating replication controller externalsvc in namespace services-2373 05/04/23 13:06:29.607
I0504 13:06:29.633808      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-2373, replica count: 2
I0504 13:06:32.686040      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 05/04/23 13:06:32.692
May  4 13:06:32.733: INFO: Creating new exec pod
May  4 13:06:32.767: INFO: Waiting up to 5m0s for pod "execpoddfrbk" in namespace "services-2373" to be "running"
May  4 13:06:32.775: INFO: Pod "execpoddfrbk": Phase="Pending", Reason="", readiness=false. Elapsed: 8.130263ms
May  4 13:06:34.783: INFO: Pod "execpoddfrbk": Phase="Running", Reason="", readiness=true. Elapsed: 2.016452178s
May  4 13:06:34.783: INFO: Pod "execpoddfrbk" satisfied condition "running"
May  4 13:06:34.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-2373 exec execpoddfrbk -- /bin/sh -x -c nslookup nodeport-service.services-2373.svc.cluster.local'
May  4 13:06:34.986: INFO: stderr: "+ nslookup nodeport-service.services-2373.svc.cluster.local\n"
May  4 13:06:34.986: INFO: stdout: "Server:\t\t10.21.0.10\nAddress:\t10.21.0.10#53\n\nnodeport-service.services-2373.svc.cluster.local\tcanonical name = externalsvc.services-2373.svc.cluster.local.\nName:\texternalsvc.services-2373.svc.cluster.local\nAddress: 10.21.24.131\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-2373, will wait for the garbage collector to delete the pods 05/04/23 13:06:34.986
May  4 13:06:35.052: INFO: Deleting ReplicationController externalsvc took: 10.456587ms
May  4 13:06:35.153: INFO: Terminating ReplicationController externalsvc pods took: 100.310769ms
May  4 13:06:37.401: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  4 13:06:37.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2373" for this suite. 05/04/23 13:06:37.461
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","completed":301,"skipped":5562,"failed":0}
------------------------------
• [SLOW TEST] [8.071 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:06:29.404
    May  4 13:06:29.404: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename services 05/04/23 13:06:29.405
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:06:29.452
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:06:29.457
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1523
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-2373 05/04/23 13:06:29.461
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 05/04/23 13:06:29.537
    STEP: creating service externalsvc in namespace services-2373 05/04/23 13:06:29.537
    STEP: creating replication controller externalsvc in namespace services-2373 05/04/23 13:06:29.607
    I0504 13:06:29.633808      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-2373, replica count: 2
    I0504 13:06:32.686040      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 05/04/23 13:06:32.692
    May  4 13:06:32.733: INFO: Creating new exec pod
    May  4 13:06:32.767: INFO: Waiting up to 5m0s for pod "execpoddfrbk" in namespace "services-2373" to be "running"
    May  4 13:06:32.775: INFO: Pod "execpoddfrbk": Phase="Pending", Reason="", readiness=false. Elapsed: 8.130263ms
    May  4 13:06:34.783: INFO: Pod "execpoddfrbk": Phase="Running", Reason="", readiness=true. Elapsed: 2.016452178s
    May  4 13:06:34.783: INFO: Pod "execpoddfrbk" satisfied condition "running"
    May  4 13:06:34.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-2373 exec execpoddfrbk -- /bin/sh -x -c nslookup nodeport-service.services-2373.svc.cluster.local'
    May  4 13:06:34.986: INFO: stderr: "+ nslookup nodeport-service.services-2373.svc.cluster.local\n"
    May  4 13:06:34.986: INFO: stdout: "Server:\t\t10.21.0.10\nAddress:\t10.21.0.10#53\n\nnodeport-service.services-2373.svc.cluster.local\tcanonical name = externalsvc.services-2373.svc.cluster.local.\nName:\texternalsvc.services-2373.svc.cluster.local\nAddress: 10.21.24.131\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-2373, will wait for the garbage collector to delete the pods 05/04/23 13:06:34.986
    May  4 13:06:35.052: INFO: Deleting ReplicationController externalsvc took: 10.456587ms
    May  4 13:06:35.153: INFO: Terminating ReplicationController externalsvc pods took: 100.310769ms
    May  4 13:06:37.401: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  4 13:06:37.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2373" for this suite. 05/04/23 13:06:37.461
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:06:37.475
May  4 13:06:37.475: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename webhook 05/04/23 13:06:37.476
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:06:37.506
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:06:37.511
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/04/23 13:06:37.537
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/04/23 13:06:37.959
STEP: Deploying the webhook pod 05/04/23 13:06:37.968
STEP: Wait for the deployment to be ready 05/04/23 13:06:37.984
May  4 13:06:37.998: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/04/23 13:06:40.018
STEP: Verifying the service has paired with the endpoint 05/04/23 13:06:40.039
May  4 13:06:41.040: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
STEP: Creating a mutating webhook configuration 05/04/23 13:06:41.05
STEP: Updating a mutating webhook configuration's rules to not include the create operation 05/04/23 13:06:41.088
STEP: Creating a configMap that should not be mutated 05/04/23 13:06:41.108
STEP: Patching a mutating webhook configuration's rules to include the create operation 05/04/23 13:06:41.132
STEP: Creating a configMap that should be mutated 05/04/23 13:06:41.153
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  4 13:06:41.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6820" for this suite. 05/04/23 13:06:41.205
STEP: Destroying namespace "webhook-6820-markers" for this suite. 05/04/23 13:06:41.218
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","completed":302,"skipped":5566,"failed":0}
------------------------------
• [3.849 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:06:37.475
    May  4 13:06:37.475: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename webhook 05/04/23 13:06:37.476
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:06:37.506
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:06:37.511
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/04/23 13:06:37.537
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/04/23 13:06:37.959
    STEP: Deploying the webhook pod 05/04/23 13:06:37.968
    STEP: Wait for the deployment to be ready 05/04/23 13:06:37.984
    May  4 13:06:37.998: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/04/23 13:06:40.018
    STEP: Verifying the service has paired with the endpoint 05/04/23 13:06:40.039
    May  4 13:06:41.040: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:507
    STEP: Creating a mutating webhook configuration 05/04/23 13:06:41.05
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 05/04/23 13:06:41.088
    STEP: Creating a configMap that should not be mutated 05/04/23 13:06:41.108
    STEP: Patching a mutating webhook configuration's rules to include the create operation 05/04/23 13:06:41.132
    STEP: Creating a configMap that should be mutated 05/04/23 13:06:41.153
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  4 13:06:41.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6820" for this suite. 05/04/23 13:06:41.205
    STEP: Destroying namespace "webhook-6820-markers" for this suite. 05/04/23 13:06:41.218
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:06:41.326
May  4 13:06:41.326: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename job 05/04/23 13:06:41.326
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:06:41.372
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:06:41.378
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
STEP: Creating Indexed job 05/04/23 13:06:41.381
STEP: Ensuring job reaches completions 05/04/23 13:06:41.389
STEP: Ensuring pods with index for job exist 05/04/23 13:06:51.395
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
May  4 13:06:51.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8259" for this suite. 05/04/23 13:06:51.411
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","completed":303,"skipped":5574,"failed":0}
------------------------------
• [SLOW TEST] [10.100 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:06:41.326
    May  4 13:06:41.326: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename job 05/04/23 13:06:41.326
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:06:41.372
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:06:41.378
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:194
    STEP: Creating Indexed job 05/04/23 13:06:41.381
    STEP: Ensuring job reaches completions 05/04/23 13:06:41.389
    STEP: Ensuring pods with index for job exist 05/04/23 13:06:51.395
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    May  4 13:06:51.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-8259" for this suite. 05/04/23 13:06:51.411
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:06:51.427
May  4 13:06:51.427: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename replicaset 05/04/23 13:06:51.428
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:06:51.453
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:06:51.458
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 05/04/23 13:06:51.462
May  4 13:06:51.474: INFO: Pod name sample-pod: Found 0 pods out of 1
May  4 13:06:56.483: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 05/04/23 13:06:56.483
STEP: getting scale subresource 05/04/23 13:06:56.484
STEP: updating a scale subresource 05/04/23 13:06:56.492
STEP: verifying the replicaset Spec.Replicas was modified 05/04/23 13:06:56.507
STEP: Patch a scale subresource 05/04/23 13:06:56.52
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
May  4 13:06:56.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8288" for this suite. 05/04/23 13:06:56.559
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","completed":304,"skipped":5605,"failed":0}
------------------------------
• [SLOW TEST] [5.163 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:06:51.427
    May  4 13:06:51.427: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename replicaset 05/04/23 13:06:51.428
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:06:51.453
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:06:51.458
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 05/04/23 13:06:51.462
    May  4 13:06:51.474: INFO: Pod name sample-pod: Found 0 pods out of 1
    May  4 13:06:56.483: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 05/04/23 13:06:56.483
    STEP: getting scale subresource 05/04/23 13:06:56.484
    STEP: updating a scale subresource 05/04/23 13:06:56.492
    STEP: verifying the replicaset Spec.Replicas was modified 05/04/23 13:06:56.507
    STEP: Patch a scale subresource 05/04/23 13:06:56.52
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    May  4 13:06:56.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-8288" for this suite. 05/04/23 13:06:56.559
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:06:56.592
May  4 13:06:56.592: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename downward-api 05/04/23 13:06:56.593
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:06:56.651
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:06:56.659
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
STEP: Creating a pod to test downward API volume plugin 05/04/23 13:06:56.666
May  4 13:06:56.682: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3b360a02-0576-4f21-bc77-d7fe20e58f03" in namespace "downward-api-4758" to be "Succeeded or Failed"
May  4 13:06:56.689: INFO: Pod "downwardapi-volume-3b360a02-0576-4f21-bc77-d7fe20e58f03": Phase="Pending", Reason="", readiness=false. Elapsed: 6.498111ms
May  4 13:06:58.695: INFO: Pod "downwardapi-volume-3b360a02-0576-4f21-bc77-d7fe20e58f03": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012611309s
May  4 13:07:00.695: INFO: Pod "downwardapi-volume-3b360a02-0576-4f21-bc77-d7fe20e58f03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012801877s
STEP: Saw pod success 05/04/23 13:07:00.695
May  4 13:07:00.695: INFO: Pod "downwardapi-volume-3b360a02-0576-4f21-bc77-d7fe20e58f03" satisfied condition "Succeeded or Failed"
May  4 13:07:00.701: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod downwardapi-volume-3b360a02-0576-4f21-bc77-d7fe20e58f03 container client-container: <nil>
STEP: delete the pod 05/04/23 13:07:00.721
May  4 13:07:00.744: INFO: Waiting for pod downwardapi-volume-3b360a02-0576-4f21-bc77-d7fe20e58f03 to disappear
May  4 13:07:00.752: INFO: Pod downwardapi-volume-3b360a02-0576-4f21-bc77-d7fe20e58f03 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
May  4 13:07:00.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4758" for this suite. 05/04/23 13:07:00.762
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":305,"skipped":5642,"failed":0}
------------------------------
• [4.184 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:06:56.592
    May  4 13:06:56.592: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename downward-api 05/04/23 13:06:56.593
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:06:56.651
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:06:56.659
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:67
    STEP: Creating a pod to test downward API volume plugin 05/04/23 13:06:56.666
    May  4 13:06:56.682: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3b360a02-0576-4f21-bc77-d7fe20e58f03" in namespace "downward-api-4758" to be "Succeeded or Failed"
    May  4 13:06:56.689: INFO: Pod "downwardapi-volume-3b360a02-0576-4f21-bc77-d7fe20e58f03": Phase="Pending", Reason="", readiness=false. Elapsed: 6.498111ms
    May  4 13:06:58.695: INFO: Pod "downwardapi-volume-3b360a02-0576-4f21-bc77-d7fe20e58f03": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012611309s
    May  4 13:07:00.695: INFO: Pod "downwardapi-volume-3b360a02-0576-4f21-bc77-d7fe20e58f03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012801877s
    STEP: Saw pod success 05/04/23 13:07:00.695
    May  4 13:07:00.695: INFO: Pod "downwardapi-volume-3b360a02-0576-4f21-bc77-d7fe20e58f03" satisfied condition "Succeeded or Failed"
    May  4 13:07:00.701: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod downwardapi-volume-3b360a02-0576-4f21-bc77-d7fe20e58f03 container client-container: <nil>
    STEP: delete the pod 05/04/23 13:07:00.721
    May  4 13:07:00.744: INFO: Waiting for pod downwardapi-volume-3b360a02-0576-4f21-bc77-d7fe20e58f03 to disappear
    May  4 13:07:00.752: INFO: Pod downwardapi-volume-3b360a02-0576-4f21-bc77-d7fe20e58f03 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    May  4 13:07:00.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-4758" for this suite. 05/04/23 13:07:00.762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:07:00.779
May  4 13:07:00.779: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename configmap 05/04/23 13:07:00.78
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:07:00.801
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:07:00.804
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
STEP: Creating configMap configmap-4354/configmap-test-7509c03c-7f8a-40b3-81cf-47445f65c100 05/04/23 13:07:00.806
STEP: Creating a pod to test consume configMaps 05/04/23 13:07:00.812
May  4 13:07:00.832: INFO: Waiting up to 5m0s for pod "pod-configmaps-7f5ad9d1-edab-44f9-9cb5-bf89c1fb5f9c" in namespace "configmap-4354" to be "Succeeded or Failed"
May  4 13:07:00.842: INFO: Pod "pod-configmaps-7f5ad9d1-edab-44f9-9cb5-bf89c1fb5f9c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.972379ms
May  4 13:07:02.847: INFO: Pod "pod-configmaps-7f5ad9d1-edab-44f9-9cb5-bf89c1fb5f9c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015321071s
May  4 13:07:04.848: INFO: Pod "pod-configmaps-7f5ad9d1-edab-44f9-9cb5-bf89c1fb5f9c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015988714s
STEP: Saw pod success 05/04/23 13:07:04.848
May  4 13:07:04.848: INFO: Pod "pod-configmaps-7f5ad9d1-edab-44f9-9cb5-bf89c1fb5f9c" satisfied condition "Succeeded or Failed"
May  4 13:07:04.860: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-configmaps-7f5ad9d1-edab-44f9-9cb5-bf89c1fb5f9c container env-test: <nil>
STEP: delete the pod 05/04/23 13:07:04.87
May  4 13:07:04.890: INFO: Waiting for pod pod-configmaps-7f5ad9d1-edab-44f9-9cb5-bf89c1fb5f9c to disappear
May  4 13:07:04.895: INFO: Pod pod-configmaps-7f5ad9d1-edab-44f9-9cb5-bf89c1fb5f9c no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
May  4 13:07:04.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4354" for this suite. 05/04/23 13:07:04.906
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","completed":306,"skipped":5706,"failed":0}
------------------------------
• [4.143 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:07:00.779
    May  4 13:07:00.779: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename configmap 05/04/23 13:07:00.78
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:07:00.801
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:07:00.804
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:44
    STEP: Creating configMap configmap-4354/configmap-test-7509c03c-7f8a-40b3-81cf-47445f65c100 05/04/23 13:07:00.806
    STEP: Creating a pod to test consume configMaps 05/04/23 13:07:00.812
    May  4 13:07:00.832: INFO: Waiting up to 5m0s for pod "pod-configmaps-7f5ad9d1-edab-44f9-9cb5-bf89c1fb5f9c" in namespace "configmap-4354" to be "Succeeded or Failed"
    May  4 13:07:00.842: INFO: Pod "pod-configmaps-7f5ad9d1-edab-44f9-9cb5-bf89c1fb5f9c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.972379ms
    May  4 13:07:02.847: INFO: Pod "pod-configmaps-7f5ad9d1-edab-44f9-9cb5-bf89c1fb5f9c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015321071s
    May  4 13:07:04.848: INFO: Pod "pod-configmaps-7f5ad9d1-edab-44f9-9cb5-bf89c1fb5f9c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015988714s
    STEP: Saw pod success 05/04/23 13:07:04.848
    May  4 13:07:04.848: INFO: Pod "pod-configmaps-7f5ad9d1-edab-44f9-9cb5-bf89c1fb5f9c" satisfied condition "Succeeded or Failed"
    May  4 13:07:04.860: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-configmaps-7f5ad9d1-edab-44f9-9cb5-bf89c1fb5f9c container env-test: <nil>
    STEP: delete the pod 05/04/23 13:07:04.87
    May  4 13:07:04.890: INFO: Waiting for pod pod-configmaps-7f5ad9d1-edab-44f9-9cb5-bf89c1fb5f9c to disappear
    May  4 13:07:04.895: INFO: Pod pod-configmaps-7f5ad9d1-edab-44f9-9cb5-bf89c1fb5f9c no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    May  4 13:07:04.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4354" for this suite. 05/04/23 13:07:04.906
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:07:04.922
May  4 13:07:04.922: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename server-version 05/04/23 13:07:04.923
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:07:04.955
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:07:04.963
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 05/04/23 13:07:04.969
STEP: Confirm major version 05/04/23 13:07:04.972
May  4 13:07:04.972: INFO: Major version: 1
STEP: Confirm minor version 05/04/23 13:07:04.972
May  4 13:07:04.972: INFO: cleanMinorVersion: 25
May  4 13:07:04.972: INFO: Minor version: 25
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
May  4 13:07:04.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-8636" for this suite. 05/04/23 13:07:04.996
{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","completed":307,"skipped":5711,"failed":0}
------------------------------
• [0.086 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:07:04.922
    May  4 13:07:04.922: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename server-version 05/04/23 13:07:04.923
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:07:04.955
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:07:04.963
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 05/04/23 13:07:04.969
    STEP: Confirm major version 05/04/23 13:07:04.972
    May  4 13:07:04.972: INFO: Major version: 1
    STEP: Confirm minor version 05/04/23 13:07:04.972
    May  4 13:07:04.972: INFO: cleanMinorVersion: 25
    May  4 13:07:04.972: INFO: Minor version: 25
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:187
    May  4 13:07:04.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "server-version-8636" for this suite. 05/04/23 13:07:04.996
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:07:05.008
May  4 13:07:05.008: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename pods 05/04/23 13:07:05.009
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:07:05.045
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:07:05.05
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
STEP: creating the pod 05/04/23 13:07:05.053
STEP: submitting the pod to kubernetes 05/04/23 13:07:05.053
May  4 13:07:05.066: INFO: Waiting up to 5m0s for pod "pod-update-b88897ec-8560-4d5c-b4c4-301ee5654bc0" in namespace "pods-3901" to be "running and ready"
May  4 13:07:05.071: INFO: Pod "pod-update-b88897ec-8560-4d5c-b4c4-301ee5654bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.942122ms
May  4 13:07:05.071: INFO: The phase of Pod pod-update-b88897ec-8560-4d5c-b4c4-301ee5654bc0 is Pending, waiting for it to be Running (with Ready = true)
May  4 13:07:07.077: INFO: Pod "pod-update-b88897ec-8560-4d5c-b4c4-301ee5654bc0": Phase="Running", Reason="", readiness=true. Elapsed: 2.011197348s
May  4 13:07:07.077: INFO: The phase of Pod pod-update-b88897ec-8560-4d5c-b4c4-301ee5654bc0 is Running (Ready = true)
May  4 13:07:07.077: INFO: Pod "pod-update-b88897ec-8560-4d5c-b4c4-301ee5654bc0" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 05/04/23 13:07:07.082
STEP: updating the pod 05/04/23 13:07:07.087
May  4 13:07:07.603: INFO: Successfully updated pod "pod-update-b88897ec-8560-4d5c-b4c4-301ee5654bc0"
May  4 13:07:07.603: INFO: Waiting up to 5m0s for pod "pod-update-b88897ec-8560-4d5c-b4c4-301ee5654bc0" in namespace "pods-3901" to be "running"
May  4 13:07:07.607: INFO: Pod "pod-update-b88897ec-8560-4d5c-b4c4-301ee5654bc0": Phase="Running", Reason="", readiness=true. Elapsed: 3.921528ms
May  4 13:07:07.607: INFO: Pod "pod-update-b88897ec-8560-4d5c-b4c4-301ee5654bc0" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 05/04/23 13:07:07.607
May  4 13:07:07.611: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
May  4 13:07:07.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3901" for this suite. 05/04/23 13:07:07.621
{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","completed":308,"skipped":5719,"failed":0}
------------------------------
• [2.626 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:07:05.008
    May  4 13:07:05.008: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename pods 05/04/23 13:07:05.009
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:07:05.045
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:07:05.05
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:343
    STEP: creating the pod 05/04/23 13:07:05.053
    STEP: submitting the pod to kubernetes 05/04/23 13:07:05.053
    May  4 13:07:05.066: INFO: Waiting up to 5m0s for pod "pod-update-b88897ec-8560-4d5c-b4c4-301ee5654bc0" in namespace "pods-3901" to be "running and ready"
    May  4 13:07:05.071: INFO: Pod "pod-update-b88897ec-8560-4d5c-b4c4-301ee5654bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.942122ms
    May  4 13:07:05.071: INFO: The phase of Pod pod-update-b88897ec-8560-4d5c-b4c4-301ee5654bc0 is Pending, waiting for it to be Running (with Ready = true)
    May  4 13:07:07.077: INFO: Pod "pod-update-b88897ec-8560-4d5c-b4c4-301ee5654bc0": Phase="Running", Reason="", readiness=true. Elapsed: 2.011197348s
    May  4 13:07:07.077: INFO: The phase of Pod pod-update-b88897ec-8560-4d5c-b4c4-301ee5654bc0 is Running (Ready = true)
    May  4 13:07:07.077: INFO: Pod "pod-update-b88897ec-8560-4d5c-b4c4-301ee5654bc0" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 05/04/23 13:07:07.082
    STEP: updating the pod 05/04/23 13:07:07.087
    May  4 13:07:07.603: INFO: Successfully updated pod "pod-update-b88897ec-8560-4d5c-b4c4-301ee5654bc0"
    May  4 13:07:07.603: INFO: Waiting up to 5m0s for pod "pod-update-b88897ec-8560-4d5c-b4c4-301ee5654bc0" in namespace "pods-3901" to be "running"
    May  4 13:07:07.607: INFO: Pod "pod-update-b88897ec-8560-4d5c-b4c4-301ee5654bc0": Phase="Running", Reason="", readiness=true. Elapsed: 3.921528ms
    May  4 13:07:07.607: INFO: Pod "pod-update-b88897ec-8560-4d5c-b4c4-301ee5654bc0" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 05/04/23 13:07:07.607
    May  4 13:07:07.611: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    May  4 13:07:07.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-3901" for this suite. 05/04/23 13:07:07.621
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:07:07.635
May  4 13:07:07.635: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename secrets 05/04/23 13:07:07.636
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:07:07.659
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:07:07.671
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
STEP: Creating secret with name secret-test-map-d9cf7549-c8f7-496e-92dc-13069ba37a8c 05/04/23 13:07:07.673
STEP: Creating a pod to test consume secrets 05/04/23 13:07:07.681
May  4 13:07:07.694: INFO: Waiting up to 5m0s for pod "pod-secrets-180cf1cb-b2d0-4c93-b513-fd6ab91728ae" in namespace "secrets-8154" to be "Succeeded or Failed"
May  4 13:07:07.698: INFO: Pod "pod-secrets-180cf1cb-b2d0-4c93-b513-fd6ab91728ae": Phase="Pending", Reason="", readiness=false. Elapsed: 4.101865ms
May  4 13:07:09.704: INFO: Pod "pod-secrets-180cf1cb-b2d0-4c93-b513-fd6ab91728ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009745088s
May  4 13:07:11.706: INFO: Pod "pod-secrets-180cf1cb-b2d0-4c93-b513-fd6ab91728ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012226238s
STEP: Saw pod success 05/04/23 13:07:11.706
May  4 13:07:11.706: INFO: Pod "pod-secrets-180cf1cb-b2d0-4c93-b513-fd6ab91728ae" satisfied condition "Succeeded or Failed"
May  4 13:07:11.712: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-secrets-180cf1cb-b2d0-4c93-b513-fd6ab91728ae container secret-volume-test: <nil>
STEP: delete the pod 05/04/23 13:07:11.724
May  4 13:07:11.744: INFO: Waiting for pod pod-secrets-180cf1cb-b2d0-4c93-b513-fd6ab91728ae to disappear
May  4 13:07:11.749: INFO: Pod pod-secrets-180cf1cb-b2d0-4c93-b513-fd6ab91728ae no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
May  4 13:07:11.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8154" for this suite. 05/04/23 13:07:11.761
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":309,"skipped":5721,"failed":0}
------------------------------
• [4.134 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:07:07.635
    May  4 13:07:07.635: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename secrets 05/04/23 13:07:07.636
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:07:07.659
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:07:07.671
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:78
    STEP: Creating secret with name secret-test-map-d9cf7549-c8f7-496e-92dc-13069ba37a8c 05/04/23 13:07:07.673
    STEP: Creating a pod to test consume secrets 05/04/23 13:07:07.681
    May  4 13:07:07.694: INFO: Waiting up to 5m0s for pod "pod-secrets-180cf1cb-b2d0-4c93-b513-fd6ab91728ae" in namespace "secrets-8154" to be "Succeeded or Failed"
    May  4 13:07:07.698: INFO: Pod "pod-secrets-180cf1cb-b2d0-4c93-b513-fd6ab91728ae": Phase="Pending", Reason="", readiness=false. Elapsed: 4.101865ms
    May  4 13:07:09.704: INFO: Pod "pod-secrets-180cf1cb-b2d0-4c93-b513-fd6ab91728ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009745088s
    May  4 13:07:11.706: INFO: Pod "pod-secrets-180cf1cb-b2d0-4c93-b513-fd6ab91728ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012226238s
    STEP: Saw pod success 05/04/23 13:07:11.706
    May  4 13:07:11.706: INFO: Pod "pod-secrets-180cf1cb-b2d0-4c93-b513-fd6ab91728ae" satisfied condition "Succeeded or Failed"
    May  4 13:07:11.712: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-secrets-180cf1cb-b2d0-4c93-b513-fd6ab91728ae container secret-volume-test: <nil>
    STEP: delete the pod 05/04/23 13:07:11.724
    May  4 13:07:11.744: INFO: Waiting for pod pod-secrets-180cf1cb-b2d0-4c93-b513-fd6ab91728ae to disappear
    May  4 13:07:11.749: INFO: Pod pod-secrets-180cf1cb-b2d0-4c93-b513-fd6ab91728ae no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    May  4 13:07:11.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8154" for this suite. 05/04/23 13:07:11.761
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:07:11.769
May  4 13:07:11.769: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename var-expansion 05/04/23 13:07:11.77
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:07:11.787
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:07:11.795
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
STEP: Creating a pod to test substitution in container's command 05/04/23 13:07:11.798
May  4 13:07:11.812: INFO: Waiting up to 5m0s for pod "var-expansion-e9369a12-7b96-455e-a99d-4b81fb1e477a" in namespace "var-expansion-9706" to be "Succeeded or Failed"
May  4 13:07:11.816: INFO: Pod "var-expansion-e9369a12-7b96-455e-a99d-4b81fb1e477a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.394117ms
May  4 13:07:13.822: INFO: Pod "var-expansion-e9369a12-7b96-455e-a99d-4b81fb1e477a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009885908s
May  4 13:07:15.822: INFO: Pod "var-expansion-e9369a12-7b96-455e-a99d-4b81fb1e477a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010170182s
STEP: Saw pod success 05/04/23 13:07:15.822
May  4 13:07:15.823: INFO: Pod "var-expansion-e9369a12-7b96-455e-a99d-4b81fb1e477a" satisfied condition "Succeeded or Failed"
May  4 13:07:15.827: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod var-expansion-e9369a12-7b96-455e-a99d-4b81fb1e477a container dapi-container: <nil>
STEP: delete the pod 05/04/23 13:07:15.837
May  4 13:07:15.853: INFO: Waiting for pod var-expansion-e9369a12-7b96-455e-a99d-4b81fb1e477a to disappear
May  4 13:07:15.858: INFO: Pod var-expansion-e9369a12-7b96-455e-a99d-4b81fb1e477a no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
May  4 13:07:15.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9706" for this suite. 05/04/23 13:07:15.882
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","completed":310,"skipped":5732,"failed":0}
------------------------------
• [4.121 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:07:11.769
    May  4 13:07:11.769: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename var-expansion 05/04/23 13:07:11.77
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:07:11.787
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:07:11.795
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:72
    STEP: Creating a pod to test substitution in container's command 05/04/23 13:07:11.798
    May  4 13:07:11.812: INFO: Waiting up to 5m0s for pod "var-expansion-e9369a12-7b96-455e-a99d-4b81fb1e477a" in namespace "var-expansion-9706" to be "Succeeded or Failed"
    May  4 13:07:11.816: INFO: Pod "var-expansion-e9369a12-7b96-455e-a99d-4b81fb1e477a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.394117ms
    May  4 13:07:13.822: INFO: Pod "var-expansion-e9369a12-7b96-455e-a99d-4b81fb1e477a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009885908s
    May  4 13:07:15.822: INFO: Pod "var-expansion-e9369a12-7b96-455e-a99d-4b81fb1e477a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010170182s
    STEP: Saw pod success 05/04/23 13:07:15.822
    May  4 13:07:15.823: INFO: Pod "var-expansion-e9369a12-7b96-455e-a99d-4b81fb1e477a" satisfied condition "Succeeded or Failed"
    May  4 13:07:15.827: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod var-expansion-e9369a12-7b96-455e-a99d-4b81fb1e477a container dapi-container: <nil>
    STEP: delete the pod 05/04/23 13:07:15.837
    May  4 13:07:15.853: INFO: Waiting for pod var-expansion-e9369a12-7b96-455e-a99d-4b81fb1e477a to disappear
    May  4 13:07:15.858: INFO: Pod var-expansion-e9369a12-7b96-455e-a99d-4b81fb1e477a no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    May  4 13:07:15.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-9706" for this suite. 05/04/23 13:07:15.882
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:07:15.893
May  4 13:07:15.893: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename resourcequota 05/04/23 13:07:15.894
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:07:15.929
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:07:15.934
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
STEP: Creating a ResourceQuota 05/04/23 13:07:15.939
STEP: Getting a ResourceQuota 05/04/23 13:07:15.947
STEP: Listing all ResourceQuotas with LabelSelector 05/04/23 13:07:15.957
STEP: Patching the ResourceQuota 05/04/23 13:07:15.965
STEP: Deleting a Collection of ResourceQuotas 05/04/23 13:07:15.975
STEP: Verifying the deleted ResourceQuota 05/04/23 13:07:15.989
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
May  4 13:07:16.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9646" for this suite. 05/04/23 13:07:16.01
{"msg":"PASSED [sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]","completed":311,"skipped":5754,"failed":0}
------------------------------
• [0.127 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:07:15.893
    May  4 13:07:15.893: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename resourcequota 05/04/23 13:07:15.894
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:07:15.929
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:07:15.934
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:933
    STEP: Creating a ResourceQuota 05/04/23 13:07:15.939
    STEP: Getting a ResourceQuota 05/04/23 13:07:15.947
    STEP: Listing all ResourceQuotas with LabelSelector 05/04/23 13:07:15.957
    STEP: Patching the ResourceQuota 05/04/23 13:07:15.965
    STEP: Deleting a Collection of ResourceQuotas 05/04/23 13:07:15.975
    STEP: Verifying the deleted ResourceQuota 05/04/23 13:07:15.989
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    May  4 13:07:16.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9646" for this suite. 05/04/23 13:07:16.01
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:07:16.021
May  4 13:07:16.022: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename cronjob 05/04/23 13:07:16.023
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:07:16.042
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:07:16.045
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 05/04/23 13:07:16.05
STEP: Ensuring no jobs are scheduled 05/04/23 13:07:16.059
STEP: Ensuring no job exists by listing jobs explicitly 05/04/23 13:12:16.07
STEP: Removing cronjob 05/04/23 13:12:16.076
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
May  4 13:12:16.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-4803" for this suite. 05/04/23 13:12:16.094
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","completed":312,"skipped":5767,"failed":0}
------------------------------
• [SLOW TEST] [300.081 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:07:16.021
    May  4 13:07:16.022: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename cronjob 05/04/23 13:07:16.023
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:07:16.042
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:07:16.045
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 05/04/23 13:07:16.05
    STEP: Ensuring no jobs are scheduled 05/04/23 13:07:16.059
    STEP: Ensuring no job exists by listing jobs explicitly 05/04/23 13:12:16.07
    STEP: Removing cronjob 05/04/23 13:12:16.076
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    May  4 13:12:16.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-4803" for this suite. 05/04/23 13:12:16.094
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:12:16.106
May  4 13:12:16.107: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename projected 05/04/23 13:12:16.107
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:12:16.132
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:12:16.136
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
STEP: Creating a pod to test downward API volume plugin 05/04/23 13:12:16.139
May  4 13:12:16.161: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2053040c-89a1-47f7-b516-c8bccb6021d3" in namespace "projected-3239" to be "Succeeded or Failed"
May  4 13:12:16.178: INFO: Pod "downwardapi-volume-2053040c-89a1-47f7-b516-c8bccb6021d3": Phase="Pending", Reason="", readiness=false. Elapsed: 16.757376ms
May  4 13:12:18.184: INFO: Pod "downwardapi-volume-2053040c-89a1-47f7-b516-c8bccb6021d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022705626s
May  4 13:12:20.185: INFO: Pod "downwardapi-volume-2053040c-89a1-47f7-b516-c8bccb6021d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023737267s
STEP: Saw pod success 05/04/23 13:12:20.185
May  4 13:12:20.185: INFO: Pod "downwardapi-volume-2053040c-89a1-47f7-b516-c8bccb6021d3" satisfied condition "Succeeded or Failed"
May  4 13:12:20.198: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod downwardapi-volume-2053040c-89a1-47f7-b516-c8bccb6021d3 container client-container: <nil>
STEP: delete the pod 05/04/23 13:12:20.217
May  4 13:12:20.263: INFO: Waiting for pod downwardapi-volume-2053040c-89a1-47f7-b516-c8bccb6021d3 to disappear
May  4 13:12:20.273: INFO: Pod downwardapi-volume-2053040c-89a1-47f7-b516-c8bccb6021d3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
May  4 13:12:20.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3239" for this suite. 05/04/23 13:12:20.29
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","completed":313,"skipped":5791,"failed":0}
------------------------------
• [4.200 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:12:16.106
    May  4 13:12:16.107: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename projected 05/04/23 13:12:16.107
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:12:16.132
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:12:16.136
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:206
    STEP: Creating a pod to test downward API volume plugin 05/04/23 13:12:16.139
    May  4 13:12:16.161: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2053040c-89a1-47f7-b516-c8bccb6021d3" in namespace "projected-3239" to be "Succeeded or Failed"
    May  4 13:12:16.178: INFO: Pod "downwardapi-volume-2053040c-89a1-47f7-b516-c8bccb6021d3": Phase="Pending", Reason="", readiness=false. Elapsed: 16.757376ms
    May  4 13:12:18.184: INFO: Pod "downwardapi-volume-2053040c-89a1-47f7-b516-c8bccb6021d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022705626s
    May  4 13:12:20.185: INFO: Pod "downwardapi-volume-2053040c-89a1-47f7-b516-c8bccb6021d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023737267s
    STEP: Saw pod success 05/04/23 13:12:20.185
    May  4 13:12:20.185: INFO: Pod "downwardapi-volume-2053040c-89a1-47f7-b516-c8bccb6021d3" satisfied condition "Succeeded or Failed"
    May  4 13:12:20.198: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod downwardapi-volume-2053040c-89a1-47f7-b516-c8bccb6021d3 container client-container: <nil>
    STEP: delete the pod 05/04/23 13:12:20.217
    May  4 13:12:20.263: INFO: Waiting for pod downwardapi-volume-2053040c-89a1-47f7-b516-c8bccb6021d3 to disappear
    May  4 13:12:20.273: INFO: Pod downwardapi-volume-2053040c-89a1-47f7-b516-c8bccb6021d3 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    May  4 13:12:20.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3239" for this suite. 05/04/23 13:12:20.29
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:12:20.307
May  4 13:12:20.308: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename pods 05/04/23 13:12:20.308
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:12:20.342
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:12:20.345
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
STEP: Create set of pods 05/04/23 13:12:20.349
May  4 13:12:20.367: INFO: created test-pod-1
May  4 13:12:20.387: INFO: created test-pod-2
May  4 13:12:20.402: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 05/04/23 13:12:20.402
May  4 13:12:20.402: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-5896' to be running and ready
May  4 13:12:20.420: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
May  4 13:12:20.420: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
May  4 13:12:20.420: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
May  4 13:12:20.420: INFO: 0 / 3 pods in namespace 'pods-5896' are running and ready (0 seconds elapsed)
May  4 13:12:20.420: INFO: expected 0 pod replicas in namespace 'pods-5896', 0 are Running and Ready.
May  4 13:12:20.420: INFO: POD         NODE                                      PHASE    GRACE  CONDITIONS
May  4 13:12:20.420: INFO: test-pod-1  ip-10-0-1-224.us-west-2.compute.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:12:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:12:20 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:12:20 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:12:20 +0000 UTC  }]
May  4 13:12:20.420: INFO: test-pod-2                                            Pending         []
May  4 13:12:20.420: INFO: test-pod-3                                            Pending         []
May  4 13:12:20.420: INFO: 
May  4 13:12:22.447: INFO: 3 / 3 pods in namespace 'pods-5896' are running and ready (2 seconds elapsed)
May  4 13:12:22.447: INFO: expected 0 pod replicas in namespace 'pods-5896', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 05/04/23 13:12:22.484
May  4 13:12:22.489: INFO: Pod quantity 3 is different from expected quantity 0
May  4 13:12:23.495: INFO: Pod quantity 3 is different from expected quantity 0
May  4 13:12:24.498: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
May  4 13:12:25.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5896" for this suite. 05/04/23 13:12:25.511
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","completed":314,"skipped":5792,"failed":0}
------------------------------
• [SLOW TEST] [5.222 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:12:20.307
    May  4 13:12:20.308: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename pods 05/04/23 13:12:20.308
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:12:20.342
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:12:20.345
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:844
    STEP: Create set of pods 05/04/23 13:12:20.349
    May  4 13:12:20.367: INFO: created test-pod-1
    May  4 13:12:20.387: INFO: created test-pod-2
    May  4 13:12:20.402: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 05/04/23 13:12:20.402
    May  4 13:12:20.402: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-5896' to be running and ready
    May  4 13:12:20.420: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    May  4 13:12:20.420: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    May  4 13:12:20.420: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    May  4 13:12:20.420: INFO: 0 / 3 pods in namespace 'pods-5896' are running and ready (0 seconds elapsed)
    May  4 13:12:20.420: INFO: expected 0 pod replicas in namespace 'pods-5896', 0 are Running and Ready.
    May  4 13:12:20.420: INFO: POD         NODE                                      PHASE    GRACE  CONDITIONS
    May  4 13:12:20.420: INFO: test-pod-1  ip-10-0-1-224.us-west-2.compute.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:12:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:12:20 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:12:20 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:12:20 +0000 UTC  }]
    May  4 13:12:20.420: INFO: test-pod-2                                            Pending         []
    May  4 13:12:20.420: INFO: test-pod-3                                            Pending         []
    May  4 13:12:20.420: INFO: 
    May  4 13:12:22.447: INFO: 3 / 3 pods in namespace 'pods-5896' are running and ready (2 seconds elapsed)
    May  4 13:12:22.447: INFO: expected 0 pod replicas in namespace 'pods-5896', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 05/04/23 13:12:22.484
    May  4 13:12:22.489: INFO: Pod quantity 3 is different from expected quantity 0
    May  4 13:12:23.495: INFO: Pod quantity 3 is different from expected quantity 0
    May  4 13:12:24.498: INFO: Pod quantity 3 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    May  4 13:12:25.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-5896" for this suite. 05/04/23 13:12:25.511
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:12:25.53
May  4 13:12:25.530: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename disruption 05/04/23 13:12:25.532
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:12:25.562
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:12:25.566
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
STEP: Creating a pdb that targets all three pods in a test replica set 05/04/23 13:12:25.569
STEP: Waiting for the pdb to be processed 05/04/23 13:12:25.58
STEP: First trying to evict a pod which shouldn't be evictable 05/04/23 13:12:25.598
STEP: Waiting for all pods to be running 05/04/23 13:12:25.598
May  4 13:12:25.626: INFO: running pods: 0 < 3
May  4 13:12:27.632: INFO: running pods: 2 < 3
STEP: locating a running pod 05/04/23 13:12:29.634
STEP: Updating the pdb to allow a pod to be evicted 05/04/23 13:12:29.648
STEP: Waiting for the pdb to be processed 05/04/23 13:12:29.66
STEP: Trying to evict the same pod we tried earlier which should now be evictable 05/04/23 13:12:31.671
STEP: Waiting for all pods to be running 05/04/23 13:12:31.671
STEP: Waiting for the pdb to observed all healthy pods 05/04/23 13:12:31.679
STEP: Patching the pdb to disallow a pod to be evicted 05/04/23 13:12:31.721
STEP: Waiting for the pdb to be processed 05/04/23 13:12:31.753
STEP: Waiting for all pods to be running 05/04/23 13:12:31.761
May  4 13:12:31.781: INFO: running pods: 2 < 3
STEP: locating a running pod 05/04/23 13:12:33.789
STEP: Deleting the pdb to allow a pod to be evicted 05/04/23 13:12:33.803
STEP: Waiting for the pdb to be deleted 05/04/23 13:12:33.818
STEP: Trying to evict the same pod we tried earlier which should now be evictable 05/04/23 13:12:33.822
STEP: Waiting for all pods to be running 05/04/23 13:12:33.822
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
May  4 13:12:33.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-4794" for this suite. 05/04/23 13:12:33.956
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","completed":315,"skipped":5808,"failed":0}
------------------------------
• [SLOW TEST] [8.442 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:12:25.53
    May  4 13:12:25.530: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename disruption 05/04/23 13:12:25.532
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:12:25.562
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:12:25.566
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:346
    STEP: Creating a pdb that targets all three pods in a test replica set 05/04/23 13:12:25.569
    STEP: Waiting for the pdb to be processed 05/04/23 13:12:25.58
    STEP: First trying to evict a pod which shouldn't be evictable 05/04/23 13:12:25.598
    STEP: Waiting for all pods to be running 05/04/23 13:12:25.598
    May  4 13:12:25.626: INFO: running pods: 0 < 3
    May  4 13:12:27.632: INFO: running pods: 2 < 3
    STEP: locating a running pod 05/04/23 13:12:29.634
    STEP: Updating the pdb to allow a pod to be evicted 05/04/23 13:12:29.648
    STEP: Waiting for the pdb to be processed 05/04/23 13:12:29.66
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 05/04/23 13:12:31.671
    STEP: Waiting for all pods to be running 05/04/23 13:12:31.671
    STEP: Waiting for the pdb to observed all healthy pods 05/04/23 13:12:31.679
    STEP: Patching the pdb to disallow a pod to be evicted 05/04/23 13:12:31.721
    STEP: Waiting for the pdb to be processed 05/04/23 13:12:31.753
    STEP: Waiting for all pods to be running 05/04/23 13:12:31.761
    May  4 13:12:31.781: INFO: running pods: 2 < 3
    STEP: locating a running pod 05/04/23 13:12:33.789
    STEP: Deleting the pdb to allow a pod to be evicted 05/04/23 13:12:33.803
    STEP: Waiting for the pdb to be deleted 05/04/23 13:12:33.818
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 05/04/23 13:12:33.822
    STEP: Waiting for all pods to be running 05/04/23 13:12:33.822
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    May  4 13:12:33.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-4794" for this suite. 05/04/23 13:12:33.956
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:12:33.973
May  4 13:12:33.973: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename daemonsets 05/04/23 13:12:33.974
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:12:34.054
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:12:34.061
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
STEP: Creating a simple DaemonSet "daemon-set" 05/04/23 13:12:34.134
STEP: Check that daemon pods launch on every node of the cluster. 05/04/23 13:12:34.149
May  4 13:12:34.165: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 13:12:34.165: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 13:12:34.165: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 13:12:34.177: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  4 13:12:34.177: INFO: Node ip-10-0-1-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
May  4 13:12:35.190: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 13:12:35.190: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 13:12:35.190: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 13:12:35.202: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May  4 13:12:35.202: INFO: Node ip-10-0-1-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
May  4 13:12:36.187: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 13:12:36.187: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 13:12:36.188: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 13:12:36.200: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
May  4 13:12:36.200: INFO: Node ip-10-0-1-253.us-west-2.compute.internal is running 0 daemon pod, expected 1
May  4 13:12:37.190: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 13:12:37.190: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 13:12:37.190: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 13:12:37.200: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
May  4 13:12:37.200: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 05/04/23 13:12:37.207
May  4 13:12:37.246: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 13:12:37.246: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 13:12:37.246: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 13:12:37.260: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
May  4 13:12:37.260: INFO: Node ip-10-0-1-224.us-west-2.compute.internal is running 0 daemon pod, expected 1
May  4 13:12:38.272: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 13:12:38.272: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 13:12:38.272: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 13:12:38.280: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
May  4 13:12:38.280: INFO: Node ip-10-0-1-224.us-west-2.compute.internal is running 0 daemon pod, expected 1
May  4 13:12:39.270: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 13:12:39.270: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 13:12:39.270: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  4 13:12:39.276: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
May  4 13:12:39.276: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 05/04/23 13:12:39.276
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 05/04/23 13:12:39.287
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6845, will wait for the garbage collector to delete the pods 05/04/23 13:12:39.287
May  4 13:12:39.355: INFO: Deleting DaemonSet.extensions daemon-set took: 13.626455ms
May  4 13:12:39.455: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.814086ms
May  4 13:12:42.260: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  4 13:12:42.260: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May  4 13:12:42.265: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"43899"},"items":null}

May  4 13:12:42.270: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"43899"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
May  4 13:12:42.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6845" for this suite. 05/04/23 13:12:42.326
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","completed":316,"skipped":5811,"failed":0}
------------------------------
• [SLOW TEST] [8.363 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:12:33.973
    May  4 13:12:33.973: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename daemonsets 05/04/23 13:12:33.974
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:12:34.054
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:12:34.061
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:293
    STEP: Creating a simple DaemonSet "daemon-set" 05/04/23 13:12:34.134
    STEP: Check that daemon pods launch on every node of the cluster. 05/04/23 13:12:34.149
    May  4 13:12:34.165: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 13:12:34.165: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 13:12:34.165: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 13:12:34.177: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  4 13:12:34.177: INFO: Node ip-10-0-1-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
    May  4 13:12:35.190: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 13:12:35.190: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 13:12:35.190: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 13:12:35.202: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May  4 13:12:35.202: INFO: Node ip-10-0-1-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
    May  4 13:12:36.187: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 13:12:36.187: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 13:12:36.188: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 13:12:36.200: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    May  4 13:12:36.200: INFO: Node ip-10-0-1-253.us-west-2.compute.internal is running 0 daemon pod, expected 1
    May  4 13:12:37.190: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 13:12:37.190: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 13:12:37.190: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 13:12:37.200: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    May  4 13:12:37.200: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 05/04/23 13:12:37.207
    May  4 13:12:37.246: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 13:12:37.246: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 13:12:37.246: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 13:12:37.260: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    May  4 13:12:37.260: INFO: Node ip-10-0-1-224.us-west-2.compute.internal is running 0 daemon pod, expected 1
    May  4 13:12:38.272: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 13:12:38.272: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 13:12:38.272: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 13:12:38.280: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    May  4 13:12:38.280: INFO: Node ip-10-0-1-224.us-west-2.compute.internal is running 0 daemon pod, expected 1
    May  4 13:12:39.270: INFO: DaemonSet pods can't tolerate node ip-10-0-1-168.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 13:12:39.270: INFO: DaemonSet pods can't tolerate node ip-10-0-1-223.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 13:12:39.270: INFO: DaemonSet pods can't tolerate node ip-10-0-1-58.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  4 13:12:39.276: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    May  4 13:12:39.276: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 05/04/23 13:12:39.276
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 05/04/23 13:12:39.287
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6845, will wait for the garbage collector to delete the pods 05/04/23 13:12:39.287
    May  4 13:12:39.355: INFO: Deleting DaemonSet.extensions daemon-set took: 13.626455ms
    May  4 13:12:39.455: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.814086ms
    May  4 13:12:42.260: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  4 13:12:42.260: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    May  4 13:12:42.265: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"43899"},"items":null}

    May  4 13:12:42.270: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"43899"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    May  4 13:12:42.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-6845" for this suite. 05/04/23 13:12:42.326
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:12:42.338
May  4 13:12:42.338: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename webhook 05/04/23 13:12:42.339
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:12:42.359
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:12:42.367
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/04/23 13:12:42.416
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/04/23 13:12:42.984
STEP: Deploying the webhook pod 05/04/23 13:12:42.995
STEP: Wait for the deployment to be ready 05/04/23 13:12:43.013
May  4 13:12:43.031: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/04/23 13:12:45.045
STEP: Verifying the service has paired with the endpoint 05/04/23 13:12:45.066
May  4 13:12:46.067: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 05/04/23 13:12:46.072
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 05/04/23 13:12:46.094
STEP: Creating a dummy validating-webhook-configuration object 05/04/23 13:12:46.12
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 05/04/23 13:12:46.139
STEP: Creating a dummy mutating-webhook-configuration object 05/04/23 13:12:46.151
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 05/04/23 13:12:46.165
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  4 13:12:46.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3422" for this suite. 05/04/23 13:12:46.2
STEP: Destroying namespace "webhook-3422-markers" for this suite. 05/04/23 13:12:46.208
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","completed":317,"skipped":5827,"failed":0}
------------------------------
• [3.947 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:12:42.338
    May  4 13:12:42.338: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename webhook 05/04/23 13:12:42.339
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:12:42.359
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:12:42.367
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/04/23 13:12:42.416
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/04/23 13:12:42.984
    STEP: Deploying the webhook pod 05/04/23 13:12:42.995
    STEP: Wait for the deployment to be ready 05/04/23 13:12:43.013
    May  4 13:12:43.031: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/04/23 13:12:45.045
    STEP: Verifying the service has paired with the endpoint 05/04/23 13:12:45.066
    May  4 13:12:46.067: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:276
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 05/04/23 13:12:46.072
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 05/04/23 13:12:46.094
    STEP: Creating a dummy validating-webhook-configuration object 05/04/23 13:12:46.12
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 05/04/23 13:12:46.139
    STEP: Creating a dummy mutating-webhook-configuration object 05/04/23 13:12:46.151
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 05/04/23 13:12:46.165
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  4 13:12:46.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3422" for this suite. 05/04/23 13:12:46.2
    STEP: Destroying namespace "webhook-3422-markers" for this suite. 05/04/23 13:12:46.208
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:12:46.285
May  4 13:12:46.286: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename kubectl 05/04/23 13:12:46.287
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:12:46.31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:12:46.315
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
STEP: starting the proxy server 05/04/23 13:12:46.319
May  4 13:12:46.319: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-7056 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 05/04/23 13:12:46.383
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  4 13:12:46.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7056" for this suite. 05/04/23 13:12:46.407
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","completed":318,"skipped":5828,"failed":0}
------------------------------
• [0.132 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1785

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:12:46.285
    May  4 13:12:46.286: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename kubectl 05/04/23 13:12:46.287
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:12:46.31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:12:46.315
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1785
    STEP: starting the proxy server 05/04/23 13:12:46.319
    May  4 13:12:46.319: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-7056 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 05/04/23 13:12:46.383
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  4 13:12:46.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7056" for this suite. 05/04/23 13:12:46.407
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:12:46.419
May  4 13:12:46.419: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename deployment 05/04/23 13:12:46.42
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:12:46.44
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:12:46.447
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
May  4 13:12:46.451: INFO: Creating deployment "webserver-deployment"
May  4 13:12:46.466: INFO: Waiting for observed generation 1
May  4 13:12:48.481: INFO: Waiting for all required pods to come up
May  4 13:12:48.488: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 05/04/23 13:12:48.488
May  4 13:12:48.489: INFO: Waiting for deployment "webserver-deployment" to complete
May  4 13:12:48.497: INFO: Updating deployment "webserver-deployment" with a non-existent image
May  4 13:12:48.515: INFO: Updating deployment webserver-deployment
May  4 13:12:48.515: INFO: Waiting for observed generation 2
May  4 13:12:50.526: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
May  4 13:12:50.531: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
May  4 13:12:50.536: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May  4 13:12:50.558: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
May  4 13:12:50.558: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
May  4 13:12:50.565: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May  4 13:12:50.575: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
May  4 13:12:50.575: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
May  4 13:12:50.592: INFO: Updating deployment webserver-deployment
May  4 13:12:50.592: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
May  4 13:12:50.604: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
May  4 13:12:50.615: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May  4 13:12:50.648: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-7444  e625f6aa-74bf-46b7-b3d5-bbb4809e92fe 44261 3 2023-05-04 13:12:46 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-04 13:12:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 13:12:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00559cb18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-05-04 13:12:48 +0000 UTC,LastTransitionTime:2023-05-04 13:12:46 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-04 13:12:50 +0000 UTC,LastTransitionTime:2023-05-04 13:12:50 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

May  4 13:12:50.665: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-7444  e7372ee1-fbf0-44fd-a2c8-c7e00f3c96b4 44256 3 2023-05-04 13:12:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment e625f6aa-74bf-46b7-b3d5-bbb4809e92fe 0xc0004c8207 0xc0004c8208}] [] [{kube-controller-manager Update apps/v1 2023-05-04 13:12:48 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-04 13:12:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e625f6aa-74bf-46b7-b3d5-bbb4809e92fe\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0004c82e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  4 13:12:50.665: INFO: All old ReplicaSets of Deployment "webserver-deployment":
May  4 13:12:50.665: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-7444  70affdcf-2ecb-4ca5-be57-b1ab0817cd14 44253 3 2023-05-04 13:12:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment e625f6aa-74bf-46b7-b3d5-bbb4809e92fe 0xc0004c8347 0xc0004c8348}] [] [{kube-controller-manager Update apps/v1 2023-05-04 13:12:48 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-04 13:12:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e625f6aa-74bf-46b7-b3d5-bbb4809e92fe\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0004c8428 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
May  4 13:12:50.692: INFO: Pod "webserver-deployment-69b7448995-49tzb" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-49tzb webserver-deployment-69b7448995- deployment-7444  d78e2180-ab21-45d2-9dca-be6f774281a7 44237 0 2023-05-04 13:12:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:17e27a1eef89a1ef8c1152f8fe0798d69e32861548bbdee1dcc76710b3c114ef cni.projectcalico.org/podIP:10.20.92.141/32 cni.projectcalico.org/podIPs:10.20.92.141/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 e7372ee1-fbf0-44fd-a2c8-c7e00f3c96b4 0xc0035c20e7 0xc0035c20e8}] [] [{kube-controller-manager Update v1 2023-05-04 13:12:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e7372ee1-fbf0-44fd-a2c8-c7e00f3c96b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-04 13:12:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-04 13:12:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7tnlp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7tnlp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-232.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.232,PodIP:,StartTime:2023-05-04 13:12:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  4 13:12:50.692: INFO: Pod "webserver-deployment-69b7448995-dwzpx" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-dwzpx webserver-deployment-69b7448995- deployment-7444  40033deb-1ee4-4bcb-b4dc-c4f2a0f94914 44265 0 2023-05-04 13:12:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 e7372ee1-fbf0-44fd-a2c8-c7e00f3c96b4 0xc0035c22f0 0xc0035c22f1}] [] [{kube-controller-manager Update v1 2023-05-04 13:12:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e7372ee1-fbf0-44fd-a2c8-c7e00f3c96b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f7skp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f7skp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-224.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  4 13:12:50.692: INFO: Pod "webserver-deployment-69b7448995-g8vm9" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-g8vm9 webserver-deployment-69b7448995- deployment-7444  ca248695-6494-4b3c-8f8b-381803b49f26 44229 0 2023-05-04 13:12:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:3c735b2cc0038a1842fe754fd05cb784cc6d80b7c50441a4c1a01e8d0601bb79 cni.projectcalico.org/podIP:10.20.11.227/32 cni.projectcalico.org/podIPs:10.20.11.227/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 e7372ee1-fbf0-44fd-a2c8-c7e00f3c96b4 0xc0035c2470 0xc0035c2471}] [] [{kube-controller-manager Update v1 2023-05-04 13:12:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e7372ee1-fbf0-44fd-a2c8-c7e00f3c96b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-04 13:12:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-04 13:12:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5lncc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5lncc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-189.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.189,PodIP:,StartTime:2023-05-04 13:12:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  4 13:12:50.693: INFO: Pod "webserver-deployment-69b7448995-j8n9h" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-j8n9h webserver-deployment-69b7448995- deployment-7444  f1bfa846-f50d-423b-9f32-28b3e4f5e378 44274 0 2023-05-04 13:12:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 e7372ee1-fbf0-44fd-a2c8-c7e00f3c96b4 0xc0035c2660 0xc0035c2661}] [] [{kube-controller-manager Update v1 2023-05-04 13:12:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e7372ee1-fbf0-44fd-a2c8-c7e00f3c96b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q6hk6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q6hk6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-216.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  4 13:12:50.693: INFO: Pod "webserver-deployment-69b7448995-k2hxb" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-k2hxb webserver-deployment-69b7448995- deployment-7444  5e755290-dd9c-4cf6-8dac-f3c79d13734f 44250 0 2023-05-04 13:12:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:1fc614e2707b33daf18dac275720c60996591354ed7ed32c6a41ea509dac59ae cni.projectcalico.org/podIP:10.20.142.149/32 cni.projectcalico.org/podIPs:10.20.142.149/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 e7372ee1-fbf0-44fd-a2c8-c7e00f3c96b4 0xc0035c27e0 0xc0035c27e1}] [] [{kube-controller-manager Update v1 2023-05-04 13:12:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e7372ee1-fbf0-44fd-a2c8-c7e00f3c96b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-04 13:12:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-04 13:12:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.142.149\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wf627,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wf627,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-216.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.216,PodIP:10.20.142.149,StartTime:2023-05-04 13:12:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.142.149,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  4 13:12:50.693: INFO: Pod "webserver-deployment-69b7448995-ld6cj" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-ld6cj webserver-deployment-69b7448995- deployment-7444  f8f2af9f-fb27-4a8b-9987-18a01353eaf0 44272 0 2023-05-04 13:12:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 e7372ee1-fbf0-44fd-a2c8-c7e00f3c96b4 0xc0035c2a00 0xc0035c2a01}] [] [{kube-controller-manager Update v1 2023-05-04 13:12:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e7372ee1-fbf0-44fd-a2c8-c7e00f3c96b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2qkjk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2qkjk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  4 13:12:50.693: INFO: Pod "webserver-deployment-69b7448995-w9smf" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-w9smf webserver-deployment-69b7448995- deployment-7444  e37ff241-316c-4413-890d-93972d655ae6 44232 0 2023-05-04 13:12:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:ea563f761120bd2d383b3d7a3756181eb14e48e315c6efbe86f9c143ada6836c cni.projectcalico.org/podIP:10.20.83.13/32 cni.projectcalico.org/podIPs:10.20.83.13/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 e7372ee1-fbf0-44fd-a2c8-c7e00f3c96b4 0xc0035c2b67 0xc0035c2b68}] [] [{kube-controller-manager Update v1 2023-05-04 13:12:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e7372ee1-fbf0-44fd-a2c8-c7e00f3c96b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-04 13:12:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-04 13:12:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q98dr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q98dr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-224.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.224,PodIP:,StartTime:2023-05-04 13:12:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  4 13:12:50.694: INFO: Pod "webserver-deployment-69b7448995-zljq5" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-zljq5 webserver-deployment-69b7448995- deployment-7444  8ec87358-4909-4971-ba5d-efb12855067f 44233 0 2023-05-04 13:12:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:0fad05a72da4e2d4efbc7ec50a7b08d94c80e41c19e0d3a87d85b017aba79763 cni.projectcalico.org/podIP:10.20.1.146/32 cni.projectcalico.org/podIPs:10.20.1.146/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 e7372ee1-fbf0-44fd-a2c8-c7e00f3c96b4 0xc0035c2d80 0xc0035c2d81}] [] [{kube-controller-manager Update v1 2023-05-04 13:12:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e7372ee1-fbf0-44fd-a2c8-c7e00f3c96b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-04 13:12:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-04 13:12:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vjqcm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vjqcm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-253.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.253,PodIP:,StartTime:2023-05-04 13:12:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  4 13:12:50.694: INFO: Pod "webserver-deployment-845c8977d9-5mmsj" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-5mmsj webserver-deployment-845c8977d9- deployment-7444  63625b17-0f40-41d6-838c-8606815dad64 44165 0 2023-05-04 13:12:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:78fbadf261582630678fd1f2cba1dc6673901cfc04bc06c0771bc90e325ff188 cni.projectcalico.org/podIP:10.20.1.145/32 cni.projectcalico.org/podIPs:10.20.1.145/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 70affdcf-2ecb-4ca5-be57-b1ab0817cd14 0xc0035c2f90 0xc0035c2f91}] [] [{kube-controller-manager Update v1 2023-05-04 13:12:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70affdcf-2ecb-4ca5-be57-b1ab0817cd14\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-04 13:12:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-04 13:12:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.1.145\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4qqnz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4qqnz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-253.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.253,PodIP:10.20.1.145,StartTime:2023-05-04 13:12:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-04 13:12:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://1f214416efb605961e47c7bc9174e8c6b692a327417a5bf1a4b6aa62911f101e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.1.145,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  4 13:12:50.694: INFO: Pod "webserver-deployment-845c8977d9-6bdjj" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-6bdjj webserver-deployment-845c8977d9- deployment-7444  f8a9bcd2-9401-48ff-b35a-c1574c5bc412 44156 0 2023-05-04 13:12:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:e34aaa8dd262004374727e1ba75fdebe5d66eb467ffad9cb209747d1ecdba953 cni.projectcalico.org/podIP:10.20.92.191/32 cni.projectcalico.org/podIPs:10.20.92.191/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 70affdcf-2ecb-4ca5-be57-b1ab0817cd14 0xc0035c31b0 0xc0035c31b1}] [] [{kube-controller-manager Update v1 2023-05-04 13:12:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70affdcf-2ecb-4ca5-be57-b1ab0817cd14\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-04 13:12:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-04 13:12:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.92.191\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gx9fj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gx9fj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-232.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.232,PodIP:10.20.92.191,StartTime:2023-05-04 13:12:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-04 13:12:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://b06c385b162919bd3038ad3b1a0fd2999dcab16ea446039f8705552ae01c7636,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.92.191,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  4 13:12:50.694: INFO: Pod "webserver-deployment-845c8977d9-6mc42" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-6mc42 webserver-deployment-845c8977d9- deployment-7444  ad6d6929-a355-47d0-975e-4111712ab315 44257 0 2023-05-04 13:12:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 70affdcf-2ecb-4ca5-be57-b1ab0817cd14 0xc0035c33b0 0xc0035c33b1}] [] [{kube-controller-manager Update v1 2023-05-04 13:12:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70affdcf-2ecb-4ca5-be57-b1ab0817cd14\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8blw9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8blw9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-224.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  4 13:12:50.695: INFO: Pod "webserver-deployment-845c8977d9-94r5f" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-94r5f webserver-deployment-845c8977d9- deployment-7444  2551aa1c-9393-4095-ad46-f4a755112772 44086 0 2023-05-04 13:12:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:a503c084b081454b662fe602577b5c1279dc4dd253bb7f72c1e06da399a9e5f8 cni.projectcalico.org/podIP:10.20.11.222/32 cni.projectcalico.org/podIPs:10.20.11.222/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 70affdcf-2ecb-4ca5-be57-b1ab0817cd14 0xc0035c3520 0xc0035c3521}] [] [{kube-controller-manager Update v1 2023-05-04 13:12:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70affdcf-2ecb-4ca5-be57-b1ab0817cd14\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-04 13:12:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-04 13:12:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.11.222\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nv5h2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nv5h2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-189.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.189,PodIP:10.20.11.222,StartTime:2023-05-04 13:12:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-04 13:12:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://9fb76919fc422c8c082e919035370180a289cebe9fdbe95891a4a68c9b0868fc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.11.222,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  4 13:12:50.695: INFO: Pod "webserver-deployment-845c8977d9-d82f7" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-d82f7 webserver-deployment-845c8977d9- deployment-7444  323259c6-d26a-4893-8c7e-196f2c932451 44097 0 2023-05-04 13:12:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:e6f2af73998ded859f19fa59e9e3328f92364d63af6cc66b499a8703c40d27c4 cni.projectcalico.org/podIP:10.20.142.144/32 cni.projectcalico.org/podIPs:10.20.142.144/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 70affdcf-2ecb-4ca5-be57-b1ab0817cd14 0xc0035c3730 0xc0035c3731}] [] [{kube-controller-manager Update v1 2023-05-04 13:12:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70affdcf-2ecb-4ca5-be57-b1ab0817cd14\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-04 13:12:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-04 13:12:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.142.144\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nhwnw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nhwnw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-216.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.216,PodIP:10.20.142.144,StartTime:2023-05-04 13:12:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-04 13:12:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://3c6ac1d9533168ff2ebfda28c0fc4b4171582bd51a385af2a9aeb1147f07fa43,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.142.144,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  4 13:12:50.695: INFO: Pod "webserver-deployment-845c8977d9-h54hh" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-h54hh webserver-deployment-845c8977d9- deployment-7444  80592909-0587-42f5-b6b4-0338fc38e073 44266 0 2023-05-04 13:12:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 70affdcf-2ecb-4ca5-be57-b1ab0817cd14 0xc0035c3920 0xc0035c3921}] [] [{kube-controller-manager Update v1 2023-05-04 13:12:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70affdcf-2ecb-4ca5-be57-b1ab0817cd14\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bqw59,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bqw59,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-224.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  4 13:12:50.695: INFO: Pod "webserver-deployment-845c8977d9-jzs4n" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-jzs4n webserver-deployment-845c8977d9- deployment-7444  cca6014c-3d81-4278-aaba-0ff53609272b 44104 0 2023-05-04 13:12:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:2365fcc3f793240eb6a3e4869e9fb0d322a86be3721bc1cd2e504ea004a8b01f cni.projectcalico.org/podIP:10.20.142.148/32 cni.projectcalico.org/podIPs:10.20.142.148/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 70affdcf-2ecb-4ca5-be57-b1ab0817cd14 0xc0035c3a90 0xc0035c3a91}] [] [{kube-controller-manager Update v1 2023-05-04 13:12:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70affdcf-2ecb-4ca5-be57-b1ab0817cd14\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-04 13:12:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-04 13:12:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.142.148\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-whv89,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-whv89,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-216.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.216,PodIP:10.20.142.148,StartTime:2023-05-04 13:12:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-04 13:12:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://53da541a41aeaa7a0a48d24ea46dae06bb9990ee67009eb2841b0d1fa795d88e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.142.148,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  4 13:12:50.695: INFO: Pod "webserver-deployment-845c8977d9-kpp5l" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-kpp5l webserver-deployment-845c8977d9- deployment-7444  16baf925-ecbf-4937-8414-de2d12b5ca24 44167 0 2023-05-04 13:12:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:8997d11ac6c61777be55f3976c42a39fa8a147776557ae655802f69516f5569f cni.projectcalico.org/podIP:10.20.1.154/32 cni.projectcalico.org/podIPs:10.20.1.154/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 70affdcf-2ecb-4ca5-be57-b1ab0817cd14 0xc0035c3ca0 0xc0035c3ca1}] [] [{kube-controller-manager Update v1 2023-05-04 13:12:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70affdcf-2ecb-4ca5-be57-b1ab0817cd14\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-04 13:12:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-04 13:12:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.1.154\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8xl6m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8xl6m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-253.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.253,PodIP:10.20.1.154,StartTime:2023-05-04 13:12:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-04 13:12:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://cc6d7651d943844a30940b068faa10cb62a9550e014c5656d194c73acd8400c1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.1.154,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  4 13:12:50.696: INFO: Pod "webserver-deployment-845c8977d9-qjrd8" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-qjrd8 webserver-deployment-845c8977d9- deployment-7444  4f7487e2-3ef8-4635-8dc0-0911f2ab7517 44153 0 2023-05-04 13:12:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:7c8e931e92e12cb25ca5485cb8c71e100356b4ed59c2a6d2b06ce145a82617ac cni.projectcalico.org/podIP:10.20.92.133/32 cni.projectcalico.org/podIPs:10.20.92.133/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 70affdcf-2ecb-4ca5-be57-b1ab0817cd14 0xc0035c3eb0 0xc0035c3eb1}] [] [{kube-controller-manager Update v1 2023-05-04 13:12:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70affdcf-2ecb-4ca5-be57-b1ab0817cd14\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-04 13:12:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-04 13:12:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.92.133\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-khbzm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-khbzm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-232.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.232,PodIP:10.20.92.133,StartTime:2023-05-04 13:12:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-04 13:12:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://a3c711ac05b8b443266cae2447602ca1279bf4749721535800d45881d324bfce,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.92.133,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  4 13:12:50.697: INFO: Pod "webserver-deployment-845c8977d9-vk7wl" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-vk7wl webserver-deployment-845c8977d9- deployment-7444  c7492344-d2ae-4d52-9518-309eac726644 44171 0 2023-05-04 13:12:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:b8c4c93a23e8f02f531a6b7fc87c1442429ef68c02cffd5eb856ee4711b377e9 cni.projectcalico.org/podIP:10.20.11.218/32 cni.projectcalico.org/podIPs:10.20.11.218/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 70affdcf-2ecb-4ca5-be57-b1ab0817cd14 0xc00344e120 0xc00344e121}] [] [{kube-controller-manager Update v1 2023-05-04 13:12:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70affdcf-2ecb-4ca5-be57-b1ab0817cd14\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-04 13:12:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-04 13:12:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.11.218\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nxdx8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nxdx8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-189.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.189,PodIP:10.20.11.218,StartTime:2023-05-04 13:12:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-04 13:12:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://1a5f07cd9ec7df6e1bc7a6e89d51c64fe3355040414e6bbe463721dded114f61,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.11.218,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  4 13:12:50.697: INFO: Pod "webserver-deployment-845c8977d9-vx285" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-vx285 webserver-deployment-845c8977d9- deployment-7444  aab882b1-0111-4f6d-9663-7dc5bc0d8a0d 44264 0 2023-05-04 13:12:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 70affdcf-2ecb-4ca5-be57-b1ab0817cd14 0xc00344e320 0xc00344e321}] [] [{kube-controller-manager Update v1 2023-05-04 13:12:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70affdcf-2ecb-4ca5-be57-b1ab0817cd14\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xz2wb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xz2wb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-224.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
May  4 13:12:50.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7444" for this suite. 05/04/23 13:12:50.756
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","completed":319,"skipped":5854,"failed":0}
------------------------------
• [4.465 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:12:46.419
    May  4 13:12:46.419: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename deployment 05/04/23 13:12:46.42
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:12:46.44
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:12:46.447
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    May  4 13:12:46.451: INFO: Creating deployment "webserver-deployment"
    May  4 13:12:46.466: INFO: Waiting for observed generation 1
    May  4 13:12:48.481: INFO: Waiting for all required pods to come up
    May  4 13:12:48.488: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 05/04/23 13:12:48.488
    May  4 13:12:48.489: INFO: Waiting for deployment "webserver-deployment" to complete
    May  4 13:12:48.497: INFO: Updating deployment "webserver-deployment" with a non-existent image
    May  4 13:12:48.515: INFO: Updating deployment webserver-deployment
    May  4 13:12:48.515: INFO: Waiting for observed generation 2
    May  4 13:12:50.526: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    May  4 13:12:50.531: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    May  4 13:12:50.536: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    May  4 13:12:50.558: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    May  4 13:12:50.558: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    May  4 13:12:50.565: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    May  4 13:12:50.575: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    May  4 13:12:50.575: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    May  4 13:12:50.592: INFO: Updating deployment webserver-deployment
    May  4 13:12:50.592: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    May  4 13:12:50.604: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    May  4 13:12:50.615: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May  4 13:12:50.648: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-7444  e625f6aa-74bf-46b7-b3d5-bbb4809e92fe 44261 3 2023-05-04 13:12:46 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-04 13:12:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 13:12:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00559cb18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-05-04 13:12:48 +0000 UTC,LastTransitionTime:2023-05-04 13:12:46 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-04 13:12:50 +0000 UTC,LastTransitionTime:2023-05-04 13:12:50 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    May  4 13:12:50.665: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-7444  e7372ee1-fbf0-44fd-a2c8-c7e00f3c96b4 44256 3 2023-05-04 13:12:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment e625f6aa-74bf-46b7-b3d5-bbb4809e92fe 0xc0004c8207 0xc0004c8208}] [] [{kube-controller-manager Update apps/v1 2023-05-04 13:12:48 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-04 13:12:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e625f6aa-74bf-46b7-b3d5-bbb4809e92fe\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0004c82e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    May  4 13:12:50.665: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    May  4 13:12:50.665: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-7444  70affdcf-2ecb-4ca5-be57-b1ab0817cd14 44253 3 2023-05-04 13:12:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment e625f6aa-74bf-46b7-b3d5-bbb4809e92fe 0xc0004c8347 0xc0004c8348}] [] [{kube-controller-manager Update apps/v1 2023-05-04 13:12:48 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-04 13:12:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e625f6aa-74bf-46b7-b3d5-bbb4809e92fe\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0004c8428 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    May  4 13:12:50.692: INFO: Pod "webserver-deployment-69b7448995-49tzb" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-49tzb webserver-deployment-69b7448995- deployment-7444  d78e2180-ab21-45d2-9dca-be6f774281a7 44237 0 2023-05-04 13:12:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:17e27a1eef89a1ef8c1152f8fe0798d69e32861548bbdee1dcc76710b3c114ef cni.projectcalico.org/podIP:10.20.92.141/32 cni.projectcalico.org/podIPs:10.20.92.141/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 e7372ee1-fbf0-44fd-a2c8-c7e00f3c96b4 0xc0035c20e7 0xc0035c20e8}] [] [{kube-controller-manager Update v1 2023-05-04 13:12:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e7372ee1-fbf0-44fd-a2c8-c7e00f3c96b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-04 13:12:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-04 13:12:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7tnlp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7tnlp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-232.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.232,PodIP:,StartTime:2023-05-04 13:12:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  4 13:12:50.692: INFO: Pod "webserver-deployment-69b7448995-dwzpx" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-dwzpx webserver-deployment-69b7448995- deployment-7444  40033deb-1ee4-4bcb-b4dc-c4f2a0f94914 44265 0 2023-05-04 13:12:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 e7372ee1-fbf0-44fd-a2c8-c7e00f3c96b4 0xc0035c22f0 0xc0035c22f1}] [] [{kube-controller-manager Update v1 2023-05-04 13:12:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e7372ee1-fbf0-44fd-a2c8-c7e00f3c96b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f7skp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f7skp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-224.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  4 13:12:50.692: INFO: Pod "webserver-deployment-69b7448995-g8vm9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-g8vm9 webserver-deployment-69b7448995- deployment-7444  ca248695-6494-4b3c-8f8b-381803b49f26 44229 0 2023-05-04 13:12:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:3c735b2cc0038a1842fe754fd05cb784cc6d80b7c50441a4c1a01e8d0601bb79 cni.projectcalico.org/podIP:10.20.11.227/32 cni.projectcalico.org/podIPs:10.20.11.227/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 e7372ee1-fbf0-44fd-a2c8-c7e00f3c96b4 0xc0035c2470 0xc0035c2471}] [] [{kube-controller-manager Update v1 2023-05-04 13:12:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e7372ee1-fbf0-44fd-a2c8-c7e00f3c96b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-04 13:12:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-04 13:12:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5lncc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5lncc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-189.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.189,PodIP:,StartTime:2023-05-04 13:12:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  4 13:12:50.693: INFO: Pod "webserver-deployment-69b7448995-j8n9h" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-j8n9h webserver-deployment-69b7448995- deployment-7444  f1bfa846-f50d-423b-9f32-28b3e4f5e378 44274 0 2023-05-04 13:12:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 e7372ee1-fbf0-44fd-a2c8-c7e00f3c96b4 0xc0035c2660 0xc0035c2661}] [] [{kube-controller-manager Update v1 2023-05-04 13:12:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e7372ee1-fbf0-44fd-a2c8-c7e00f3c96b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q6hk6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q6hk6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-216.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  4 13:12:50.693: INFO: Pod "webserver-deployment-69b7448995-k2hxb" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-k2hxb webserver-deployment-69b7448995- deployment-7444  5e755290-dd9c-4cf6-8dac-f3c79d13734f 44250 0 2023-05-04 13:12:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:1fc614e2707b33daf18dac275720c60996591354ed7ed32c6a41ea509dac59ae cni.projectcalico.org/podIP:10.20.142.149/32 cni.projectcalico.org/podIPs:10.20.142.149/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 e7372ee1-fbf0-44fd-a2c8-c7e00f3c96b4 0xc0035c27e0 0xc0035c27e1}] [] [{kube-controller-manager Update v1 2023-05-04 13:12:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e7372ee1-fbf0-44fd-a2c8-c7e00f3c96b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-04 13:12:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-04 13:12:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.142.149\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wf627,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wf627,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-216.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.216,PodIP:10.20.142.149,StartTime:2023-05-04 13:12:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.142.149,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  4 13:12:50.693: INFO: Pod "webserver-deployment-69b7448995-ld6cj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-ld6cj webserver-deployment-69b7448995- deployment-7444  f8f2af9f-fb27-4a8b-9987-18a01353eaf0 44272 0 2023-05-04 13:12:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 e7372ee1-fbf0-44fd-a2c8-c7e00f3c96b4 0xc0035c2a00 0xc0035c2a01}] [] [{kube-controller-manager Update v1 2023-05-04 13:12:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e7372ee1-fbf0-44fd-a2c8-c7e00f3c96b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2qkjk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2qkjk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  4 13:12:50.693: INFO: Pod "webserver-deployment-69b7448995-w9smf" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-w9smf webserver-deployment-69b7448995- deployment-7444  e37ff241-316c-4413-890d-93972d655ae6 44232 0 2023-05-04 13:12:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:ea563f761120bd2d383b3d7a3756181eb14e48e315c6efbe86f9c143ada6836c cni.projectcalico.org/podIP:10.20.83.13/32 cni.projectcalico.org/podIPs:10.20.83.13/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 e7372ee1-fbf0-44fd-a2c8-c7e00f3c96b4 0xc0035c2b67 0xc0035c2b68}] [] [{kube-controller-manager Update v1 2023-05-04 13:12:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e7372ee1-fbf0-44fd-a2c8-c7e00f3c96b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-04 13:12:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-04 13:12:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q98dr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q98dr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-224.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.224,PodIP:,StartTime:2023-05-04 13:12:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  4 13:12:50.694: INFO: Pod "webserver-deployment-69b7448995-zljq5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-zljq5 webserver-deployment-69b7448995- deployment-7444  8ec87358-4909-4971-ba5d-efb12855067f 44233 0 2023-05-04 13:12:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:0fad05a72da4e2d4efbc7ec50a7b08d94c80e41c19e0d3a87d85b017aba79763 cni.projectcalico.org/podIP:10.20.1.146/32 cni.projectcalico.org/podIPs:10.20.1.146/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 e7372ee1-fbf0-44fd-a2c8-c7e00f3c96b4 0xc0035c2d80 0xc0035c2d81}] [] [{kube-controller-manager Update v1 2023-05-04 13:12:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e7372ee1-fbf0-44fd-a2c8-c7e00f3c96b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-04 13:12:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-04 13:12:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vjqcm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vjqcm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-253.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.253,PodIP:,StartTime:2023-05-04 13:12:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  4 13:12:50.694: INFO: Pod "webserver-deployment-845c8977d9-5mmsj" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-5mmsj webserver-deployment-845c8977d9- deployment-7444  63625b17-0f40-41d6-838c-8606815dad64 44165 0 2023-05-04 13:12:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:78fbadf261582630678fd1f2cba1dc6673901cfc04bc06c0771bc90e325ff188 cni.projectcalico.org/podIP:10.20.1.145/32 cni.projectcalico.org/podIPs:10.20.1.145/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 70affdcf-2ecb-4ca5-be57-b1ab0817cd14 0xc0035c2f90 0xc0035c2f91}] [] [{kube-controller-manager Update v1 2023-05-04 13:12:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70affdcf-2ecb-4ca5-be57-b1ab0817cd14\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-04 13:12:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-04 13:12:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.1.145\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4qqnz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4qqnz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-253.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.253,PodIP:10.20.1.145,StartTime:2023-05-04 13:12:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-04 13:12:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://1f214416efb605961e47c7bc9174e8c6b692a327417a5bf1a4b6aa62911f101e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.1.145,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  4 13:12:50.694: INFO: Pod "webserver-deployment-845c8977d9-6bdjj" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-6bdjj webserver-deployment-845c8977d9- deployment-7444  f8a9bcd2-9401-48ff-b35a-c1574c5bc412 44156 0 2023-05-04 13:12:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:e34aaa8dd262004374727e1ba75fdebe5d66eb467ffad9cb209747d1ecdba953 cni.projectcalico.org/podIP:10.20.92.191/32 cni.projectcalico.org/podIPs:10.20.92.191/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 70affdcf-2ecb-4ca5-be57-b1ab0817cd14 0xc0035c31b0 0xc0035c31b1}] [] [{kube-controller-manager Update v1 2023-05-04 13:12:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70affdcf-2ecb-4ca5-be57-b1ab0817cd14\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-04 13:12:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-04 13:12:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.92.191\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gx9fj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gx9fj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-232.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.232,PodIP:10.20.92.191,StartTime:2023-05-04 13:12:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-04 13:12:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://b06c385b162919bd3038ad3b1a0fd2999dcab16ea446039f8705552ae01c7636,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.92.191,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  4 13:12:50.694: INFO: Pod "webserver-deployment-845c8977d9-6mc42" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-6mc42 webserver-deployment-845c8977d9- deployment-7444  ad6d6929-a355-47d0-975e-4111712ab315 44257 0 2023-05-04 13:12:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 70affdcf-2ecb-4ca5-be57-b1ab0817cd14 0xc0035c33b0 0xc0035c33b1}] [] [{kube-controller-manager Update v1 2023-05-04 13:12:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70affdcf-2ecb-4ca5-be57-b1ab0817cd14\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8blw9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8blw9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-224.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  4 13:12:50.695: INFO: Pod "webserver-deployment-845c8977d9-94r5f" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-94r5f webserver-deployment-845c8977d9- deployment-7444  2551aa1c-9393-4095-ad46-f4a755112772 44086 0 2023-05-04 13:12:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:a503c084b081454b662fe602577b5c1279dc4dd253bb7f72c1e06da399a9e5f8 cni.projectcalico.org/podIP:10.20.11.222/32 cni.projectcalico.org/podIPs:10.20.11.222/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 70affdcf-2ecb-4ca5-be57-b1ab0817cd14 0xc0035c3520 0xc0035c3521}] [] [{kube-controller-manager Update v1 2023-05-04 13:12:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70affdcf-2ecb-4ca5-be57-b1ab0817cd14\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-04 13:12:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-04 13:12:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.11.222\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nv5h2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nv5h2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-189.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.189,PodIP:10.20.11.222,StartTime:2023-05-04 13:12:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-04 13:12:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://9fb76919fc422c8c082e919035370180a289cebe9fdbe95891a4a68c9b0868fc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.11.222,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  4 13:12:50.695: INFO: Pod "webserver-deployment-845c8977d9-d82f7" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-d82f7 webserver-deployment-845c8977d9- deployment-7444  323259c6-d26a-4893-8c7e-196f2c932451 44097 0 2023-05-04 13:12:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:e6f2af73998ded859f19fa59e9e3328f92364d63af6cc66b499a8703c40d27c4 cni.projectcalico.org/podIP:10.20.142.144/32 cni.projectcalico.org/podIPs:10.20.142.144/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 70affdcf-2ecb-4ca5-be57-b1ab0817cd14 0xc0035c3730 0xc0035c3731}] [] [{kube-controller-manager Update v1 2023-05-04 13:12:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70affdcf-2ecb-4ca5-be57-b1ab0817cd14\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-04 13:12:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-04 13:12:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.142.144\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nhwnw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nhwnw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-216.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.216,PodIP:10.20.142.144,StartTime:2023-05-04 13:12:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-04 13:12:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://3c6ac1d9533168ff2ebfda28c0fc4b4171582bd51a385af2a9aeb1147f07fa43,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.142.144,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  4 13:12:50.695: INFO: Pod "webserver-deployment-845c8977d9-h54hh" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-h54hh webserver-deployment-845c8977d9- deployment-7444  80592909-0587-42f5-b6b4-0338fc38e073 44266 0 2023-05-04 13:12:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 70affdcf-2ecb-4ca5-be57-b1ab0817cd14 0xc0035c3920 0xc0035c3921}] [] [{kube-controller-manager Update v1 2023-05-04 13:12:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70affdcf-2ecb-4ca5-be57-b1ab0817cd14\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bqw59,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bqw59,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-224.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  4 13:12:50.695: INFO: Pod "webserver-deployment-845c8977d9-jzs4n" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-jzs4n webserver-deployment-845c8977d9- deployment-7444  cca6014c-3d81-4278-aaba-0ff53609272b 44104 0 2023-05-04 13:12:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:2365fcc3f793240eb6a3e4869e9fb0d322a86be3721bc1cd2e504ea004a8b01f cni.projectcalico.org/podIP:10.20.142.148/32 cni.projectcalico.org/podIPs:10.20.142.148/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 70affdcf-2ecb-4ca5-be57-b1ab0817cd14 0xc0035c3a90 0xc0035c3a91}] [] [{kube-controller-manager Update v1 2023-05-04 13:12:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70affdcf-2ecb-4ca5-be57-b1ab0817cd14\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-04 13:12:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-04 13:12:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.142.148\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-whv89,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-whv89,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-216.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.216,PodIP:10.20.142.148,StartTime:2023-05-04 13:12:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-04 13:12:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://53da541a41aeaa7a0a48d24ea46dae06bb9990ee67009eb2841b0d1fa795d88e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.142.148,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  4 13:12:50.695: INFO: Pod "webserver-deployment-845c8977d9-kpp5l" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-kpp5l webserver-deployment-845c8977d9- deployment-7444  16baf925-ecbf-4937-8414-de2d12b5ca24 44167 0 2023-05-04 13:12:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:8997d11ac6c61777be55f3976c42a39fa8a147776557ae655802f69516f5569f cni.projectcalico.org/podIP:10.20.1.154/32 cni.projectcalico.org/podIPs:10.20.1.154/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 70affdcf-2ecb-4ca5-be57-b1ab0817cd14 0xc0035c3ca0 0xc0035c3ca1}] [] [{kube-controller-manager Update v1 2023-05-04 13:12:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70affdcf-2ecb-4ca5-be57-b1ab0817cd14\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-04 13:12:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-04 13:12:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.1.154\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8xl6m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8xl6m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-253.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.253,PodIP:10.20.1.154,StartTime:2023-05-04 13:12:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-04 13:12:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://cc6d7651d943844a30940b068faa10cb62a9550e014c5656d194c73acd8400c1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.1.154,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  4 13:12:50.696: INFO: Pod "webserver-deployment-845c8977d9-qjrd8" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-qjrd8 webserver-deployment-845c8977d9- deployment-7444  4f7487e2-3ef8-4635-8dc0-0911f2ab7517 44153 0 2023-05-04 13:12:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:7c8e931e92e12cb25ca5485cb8c71e100356b4ed59c2a6d2b06ce145a82617ac cni.projectcalico.org/podIP:10.20.92.133/32 cni.projectcalico.org/podIPs:10.20.92.133/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 70affdcf-2ecb-4ca5-be57-b1ab0817cd14 0xc0035c3eb0 0xc0035c3eb1}] [] [{kube-controller-manager Update v1 2023-05-04 13:12:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70affdcf-2ecb-4ca5-be57-b1ab0817cd14\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-04 13:12:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-04 13:12:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.92.133\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-khbzm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-khbzm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-232.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.232,PodIP:10.20.92.133,StartTime:2023-05-04 13:12:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-04 13:12:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://a3c711ac05b8b443266cae2447602ca1279bf4749721535800d45881d324bfce,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.92.133,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  4 13:12:50.697: INFO: Pod "webserver-deployment-845c8977d9-vk7wl" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-vk7wl webserver-deployment-845c8977d9- deployment-7444  c7492344-d2ae-4d52-9518-309eac726644 44171 0 2023-05-04 13:12:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:b8c4c93a23e8f02f531a6b7fc87c1442429ef68c02cffd5eb856ee4711b377e9 cni.projectcalico.org/podIP:10.20.11.218/32 cni.projectcalico.org/podIPs:10.20.11.218/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 70affdcf-2ecb-4ca5-be57-b1ab0817cd14 0xc00344e120 0xc00344e121}] [] [{kube-controller-manager Update v1 2023-05-04 13:12:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70affdcf-2ecb-4ca5-be57-b1ab0817cd14\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-04 13:12:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-04 13:12:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.11.218\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nxdx8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nxdx8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-189.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.189,PodIP:10.20.11.218,StartTime:2023-05-04 13:12:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-04 13:12:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://1a5f07cd9ec7df6e1bc7a6e89d51c64fe3355040414e6bbe463721dded114f61,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.11.218,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  4 13:12:50.697: INFO: Pod "webserver-deployment-845c8977d9-vx285" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-vx285 webserver-deployment-845c8977d9- deployment-7444  aab882b1-0111-4f6d-9663-7dc5bc0d8a0d 44264 0 2023-05-04 13:12:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 70affdcf-2ecb-4ca5-be57-b1ab0817cd14 0xc00344e320 0xc00344e321}] [] [{kube-controller-manager Update v1 2023-05-04 13:12:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70affdcf-2ecb-4ca5-be57-b1ab0817cd14\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xz2wb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xz2wb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-224.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:12:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    May  4 13:12:50.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-7444" for this suite. 05/04/23 13:12:50.756
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:12:50.886
May  4 13:12:50.886: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 05/04/23 13:12:50.886
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:12:50.919
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:12:50.925
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 05/04/23 13:12:50.93
STEP: Creating hostNetwork=false pod 05/04/23 13:12:50.93
May  4 13:12:50.945: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-3115" to be "running and ready"
May  4 13:12:50.966: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 20.997647ms
May  4 13:12:50.966: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
May  4 13:12:52.971: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026687719s
May  4 13:12:52.971: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
May  4 13:12:54.973: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028001184s
May  4 13:12:54.973: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
May  4 13:12:56.972: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.026998617s
May  4 13:12:56.972: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
May  4 13:12:58.975: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.030369663s
May  4 13:12:58.975: INFO: The phase of Pod test-pod is Running (Ready = true)
May  4 13:12:58.975: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 05/04/23 13:12:58.981
May  4 13:12:58.991: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-3115" to be "running and ready"
May  4 13:12:58.997: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.200009ms
May  4 13:12:58.997: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
May  4 13:13:01.012: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021185084s
May  4 13:13:01.012: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
May  4 13:13:03.003: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.012575829s
May  4 13:13:03.003: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
May  4 13:13:03.003: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 05/04/23 13:13:03.012
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 05/04/23 13:13:03.012
May  4 13:13:03.012: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3115 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  4 13:13:03.012: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
May  4 13:13:03.012: INFO: ExecWithOptions: Clientset creation
May  4 13:13:03.012: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3115/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
May  4 13:13:03.119: INFO: Exec stderr: ""
May  4 13:13:03.119: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3115 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  4 13:13:03.119: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
May  4 13:13:03.120: INFO: ExecWithOptions: Clientset creation
May  4 13:13:03.120: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3115/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
May  4 13:13:03.192: INFO: Exec stderr: ""
May  4 13:13:03.192: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3115 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  4 13:13:03.192: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
May  4 13:13:03.192: INFO: ExecWithOptions: Clientset creation
May  4 13:13:03.193: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3115/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
May  4 13:13:03.286: INFO: Exec stderr: ""
May  4 13:13:03.286: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3115 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  4 13:13:03.286: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
May  4 13:13:03.287: INFO: ExecWithOptions: Clientset creation
May  4 13:13:03.287: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3115/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
May  4 13:13:03.411: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 05/04/23 13:13:03.411
May  4 13:13:03.411: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3115 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  4 13:13:03.411: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
May  4 13:13:03.412: INFO: ExecWithOptions: Clientset creation
May  4 13:13:03.412: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3115/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
May  4 13:13:03.492: INFO: Exec stderr: ""
May  4 13:13:03.492: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3115 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  4 13:13:03.492: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
May  4 13:13:03.493: INFO: ExecWithOptions: Clientset creation
May  4 13:13:03.493: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3115/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
May  4 13:13:03.576: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 05/04/23 13:13:03.576
May  4 13:13:03.576: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3115 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  4 13:13:03.576: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
May  4 13:13:03.577: INFO: ExecWithOptions: Clientset creation
May  4 13:13:03.577: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3115/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
May  4 13:13:03.672: INFO: Exec stderr: ""
May  4 13:13:03.672: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3115 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  4 13:13:03.672: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
May  4 13:13:03.673: INFO: ExecWithOptions: Clientset creation
May  4 13:13:03.673: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3115/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
May  4 13:13:03.778: INFO: Exec stderr: ""
May  4 13:13:03.778: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3115 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  4 13:13:03.778: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
May  4 13:13:03.779: INFO: ExecWithOptions: Clientset creation
May  4 13:13:03.779: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3115/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
May  4 13:13:03.854: INFO: Exec stderr: ""
May  4 13:13:03.854: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3115 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  4 13:13:03.854: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
May  4 13:13:03.854: INFO: ExecWithOptions: Clientset creation
May  4 13:13:03.854: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3115/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
May  4 13:13:03.959: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
May  4 13:13:03.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-3115" for this suite. 05/04/23 13:13:03.99
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","completed":320,"skipped":5856,"failed":0}
------------------------------
• [SLOW TEST] [13.131 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:12:50.886
    May  4 13:12:50.886: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 05/04/23 13:12:50.886
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:12:50.919
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:12:50.925
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 05/04/23 13:12:50.93
    STEP: Creating hostNetwork=false pod 05/04/23 13:12:50.93
    May  4 13:12:50.945: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-3115" to be "running and ready"
    May  4 13:12:50.966: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 20.997647ms
    May  4 13:12:50.966: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    May  4 13:12:52.971: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026687719s
    May  4 13:12:52.971: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    May  4 13:12:54.973: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028001184s
    May  4 13:12:54.973: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    May  4 13:12:56.972: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.026998617s
    May  4 13:12:56.972: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    May  4 13:12:58.975: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.030369663s
    May  4 13:12:58.975: INFO: The phase of Pod test-pod is Running (Ready = true)
    May  4 13:12:58.975: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 05/04/23 13:12:58.981
    May  4 13:12:58.991: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-3115" to be "running and ready"
    May  4 13:12:58.997: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.200009ms
    May  4 13:12:58.997: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    May  4 13:13:01.012: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021185084s
    May  4 13:13:01.012: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    May  4 13:13:03.003: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.012575829s
    May  4 13:13:03.003: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    May  4 13:13:03.003: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 05/04/23 13:13:03.012
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 05/04/23 13:13:03.012
    May  4 13:13:03.012: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3115 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  4 13:13:03.012: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    May  4 13:13:03.012: INFO: ExecWithOptions: Clientset creation
    May  4 13:13:03.012: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3115/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    May  4 13:13:03.119: INFO: Exec stderr: ""
    May  4 13:13:03.119: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3115 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  4 13:13:03.119: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    May  4 13:13:03.120: INFO: ExecWithOptions: Clientset creation
    May  4 13:13:03.120: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3115/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    May  4 13:13:03.192: INFO: Exec stderr: ""
    May  4 13:13:03.192: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3115 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  4 13:13:03.192: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    May  4 13:13:03.192: INFO: ExecWithOptions: Clientset creation
    May  4 13:13:03.193: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3115/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    May  4 13:13:03.286: INFO: Exec stderr: ""
    May  4 13:13:03.286: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3115 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  4 13:13:03.286: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    May  4 13:13:03.287: INFO: ExecWithOptions: Clientset creation
    May  4 13:13:03.287: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3115/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    May  4 13:13:03.411: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 05/04/23 13:13:03.411
    May  4 13:13:03.411: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3115 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  4 13:13:03.411: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    May  4 13:13:03.412: INFO: ExecWithOptions: Clientset creation
    May  4 13:13:03.412: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3115/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    May  4 13:13:03.492: INFO: Exec stderr: ""
    May  4 13:13:03.492: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3115 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  4 13:13:03.492: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    May  4 13:13:03.493: INFO: ExecWithOptions: Clientset creation
    May  4 13:13:03.493: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3115/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    May  4 13:13:03.576: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 05/04/23 13:13:03.576
    May  4 13:13:03.576: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3115 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  4 13:13:03.576: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    May  4 13:13:03.577: INFO: ExecWithOptions: Clientset creation
    May  4 13:13:03.577: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3115/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    May  4 13:13:03.672: INFO: Exec stderr: ""
    May  4 13:13:03.672: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3115 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  4 13:13:03.672: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    May  4 13:13:03.673: INFO: ExecWithOptions: Clientset creation
    May  4 13:13:03.673: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3115/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    May  4 13:13:03.778: INFO: Exec stderr: ""
    May  4 13:13:03.778: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3115 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  4 13:13:03.778: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    May  4 13:13:03.779: INFO: ExecWithOptions: Clientset creation
    May  4 13:13:03.779: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3115/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    May  4 13:13:03.854: INFO: Exec stderr: ""
    May  4 13:13:03.854: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3115 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  4 13:13:03.854: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    May  4 13:13:03.854: INFO: ExecWithOptions: Clientset creation
    May  4 13:13:03.854: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3115/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    May  4 13:13:03.959: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:187
    May  4 13:13:03.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-3115" for this suite. 05/04/23 13:13:03.99
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:13:04.017
May  4 13:13:04.017: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename downward-api 05/04/23 13:13:04.018
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:13:04.049
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:13:04.063
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
STEP: Creating a pod to test downward API volume plugin 05/04/23 13:13:04.068
May  4 13:13:04.096: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fceeb191-dda0-45d2-8262-d5b2a2d66c51" in namespace "downward-api-480" to be "Succeeded or Failed"
May  4 13:13:04.110: INFO: Pod "downwardapi-volume-fceeb191-dda0-45d2-8262-d5b2a2d66c51": Phase="Pending", Reason="", readiness=false. Elapsed: 14.002178ms
May  4 13:13:06.121: INFO: Pod "downwardapi-volume-fceeb191-dda0-45d2-8262-d5b2a2d66c51": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024304493s
May  4 13:13:08.115: INFO: Pod "downwardapi-volume-fceeb191-dda0-45d2-8262-d5b2a2d66c51": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018619462s
STEP: Saw pod success 05/04/23 13:13:08.115
May  4 13:13:08.115: INFO: Pod "downwardapi-volume-fceeb191-dda0-45d2-8262-d5b2a2d66c51" satisfied condition "Succeeded or Failed"
May  4 13:13:08.119: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod downwardapi-volume-fceeb191-dda0-45d2-8262-d5b2a2d66c51 container client-container: <nil>
STEP: delete the pod 05/04/23 13:13:08.137
May  4 13:13:08.154: INFO: Waiting for pod downwardapi-volume-fceeb191-dda0-45d2-8262-d5b2a2d66c51 to disappear
May  4 13:13:08.159: INFO: Pod downwardapi-volume-fceeb191-dda0-45d2-8262-d5b2a2d66c51 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
May  4 13:13:08.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-480" for this suite. 05/04/23 13:13:08.167
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","completed":321,"skipped":5864,"failed":0}
------------------------------
• [4.161 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:13:04.017
    May  4 13:13:04.017: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename downward-api 05/04/23 13:13:04.018
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:13:04.049
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:13:04.063
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:52
    STEP: Creating a pod to test downward API volume plugin 05/04/23 13:13:04.068
    May  4 13:13:04.096: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fceeb191-dda0-45d2-8262-d5b2a2d66c51" in namespace "downward-api-480" to be "Succeeded or Failed"
    May  4 13:13:04.110: INFO: Pod "downwardapi-volume-fceeb191-dda0-45d2-8262-d5b2a2d66c51": Phase="Pending", Reason="", readiness=false. Elapsed: 14.002178ms
    May  4 13:13:06.121: INFO: Pod "downwardapi-volume-fceeb191-dda0-45d2-8262-d5b2a2d66c51": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024304493s
    May  4 13:13:08.115: INFO: Pod "downwardapi-volume-fceeb191-dda0-45d2-8262-d5b2a2d66c51": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018619462s
    STEP: Saw pod success 05/04/23 13:13:08.115
    May  4 13:13:08.115: INFO: Pod "downwardapi-volume-fceeb191-dda0-45d2-8262-d5b2a2d66c51" satisfied condition "Succeeded or Failed"
    May  4 13:13:08.119: INFO: Trying to get logs from node ip-10-0-1-216.us-west-2.compute.internal pod downwardapi-volume-fceeb191-dda0-45d2-8262-d5b2a2d66c51 container client-container: <nil>
    STEP: delete the pod 05/04/23 13:13:08.137
    May  4 13:13:08.154: INFO: Waiting for pod downwardapi-volume-fceeb191-dda0-45d2-8262-d5b2a2d66c51 to disappear
    May  4 13:13:08.159: INFO: Pod downwardapi-volume-fceeb191-dda0-45d2-8262-d5b2a2d66c51 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    May  4 13:13:08.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-480" for this suite. 05/04/23 13:13:08.167
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:13:08.178
May  4 13:13:08.178: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename ingressclass 05/04/23 13:13:08.179
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:13:08.198
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:13:08.204
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 05/04/23 13:13:08.208
STEP: getting /apis/networking.k8s.io 05/04/23 13:13:08.211
STEP: getting /apis/networking.k8s.iov1 05/04/23 13:13:08.212
STEP: creating 05/04/23 13:13:08.214
STEP: getting 05/04/23 13:13:08.236
STEP: listing 05/04/23 13:13:08.24
STEP: watching 05/04/23 13:13:08.243
May  4 13:13:08.244: INFO: starting watch
STEP: patching 05/04/23 13:13:08.245
STEP: updating 05/04/23 13:13:08.251
May  4 13:13:08.259: INFO: waiting for watch events with expected annotations
May  4 13:13:08.259: INFO: saw patched and updated annotations
STEP: deleting 05/04/23 13:13:08.259
STEP: deleting a collection 05/04/23 13:13:08.277
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
May  4 13:13:08.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-786" for this suite. 05/04/23 13:13:08.307
{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","completed":322,"skipped":5864,"failed":0}
------------------------------
• [0.141 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:13:08.178
    May  4 13:13:08.178: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename ingressclass 05/04/23 13:13:08.179
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:13:08.198
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:13:08.204
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 05/04/23 13:13:08.208
    STEP: getting /apis/networking.k8s.io 05/04/23 13:13:08.211
    STEP: getting /apis/networking.k8s.iov1 05/04/23 13:13:08.212
    STEP: creating 05/04/23 13:13:08.214
    STEP: getting 05/04/23 13:13:08.236
    STEP: listing 05/04/23 13:13:08.24
    STEP: watching 05/04/23 13:13:08.243
    May  4 13:13:08.244: INFO: starting watch
    STEP: patching 05/04/23 13:13:08.245
    STEP: updating 05/04/23 13:13:08.251
    May  4 13:13:08.259: INFO: waiting for watch events with expected annotations
    May  4 13:13:08.259: INFO: saw patched and updated annotations
    STEP: deleting 05/04/23 13:13:08.259
    STEP: deleting a collection 05/04/23 13:13:08.277
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:187
    May  4 13:13:08.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingressclass-786" for this suite. 05/04/23 13:13:08.307
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:13:08.319
May  4 13:13:08.319: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename kubectl 05/04/23 13:13:08.32
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:13:08.339
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:13:08.345
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
STEP: creating Agnhost RC 05/04/23 13:13:08.352
May  4 13:13:08.352: INFO: namespace kubectl-3650
May  4 13:13:08.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3650 create -f -'
May  4 13:13:10.250: INFO: stderr: ""
May  4 13:13:10.250: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 05/04/23 13:13:10.25
May  4 13:13:11.257: INFO: Selector matched 1 pods for map[app:agnhost]
May  4 13:13:11.257: INFO: Found 0 / 1
May  4 13:13:12.260: INFO: Selector matched 1 pods for map[app:agnhost]
May  4 13:13:12.260: INFO: Found 1 / 1
May  4 13:13:12.260: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May  4 13:13:12.265: INFO: Selector matched 1 pods for map[app:agnhost]
May  4 13:13:12.265: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May  4 13:13:12.265: INFO: wait on agnhost-primary startup in kubectl-3650 
May  4 13:13:12.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3650 logs agnhost-primary-7rftb agnhost-primary'
May  4 13:13:12.379: INFO: stderr: ""
May  4 13:13:12.379: INFO: stdout: "Paused\n"
STEP: exposing RC 05/04/23 13:13:12.379
May  4 13:13:12.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3650 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
May  4 13:13:12.496: INFO: stderr: ""
May  4 13:13:12.496: INFO: stdout: "service/rm2 exposed\n"
May  4 13:13:12.509: INFO: Service rm2 in namespace kubectl-3650 found.
STEP: exposing service 05/04/23 13:13:14.521
May  4 13:13:14.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3650 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
May  4 13:13:14.708: INFO: stderr: ""
May  4 13:13:14.708: INFO: stdout: "service/rm3 exposed\n"
May  4 13:13:14.716: INFO: Service rm3 in namespace kubectl-3650 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  4 13:13:16.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3650" for this suite. 05/04/23 13:13:16.738
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","completed":323,"skipped":5867,"failed":0}
------------------------------
• [SLOW TEST] [8.431 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1407
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:13:08.319
    May  4 13:13:08.319: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename kubectl 05/04/23 13:13:08.32
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:13:08.339
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:13:08.345
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1413
    STEP: creating Agnhost RC 05/04/23 13:13:08.352
    May  4 13:13:08.352: INFO: namespace kubectl-3650
    May  4 13:13:08.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3650 create -f -'
    May  4 13:13:10.250: INFO: stderr: ""
    May  4 13:13:10.250: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 05/04/23 13:13:10.25
    May  4 13:13:11.257: INFO: Selector matched 1 pods for map[app:agnhost]
    May  4 13:13:11.257: INFO: Found 0 / 1
    May  4 13:13:12.260: INFO: Selector matched 1 pods for map[app:agnhost]
    May  4 13:13:12.260: INFO: Found 1 / 1
    May  4 13:13:12.260: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    May  4 13:13:12.265: INFO: Selector matched 1 pods for map[app:agnhost]
    May  4 13:13:12.265: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    May  4 13:13:12.265: INFO: wait on agnhost-primary startup in kubectl-3650 
    May  4 13:13:12.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3650 logs agnhost-primary-7rftb agnhost-primary'
    May  4 13:13:12.379: INFO: stderr: ""
    May  4 13:13:12.379: INFO: stdout: "Paused\n"
    STEP: exposing RC 05/04/23 13:13:12.379
    May  4 13:13:12.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3650 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    May  4 13:13:12.496: INFO: stderr: ""
    May  4 13:13:12.496: INFO: stdout: "service/rm2 exposed\n"
    May  4 13:13:12.509: INFO: Service rm2 in namespace kubectl-3650 found.
    STEP: exposing service 05/04/23 13:13:14.521
    May  4 13:13:14.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-3650 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    May  4 13:13:14.708: INFO: stderr: ""
    May  4 13:13:14.708: INFO: stdout: "service/rm3 exposed\n"
    May  4 13:13:14.716: INFO: Service rm3 in namespace kubectl-3650 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  4 13:13:16.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3650" for this suite. 05/04/23 13:13:16.738
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:13:16.752
May  4 13:13:16.752: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename emptydir 05/04/23 13:13:16.752
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:13:16.776
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:13:16.78
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
STEP: Creating a pod to test emptydir 0644 on tmpfs 05/04/23 13:13:16.783
May  4 13:13:16.801: INFO: Waiting up to 5m0s for pod "pod-96022a8d-2b88-4844-9c3b-9864434744e8" in namespace "emptydir-2377" to be "Succeeded or Failed"
May  4 13:13:16.808: INFO: Pod "pod-96022a8d-2b88-4844-9c3b-9864434744e8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.432105ms
May  4 13:13:18.815: INFO: Pod "pod-96022a8d-2b88-4844-9c3b-9864434744e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014197887s
May  4 13:13:20.814: INFO: Pod "pod-96022a8d-2b88-4844-9c3b-9864434744e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013375739s
STEP: Saw pod success 05/04/23 13:13:20.814
May  4 13:13:20.814: INFO: Pod "pod-96022a8d-2b88-4844-9c3b-9864434744e8" satisfied condition "Succeeded or Failed"
May  4 13:13:20.819: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-96022a8d-2b88-4844-9c3b-9864434744e8 container test-container: <nil>
STEP: delete the pod 05/04/23 13:13:20.829
May  4 13:13:20.845: INFO: Waiting for pod pod-96022a8d-2b88-4844-9c3b-9864434744e8 to disappear
May  4 13:13:20.849: INFO: Pod pod-96022a8d-2b88-4844-9c3b-9864434744e8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May  4 13:13:20.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2377" for this suite. 05/04/23 13:13:20.856
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":324,"skipped":5904,"failed":0}
------------------------------
• [4.134 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:13:16.752
    May  4 13:13:16.752: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename emptydir 05/04/23 13:13:16.752
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:13:16.776
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:13:16.78
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:96
    STEP: Creating a pod to test emptydir 0644 on tmpfs 05/04/23 13:13:16.783
    May  4 13:13:16.801: INFO: Waiting up to 5m0s for pod "pod-96022a8d-2b88-4844-9c3b-9864434744e8" in namespace "emptydir-2377" to be "Succeeded or Failed"
    May  4 13:13:16.808: INFO: Pod "pod-96022a8d-2b88-4844-9c3b-9864434744e8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.432105ms
    May  4 13:13:18.815: INFO: Pod "pod-96022a8d-2b88-4844-9c3b-9864434744e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014197887s
    May  4 13:13:20.814: INFO: Pod "pod-96022a8d-2b88-4844-9c3b-9864434744e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013375739s
    STEP: Saw pod success 05/04/23 13:13:20.814
    May  4 13:13:20.814: INFO: Pod "pod-96022a8d-2b88-4844-9c3b-9864434744e8" satisfied condition "Succeeded or Failed"
    May  4 13:13:20.819: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-96022a8d-2b88-4844-9c3b-9864434744e8 container test-container: <nil>
    STEP: delete the pod 05/04/23 13:13:20.829
    May  4 13:13:20.845: INFO: Waiting for pod pod-96022a8d-2b88-4844-9c3b-9864434744e8 to disappear
    May  4 13:13:20.849: INFO: Pod pod-96022a8d-2b88-4844-9c3b-9864434744e8 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May  4 13:13:20.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2377" for this suite. 05/04/23 13:13:20.856
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:13:20.887
May  4 13:13:20.887: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename init-container 05/04/23 13:13:20.888
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:13:20.908
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:13:20.912
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
STEP: creating the pod 05/04/23 13:13:20.914
May  4 13:13:20.914: INFO: PodSpec: initContainers in spec.initContainers
May  4 13:14:06.405: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-4242ac32-b3bd-4e0d-84bf-0a47dc64b5db", GenerateName:"", Namespace:"init-container-8679", SelfLink:"", UID:"d3b71bfd-f060-4142-9bfd-cf0461cdf09e", ResourceVersion:"45050", Generation:0, CreationTimestamp:time.Date(2023, time.May, 4, 13, 13, 20, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"914577192"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"d8b5f7860902cfec4b531c7407a0dd0763abe7c138fa5e49edf5d9de51cdda13", "cni.projectcalico.org/podIP":"10.20.83.34/32", "cni.projectcalico.org/podIPs":"10.20.83.34/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 4, 13, 13, 20, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0036c2990), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 4, 13, 13, 21, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0036c29c0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 4, 13, 14, 6, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0036c29f0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-wg4fp", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc00281a0a0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-wg4fp", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-wg4fp", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-wg4fp", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00568a9a0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-0-1-224.us-west-2.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc008717b20), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00568aa20)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00568aa40)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00568aa48), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00568aa4c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc005505160), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 4, 13, 13, 20, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 4, 13, 13, 20, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 4, 13, 13, 20, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 4, 13, 13, 20, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.1.224", PodIP:"10.20.83.34", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.20.83.34"}}, StartTime:time.Date(2023, time.May, 4, 13, 13, 20, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc008717c00)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc008717c70)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://01e8517b2fe2ce0710c14ed810fd7c2c2053c3b142cdb0499497db374d05a9d5", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00281a240), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00281a220), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc00568aacf)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
May  4 13:14:06.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8679" for this suite. 05/04/23 13:14:06.431
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","completed":325,"skipped":5914,"failed":0}
------------------------------
• [SLOW TEST] [45.556 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:13:20.887
    May  4 13:13:20.887: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename init-container 05/04/23 13:13:20.888
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:13:20.908
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:13:20.912
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:333
    STEP: creating the pod 05/04/23 13:13:20.914
    May  4 13:13:20.914: INFO: PodSpec: initContainers in spec.initContainers
    May  4 13:14:06.405: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-4242ac32-b3bd-4e0d-84bf-0a47dc64b5db", GenerateName:"", Namespace:"init-container-8679", SelfLink:"", UID:"d3b71bfd-f060-4142-9bfd-cf0461cdf09e", ResourceVersion:"45050", Generation:0, CreationTimestamp:time.Date(2023, time.May, 4, 13, 13, 20, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"914577192"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"d8b5f7860902cfec4b531c7407a0dd0763abe7c138fa5e49edf5d9de51cdda13", "cni.projectcalico.org/podIP":"10.20.83.34/32", "cni.projectcalico.org/podIPs":"10.20.83.34/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 4, 13, 13, 20, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0036c2990), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 4, 13, 13, 21, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0036c29c0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 4, 13, 14, 6, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0036c29f0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-wg4fp", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc00281a0a0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-wg4fp", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-wg4fp", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-wg4fp", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00568a9a0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-0-1-224.us-west-2.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc008717b20), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00568aa20)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00568aa40)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00568aa48), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00568aa4c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc005505160), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 4, 13, 13, 20, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 4, 13, 13, 20, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 4, 13, 13, 20, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 4, 13, 13, 20, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.1.224", PodIP:"10.20.83.34", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.20.83.34"}}, StartTime:time.Date(2023, time.May, 4, 13, 13, 20, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc008717c00)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc008717c70)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://01e8517b2fe2ce0710c14ed810fd7c2c2053c3b142cdb0499497db374d05a9d5", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00281a240), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00281a220), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc00568aacf)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    May  4 13:14:06.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-8679" for this suite. 05/04/23 13:14:06.431
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:14:06.445
May  4 13:14:06.445: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename emptydir 05/04/23 13:14:06.446
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:14:06.466
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:14:06.469
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
STEP: Creating a pod to test emptydir 0666 on tmpfs 05/04/23 13:14:06.472
May  4 13:14:06.490: INFO: Waiting up to 5m0s for pod "pod-e2d9f585-858f-435f-bfc1-ac683b3fac0d" in namespace "emptydir-5202" to be "Succeeded or Failed"
May  4 13:14:06.496: INFO: Pod "pod-e2d9f585-858f-435f-bfc1-ac683b3fac0d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.943184ms
May  4 13:14:08.501: INFO: Pod "pod-e2d9f585-858f-435f-bfc1-ac683b3fac0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011114531s
May  4 13:14:10.502: INFO: Pod "pod-e2d9f585-858f-435f-bfc1-ac683b3fac0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011856323s
STEP: Saw pod success 05/04/23 13:14:10.502
May  4 13:14:10.502: INFO: Pod "pod-e2d9f585-858f-435f-bfc1-ac683b3fac0d" satisfied condition "Succeeded or Failed"
May  4 13:14:10.510: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-e2d9f585-858f-435f-bfc1-ac683b3fac0d container test-container: <nil>
STEP: delete the pod 05/04/23 13:14:10.522
May  4 13:14:10.553: INFO: Waiting for pod pod-e2d9f585-858f-435f-bfc1-ac683b3fac0d to disappear
May  4 13:14:10.558: INFO: Pod pod-e2d9f585-858f-435f-bfc1-ac683b3fac0d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May  4 13:14:10.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5202" for this suite. 05/04/23 13:14:10.575
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":326,"skipped":5931,"failed":0}
------------------------------
• [4.153 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:14:06.445
    May  4 13:14:06.445: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename emptydir 05/04/23 13:14:06.446
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:14:06.466
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:14:06.469
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:136
    STEP: Creating a pod to test emptydir 0666 on tmpfs 05/04/23 13:14:06.472
    May  4 13:14:06.490: INFO: Waiting up to 5m0s for pod "pod-e2d9f585-858f-435f-bfc1-ac683b3fac0d" in namespace "emptydir-5202" to be "Succeeded or Failed"
    May  4 13:14:06.496: INFO: Pod "pod-e2d9f585-858f-435f-bfc1-ac683b3fac0d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.943184ms
    May  4 13:14:08.501: INFO: Pod "pod-e2d9f585-858f-435f-bfc1-ac683b3fac0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011114531s
    May  4 13:14:10.502: INFO: Pod "pod-e2d9f585-858f-435f-bfc1-ac683b3fac0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011856323s
    STEP: Saw pod success 05/04/23 13:14:10.502
    May  4 13:14:10.502: INFO: Pod "pod-e2d9f585-858f-435f-bfc1-ac683b3fac0d" satisfied condition "Succeeded or Failed"
    May  4 13:14:10.510: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-e2d9f585-858f-435f-bfc1-ac683b3fac0d container test-container: <nil>
    STEP: delete the pod 05/04/23 13:14:10.522
    May  4 13:14:10.553: INFO: Waiting for pod pod-e2d9f585-858f-435f-bfc1-ac683b3fac0d to disappear
    May  4 13:14:10.558: INFO: Pod pod-e2d9f585-858f-435f-bfc1-ac683b3fac0d no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May  4 13:14:10.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5202" for this suite. 05/04/23 13:14:10.575
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:14:10.601
May  4 13:14:10.601: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename statefulset 05/04/23 13:14:10.603
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:14:10.649
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:14:10.652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-925 05/04/23 13:14:10.654
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
STEP: Creating statefulset ss in namespace statefulset-925 05/04/23 13:14:10.671
May  4 13:14:10.693: INFO: Found 0 stateful pods, waiting for 1
May  4 13:14:20.702: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 05/04/23 13:14:20.721
STEP: Getting /status 05/04/23 13:14:20.803
May  4 13:14:20.808: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 05/04/23 13:14:20.808
May  4 13:14:20.821: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 05/04/23 13:14:20.822
May  4 13:14:20.823: INFO: Observed &StatefulSet event: ADDED
May  4 13:14:20.824: INFO: Found Statefulset ss in namespace statefulset-925 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May  4 13:14:20.824: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 05/04/23 13:14:20.824
May  4 13:14:20.824: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
May  4 13:14:20.834: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 05/04/23 13:14:20.834
May  4 13:14:20.836: INFO: Observed &StatefulSet event: ADDED
May  4 13:14:20.836: INFO: Observed Statefulset ss in namespace statefulset-925 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May  4 13:14:20.836: INFO: Observed &StatefulSet event: MODIFIED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
May  4 13:14:20.836: INFO: Deleting all statefulset in ns statefulset-925
May  4 13:14:20.840: INFO: Scaling statefulset ss to 0
May  4 13:14:30.865: INFO: Waiting for statefulset status.replicas updated to 0
May  4 13:14:30.870: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
May  4 13:14:30.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-925" for this suite. 05/04/23 13:14:30.914
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","completed":327,"skipped":5988,"failed":0}
------------------------------
• [SLOW TEST] [20.326 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:975

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:14:10.601
    May  4 13:14:10.601: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename statefulset 05/04/23 13:14:10.603
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:14:10.649
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:14:10.652
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-925 05/04/23 13:14:10.654
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:975
    STEP: Creating statefulset ss in namespace statefulset-925 05/04/23 13:14:10.671
    May  4 13:14:10.693: INFO: Found 0 stateful pods, waiting for 1
    May  4 13:14:20.702: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 05/04/23 13:14:20.721
    STEP: Getting /status 05/04/23 13:14:20.803
    May  4 13:14:20.808: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 05/04/23 13:14:20.808
    May  4 13:14:20.821: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 05/04/23 13:14:20.822
    May  4 13:14:20.823: INFO: Observed &StatefulSet event: ADDED
    May  4 13:14:20.824: INFO: Found Statefulset ss in namespace statefulset-925 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    May  4 13:14:20.824: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 05/04/23 13:14:20.824
    May  4 13:14:20.824: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    May  4 13:14:20.834: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 05/04/23 13:14:20.834
    May  4 13:14:20.836: INFO: Observed &StatefulSet event: ADDED
    May  4 13:14:20.836: INFO: Observed Statefulset ss in namespace statefulset-925 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    May  4 13:14:20.836: INFO: Observed &StatefulSet event: MODIFIED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    May  4 13:14:20.836: INFO: Deleting all statefulset in ns statefulset-925
    May  4 13:14:20.840: INFO: Scaling statefulset ss to 0
    May  4 13:14:30.865: INFO: Waiting for statefulset status.replicas updated to 0
    May  4 13:14:30.870: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    May  4 13:14:30.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-925" for this suite. 05/04/23 13:14:30.914
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:14:30.928
May  4 13:14:30.928: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename pods 05/04/23 13:14:30.929
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:14:30.954
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:14:30.961
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 05/04/23 13:14:30.964
STEP: submitting the pod to kubernetes 05/04/23 13:14:30.965
STEP: verifying QOS class is set on the pod 05/04/23 13:14:30.983
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
May  4 13:14:30.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7334" for this suite. 05/04/23 13:14:31.004
{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","completed":328,"skipped":6001,"failed":0}
------------------------------
• [0.089 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:14:30.928
    May  4 13:14:30.928: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename pods 05/04/23 13:14:30.929
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:14:30.954
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:14:30.961
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 05/04/23 13:14:30.964
    STEP: submitting the pod to kubernetes 05/04/23 13:14:30.965
    STEP: verifying QOS class is set on the pod 05/04/23 13:14:30.983
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:187
    May  4 13:14:30.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-7334" for this suite. 05/04/23 13:14:31.004
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:14:31.018
May  4 13:14:31.018: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename endpointslice 05/04/23 13:14:31.019
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:14:31.06
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:14:31.063
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
May  4 13:14:31.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-3355" for this suite. 05/04/23 13:14:31.199
{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","completed":329,"skipped":6004,"failed":0}
------------------------------
• [0.196 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:14:31.018
    May  4 13:14:31.018: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename endpointslice 05/04/23 13:14:31.019
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:14:31.06
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:14:31.063
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:101
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    May  4 13:14:31.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-3355" for this suite. 05/04/23 13:14:31.199
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:14:31.214
May  4 13:14:31.214: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename projected 05/04/23 13:14:31.215
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:14:31.242
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:14:31.249
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
STEP: Creating a pod to test downward API volume plugin 05/04/23 13:14:31.254
May  4 13:14:31.273: INFO: Waiting up to 5m0s for pod "downwardapi-volume-907ded4a-f53e-433f-8c3f-8a7fb35a57a7" in namespace "projected-8282" to be "Succeeded or Failed"
May  4 13:14:31.283: INFO: Pod "downwardapi-volume-907ded4a-f53e-433f-8c3f-8a7fb35a57a7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.309985ms
May  4 13:14:33.290: INFO: Pod "downwardapi-volume-907ded4a-f53e-433f-8c3f-8a7fb35a57a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016524385s
May  4 13:14:35.290: INFO: Pod "downwardapi-volume-907ded4a-f53e-433f-8c3f-8a7fb35a57a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0172396s
STEP: Saw pod success 05/04/23 13:14:35.29
May  4 13:14:35.290: INFO: Pod "downwardapi-volume-907ded4a-f53e-433f-8c3f-8a7fb35a57a7" satisfied condition "Succeeded or Failed"
May  4 13:14:35.295: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod downwardapi-volume-907ded4a-f53e-433f-8c3f-8a7fb35a57a7 container client-container: <nil>
STEP: delete the pod 05/04/23 13:14:35.305
May  4 13:14:35.326: INFO: Waiting for pod downwardapi-volume-907ded4a-f53e-433f-8c3f-8a7fb35a57a7 to disappear
May  4 13:14:35.332: INFO: Pod downwardapi-volume-907ded4a-f53e-433f-8c3f-8a7fb35a57a7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
May  4 13:14:35.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8282" for this suite. 05/04/23 13:14:35.341
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":330,"skipped":6009,"failed":0}
------------------------------
• [4.141 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:14:31.214
    May  4 13:14:31.214: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename projected 05/04/23 13:14:31.215
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:14:31.242
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:14:31.249
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:248
    STEP: Creating a pod to test downward API volume plugin 05/04/23 13:14:31.254
    May  4 13:14:31.273: INFO: Waiting up to 5m0s for pod "downwardapi-volume-907ded4a-f53e-433f-8c3f-8a7fb35a57a7" in namespace "projected-8282" to be "Succeeded or Failed"
    May  4 13:14:31.283: INFO: Pod "downwardapi-volume-907ded4a-f53e-433f-8c3f-8a7fb35a57a7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.309985ms
    May  4 13:14:33.290: INFO: Pod "downwardapi-volume-907ded4a-f53e-433f-8c3f-8a7fb35a57a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016524385s
    May  4 13:14:35.290: INFO: Pod "downwardapi-volume-907ded4a-f53e-433f-8c3f-8a7fb35a57a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0172396s
    STEP: Saw pod success 05/04/23 13:14:35.29
    May  4 13:14:35.290: INFO: Pod "downwardapi-volume-907ded4a-f53e-433f-8c3f-8a7fb35a57a7" satisfied condition "Succeeded or Failed"
    May  4 13:14:35.295: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod downwardapi-volume-907ded4a-f53e-433f-8c3f-8a7fb35a57a7 container client-container: <nil>
    STEP: delete the pod 05/04/23 13:14:35.305
    May  4 13:14:35.326: INFO: Waiting for pod downwardapi-volume-907ded4a-f53e-433f-8c3f-8a7fb35a57a7 to disappear
    May  4 13:14:35.332: INFO: Pod downwardapi-volume-907ded4a-f53e-433f-8c3f-8a7fb35a57a7 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    May  4 13:14:35.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8282" for this suite. 05/04/23 13:14:35.341
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:14:35.357
May  4 13:14:35.357: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename container-runtime 05/04/23 13:14:35.358
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:14:35.384
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:14:35.388
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
STEP: create the container 05/04/23 13:14:35.391
STEP: wait for the container to reach Succeeded 05/04/23 13:14:35.403
STEP: get the container status 05/04/23 13:14:39.434
STEP: the container should be terminated 05/04/23 13:14:39.438
STEP: the termination message should be set 05/04/23 13:14:39.438
May  4 13:14:39.438: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 05/04/23 13:14:39.439
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
May  4 13:14:39.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7502" for this suite. 05/04/23 13:14:39.477
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":331,"skipped":6024,"failed":0}
------------------------------
• [4.130 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:14:35.357
    May  4 13:14:35.357: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename container-runtime 05/04/23 13:14:35.358
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:14:35.384
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:14:35.388
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231
    STEP: create the container 05/04/23 13:14:35.391
    STEP: wait for the container to reach Succeeded 05/04/23 13:14:35.403
    STEP: get the container status 05/04/23 13:14:39.434
    STEP: the container should be terminated 05/04/23 13:14:39.438
    STEP: the termination message should be set 05/04/23 13:14:39.438
    May  4 13:14:39.438: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 05/04/23 13:14:39.439
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    May  4 13:14:39.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-7502" for this suite. 05/04/23 13:14:39.477
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:14:39.491
May  4 13:14:39.491: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename dns 05/04/23 13:14:39.493
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:14:39.52
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:14:39.523
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 05/04/23 13:14:39.528
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5071 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5071;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5071 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5071;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5071.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5071.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5071.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5071.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5071.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5071.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5071.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5071.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5071.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5071.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5071.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5071.svc;check="$$(dig +notcp +noall +answer +search 101.96.21.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.21.96.101_udp@PTR;check="$$(dig +tcp +noall +answer +search 101.96.21.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.21.96.101_tcp@PTR;sleep 1; done
 05/04/23 13:14:39.569
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5071 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5071;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5071 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5071;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5071.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5071.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5071.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5071.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5071.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5071.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5071.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5071.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5071.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5071.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5071.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5071.svc;check="$$(dig +notcp +noall +answer +search 101.96.21.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.21.96.101_udp@PTR;check="$$(dig +tcp +noall +answer +search 101.96.21.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.21.96.101_tcp@PTR;sleep 1; done
 05/04/23 13:14:39.569
STEP: creating a pod to probe DNS 05/04/23 13:14:39.569
STEP: submitting the pod to kubernetes 05/04/23 13:14:39.569
May  4 13:14:39.584: INFO: Waiting up to 15m0s for pod "dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02" in namespace "dns-5071" to be "running"
May  4 13:14:39.590: INFO: Pod "dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02": Phase="Pending", Reason="", readiness=false. Elapsed: 5.746981ms
May  4 13:14:41.600: INFO: Pod "dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02": Phase="Running", Reason="", readiness=true. Elapsed: 2.01594737s
May  4 13:14:41.600: INFO: Pod "dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02" satisfied condition "running"
STEP: retrieving the pod 05/04/23 13:14:41.6
STEP: looking for the results for each expected name from probers 05/04/23 13:14:41.605
May  4 13:14:41.616: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:41.622: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:41.628: INFO: Unable to read wheezy_udp@dns-test-service.dns-5071 from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:41.634: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5071 from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:41.644: INFO: Unable to read wheezy_udp@dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:41.650: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:41.656: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:41.660: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:41.694: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:41.699: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:41.710: INFO: Unable to read jessie_udp@dns-test-service.dns-5071 from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:41.722: INFO: Unable to read jessie_tcp@dns-test-service.dns-5071 from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:41.729: INFO: Unable to read jessie_udp@dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:41.736: INFO: Unable to read jessie_tcp@dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:41.742: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:41.747: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:41.770: INFO: Lookups using dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5071 wheezy_tcp@dns-test-service.dns-5071 wheezy_udp@dns-test-service.dns-5071.svc wheezy_tcp@dns-test-service.dns-5071.svc wheezy_udp@_http._tcp.dns-test-service.dns-5071.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5071.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5071 jessie_tcp@dns-test-service.dns-5071 jessie_udp@dns-test-service.dns-5071.svc jessie_tcp@dns-test-service.dns-5071.svc jessie_udp@_http._tcp.dns-test-service.dns-5071.svc jessie_tcp@_http._tcp.dns-test-service.dns-5071.svc]

May  4 13:14:46.776: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:46.785: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:46.791: INFO: Unable to read wheezy_udp@dns-test-service.dns-5071 from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:46.796: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5071 from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:46.804: INFO: Unable to read wheezy_udp@dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:46.812: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:46.824: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:46.834: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:46.860: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:46.865: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:46.871: INFO: Unable to read jessie_udp@dns-test-service.dns-5071 from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:46.876: INFO: Unable to read jessie_tcp@dns-test-service.dns-5071 from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:46.885: INFO: Unable to read jessie_udp@dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:46.899: INFO: Unable to read jessie_tcp@dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:46.905: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:46.911: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:46.939: INFO: Lookups using dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5071 wheezy_tcp@dns-test-service.dns-5071 wheezy_udp@dns-test-service.dns-5071.svc wheezy_tcp@dns-test-service.dns-5071.svc wheezy_udp@_http._tcp.dns-test-service.dns-5071.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5071.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5071 jessie_tcp@dns-test-service.dns-5071 jessie_udp@dns-test-service.dns-5071.svc jessie_tcp@dns-test-service.dns-5071.svc jessie_udp@_http._tcp.dns-test-service.dns-5071.svc jessie_tcp@_http._tcp.dns-test-service.dns-5071.svc]

May  4 13:14:51.779: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:51.789: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:51.795: INFO: Unable to read wheezy_udp@dns-test-service.dns-5071 from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:51.810: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5071 from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:51.821: INFO: Unable to read wheezy_udp@dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:51.827: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:51.832: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:51.842: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:51.878: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:51.887: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:51.894: INFO: Unable to read jessie_udp@dns-test-service.dns-5071 from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:51.903: INFO: Unable to read jessie_tcp@dns-test-service.dns-5071 from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:51.911: INFO: Unable to read jessie_udp@dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:51.927: INFO: Unable to read jessie_tcp@dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:51.934: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:51.944: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:51.973: INFO: Lookups using dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5071 wheezy_tcp@dns-test-service.dns-5071 wheezy_udp@dns-test-service.dns-5071.svc wheezy_tcp@dns-test-service.dns-5071.svc wheezy_udp@_http._tcp.dns-test-service.dns-5071.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5071.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5071 jessie_tcp@dns-test-service.dns-5071 jessie_udp@dns-test-service.dns-5071.svc jessie_tcp@dns-test-service.dns-5071.svc jessie_udp@_http._tcp.dns-test-service.dns-5071.svc jessie_tcp@_http._tcp.dns-test-service.dns-5071.svc]

May  4 13:14:56.778: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:56.783: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:56.788: INFO: Unable to read wheezy_udp@dns-test-service.dns-5071 from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:56.793: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5071 from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:56.798: INFO: Unable to read wheezy_udp@dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:56.803: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:56.808: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:56.813: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:56.842: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:56.851: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:56.856: INFO: Unable to read jessie_udp@dns-test-service.dns-5071 from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:56.861: INFO: Unable to read jessie_tcp@dns-test-service.dns-5071 from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:56.866: INFO: Unable to read jessie_udp@dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:56.871: INFO: Unable to read jessie_tcp@dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:56.877: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:56.884: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:14:56.909: INFO: Lookups using dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5071 wheezy_tcp@dns-test-service.dns-5071 wheezy_udp@dns-test-service.dns-5071.svc wheezy_tcp@dns-test-service.dns-5071.svc wheezy_udp@_http._tcp.dns-test-service.dns-5071.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5071.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5071 jessie_tcp@dns-test-service.dns-5071 jessie_udp@dns-test-service.dns-5071.svc jessie_tcp@dns-test-service.dns-5071.svc jessie_udp@_http._tcp.dns-test-service.dns-5071.svc jessie_tcp@_http._tcp.dns-test-service.dns-5071.svc]

May  4 13:15:01.783: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:15:01.788: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:15:01.796: INFO: Unable to read wheezy_udp@dns-test-service.dns-5071 from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:15:01.806: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5071 from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:15:01.811: INFO: Unable to read wheezy_udp@dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:15:01.816: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:15:01.821: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:15:01.826: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:15:01.853: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:15:01.860: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:15:01.868: INFO: Unable to read jessie_udp@dns-test-service.dns-5071 from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:15:01.875: INFO: Unable to read jessie_tcp@dns-test-service.dns-5071 from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:15:01.881: INFO: Unable to read jessie_udp@dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:15:01.888: INFO: Unable to read jessie_tcp@dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:15:01.895: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:15:01.901: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:15:01.929: INFO: Lookups using dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5071 wheezy_tcp@dns-test-service.dns-5071 wheezy_udp@dns-test-service.dns-5071.svc wheezy_tcp@dns-test-service.dns-5071.svc wheezy_udp@_http._tcp.dns-test-service.dns-5071.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5071.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5071 jessie_tcp@dns-test-service.dns-5071 jessie_udp@dns-test-service.dns-5071.svc jessie_tcp@dns-test-service.dns-5071.svc jessie_udp@_http._tcp.dns-test-service.dns-5071.svc jessie_tcp@_http._tcp.dns-test-service.dns-5071.svc]

May  4 13:15:06.780: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:15:06.785: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:15:06.790: INFO: Unable to read wheezy_udp@dns-test-service.dns-5071 from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:15:06.798: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5071 from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:15:06.810: INFO: Unable to read wheezy_udp@dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:15:06.820: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:15:06.827: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:15:06.832: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:15:06.858: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:15:06.863: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:15:06.869: INFO: Unable to read jessie_udp@dns-test-service.dns-5071 from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:15:06.875: INFO: Unable to read jessie_tcp@dns-test-service.dns-5071 from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:15:06.880: INFO: Unable to read jessie_udp@dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:15:06.885: INFO: Unable to read jessie_tcp@dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:15:06.891: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:15:06.897: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
May  4 13:15:06.923: INFO: Lookups using dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5071 wheezy_tcp@dns-test-service.dns-5071 wheezy_udp@dns-test-service.dns-5071.svc wheezy_tcp@dns-test-service.dns-5071.svc wheezy_udp@_http._tcp.dns-test-service.dns-5071.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5071.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5071 jessie_tcp@dns-test-service.dns-5071 jessie_udp@dns-test-service.dns-5071.svc jessie_tcp@dns-test-service.dns-5071.svc jessie_udp@_http._tcp.dns-test-service.dns-5071.svc jessie_tcp@_http._tcp.dns-test-service.dns-5071.svc]

May  4 13:15:11.931: INFO: DNS probes using dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02 succeeded

STEP: deleting the pod 05/04/23 13:15:11.931
STEP: deleting the test service 05/04/23 13:15:11.961
STEP: deleting the test headless service 05/04/23 13:15:12.007
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
May  4 13:15:12.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5071" for this suite. 05/04/23 13:15:12.045
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","completed":332,"skipped":6052,"failed":0}
------------------------------
• [SLOW TEST] [32.568 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:14:39.491
    May  4 13:14:39.491: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename dns 05/04/23 13:14:39.493
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:14:39.52
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:14:39.523
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 05/04/23 13:14:39.528
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5071 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5071;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5071 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5071;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5071.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5071.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5071.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5071.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5071.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5071.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5071.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5071.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5071.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5071.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5071.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5071.svc;check="$$(dig +notcp +noall +answer +search 101.96.21.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.21.96.101_udp@PTR;check="$$(dig +tcp +noall +answer +search 101.96.21.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.21.96.101_tcp@PTR;sleep 1; done
     05/04/23 13:14:39.569
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5071 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5071;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5071 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5071;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5071.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5071.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5071.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5071.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5071.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5071.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5071.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5071.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5071.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5071.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5071.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5071.svc;check="$$(dig +notcp +noall +answer +search 101.96.21.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.21.96.101_udp@PTR;check="$$(dig +tcp +noall +answer +search 101.96.21.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.21.96.101_tcp@PTR;sleep 1; done
     05/04/23 13:14:39.569
    STEP: creating a pod to probe DNS 05/04/23 13:14:39.569
    STEP: submitting the pod to kubernetes 05/04/23 13:14:39.569
    May  4 13:14:39.584: INFO: Waiting up to 15m0s for pod "dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02" in namespace "dns-5071" to be "running"
    May  4 13:14:39.590: INFO: Pod "dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02": Phase="Pending", Reason="", readiness=false. Elapsed: 5.746981ms
    May  4 13:14:41.600: INFO: Pod "dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02": Phase="Running", Reason="", readiness=true. Elapsed: 2.01594737s
    May  4 13:14:41.600: INFO: Pod "dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02" satisfied condition "running"
    STEP: retrieving the pod 05/04/23 13:14:41.6
    STEP: looking for the results for each expected name from probers 05/04/23 13:14:41.605
    May  4 13:14:41.616: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:41.622: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:41.628: INFO: Unable to read wheezy_udp@dns-test-service.dns-5071 from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:41.634: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5071 from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:41.644: INFO: Unable to read wheezy_udp@dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:41.650: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:41.656: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:41.660: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:41.694: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:41.699: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:41.710: INFO: Unable to read jessie_udp@dns-test-service.dns-5071 from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:41.722: INFO: Unable to read jessie_tcp@dns-test-service.dns-5071 from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:41.729: INFO: Unable to read jessie_udp@dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:41.736: INFO: Unable to read jessie_tcp@dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:41.742: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:41.747: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:41.770: INFO: Lookups using dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5071 wheezy_tcp@dns-test-service.dns-5071 wheezy_udp@dns-test-service.dns-5071.svc wheezy_tcp@dns-test-service.dns-5071.svc wheezy_udp@_http._tcp.dns-test-service.dns-5071.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5071.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5071 jessie_tcp@dns-test-service.dns-5071 jessie_udp@dns-test-service.dns-5071.svc jessie_tcp@dns-test-service.dns-5071.svc jessie_udp@_http._tcp.dns-test-service.dns-5071.svc jessie_tcp@_http._tcp.dns-test-service.dns-5071.svc]

    May  4 13:14:46.776: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:46.785: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:46.791: INFO: Unable to read wheezy_udp@dns-test-service.dns-5071 from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:46.796: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5071 from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:46.804: INFO: Unable to read wheezy_udp@dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:46.812: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:46.824: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:46.834: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:46.860: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:46.865: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:46.871: INFO: Unable to read jessie_udp@dns-test-service.dns-5071 from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:46.876: INFO: Unable to read jessie_tcp@dns-test-service.dns-5071 from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:46.885: INFO: Unable to read jessie_udp@dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:46.899: INFO: Unable to read jessie_tcp@dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:46.905: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:46.911: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:46.939: INFO: Lookups using dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5071 wheezy_tcp@dns-test-service.dns-5071 wheezy_udp@dns-test-service.dns-5071.svc wheezy_tcp@dns-test-service.dns-5071.svc wheezy_udp@_http._tcp.dns-test-service.dns-5071.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5071.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5071 jessie_tcp@dns-test-service.dns-5071 jessie_udp@dns-test-service.dns-5071.svc jessie_tcp@dns-test-service.dns-5071.svc jessie_udp@_http._tcp.dns-test-service.dns-5071.svc jessie_tcp@_http._tcp.dns-test-service.dns-5071.svc]

    May  4 13:14:51.779: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:51.789: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:51.795: INFO: Unable to read wheezy_udp@dns-test-service.dns-5071 from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:51.810: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5071 from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:51.821: INFO: Unable to read wheezy_udp@dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:51.827: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:51.832: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:51.842: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:51.878: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:51.887: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:51.894: INFO: Unable to read jessie_udp@dns-test-service.dns-5071 from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:51.903: INFO: Unable to read jessie_tcp@dns-test-service.dns-5071 from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:51.911: INFO: Unable to read jessie_udp@dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:51.927: INFO: Unable to read jessie_tcp@dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:51.934: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:51.944: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:51.973: INFO: Lookups using dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5071 wheezy_tcp@dns-test-service.dns-5071 wheezy_udp@dns-test-service.dns-5071.svc wheezy_tcp@dns-test-service.dns-5071.svc wheezy_udp@_http._tcp.dns-test-service.dns-5071.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5071.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5071 jessie_tcp@dns-test-service.dns-5071 jessie_udp@dns-test-service.dns-5071.svc jessie_tcp@dns-test-service.dns-5071.svc jessie_udp@_http._tcp.dns-test-service.dns-5071.svc jessie_tcp@_http._tcp.dns-test-service.dns-5071.svc]

    May  4 13:14:56.778: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:56.783: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:56.788: INFO: Unable to read wheezy_udp@dns-test-service.dns-5071 from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:56.793: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5071 from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:56.798: INFO: Unable to read wheezy_udp@dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:56.803: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:56.808: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:56.813: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:56.842: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:56.851: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:56.856: INFO: Unable to read jessie_udp@dns-test-service.dns-5071 from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:56.861: INFO: Unable to read jessie_tcp@dns-test-service.dns-5071 from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:56.866: INFO: Unable to read jessie_udp@dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:56.871: INFO: Unable to read jessie_tcp@dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:56.877: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:56.884: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:14:56.909: INFO: Lookups using dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5071 wheezy_tcp@dns-test-service.dns-5071 wheezy_udp@dns-test-service.dns-5071.svc wheezy_tcp@dns-test-service.dns-5071.svc wheezy_udp@_http._tcp.dns-test-service.dns-5071.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5071.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5071 jessie_tcp@dns-test-service.dns-5071 jessie_udp@dns-test-service.dns-5071.svc jessie_tcp@dns-test-service.dns-5071.svc jessie_udp@_http._tcp.dns-test-service.dns-5071.svc jessie_tcp@_http._tcp.dns-test-service.dns-5071.svc]

    May  4 13:15:01.783: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:15:01.788: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:15:01.796: INFO: Unable to read wheezy_udp@dns-test-service.dns-5071 from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:15:01.806: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5071 from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:15:01.811: INFO: Unable to read wheezy_udp@dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:15:01.816: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:15:01.821: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:15:01.826: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:15:01.853: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:15:01.860: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:15:01.868: INFO: Unable to read jessie_udp@dns-test-service.dns-5071 from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:15:01.875: INFO: Unable to read jessie_tcp@dns-test-service.dns-5071 from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:15:01.881: INFO: Unable to read jessie_udp@dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:15:01.888: INFO: Unable to read jessie_tcp@dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:15:01.895: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:15:01.901: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:15:01.929: INFO: Lookups using dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5071 wheezy_tcp@dns-test-service.dns-5071 wheezy_udp@dns-test-service.dns-5071.svc wheezy_tcp@dns-test-service.dns-5071.svc wheezy_udp@_http._tcp.dns-test-service.dns-5071.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5071.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5071 jessie_tcp@dns-test-service.dns-5071 jessie_udp@dns-test-service.dns-5071.svc jessie_tcp@dns-test-service.dns-5071.svc jessie_udp@_http._tcp.dns-test-service.dns-5071.svc jessie_tcp@_http._tcp.dns-test-service.dns-5071.svc]

    May  4 13:15:06.780: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:15:06.785: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:15:06.790: INFO: Unable to read wheezy_udp@dns-test-service.dns-5071 from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:15:06.798: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5071 from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:15:06.810: INFO: Unable to read wheezy_udp@dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:15:06.820: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:15:06.827: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:15:06.832: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:15:06.858: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:15:06.863: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:15:06.869: INFO: Unable to read jessie_udp@dns-test-service.dns-5071 from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:15:06.875: INFO: Unable to read jessie_tcp@dns-test-service.dns-5071 from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:15:06.880: INFO: Unable to read jessie_udp@dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:15:06.885: INFO: Unable to read jessie_tcp@dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:15:06.891: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:15:06.897: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5071.svc from pod dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02: the server could not find the requested resource (get pods dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02)
    May  4 13:15:06.923: INFO: Lookups using dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5071 wheezy_tcp@dns-test-service.dns-5071 wheezy_udp@dns-test-service.dns-5071.svc wheezy_tcp@dns-test-service.dns-5071.svc wheezy_udp@_http._tcp.dns-test-service.dns-5071.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5071.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5071 jessie_tcp@dns-test-service.dns-5071 jessie_udp@dns-test-service.dns-5071.svc jessie_tcp@dns-test-service.dns-5071.svc jessie_udp@_http._tcp.dns-test-service.dns-5071.svc jessie_tcp@_http._tcp.dns-test-service.dns-5071.svc]

    May  4 13:15:11.931: INFO: DNS probes using dns-5071/dns-test-c1b7dc39-baa7-4ce2-9216-92e307a41a02 succeeded

    STEP: deleting the pod 05/04/23 13:15:11.931
    STEP: deleting the test service 05/04/23 13:15:11.961
    STEP: deleting the test headless service 05/04/23 13:15:12.007
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    May  4 13:15:12.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-5071" for this suite. 05/04/23 13:15:12.045
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:15:12.06
May  4 13:15:12.060: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename statefulset 05/04/23 13:15:12.062
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:15:12.128
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:15:12.14
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-8449 05/04/23 13:15:12.143
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
May  4 13:15:12.196: INFO: Found 0 stateful pods, waiting for 1
May  4 13:15:22.206: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 05/04/23 13:15:22.218
W0504 13:15:22.230021      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
May  4 13:15:22.240: INFO: Found 1 stateful pods, waiting for 2
May  4 13:15:32.250: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
May  4 13:15:32.250: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 05/04/23 13:15:32.261
STEP: Delete all of the StatefulSets 05/04/23 13:15:32.266
STEP: Verify that StatefulSets have been deleted 05/04/23 13:15:32.279
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
May  4 13:15:32.289: INFO: Deleting all statefulset in ns statefulset-8449
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
May  4 13:15:32.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8449" for this suite. 05/04/23 13:15:32.338
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","completed":333,"skipped":6064,"failed":0}
------------------------------
• [SLOW TEST] [20.289 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:906

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:15:12.06
    May  4 13:15:12.060: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename statefulset 05/04/23 13:15:12.062
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:15:12.128
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:15:12.14
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-8449 05/04/23 13:15:12.143
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:906
    May  4 13:15:12.196: INFO: Found 0 stateful pods, waiting for 1
    May  4 13:15:22.206: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 05/04/23 13:15:22.218
    W0504 13:15:22.230021      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    May  4 13:15:22.240: INFO: Found 1 stateful pods, waiting for 2
    May  4 13:15:32.250: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    May  4 13:15:32.250: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 05/04/23 13:15:32.261
    STEP: Delete all of the StatefulSets 05/04/23 13:15:32.266
    STEP: Verify that StatefulSets have been deleted 05/04/23 13:15:32.279
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    May  4 13:15:32.289: INFO: Deleting all statefulset in ns statefulset-8449
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    May  4 13:15:32.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-8449" for this suite. 05/04/23 13:15:32.338
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:15:32.35
May  4 13:15:32.350: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename resourcequota 05/04/23 13:15:32.351
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:15:32.372
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:15:32.376
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
STEP: Counting existing ResourceQuota 05/04/23 13:15:49.392
STEP: Creating a ResourceQuota 05/04/23 13:15:54.397
STEP: Ensuring resource quota status is calculated 05/04/23 13:15:54.405
STEP: Creating a ConfigMap 05/04/23 13:15:56.411
STEP: Ensuring resource quota status captures configMap creation 05/04/23 13:15:56.426
STEP: Deleting a ConfigMap 05/04/23 13:15:58.431
STEP: Ensuring resource quota status released usage 05/04/23 13:15:58.441
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
May  4 13:16:00.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5077" for this suite. 05/04/23 13:16:00.455
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","completed":334,"skipped":6069,"failed":0}
------------------------------
• [SLOW TEST] [28.114 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:15:32.35
    May  4 13:15:32.350: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename resourcequota 05/04/23 13:15:32.351
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:15:32.372
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:15:32.376
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:316
    STEP: Counting existing ResourceQuota 05/04/23 13:15:49.392
    STEP: Creating a ResourceQuota 05/04/23 13:15:54.397
    STEP: Ensuring resource quota status is calculated 05/04/23 13:15:54.405
    STEP: Creating a ConfigMap 05/04/23 13:15:56.411
    STEP: Ensuring resource quota status captures configMap creation 05/04/23 13:15:56.426
    STEP: Deleting a ConfigMap 05/04/23 13:15:58.431
    STEP: Ensuring resource quota status released usage 05/04/23 13:15:58.441
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    May  4 13:16:00.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-5077" for this suite. 05/04/23 13:16:00.455
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:16:00.465
May  4 13:16:00.466: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename watch 05/04/23 13:16:00.466
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:16:00.493
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:16:00.498
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 05/04/23 13:16:00.502
STEP: creating a new configmap 05/04/23 13:16:00.504
STEP: modifying the configmap once 05/04/23 13:16:00.523
STEP: changing the label value of the configmap 05/04/23 13:16:00.537
STEP: Expecting to observe a delete notification for the watched object 05/04/23 13:16:00.549
May  4 13:16:00.549: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2200  ac5ad76e-ec86-4840-8b80-4f026d8fc2d8 45725 0 2023-05-04 13:16:00 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-04 13:16:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May  4 13:16:00.549: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2200  ac5ad76e-ec86-4840-8b80-4f026d8fc2d8 45726 0 2023-05-04 13:16:00 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-04 13:16:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
May  4 13:16:00.549: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2200  ac5ad76e-ec86-4840-8b80-4f026d8fc2d8 45727 0 2023-05-04 13:16:00 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-04 13:16:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 05/04/23 13:16:00.549
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 05/04/23 13:16:00.561
STEP: changing the label value of the configmap back 05/04/23 13:16:10.565
STEP: modifying the configmap a third time 05/04/23 13:16:10.589
STEP: deleting the configmap 05/04/23 13:16:10.631
STEP: Expecting to observe an add notification for the watched object when the label value was restored 05/04/23 13:16:10.645
May  4 13:16:10.645: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2200  ac5ad76e-ec86-4840-8b80-4f026d8fc2d8 45757 0 2023-05-04 13:16:00 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-04 13:16:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May  4 13:16:10.646: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2200  ac5ad76e-ec86-4840-8b80-4f026d8fc2d8 45758 0 2023-05-04 13:16:00 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-04 13:16:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
May  4 13:16:10.646: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2200  ac5ad76e-ec86-4840-8b80-4f026d8fc2d8 45759 0 2023-05-04 13:16:00 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-04 13:16:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
May  4 13:16:10.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2200" for this suite. 05/04/23 13:16:10.66
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","completed":335,"skipped":6082,"failed":0}
------------------------------
• [SLOW TEST] [10.205 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:16:00.465
    May  4 13:16:00.466: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename watch 05/04/23 13:16:00.466
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:16:00.493
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:16:00.498
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 05/04/23 13:16:00.502
    STEP: creating a new configmap 05/04/23 13:16:00.504
    STEP: modifying the configmap once 05/04/23 13:16:00.523
    STEP: changing the label value of the configmap 05/04/23 13:16:00.537
    STEP: Expecting to observe a delete notification for the watched object 05/04/23 13:16:00.549
    May  4 13:16:00.549: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2200  ac5ad76e-ec86-4840-8b80-4f026d8fc2d8 45725 0 2023-05-04 13:16:00 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-04 13:16:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    May  4 13:16:00.549: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2200  ac5ad76e-ec86-4840-8b80-4f026d8fc2d8 45726 0 2023-05-04 13:16:00 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-04 13:16:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    May  4 13:16:00.549: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2200  ac5ad76e-ec86-4840-8b80-4f026d8fc2d8 45727 0 2023-05-04 13:16:00 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-04 13:16:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 05/04/23 13:16:00.549
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 05/04/23 13:16:00.561
    STEP: changing the label value of the configmap back 05/04/23 13:16:10.565
    STEP: modifying the configmap a third time 05/04/23 13:16:10.589
    STEP: deleting the configmap 05/04/23 13:16:10.631
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 05/04/23 13:16:10.645
    May  4 13:16:10.645: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2200  ac5ad76e-ec86-4840-8b80-4f026d8fc2d8 45757 0 2023-05-04 13:16:00 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-04 13:16:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    May  4 13:16:10.646: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2200  ac5ad76e-ec86-4840-8b80-4f026d8fc2d8 45758 0 2023-05-04 13:16:00 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-04 13:16:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    May  4 13:16:10.646: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2200  ac5ad76e-ec86-4840-8b80-4f026d8fc2d8 45759 0 2023-05-04 13:16:00 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-04 13:16:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    May  4 13:16:10.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-2200" for this suite. 05/04/23 13:16:10.66
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:16:10.671
May  4 13:16:10.672: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename job 05/04/23 13:16:10.672
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:16:10.723
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:16:10.726
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
STEP: Creating a suspended job 05/04/23 13:16:10.734
STEP: Patching the Job 05/04/23 13:16:10.743
STEP: Watching for Job to be patched 05/04/23 13:16:10.812
May  4 13:16:10.813: INFO: Event ADDED observed for Job e2e-w2gtp in namespace job-5244 with labels: map[e2e-job-label:e2e-w2gtp] and annotations: map[batch.kubernetes.io/job-tracking:]
May  4 13:16:10.813: INFO: Event MODIFIED observed for Job e2e-w2gtp in namespace job-5244 with labels: map[e2e-job-label:e2e-w2gtp] and annotations: map[batch.kubernetes.io/job-tracking:]
May  4 13:16:10.813: INFO: Event MODIFIED found for Job e2e-w2gtp in namespace job-5244 with labels: map[e2e-job-label:e2e-w2gtp e2e-w2gtp:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 05/04/23 13:16:10.813
STEP: Watching for Job to be updated 05/04/23 13:16:10.826
May  4 13:16:10.828: INFO: Event MODIFIED found for Job e2e-w2gtp in namespace job-5244 with labels: map[e2e-job-label:e2e-w2gtp e2e-w2gtp:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May  4 13:16:10.828: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 05/04/23 13:16:10.828
May  4 13:16:10.832: INFO: Job: e2e-w2gtp as labels: map[e2e-job-label:e2e-w2gtp e2e-w2gtp:patched]
STEP: Waiting for job to complete 05/04/23 13:16:10.832
STEP: Delete a job collection with a labelselector 05/04/23 13:16:20.837
STEP: Watching for Job to be deleted 05/04/23 13:16:20.848
May  4 13:16:20.851: INFO: Event MODIFIED observed for Job e2e-w2gtp in namespace job-5244 with labels: map[e2e-job-label:e2e-w2gtp e2e-w2gtp:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May  4 13:16:20.851: INFO: Event MODIFIED observed for Job e2e-w2gtp in namespace job-5244 with labels: map[e2e-job-label:e2e-w2gtp e2e-w2gtp:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May  4 13:16:20.851: INFO: Event MODIFIED observed for Job e2e-w2gtp in namespace job-5244 with labels: map[e2e-job-label:e2e-w2gtp e2e-w2gtp:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May  4 13:16:20.851: INFO: Event MODIFIED observed for Job e2e-w2gtp in namespace job-5244 with labels: map[e2e-job-label:e2e-w2gtp e2e-w2gtp:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May  4 13:16:20.851: INFO: Event MODIFIED observed for Job e2e-w2gtp in namespace job-5244 with labels: map[e2e-job-label:e2e-w2gtp e2e-w2gtp:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May  4 13:16:20.851: INFO: Event DELETED found for Job e2e-w2gtp in namespace job-5244 with labels: map[e2e-job-label:e2e-w2gtp e2e-w2gtp:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 05/04/23 13:16:20.851
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
May  4 13:16:20.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5244" for this suite. 05/04/23 13:16:20.864
{"msg":"PASSED [sig-apps] Job should manage the lifecycle of a job [Conformance]","completed":336,"skipped":6088,"failed":0}
------------------------------
• [SLOW TEST] [10.202 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:16:10.671
    May  4 13:16:10.672: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename job 05/04/23 13:16:10.672
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:16:10.723
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:16:10.726
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:531
    STEP: Creating a suspended job 05/04/23 13:16:10.734
    STEP: Patching the Job 05/04/23 13:16:10.743
    STEP: Watching for Job to be patched 05/04/23 13:16:10.812
    May  4 13:16:10.813: INFO: Event ADDED observed for Job e2e-w2gtp in namespace job-5244 with labels: map[e2e-job-label:e2e-w2gtp] and annotations: map[batch.kubernetes.io/job-tracking:]
    May  4 13:16:10.813: INFO: Event MODIFIED observed for Job e2e-w2gtp in namespace job-5244 with labels: map[e2e-job-label:e2e-w2gtp] and annotations: map[batch.kubernetes.io/job-tracking:]
    May  4 13:16:10.813: INFO: Event MODIFIED found for Job e2e-w2gtp in namespace job-5244 with labels: map[e2e-job-label:e2e-w2gtp e2e-w2gtp:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 05/04/23 13:16:10.813
    STEP: Watching for Job to be updated 05/04/23 13:16:10.826
    May  4 13:16:10.828: INFO: Event MODIFIED found for Job e2e-w2gtp in namespace job-5244 with labels: map[e2e-job-label:e2e-w2gtp e2e-w2gtp:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May  4 13:16:10.828: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 05/04/23 13:16:10.828
    May  4 13:16:10.832: INFO: Job: e2e-w2gtp as labels: map[e2e-job-label:e2e-w2gtp e2e-w2gtp:patched]
    STEP: Waiting for job to complete 05/04/23 13:16:10.832
    STEP: Delete a job collection with a labelselector 05/04/23 13:16:20.837
    STEP: Watching for Job to be deleted 05/04/23 13:16:20.848
    May  4 13:16:20.851: INFO: Event MODIFIED observed for Job e2e-w2gtp in namespace job-5244 with labels: map[e2e-job-label:e2e-w2gtp e2e-w2gtp:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May  4 13:16:20.851: INFO: Event MODIFIED observed for Job e2e-w2gtp in namespace job-5244 with labels: map[e2e-job-label:e2e-w2gtp e2e-w2gtp:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May  4 13:16:20.851: INFO: Event MODIFIED observed for Job e2e-w2gtp in namespace job-5244 with labels: map[e2e-job-label:e2e-w2gtp e2e-w2gtp:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May  4 13:16:20.851: INFO: Event MODIFIED observed for Job e2e-w2gtp in namespace job-5244 with labels: map[e2e-job-label:e2e-w2gtp e2e-w2gtp:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May  4 13:16:20.851: INFO: Event MODIFIED observed for Job e2e-w2gtp in namespace job-5244 with labels: map[e2e-job-label:e2e-w2gtp e2e-w2gtp:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May  4 13:16:20.851: INFO: Event DELETED found for Job e2e-w2gtp in namespace job-5244 with labels: map[e2e-job-label:e2e-w2gtp e2e-w2gtp:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 05/04/23 13:16:20.851
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    May  4 13:16:20.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-5244" for this suite. 05/04/23 13:16:20.864
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:16:20.874
May  4 13:16:20.874: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename endpointslice 05/04/23 13:16:20.875
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:16:20.906
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:16:20.912
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
STEP: getting /apis 05/04/23 13:16:20.916
STEP: getting /apis/discovery.k8s.io 05/04/23 13:16:20.92
STEP: getting /apis/discovery.k8s.iov1 05/04/23 13:16:20.922
STEP: creating 05/04/23 13:16:20.923
STEP: getting 05/04/23 13:16:20.943
STEP: listing 05/04/23 13:16:20.95
STEP: watching 05/04/23 13:16:20.957
May  4 13:16:20.957: INFO: starting watch
STEP: cluster-wide listing 05/04/23 13:16:20.958
STEP: cluster-wide watching 05/04/23 13:16:20.965
May  4 13:16:20.965: INFO: starting watch
STEP: patching 05/04/23 13:16:20.966
STEP: updating 05/04/23 13:16:20.975
May  4 13:16:20.988: INFO: waiting for watch events with expected annotations
May  4 13:16:20.988: INFO: saw patched and updated annotations
STEP: deleting 05/04/23 13:16:20.989
STEP: deleting a collection 05/04/23 13:16:21.019
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
May  4 13:16:21.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-8016" for this suite. 05/04/23 13:16:21.058
{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","completed":337,"skipped":6088,"failed":0}
------------------------------
• [0.194 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:16:20.874
    May  4 13:16:20.874: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename endpointslice 05/04/23 13:16:20.875
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:16:20.906
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:16:20.912
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:352
    STEP: getting /apis 05/04/23 13:16:20.916
    STEP: getting /apis/discovery.k8s.io 05/04/23 13:16:20.92
    STEP: getting /apis/discovery.k8s.iov1 05/04/23 13:16:20.922
    STEP: creating 05/04/23 13:16:20.923
    STEP: getting 05/04/23 13:16:20.943
    STEP: listing 05/04/23 13:16:20.95
    STEP: watching 05/04/23 13:16:20.957
    May  4 13:16:20.957: INFO: starting watch
    STEP: cluster-wide listing 05/04/23 13:16:20.958
    STEP: cluster-wide watching 05/04/23 13:16:20.965
    May  4 13:16:20.965: INFO: starting watch
    STEP: patching 05/04/23 13:16:20.966
    STEP: updating 05/04/23 13:16:20.975
    May  4 13:16:20.988: INFO: waiting for watch events with expected annotations
    May  4 13:16:20.988: INFO: saw patched and updated annotations
    STEP: deleting 05/04/23 13:16:20.989
    STEP: deleting a collection 05/04/23 13:16:21.019
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    May  4 13:16:21.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-8016" for this suite. 05/04/23 13:16:21.058
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:16:21.069
May  4 13:16:21.069: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename webhook 05/04/23 13:16:21.07
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:16:21.098
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:16:21.101
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/04/23 13:16:21.124
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/04/23 13:16:21.294
STEP: Deploying the webhook pod 05/04/23 13:16:21.308
STEP: Wait for the deployment to be ready 05/04/23 13:16:21.334
May  4 13:16:21.350: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/04/23 13:16:23.364
STEP: Verifying the service has paired with the endpoint 05/04/23 13:16:23.379
May  4 13:16:24.379: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
May  4 13:16:24.387: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8187-crds.webhook.example.com via the AdmissionRegistration API 05/04/23 13:16:24.902
STEP: Creating a custom resource while v1 is storage version 05/04/23 13:16:24.921
STEP: Patching Custom Resource Definition to set v2 as storage 05/04/23 13:16:26.993
STEP: Patching the custom resource while v2 is storage version 05/04/23 13:16:27.014
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  4 13:16:27.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9211" for this suite. 05/04/23 13:16:27.65
STEP: Destroying namespace "webhook-9211-markers" for this suite. 05/04/23 13:16:27.658
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","completed":338,"skipped":6101,"failed":0}
------------------------------
• [SLOW TEST] [6.658 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:16:21.069
    May  4 13:16:21.069: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename webhook 05/04/23 13:16:21.07
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:16:21.098
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:16:21.101
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/04/23 13:16:21.124
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/04/23 13:16:21.294
    STEP: Deploying the webhook pod 05/04/23 13:16:21.308
    STEP: Wait for the deployment to be ready 05/04/23 13:16:21.334
    May  4 13:16:21.350: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/04/23 13:16:23.364
    STEP: Verifying the service has paired with the endpoint 05/04/23 13:16:23.379
    May  4 13:16:24.379: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:322
    May  4 13:16:24.387: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8187-crds.webhook.example.com via the AdmissionRegistration API 05/04/23 13:16:24.902
    STEP: Creating a custom resource while v1 is storage version 05/04/23 13:16:24.921
    STEP: Patching Custom Resource Definition to set v2 as storage 05/04/23 13:16:26.993
    STEP: Patching the custom resource while v2 is storage version 05/04/23 13:16:27.014
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  4 13:16:27.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9211" for this suite. 05/04/23 13:16:27.65
    STEP: Destroying namespace "webhook-9211-markers" for this suite. 05/04/23 13:16:27.658
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:16:27.732
May  4 13:16:27.732: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename taint-multiple-pods 05/04/23 13:16:27.733
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:16:27.752
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:16:27.756
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
May  4 13:16:27.759: INFO: Waiting up to 1m0s for all nodes to be ready
May  4 13:17:27.822: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
May  4 13:17:27.828: INFO: Starting informer...
STEP: Starting pods... 05/04/23 13:17:27.829
May  4 13:17:28.055: INFO: Pod1 is running on ip-10-0-1-224.us-west-2.compute.internal. Tainting Node
May  4 13:17:28.268: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-5110" to be "running"
May  4 13:17:28.273: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.933726ms
May  4 13:17:30.279: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.010785223s
May  4 13:17:30.279: INFO: Pod "taint-eviction-b1" satisfied condition "running"
May  4 13:17:30.279: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-5110" to be "running"
May  4 13:17:30.286: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 6.894428ms
May  4 13:17:30.286: INFO: Pod "taint-eviction-b2" satisfied condition "running"
May  4 13:17:30.286: INFO: Pod2 is running on ip-10-0-1-224.us-west-2.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node 05/04/23 13:17:30.286
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/04/23 13:17:30.303
STEP: Waiting for Pod1 and Pod2 to be deleted 05/04/23 13:17:30.309
May  4 13:17:35.999: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
May  4 13:17:56.043: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/04/23 13:17:56.063
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
May  4 13:17:56.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-5110" for this suite. 05/04/23 13:17:56.086
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","completed":339,"skipped":6103,"failed":0}
------------------------------
• [SLOW TEST] [88.366 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:16:27.732
    May  4 13:16:27.732: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename taint-multiple-pods 05/04/23 13:16:27.733
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:16:27.752
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:16:27.756
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:348
    May  4 13:16:27.759: INFO: Waiting up to 1m0s for all nodes to be ready
    May  4 13:17:27.822: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:420
    May  4 13:17:27.828: INFO: Starting informer...
    STEP: Starting pods... 05/04/23 13:17:27.829
    May  4 13:17:28.055: INFO: Pod1 is running on ip-10-0-1-224.us-west-2.compute.internal. Tainting Node
    May  4 13:17:28.268: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-5110" to be "running"
    May  4 13:17:28.273: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.933726ms
    May  4 13:17:30.279: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.010785223s
    May  4 13:17:30.279: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    May  4 13:17:30.279: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-5110" to be "running"
    May  4 13:17:30.286: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 6.894428ms
    May  4 13:17:30.286: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    May  4 13:17:30.286: INFO: Pod2 is running on ip-10-0-1-224.us-west-2.compute.internal. Tainting Node
    STEP: Trying to apply a taint on the Node 05/04/23 13:17:30.286
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/04/23 13:17:30.303
    STEP: Waiting for Pod1 and Pod2 to be deleted 05/04/23 13:17:30.309
    May  4 13:17:35.999: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    May  4 13:17:56.043: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/04/23 13:17:56.063
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:187
    May  4 13:17:56.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-multiple-pods-5110" for this suite. 05/04/23 13:17:56.086
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:17:56.098
May  4 13:17:56.098: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename container-runtime 05/04/23 13:17:56.099
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:17:56.124
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:17:56.135
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 05/04/23 13:17:56.162
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 05/04/23 13:18:14.283
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 05/04/23 13:18:14.289
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 05/04/23 13:18:14.3
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 05/04/23 13:18:14.3
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 05/04/23 13:18:14.351
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 05/04/23 13:18:17.372
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 05/04/23 13:18:19.387
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 05/04/23 13:18:19.397
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 05/04/23 13:18:19.397
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 05/04/23 13:18:19.434
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 05/04/23 13:18:20.451
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 05/04/23 13:18:23.484
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 05/04/23 13:18:23.499
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 05/04/23 13:18:23.499
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
May  4 13:18:23.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9427" for this suite. 05/04/23 13:18:23.544
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","completed":340,"skipped":6105,"failed":0}
------------------------------
• [SLOW TEST] [27.456 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:17:56.098
    May  4 13:17:56.098: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename container-runtime 05/04/23 13:17:56.099
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:17:56.124
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:17:56.135
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 05/04/23 13:17:56.162
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 05/04/23 13:18:14.283
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 05/04/23 13:18:14.289
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 05/04/23 13:18:14.3
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 05/04/23 13:18:14.3
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 05/04/23 13:18:14.351
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 05/04/23 13:18:17.372
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 05/04/23 13:18:19.387
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 05/04/23 13:18:19.397
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 05/04/23 13:18:19.397
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 05/04/23 13:18:19.434
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 05/04/23 13:18:20.451
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 05/04/23 13:18:23.484
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 05/04/23 13:18:23.499
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 05/04/23 13:18:23.499
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    May  4 13:18:23.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-9427" for this suite. 05/04/23 13:18:23.544
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:18:23.554
May  4 13:18:23.555: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename projected 05/04/23 13:18:23.555
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:18:23.579
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:18:23.584
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
STEP: Creating a pod to test downward API volume plugin 05/04/23 13:18:23.591
May  4 13:18:23.605: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fdf7b1bb-3b82-47ce-bc9c-f1c222074cf8" in namespace "projected-1091" to be "Succeeded or Failed"
May  4 13:18:23.612: INFO: Pod "downwardapi-volume-fdf7b1bb-3b82-47ce-bc9c-f1c222074cf8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.680266ms
May  4 13:18:25.618: INFO: Pod "downwardapi-volume-fdf7b1bb-3b82-47ce-bc9c-f1c222074cf8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013607237s
May  4 13:18:27.624: INFO: Pod "downwardapi-volume-fdf7b1bb-3b82-47ce-bc9c-f1c222074cf8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019561229s
STEP: Saw pod success 05/04/23 13:18:27.624
May  4 13:18:27.624: INFO: Pod "downwardapi-volume-fdf7b1bb-3b82-47ce-bc9c-f1c222074cf8" satisfied condition "Succeeded or Failed"
May  4 13:18:27.633: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod downwardapi-volume-fdf7b1bb-3b82-47ce-bc9c-f1c222074cf8 container client-container: <nil>
STEP: delete the pod 05/04/23 13:18:27.664
May  4 13:18:27.691: INFO: Waiting for pod downwardapi-volume-fdf7b1bb-3b82-47ce-bc9c-f1c222074cf8 to disappear
May  4 13:18:27.698: INFO: Pod downwardapi-volume-fdf7b1bb-3b82-47ce-bc9c-f1c222074cf8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
May  4 13:18:27.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1091" for this suite. 05/04/23 13:18:27.709
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":341,"skipped":6114,"failed":0}
------------------------------
• [4.164 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:18:23.554
    May  4 13:18:23.555: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename projected 05/04/23 13:18:23.555
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:18:23.579
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:18:23.584
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:67
    STEP: Creating a pod to test downward API volume plugin 05/04/23 13:18:23.591
    May  4 13:18:23.605: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fdf7b1bb-3b82-47ce-bc9c-f1c222074cf8" in namespace "projected-1091" to be "Succeeded or Failed"
    May  4 13:18:23.612: INFO: Pod "downwardapi-volume-fdf7b1bb-3b82-47ce-bc9c-f1c222074cf8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.680266ms
    May  4 13:18:25.618: INFO: Pod "downwardapi-volume-fdf7b1bb-3b82-47ce-bc9c-f1c222074cf8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013607237s
    May  4 13:18:27.624: INFO: Pod "downwardapi-volume-fdf7b1bb-3b82-47ce-bc9c-f1c222074cf8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019561229s
    STEP: Saw pod success 05/04/23 13:18:27.624
    May  4 13:18:27.624: INFO: Pod "downwardapi-volume-fdf7b1bb-3b82-47ce-bc9c-f1c222074cf8" satisfied condition "Succeeded or Failed"
    May  4 13:18:27.633: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod downwardapi-volume-fdf7b1bb-3b82-47ce-bc9c-f1c222074cf8 container client-container: <nil>
    STEP: delete the pod 05/04/23 13:18:27.664
    May  4 13:18:27.691: INFO: Waiting for pod downwardapi-volume-fdf7b1bb-3b82-47ce-bc9c-f1c222074cf8 to disappear
    May  4 13:18:27.698: INFO: Pod downwardapi-volume-fdf7b1bb-3b82-47ce-bc9c-f1c222074cf8 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    May  4 13:18:27.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1091" for this suite. 05/04/23 13:18:27.709
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:18:27.719
May  4 13:18:27.719: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename projected 05/04/23 13:18:27.72
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:18:27.741
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:18:27.745
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
STEP: Creating projection with configMap that has name projected-configmap-test-upd-e50e2942-debd-4bf4-ad27-9f65fe713b27 05/04/23 13:18:27.757
STEP: Creating the pod 05/04/23 13:18:27.767
May  4 13:18:27.780: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a8c7066a-74f4-4617-8415-2dc9d196b4d3" in namespace "projected-1951" to be "running and ready"
May  4 13:18:27.788: INFO: Pod "pod-projected-configmaps-a8c7066a-74f4-4617-8415-2dc9d196b4d3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.371257ms
May  4 13:18:27.788: INFO: The phase of Pod pod-projected-configmaps-a8c7066a-74f4-4617-8415-2dc9d196b4d3 is Pending, waiting for it to be Running (with Ready = true)
May  4 13:18:29.794: INFO: Pod "pod-projected-configmaps-a8c7066a-74f4-4617-8415-2dc9d196b4d3": Phase="Running", Reason="", readiness=true. Elapsed: 2.013785874s
May  4 13:18:29.794: INFO: The phase of Pod pod-projected-configmaps-a8c7066a-74f4-4617-8415-2dc9d196b4d3 is Running (Ready = true)
May  4 13:18:29.794: INFO: Pod "pod-projected-configmaps-a8c7066a-74f4-4617-8415-2dc9d196b4d3" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-e50e2942-debd-4bf4-ad27-9f65fe713b27 05/04/23 13:18:29.806
STEP: waiting to observe update in volume 05/04/23 13:18:29.816
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
May  4 13:18:31.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1951" for this suite. 05/04/23 13:18:31.855
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":342,"skipped":6126,"failed":0}
------------------------------
• [4.148 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:18:27.719
    May  4 13:18:27.719: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename projected 05/04/23 13:18:27.72
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:18:27.741
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:18:27.745
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:123
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-e50e2942-debd-4bf4-ad27-9f65fe713b27 05/04/23 13:18:27.757
    STEP: Creating the pod 05/04/23 13:18:27.767
    May  4 13:18:27.780: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a8c7066a-74f4-4617-8415-2dc9d196b4d3" in namespace "projected-1951" to be "running and ready"
    May  4 13:18:27.788: INFO: Pod "pod-projected-configmaps-a8c7066a-74f4-4617-8415-2dc9d196b4d3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.371257ms
    May  4 13:18:27.788: INFO: The phase of Pod pod-projected-configmaps-a8c7066a-74f4-4617-8415-2dc9d196b4d3 is Pending, waiting for it to be Running (with Ready = true)
    May  4 13:18:29.794: INFO: Pod "pod-projected-configmaps-a8c7066a-74f4-4617-8415-2dc9d196b4d3": Phase="Running", Reason="", readiness=true. Elapsed: 2.013785874s
    May  4 13:18:29.794: INFO: The phase of Pod pod-projected-configmaps-a8c7066a-74f4-4617-8415-2dc9d196b4d3 is Running (Ready = true)
    May  4 13:18:29.794: INFO: Pod "pod-projected-configmaps-a8c7066a-74f4-4617-8415-2dc9d196b4d3" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-e50e2942-debd-4bf4-ad27-9f65fe713b27 05/04/23 13:18:29.806
    STEP: waiting to observe update in volume 05/04/23 13:18:29.816
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    May  4 13:18:31.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1951" for this suite. 05/04/23 13:18:31.855
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:18:31.868
May  4 13:18:31.868: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename pods 05/04/23 13:18:31.869
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:18:31.891
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:18:31.909
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
May  4 13:18:31.938: INFO: Waiting up to 5m0s for pod "server-envvars-651d8a81-8e99-4048-a3a9-4aa422e4a447" in namespace "pods-4986" to be "running and ready"
May  4 13:18:31.944: INFO: Pod "server-envvars-651d8a81-8e99-4048-a3a9-4aa422e4a447": Phase="Pending", Reason="", readiness=false. Elapsed: 6.142189ms
May  4 13:18:31.944: INFO: The phase of Pod server-envvars-651d8a81-8e99-4048-a3a9-4aa422e4a447 is Pending, waiting for it to be Running (with Ready = true)
May  4 13:18:33.951: INFO: Pod "server-envvars-651d8a81-8e99-4048-a3a9-4aa422e4a447": Phase="Running", Reason="", readiness=true. Elapsed: 2.012813704s
May  4 13:18:33.951: INFO: The phase of Pod server-envvars-651d8a81-8e99-4048-a3a9-4aa422e4a447 is Running (Ready = true)
May  4 13:18:33.951: INFO: Pod "server-envvars-651d8a81-8e99-4048-a3a9-4aa422e4a447" satisfied condition "running and ready"
May  4 13:18:33.987: INFO: Waiting up to 5m0s for pod "client-envvars-8af6554c-f697-456f-af3a-039187a77be6" in namespace "pods-4986" to be "Succeeded or Failed"
May  4 13:18:33.999: INFO: Pod "client-envvars-8af6554c-f697-456f-af3a-039187a77be6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.673417ms
May  4 13:18:36.005: INFO: Pod "client-envvars-8af6554c-f697-456f-af3a-039187a77be6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01796398s
May  4 13:18:38.006: INFO: Pod "client-envvars-8af6554c-f697-456f-af3a-039187a77be6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018917222s
STEP: Saw pod success 05/04/23 13:18:38.006
May  4 13:18:38.006: INFO: Pod "client-envvars-8af6554c-f697-456f-af3a-039187a77be6" satisfied condition "Succeeded or Failed"
May  4 13:18:38.012: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod client-envvars-8af6554c-f697-456f-af3a-039187a77be6 container env3cont: <nil>
STEP: delete the pod 05/04/23 13:18:38.02
May  4 13:18:38.044: INFO: Waiting for pod client-envvars-8af6554c-f697-456f-af3a-039187a77be6 to disappear
May  4 13:18:38.049: INFO: Pod client-envvars-8af6554c-f697-456f-af3a-039187a77be6 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
May  4 13:18:38.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4986" for this suite. 05/04/23 13:18:38.062
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","completed":343,"skipped":6158,"failed":0}
------------------------------
• [SLOW TEST] [6.205 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:18:31.868
    May  4 13:18:31.868: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename pods 05/04/23 13:18:31.869
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:18:31.891
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:18:31.909
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:443
    May  4 13:18:31.938: INFO: Waiting up to 5m0s for pod "server-envvars-651d8a81-8e99-4048-a3a9-4aa422e4a447" in namespace "pods-4986" to be "running and ready"
    May  4 13:18:31.944: INFO: Pod "server-envvars-651d8a81-8e99-4048-a3a9-4aa422e4a447": Phase="Pending", Reason="", readiness=false. Elapsed: 6.142189ms
    May  4 13:18:31.944: INFO: The phase of Pod server-envvars-651d8a81-8e99-4048-a3a9-4aa422e4a447 is Pending, waiting for it to be Running (with Ready = true)
    May  4 13:18:33.951: INFO: Pod "server-envvars-651d8a81-8e99-4048-a3a9-4aa422e4a447": Phase="Running", Reason="", readiness=true. Elapsed: 2.012813704s
    May  4 13:18:33.951: INFO: The phase of Pod server-envvars-651d8a81-8e99-4048-a3a9-4aa422e4a447 is Running (Ready = true)
    May  4 13:18:33.951: INFO: Pod "server-envvars-651d8a81-8e99-4048-a3a9-4aa422e4a447" satisfied condition "running and ready"
    May  4 13:18:33.987: INFO: Waiting up to 5m0s for pod "client-envvars-8af6554c-f697-456f-af3a-039187a77be6" in namespace "pods-4986" to be "Succeeded or Failed"
    May  4 13:18:33.999: INFO: Pod "client-envvars-8af6554c-f697-456f-af3a-039187a77be6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.673417ms
    May  4 13:18:36.005: INFO: Pod "client-envvars-8af6554c-f697-456f-af3a-039187a77be6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01796398s
    May  4 13:18:38.006: INFO: Pod "client-envvars-8af6554c-f697-456f-af3a-039187a77be6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018917222s
    STEP: Saw pod success 05/04/23 13:18:38.006
    May  4 13:18:38.006: INFO: Pod "client-envvars-8af6554c-f697-456f-af3a-039187a77be6" satisfied condition "Succeeded or Failed"
    May  4 13:18:38.012: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod client-envvars-8af6554c-f697-456f-af3a-039187a77be6 container env3cont: <nil>
    STEP: delete the pod 05/04/23 13:18:38.02
    May  4 13:18:38.044: INFO: Waiting for pod client-envvars-8af6554c-f697-456f-af3a-039187a77be6 to disappear
    May  4 13:18:38.049: INFO: Pod client-envvars-8af6554c-f697-456f-af3a-039187a77be6 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    May  4 13:18:38.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4986" for this suite. 05/04/23 13:18:38.062
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:18:38.074
May  4 13:18:38.075: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename configmap 05/04/23 13:18:38.076
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:18:38.1
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:18:38.103
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
STEP: Creating configMap with name configmap-test-volume-map-0e896650-cbd7-4bff-a0c7-f7dae8b9bf89 05/04/23 13:18:38.105
STEP: Creating a pod to test consume configMaps 05/04/23 13:18:38.116
May  4 13:18:38.126: INFO: Waiting up to 5m0s for pod "pod-configmaps-efc870c6-a51d-4141-9a84-a1f7ac20da2a" in namespace "configmap-6820" to be "Succeeded or Failed"
May  4 13:18:38.131: INFO: Pod "pod-configmaps-efc870c6-a51d-4141-9a84-a1f7ac20da2a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.147249ms
May  4 13:18:40.137: INFO: Pod "pod-configmaps-efc870c6-a51d-4141-9a84-a1f7ac20da2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010857695s
May  4 13:18:42.140: INFO: Pod "pod-configmaps-efc870c6-a51d-4141-9a84-a1f7ac20da2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013510372s
STEP: Saw pod success 05/04/23 13:18:42.14
May  4 13:18:42.140: INFO: Pod "pod-configmaps-efc870c6-a51d-4141-9a84-a1f7ac20da2a" satisfied condition "Succeeded or Failed"
May  4 13:18:42.145: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-configmaps-efc870c6-a51d-4141-9a84-a1f7ac20da2a container agnhost-container: <nil>
STEP: delete the pod 05/04/23 13:18:42.153
May  4 13:18:42.200: INFO: Waiting for pod pod-configmaps-efc870c6-a51d-4141-9a84-a1f7ac20da2a to disappear
May  4 13:18:42.203: INFO: Pod pod-configmaps-efc870c6-a51d-4141-9a84-a1f7ac20da2a no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
May  4 13:18:42.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6820" for this suite. 05/04/23 13:18:42.21
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":344,"skipped":6177,"failed":0}
------------------------------
• [4.146 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:18:38.074
    May  4 13:18:38.075: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename configmap 05/04/23 13:18:38.076
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:18:38.1
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:18:38.103
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:88
    STEP: Creating configMap with name configmap-test-volume-map-0e896650-cbd7-4bff-a0c7-f7dae8b9bf89 05/04/23 13:18:38.105
    STEP: Creating a pod to test consume configMaps 05/04/23 13:18:38.116
    May  4 13:18:38.126: INFO: Waiting up to 5m0s for pod "pod-configmaps-efc870c6-a51d-4141-9a84-a1f7ac20da2a" in namespace "configmap-6820" to be "Succeeded or Failed"
    May  4 13:18:38.131: INFO: Pod "pod-configmaps-efc870c6-a51d-4141-9a84-a1f7ac20da2a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.147249ms
    May  4 13:18:40.137: INFO: Pod "pod-configmaps-efc870c6-a51d-4141-9a84-a1f7ac20da2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010857695s
    May  4 13:18:42.140: INFO: Pod "pod-configmaps-efc870c6-a51d-4141-9a84-a1f7ac20da2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013510372s
    STEP: Saw pod success 05/04/23 13:18:42.14
    May  4 13:18:42.140: INFO: Pod "pod-configmaps-efc870c6-a51d-4141-9a84-a1f7ac20da2a" satisfied condition "Succeeded or Failed"
    May  4 13:18:42.145: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-configmaps-efc870c6-a51d-4141-9a84-a1f7ac20da2a container agnhost-container: <nil>
    STEP: delete the pod 05/04/23 13:18:42.153
    May  4 13:18:42.200: INFO: Waiting for pod pod-configmaps-efc870c6-a51d-4141-9a84-a1f7ac20da2a to disappear
    May  4 13:18:42.203: INFO: Pod pod-configmaps-efc870c6-a51d-4141-9a84-a1f7ac20da2a no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    May  4 13:18:42.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6820" for this suite. 05/04/23 13:18:42.21
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:18:42.221
May  4 13:18:42.221: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename configmap 05/04/23 13:18:42.222
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:18:42.241
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:18:42.245
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
STEP: Creating configMap with name configmap-test-volume-e40c1cac-c69e-4e0a-b933-cb523208cc23 05/04/23 13:18:42.248
STEP: Creating a pod to test consume configMaps 05/04/23 13:18:42.255
May  4 13:18:42.272: INFO: Waiting up to 5m0s for pod "pod-configmaps-b51c6404-f892-4065-a4ed-703b7c6b854f" in namespace "configmap-19" to be "Succeeded or Failed"
May  4 13:18:42.276: INFO: Pod "pod-configmaps-b51c6404-f892-4065-a4ed-703b7c6b854f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.511696ms
May  4 13:18:44.282: INFO: Pod "pod-configmaps-b51c6404-f892-4065-a4ed-703b7c6b854f": Phase="Running", Reason="", readiness=false. Elapsed: 2.010257086s
May  4 13:18:46.286: INFO: Pod "pod-configmaps-b51c6404-f892-4065-a4ed-703b7c6b854f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01394775s
STEP: Saw pod success 05/04/23 13:18:46.286
May  4 13:18:46.286: INFO: Pod "pod-configmaps-b51c6404-f892-4065-a4ed-703b7c6b854f" satisfied condition "Succeeded or Failed"
May  4 13:18:46.292: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-configmaps-b51c6404-f892-4065-a4ed-703b7c6b854f container agnhost-container: <nil>
STEP: delete the pod 05/04/23 13:18:46.3
May  4 13:18:46.317: INFO: Waiting for pod pod-configmaps-b51c6404-f892-4065-a4ed-703b7c6b854f to disappear
May  4 13:18:46.321: INFO: Pod pod-configmaps-b51c6404-f892-4065-a4ed-703b7c6b854f no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
May  4 13:18:46.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-19" for this suite. 05/04/23 13:18:46.33
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":345,"skipped":6177,"failed":0}
------------------------------
• [4.118 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:18:42.221
    May  4 13:18:42.221: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename configmap 05/04/23 13:18:42.222
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:18:42.241
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:18:42.245
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:46
    STEP: Creating configMap with name configmap-test-volume-e40c1cac-c69e-4e0a-b933-cb523208cc23 05/04/23 13:18:42.248
    STEP: Creating a pod to test consume configMaps 05/04/23 13:18:42.255
    May  4 13:18:42.272: INFO: Waiting up to 5m0s for pod "pod-configmaps-b51c6404-f892-4065-a4ed-703b7c6b854f" in namespace "configmap-19" to be "Succeeded or Failed"
    May  4 13:18:42.276: INFO: Pod "pod-configmaps-b51c6404-f892-4065-a4ed-703b7c6b854f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.511696ms
    May  4 13:18:44.282: INFO: Pod "pod-configmaps-b51c6404-f892-4065-a4ed-703b7c6b854f": Phase="Running", Reason="", readiness=false. Elapsed: 2.010257086s
    May  4 13:18:46.286: INFO: Pod "pod-configmaps-b51c6404-f892-4065-a4ed-703b7c6b854f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01394775s
    STEP: Saw pod success 05/04/23 13:18:46.286
    May  4 13:18:46.286: INFO: Pod "pod-configmaps-b51c6404-f892-4065-a4ed-703b7c6b854f" satisfied condition "Succeeded or Failed"
    May  4 13:18:46.292: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-configmaps-b51c6404-f892-4065-a4ed-703b7c6b854f container agnhost-container: <nil>
    STEP: delete the pod 05/04/23 13:18:46.3
    May  4 13:18:46.317: INFO: Waiting for pod pod-configmaps-b51c6404-f892-4065-a4ed-703b7c6b854f to disappear
    May  4 13:18:46.321: INFO: Pod pod-configmaps-b51c6404-f892-4065-a4ed-703b7c6b854f no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    May  4 13:18:46.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-19" for this suite. 05/04/23 13:18:46.33
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:18:46.342
May  4 13:18:46.342: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename crd-webhook 05/04/23 13:18:46.343
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:18:46.362
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:18:46.366
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 05/04/23 13:18:46.371
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 05/04/23 13:18:46.868
STEP: Deploying the custom resource conversion webhook pod 05/04/23 13:18:46.883
STEP: Wait for the deployment to be ready 05/04/23 13:18:46.9
May  4 13:18:46.912: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/04/23 13:18:48.926
STEP: Verifying the service has paired with the endpoint 05/04/23 13:18:48.952
May  4 13:18:49.952: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
May  4 13:18:49.957: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Creating a v1 custom resource 05/04/23 13:18:52.544
STEP: Create a v2 custom resource 05/04/23 13:18:52.564
STEP: List CRs in v1 05/04/23 13:18:52.618
STEP: List CRs in v2 05/04/23 13:18:52.625
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  4 13:18:53.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-1624" for this suite. 05/04/23 13:18:53.179
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","completed":346,"skipped":6217,"failed":0}
------------------------------
• [SLOW TEST] [6.993 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:18:46.342
    May  4 13:18:46.342: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename crd-webhook 05/04/23 13:18:46.343
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:18:46.362
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:18:46.366
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 05/04/23 13:18:46.371
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 05/04/23 13:18:46.868
    STEP: Deploying the custom resource conversion webhook pod 05/04/23 13:18:46.883
    STEP: Wait for the deployment to be ready 05/04/23 13:18:46.9
    May  4 13:18:46.912: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/04/23 13:18:48.926
    STEP: Verifying the service has paired with the endpoint 05/04/23 13:18:48.952
    May  4 13:18:49.952: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    May  4 13:18:49.957: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Creating a v1 custom resource 05/04/23 13:18:52.544
    STEP: Create a v2 custom resource 05/04/23 13:18:52.564
    STEP: List CRs in v1 05/04/23 13:18:52.618
    STEP: List CRs in v2 05/04/23 13:18:52.625
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  4 13:18:53.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-1624" for this suite. 05/04/23 13:18:53.179
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:18:53.338
May  4 13:18:53.338: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename kubectl 05/04/23 13:18:53.34
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:18:53.447
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:18:53.461
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
STEP: validating cluster-info 05/04/23 13:18:53.466
May  4 13:18:53.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-4054 cluster-info'
May  4 13:18:53.593: INFO: stderr: ""
May  4 13:18:53.594: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.21.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  4 13:18:53.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4054" for this suite. 05/04/23 13:18:53.613
{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","completed":347,"skipped":6245,"failed":0}
------------------------------
• [0.295 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1242
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:18:53.338
    May  4 13:18:53.338: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename kubectl 05/04/23 13:18:53.34
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:18:53.447
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:18:53.461
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1248
    STEP: validating cluster-info 05/04/23 13:18:53.466
    May  4 13:18:53.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-4054 cluster-info'
    May  4 13:18:53.593: INFO: stderr: ""
    May  4 13:18:53.594: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.21.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  4 13:18:53.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4054" for this suite. 05/04/23 13:18:53.613
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:18:53.634
May  4 13:18:53.635: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename kubectl 05/04/23 13:18:53.635
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:18:53.669
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:18:53.678
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
May  4 13:18:53.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-7608 version'
May  4 13:18:53.779: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
May  4 13:18:53.779: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.5\", GitCommit:\"804d6167111f6858541cef440ccc53887fbbc96a\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T10:15:02Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.5\", GitCommit:\"804d6167111f6858541cef440ccc53887fbbc96a\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T10:08:09Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  4 13:18:53.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7608" for this suite. 05/04/23 13:18:53.807
{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","completed":348,"skipped":6261,"failed":0}
------------------------------
• [0.187 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1677
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1683

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:18:53.634
    May  4 13:18:53.635: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename kubectl 05/04/23 13:18:53.635
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:18:53.669
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:18:53.678
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1683
    May  4 13:18:53.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-7608 version'
    May  4 13:18:53.779: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    May  4 13:18:53.779: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.5\", GitCommit:\"804d6167111f6858541cef440ccc53887fbbc96a\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T10:15:02Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.5\", GitCommit:\"804d6167111f6858541cef440ccc53887fbbc96a\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T10:08:09Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  4 13:18:53.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7608" for this suite. 05/04/23 13:18:53.807
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:18:53.824
May  4 13:18:53.824: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename projected 05/04/23 13:18:53.825
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:18:53.861
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:18:53.864
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
STEP: Creating configMap with name projected-configmap-test-volume-5e31ae3f-db2c-4c72-a405-27a134bf9913 05/04/23 13:18:53.868
STEP: Creating a pod to test consume configMaps 05/04/23 13:18:53.873
May  4 13:18:53.904: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1b9bd372-3376-43b8-8997-c2e651dd416e" in namespace "projected-5782" to be "Succeeded or Failed"
May  4 13:18:53.915: INFO: Pod "pod-projected-configmaps-1b9bd372-3376-43b8-8997-c2e651dd416e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.032999ms
May  4 13:18:55.920: INFO: Pod "pod-projected-configmaps-1b9bd372-3376-43b8-8997-c2e651dd416e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015412339s
May  4 13:18:57.920: INFO: Pod "pod-projected-configmaps-1b9bd372-3376-43b8-8997-c2e651dd416e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015544186s
STEP: Saw pod success 05/04/23 13:18:57.92
May  4 13:18:57.920: INFO: Pod "pod-projected-configmaps-1b9bd372-3376-43b8-8997-c2e651dd416e" satisfied condition "Succeeded or Failed"
May  4 13:18:57.929: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-projected-configmaps-1b9bd372-3376-43b8-8997-c2e651dd416e container projected-configmap-volume-test: <nil>
STEP: delete the pod 05/04/23 13:18:57.942
May  4 13:18:57.970: INFO: Waiting for pod pod-projected-configmaps-1b9bd372-3376-43b8-8997-c2e651dd416e to disappear
May  4 13:18:57.974: INFO: Pod pod-projected-configmaps-1b9bd372-3376-43b8-8997-c2e651dd416e no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
May  4 13:18:57.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5782" for this suite. 05/04/23 13:18:57.984
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":349,"skipped":6295,"failed":0}
------------------------------
• [4.170 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:18:53.824
    May  4 13:18:53.824: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename projected 05/04/23 13:18:53.825
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:18:53.861
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:18:53.864
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:374
    STEP: Creating configMap with name projected-configmap-test-volume-5e31ae3f-db2c-4c72-a405-27a134bf9913 05/04/23 13:18:53.868
    STEP: Creating a pod to test consume configMaps 05/04/23 13:18:53.873
    May  4 13:18:53.904: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1b9bd372-3376-43b8-8997-c2e651dd416e" in namespace "projected-5782" to be "Succeeded or Failed"
    May  4 13:18:53.915: INFO: Pod "pod-projected-configmaps-1b9bd372-3376-43b8-8997-c2e651dd416e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.032999ms
    May  4 13:18:55.920: INFO: Pod "pod-projected-configmaps-1b9bd372-3376-43b8-8997-c2e651dd416e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015412339s
    May  4 13:18:57.920: INFO: Pod "pod-projected-configmaps-1b9bd372-3376-43b8-8997-c2e651dd416e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015544186s
    STEP: Saw pod success 05/04/23 13:18:57.92
    May  4 13:18:57.920: INFO: Pod "pod-projected-configmaps-1b9bd372-3376-43b8-8997-c2e651dd416e" satisfied condition "Succeeded or Failed"
    May  4 13:18:57.929: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod pod-projected-configmaps-1b9bd372-3376-43b8-8997-c2e651dd416e container projected-configmap-volume-test: <nil>
    STEP: delete the pod 05/04/23 13:18:57.942
    May  4 13:18:57.970: INFO: Waiting for pod pod-projected-configmaps-1b9bd372-3376-43b8-8997-c2e651dd416e to disappear
    May  4 13:18:57.974: INFO: Pod pod-projected-configmaps-1b9bd372-3376-43b8-8997-c2e651dd416e no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    May  4 13:18:57.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5782" for this suite. 05/04/23 13:18:57.984
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:18:57.995
May  4 13:18:57.995: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename cronjob 05/04/23 13:18:57.996
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:18:58.022
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:18:58.028
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 05/04/23 13:18:58.031
STEP: Ensuring more than one job is running at a time 05/04/23 13:18:58.094
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 05/04/23 13:20:02.103
STEP: Removing cronjob 05/04/23 13:20:02.109
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
May  4 13:20:02.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-1365" for this suite. 05/04/23 13:20:02.133
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","completed":350,"skipped":6314,"failed":0}
------------------------------
• [SLOW TEST] [64.157 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:18:57.995
    May  4 13:18:57.995: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename cronjob 05/04/23 13:18:57.996
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:18:58.022
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:18:58.028
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 05/04/23 13:18:58.031
    STEP: Ensuring more than one job is running at a time 05/04/23 13:18:58.094
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 05/04/23 13:20:02.103
    STEP: Removing cronjob 05/04/23 13:20:02.109
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    May  4 13:20:02.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-1365" for this suite. 05/04/23 13:20:02.133
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:20:02.153
May  4 13:20:02.153: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename deployment 05/04/23 13:20:02.154
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:20:02.188
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:20:02.194
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 05/04/23 13:20:02.22
STEP: waiting for Deployment to be created 05/04/23 13:20:02.233
STEP: waiting for all Replicas to be Ready 05/04/23 13:20:02.235
May  4 13:20:02.236: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May  4 13:20:02.236: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May  4 13:20:02.291: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May  4 13:20:02.291: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May  4 13:20:02.369: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May  4 13:20:02.370: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May  4 13:20:02.481: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May  4 13:20:02.481: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May  4 13:20:03.407: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 1 and labels map[test-deployment-static:true]
May  4 13:20:03.407: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 1 and labels map[test-deployment-static:true]
May  4 13:20:03.420: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 05/04/23 13:20:03.42
W0504 13:20:03.438673      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
May  4 13:20:03.440: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 05/04/23 13:20:03.44
May  4 13:20:03.443: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 0
May  4 13:20:03.443: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 0
May  4 13:20:03.443: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 0
May  4 13:20:03.443: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 0
May  4 13:20:03.445: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 0
May  4 13:20:03.445: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 0
May  4 13:20:03.445: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 0
May  4 13:20:03.445: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 0
May  4 13:20:03.445: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 1
May  4 13:20:03.445: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 1
May  4 13:20:03.445: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 2
May  4 13:20:03.445: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 2
May  4 13:20:03.445: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 2
May  4 13:20:03.445: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 2
May  4 13:20:03.456: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 2
May  4 13:20:03.456: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 2
May  4 13:20:03.489: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 2
May  4 13:20:03.489: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 2
May  4 13:20:03.505: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 1
May  4 13:20:03.505: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 1
May  4 13:20:03.518: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 1
May  4 13:20:03.518: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 1
May  4 13:20:05.418: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 2
May  4 13:20:05.418: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 2
May  4 13:20:05.460: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 1
STEP: listing Deployments 05/04/23 13:20:05.46
May  4 13:20:05.471: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 05/04/23 13:20:05.471
May  4 13:20:05.487: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 05/04/23 13:20:05.488
May  4 13:20:05.495: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May  4 13:20:05.502: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May  4 13:20:05.537: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May  4 13:20:05.558: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May  4 13:20:05.570: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May  4 13:20:06.442: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
May  4 13:20:06.504: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
May  4 13:20:06.533: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
May  4 13:20:07.486: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 05/04/23 13:20:07.611
STEP: fetching the DeploymentStatus 05/04/23 13:20:07.621
May  4 13:20:07.628: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 1
May  4 13:20:07.628: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 1
May  4 13:20:07.628: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 1
May  4 13:20:07.628: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 1
May  4 13:20:07.628: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 1
May  4 13:20:07.628: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 2
May  4 13:20:07.628: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 2
May  4 13:20:07.629: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 2
May  4 13:20:07.629: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 3
STEP: deleting the Deployment 05/04/23 13:20:07.629
May  4 13:20:07.651: INFO: observed event type MODIFIED
May  4 13:20:07.652: INFO: observed event type MODIFIED
May  4 13:20:07.652: INFO: observed event type MODIFIED
May  4 13:20:07.652: INFO: observed event type MODIFIED
May  4 13:20:07.652: INFO: observed event type MODIFIED
May  4 13:20:07.653: INFO: observed event type MODIFIED
May  4 13:20:07.653: INFO: observed event type MODIFIED
May  4 13:20:07.653: INFO: observed event type MODIFIED
May  4 13:20:07.653: INFO: observed event type MODIFIED
May  4 13:20:07.653: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May  4 13:20:07.663: INFO: Log out all the ReplicaSets if there is no deployment created
May  4 13:20:07.674: INFO: ReplicaSet "test-deployment-54cc775c4b":
&ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-7805  b05211cc-1e14-47f8-9d1f-2f78518ac9a0 47191 4 2023-05-04 13:20:03 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 8290f4f8-9e98-4a5c-acdc-b20a555ee4e8 0xc004f8e0e7 0xc004f8e0e8}] [] [{kube-controller-manager Update apps/v1 2023-05-04 13:20:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8290f4f8-9e98-4a5c-acdc-b20a555ee4e8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 13:20:07 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004f8e170 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

May  4 13:20:07.688: INFO: pod: "test-deployment-54cc775c4b-hxzzk":
&Pod{ObjectMeta:{test-deployment-54cc775c4b-hxzzk test-deployment-54cc775c4b- deployment-7805  82bf4dc2-7cac-4d4b-a791-fbd24fd44d26 47185 0 2023-05-04 13:20:03 +0000 UTC 2023-05-04 13:20:08 +0000 UTC 0xc0062dea98 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:72419fa8907be9ae0ab1e514fbb7052ffefeeba2dae5751097f49d71c01e119c cni.projectcalico.org/podIP:10.20.83.56/32 cni.projectcalico.org/podIPs:10.20.83.56/32] [{apps/v1 ReplicaSet test-deployment-54cc775c4b b05211cc-1e14-47f8-9d1f-2f78518ac9a0 0xc0062debf7 0xc0062debf8}] [] [{kube-controller-manager Update v1 2023-05-04 13:20:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b05211cc-1e14-47f8-9d1f-2f78518ac9a0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-04 13:20:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-04 13:20:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.83.56\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7ccmh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7ccmh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-224.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:20:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:20:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:20:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:20:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.224,PodIP:10.20.83.56,StartTime:2023-05-04 13:20:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-04 13:20:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://797d014a62326a7234b1e20e36ec3ac21779e6cbb4aaa5a21eba9e155d90ed67,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.83.56,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

May  4 13:20:07.688: INFO: pod: "test-deployment-54cc775c4b-rgfmb":
&Pod{ObjectMeta:{test-deployment-54cc775c4b-rgfmb test-deployment-54cc775c4b- deployment-7805  e5476a43-460d-49c3-a6c6-80a756b53bc1 47190 0 2023-05-04 13:20:05 +0000 UTC 2023-05-04 13:20:07 +0000 UTC 0xc0062df0b0 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:e0b32b4f035260e7b6afcc62759610479d6edba2751d00032a11a4757ee27303 cni.projectcalico.org/podIP: cni.projectcalico.org/podIPs:] [{apps/v1 ReplicaSet test-deployment-54cc775c4b b05211cc-1e14-47f8-9d1f-2f78518ac9a0 0xc0062df0e7 0xc0062df0e8}] [] [{kube-controller-manager Update v1 2023-05-04 13:20:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b05211cc-1e14-47f8-9d1f-2f78518ac9a0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-04 13:20:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-04 13:20:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.92.135\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-72cnr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-72cnr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-232.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:20:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:20:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:20:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:20:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.232,PodIP:10.20.92.135,StartTime:2023-05-04 13:20:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-04 13:20:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://0a1380f4e78bcbf248c5209ba5d42069b24873dae70bb0a78ecc679d1b296205,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.92.135,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

May  4 13:20:07.689: INFO: ReplicaSet "test-deployment-8594bb6fdd":
&ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-7805  609544cf-4f65-4d86-8016-aef794d7de4f 47082 3 2023-05-04 13:20:02 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 8290f4f8-9e98-4a5c-acdc-b20a555ee4e8 0xc004f8e1f7 0xc004f8e1f8}] [] [{kube-controller-manager Update apps/v1 2023-05-04 13:20:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8290f4f8-9e98-4a5c-acdc-b20a555ee4e8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 13:20:05 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004f8e280 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
May  4 13:20:07.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7805" for this suite. 05/04/23 13:20:07.737
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","completed":351,"skipped":6321,"failed":0}
------------------------------
• [SLOW TEST] [5.615 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:20:02.153
    May  4 13:20:02.153: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename deployment 05/04/23 13:20:02.154
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:20:02.188
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:20:02.194
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 05/04/23 13:20:02.22
    STEP: waiting for Deployment to be created 05/04/23 13:20:02.233
    STEP: waiting for all Replicas to be Ready 05/04/23 13:20:02.235
    May  4 13:20:02.236: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May  4 13:20:02.236: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May  4 13:20:02.291: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May  4 13:20:02.291: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May  4 13:20:02.369: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May  4 13:20:02.370: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May  4 13:20:02.481: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May  4 13:20:02.481: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May  4 13:20:03.407: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    May  4 13:20:03.407: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    May  4 13:20:03.420: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 05/04/23 13:20:03.42
    W0504 13:20:03.438673      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    May  4 13:20:03.440: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 05/04/23 13:20:03.44
    May  4 13:20:03.443: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 0
    May  4 13:20:03.443: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 0
    May  4 13:20:03.443: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 0
    May  4 13:20:03.443: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 0
    May  4 13:20:03.445: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 0
    May  4 13:20:03.445: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 0
    May  4 13:20:03.445: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 0
    May  4 13:20:03.445: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 0
    May  4 13:20:03.445: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 1
    May  4 13:20:03.445: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 1
    May  4 13:20:03.445: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 2
    May  4 13:20:03.445: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 2
    May  4 13:20:03.445: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 2
    May  4 13:20:03.445: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 2
    May  4 13:20:03.456: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 2
    May  4 13:20:03.456: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 2
    May  4 13:20:03.489: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 2
    May  4 13:20:03.489: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 2
    May  4 13:20:03.505: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 1
    May  4 13:20:03.505: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 1
    May  4 13:20:03.518: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 1
    May  4 13:20:03.518: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 1
    May  4 13:20:05.418: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 2
    May  4 13:20:05.418: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 2
    May  4 13:20:05.460: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 1
    STEP: listing Deployments 05/04/23 13:20:05.46
    May  4 13:20:05.471: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 05/04/23 13:20:05.471
    May  4 13:20:05.487: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 05/04/23 13:20:05.488
    May  4 13:20:05.495: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    May  4 13:20:05.502: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    May  4 13:20:05.537: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    May  4 13:20:05.558: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    May  4 13:20:05.570: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    May  4 13:20:06.442: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    May  4 13:20:06.504: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    May  4 13:20:06.533: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    May  4 13:20:07.486: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 05/04/23 13:20:07.611
    STEP: fetching the DeploymentStatus 05/04/23 13:20:07.621
    May  4 13:20:07.628: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 1
    May  4 13:20:07.628: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 1
    May  4 13:20:07.628: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 1
    May  4 13:20:07.628: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 1
    May  4 13:20:07.628: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 1
    May  4 13:20:07.628: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 2
    May  4 13:20:07.628: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 2
    May  4 13:20:07.629: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 2
    May  4 13:20:07.629: INFO: observed Deployment test-deployment in namespace deployment-7805 with ReadyReplicas 3
    STEP: deleting the Deployment 05/04/23 13:20:07.629
    May  4 13:20:07.651: INFO: observed event type MODIFIED
    May  4 13:20:07.652: INFO: observed event type MODIFIED
    May  4 13:20:07.652: INFO: observed event type MODIFIED
    May  4 13:20:07.652: INFO: observed event type MODIFIED
    May  4 13:20:07.652: INFO: observed event type MODIFIED
    May  4 13:20:07.653: INFO: observed event type MODIFIED
    May  4 13:20:07.653: INFO: observed event type MODIFIED
    May  4 13:20:07.653: INFO: observed event type MODIFIED
    May  4 13:20:07.653: INFO: observed event type MODIFIED
    May  4 13:20:07.653: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May  4 13:20:07.663: INFO: Log out all the ReplicaSets if there is no deployment created
    May  4 13:20:07.674: INFO: ReplicaSet "test-deployment-54cc775c4b":
    &ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-7805  b05211cc-1e14-47f8-9d1f-2f78518ac9a0 47191 4 2023-05-04 13:20:03 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 8290f4f8-9e98-4a5c-acdc-b20a555ee4e8 0xc004f8e0e7 0xc004f8e0e8}] [] [{kube-controller-manager Update apps/v1 2023-05-04 13:20:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8290f4f8-9e98-4a5c-acdc-b20a555ee4e8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 13:20:07 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004f8e170 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    May  4 13:20:07.688: INFO: pod: "test-deployment-54cc775c4b-hxzzk":
    &Pod{ObjectMeta:{test-deployment-54cc775c4b-hxzzk test-deployment-54cc775c4b- deployment-7805  82bf4dc2-7cac-4d4b-a791-fbd24fd44d26 47185 0 2023-05-04 13:20:03 +0000 UTC 2023-05-04 13:20:08 +0000 UTC 0xc0062dea98 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:72419fa8907be9ae0ab1e514fbb7052ffefeeba2dae5751097f49d71c01e119c cni.projectcalico.org/podIP:10.20.83.56/32 cni.projectcalico.org/podIPs:10.20.83.56/32] [{apps/v1 ReplicaSet test-deployment-54cc775c4b b05211cc-1e14-47f8-9d1f-2f78518ac9a0 0xc0062debf7 0xc0062debf8}] [] [{kube-controller-manager Update v1 2023-05-04 13:20:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b05211cc-1e14-47f8-9d1f-2f78518ac9a0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-04 13:20:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-04 13:20:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.83.56\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7ccmh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7ccmh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-224.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:20:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:20:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:20:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:20:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.224,PodIP:10.20.83.56,StartTime:2023-05-04 13:20:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-04 13:20:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://797d014a62326a7234b1e20e36ec3ac21779e6cbb4aaa5a21eba9e155d90ed67,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.83.56,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    May  4 13:20:07.688: INFO: pod: "test-deployment-54cc775c4b-rgfmb":
    &Pod{ObjectMeta:{test-deployment-54cc775c4b-rgfmb test-deployment-54cc775c4b- deployment-7805  e5476a43-460d-49c3-a6c6-80a756b53bc1 47190 0 2023-05-04 13:20:05 +0000 UTC 2023-05-04 13:20:07 +0000 UTC 0xc0062df0b0 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:e0b32b4f035260e7b6afcc62759610479d6edba2751d00032a11a4757ee27303 cni.projectcalico.org/podIP: cni.projectcalico.org/podIPs:] [{apps/v1 ReplicaSet test-deployment-54cc775c4b b05211cc-1e14-47f8-9d1f-2f78518ac9a0 0xc0062df0e7 0xc0062df0e8}] [] [{kube-controller-manager Update v1 2023-05-04 13:20:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b05211cc-1e14-47f8-9d1f-2f78518ac9a0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-04 13:20:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-04 13:20:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.92.135\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-72cnr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-72cnr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-232.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:20:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:20:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:20:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 13:20:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.232,PodIP:10.20.92.135,StartTime:2023-05-04 13:20:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-04 13:20:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://0a1380f4e78bcbf248c5209ba5d42069b24873dae70bb0a78ecc679d1b296205,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.92.135,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    May  4 13:20:07.689: INFO: ReplicaSet "test-deployment-8594bb6fdd":
    &ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-7805  609544cf-4f65-4d86-8016-aef794d7de4f 47082 3 2023-05-04 13:20:02 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 8290f4f8-9e98-4a5c-acdc-b20a555ee4e8 0xc004f8e1f7 0xc004f8e1f8}] [] [{kube-controller-manager Update apps/v1 2023-05-04 13:20:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8290f4f8-9e98-4a5c-acdc-b20a555ee4e8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 13:20:05 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004f8e280 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    May  4 13:20:07.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-7805" for this suite. 05/04/23 13:20:07.737
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:20:07.77
May  4 13:20:07.770: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename kubectl 05/04/23 13:20:07.771
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:20:07.815
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:20:07.82
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
STEP: Starting the proxy 05/04/23 13:20:07.828
May  4 13:20:07.828: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-1958 proxy --unix-socket=/tmp/kubectl-proxy-unix324874969/test'
STEP: retrieving proxy /api/ output 05/04/23 13:20:07.892
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  4 13:20:07.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1958" for this suite. 05/04/23 13:20:07.907
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","completed":352,"skipped":6328,"failed":0}
------------------------------
• [0.154 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:20:07.77
    May  4 13:20:07.770: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename kubectl 05/04/23 13:20:07.771
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:20:07.815
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:20:07.82
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1810
    STEP: Starting the proxy 05/04/23 13:20:07.828
    May  4 13:20:07.828: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-1958 proxy --unix-socket=/tmp/kubectl-proxy-unix324874969/test'
    STEP: retrieving proxy /api/ output 05/04/23 13:20:07.892
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  4 13:20:07.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1958" for this suite. 05/04/23 13:20:07.907
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:20:07.925
May  4 13:20:07.925: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename projected 05/04/23 13:20:07.926
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:20:07.954
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:20:07.959
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
STEP: Creating a pod to test downward API volume plugin 05/04/23 13:20:07.964
May  4 13:20:07.992: INFO: Waiting up to 5m0s for pod "downwardapi-volume-362bdeea-9529-432f-acc8-e8eea71810e5" in namespace "projected-4486" to be "Succeeded or Failed"
May  4 13:20:08.006: INFO: Pod "downwardapi-volume-362bdeea-9529-432f-acc8-e8eea71810e5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.505237ms
May  4 13:20:10.013: INFO: Pod "downwardapi-volume-362bdeea-9529-432f-acc8-e8eea71810e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021083003s
May  4 13:20:12.012: INFO: Pod "downwardapi-volume-362bdeea-9529-432f-acc8-e8eea71810e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020301458s
STEP: Saw pod success 05/04/23 13:20:12.012
May  4 13:20:12.012: INFO: Pod "downwardapi-volume-362bdeea-9529-432f-acc8-e8eea71810e5" satisfied condition "Succeeded or Failed"
May  4 13:20:12.019: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod downwardapi-volume-362bdeea-9529-432f-acc8-e8eea71810e5 container client-container: <nil>
STEP: delete the pod 05/04/23 13:20:12.031
May  4 13:20:12.055: INFO: Waiting for pod downwardapi-volume-362bdeea-9529-432f-acc8-e8eea71810e5 to disappear
May  4 13:20:12.060: INFO: Pod downwardapi-volume-362bdeea-9529-432f-acc8-e8eea71810e5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
May  4 13:20:12.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4486" for this suite. 05/04/23 13:20:12.069
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","completed":353,"skipped":6332,"failed":0}
------------------------------
• [4.156 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:20:07.925
    May  4 13:20:07.925: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename projected 05/04/23 13:20:07.926
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:20:07.954
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:20:07.959
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:52
    STEP: Creating a pod to test downward API volume plugin 05/04/23 13:20:07.964
    May  4 13:20:07.992: INFO: Waiting up to 5m0s for pod "downwardapi-volume-362bdeea-9529-432f-acc8-e8eea71810e5" in namespace "projected-4486" to be "Succeeded or Failed"
    May  4 13:20:08.006: INFO: Pod "downwardapi-volume-362bdeea-9529-432f-acc8-e8eea71810e5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.505237ms
    May  4 13:20:10.013: INFO: Pod "downwardapi-volume-362bdeea-9529-432f-acc8-e8eea71810e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021083003s
    May  4 13:20:12.012: INFO: Pod "downwardapi-volume-362bdeea-9529-432f-acc8-e8eea71810e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020301458s
    STEP: Saw pod success 05/04/23 13:20:12.012
    May  4 13:20:12.012: INFO: Pod "downwardapi-volume-362bdeea-9529-432f-acc8-e8eea71810e5" satisfied condition "Succeeded or Failed"
    May  4 13:20:12.019: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod downwardapi-volume-362bdeea-9529-432f-acc8-e8eea71810e5 container client-container: <nil>
    STEP: delete the pod 05/04/23 13:20:12.031
    May  4 13:20:12.055: INFO: Waiting for pod downwardapi-volume-362bdeea-9529-432f-acc8-e8eea71810e5 to disappear
    May  4 13:20:12.060: INFO: Pod downwardapi-volume-362bdeea-9529-432f-acc8-e8eea71810e5 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    May  4 13:20:12.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4486" for this suite. 05/04/23 13:20:12.069
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:20:12.081
May  4 13:20:12.081: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename statefulset 05/04/23 13:20:12.082
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:20:12.109
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:20:12.113
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1023 05/04/23 13:20:12.117
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
STEP: Creating stateful set ss in namespace statefulset-1023 05/04/23 13:20:12.128
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1023 05/04/23 13:20:12.142
May  4 13:20:12.150: INFO: Found 0 stateful pods, waiting for 1
May  4 13:20:22.158: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 05/04/23 13:20:22.158
May  4 13:20:22.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=statefulset-1023 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  4 13:20:22.406: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  4 13:20:22.406: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  4 13:20:22.406: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  4 13:20:22.415: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May  4 13:20:32.423: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May  4 13:20:32.423: INFO: Waiting for statefulset status.replicas updated to 0
May  4 13:20:32.452: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
May  4 13:20:32.452: INFO: ss-0  ip-10-0-1-224.us-west-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:20:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:20:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:20:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:20:12 +0000 UTC  }]
May  4 13:20:32.453: INFO: ss-1                                            Pending         []
May  4 13:20:32.453: INFO: 
May  4 13:20:32.453: INFO: StatefulSet ss has not reached scale 3, at 2
May  4 13:20:33.460: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.987601394s
May  4 13:20:34.469: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.979837451s
May  4 13:20:35.475: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.970978547s
May  4 13:20:36.480: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.965420414s
May  4 13:20:37.486: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.960103131s
May  4 13:20:38.494: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.954188321s
May  4 13:20:39.503: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.94523565s
May  4 13:20:40.509: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.936457824s
May  4 13:20:41.521: INFO: Verifying statefulset ss doesn't scale past 3 for another 930.139709ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1023 05/04/23 13:20:42.522
May  4 13:20:42.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=statefulset-1023 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  4 13:20:42.713: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  4 13:20:42.713: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  4 13:20:42.713: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  4 13:20:42.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=statefulset-1023 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  4 13:20:42.894: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May  4 13:20:42.894: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  4 13:20:42.894: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  4 13:20:42.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=statefulset-1023 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  4 13:20:43.087: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May  4 13:20:43.087: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  4 13:20:43.087: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  4 13:20:43.095: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
May  4 13:20:53.101: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May  4 13:20:53.101: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May  4 13:20:53.101: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 05/04/23 13:20:53.101
May  4 13:20:53.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=statefulset-1023 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  4 13:20:53.251: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  4 13:20:53.251: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  4 13:20:53.251: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  4 13:20:53.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=statefulset-1023 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  4 13:20:53.449: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  4 13:20:53.449: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  4 13:20:53.449: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  4 13:20:53.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=statefulset-1023 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  4 13:20:53.664: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  4 13:20:53.664: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  4 13:20:53.664: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  4 13:20:53.664: INFO: Waiting for statefulset status.replicas updated to 0
May  4 13:20:53.669: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
May  4 13:21:03.680: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May  4 13:21:03.681: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May  4 13:21:03.681: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May  4 13:21:03.709: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
May  4 13:21:03.709: INFO: ss-0  ip-10-0-1-224.us-west-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:20:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:20:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:20:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:20:12 +0000 UTC  }]
May  4 13:21:03.709: INFO: ss-1  ip-10-0-1-216.us-west-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:20:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:20:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:20:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:20:32 +0000 UTC  }]
May  4 13:21:03.709: INFO: ss-2  ip-10-0-1-232.us-west-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:20:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:20:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:20:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:20:32 +0000 UTC  }]
May  4 13:21:03.709: INFO: 
May  4 13:21:03.709: INFO: StatefulSet ss has not reached scale 0, at 3
May  4 13:21:04.715: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
May  4 13:21:04.715: INFO: ss-1  ip-10-0-1-216.us-west-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:20:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:20:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:20:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:20:32 +0000 UTC  }]
May  4 13:21:04.715: INFO: ss-2  ip-10-0-1-232.us-west-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:20:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:20:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:20:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:20:32 +0000 UTC  }]
May  4 13:21:04.715: INFO: 
May  4 13:21:04.715: INFO: StatefulSet ss has not reached scale 0, at 2
May  4 13:21:05.720: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.97940872s
May  4 13:21:06.724: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.974760677s
May  4 13:21:07.730: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.970074997s
May  4 13:21:08.734: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.964729211s
May  4 13:21:09.741: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.959805916s
May  4 13:21:10.746: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.953239164s
May  4 13:21:11.753: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.947624342s
May  4 13:21:12.758: INFO: Verifying statefulset ss doesn't scale past 0 for another 940.808631ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1023 05/04/23 13:21:13.759
May  4 13:21:13.768: INFO: Scaling statefulset ss to 0
May  4 13:21:13.783: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
May  4 13:21:13.789: INFO: Deleting all statefulset in ns statefulset-1023
May  4 13:21:13.793: INFO: Scaling statefulset ss to 0
May  4 13:21:13.815: INFO: Waiting for statefulset status.replicas updated to 0
May  4 13:21:13.819: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
May  4 13:21:13.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1023" for this suite. 05/04/23 13:21:13.856
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","completed":354,"skipped":6333,"failed":0}
------------------------------
• [SLOW TEST] [61.789 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:695

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:20:12.081
    May  4 13:20:12.081: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename statefulset 05/04/23 13:20:12.082
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:20:12.109
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:20:12.113
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-1023 05/04/23 13:20:12.117
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:695
    STEP: Creating stateful set ss in namespace statefulset-1023 05/04/23 13:20:12.128
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1023 05/04/23 13:20:12.142
    May  4 13:20:12.150: INFO: Found 0 stateful pods, waiting for 1
    May  4 13:20:22.158: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 05/04/23 13:20:22.158
    May  4 13:20:22.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=statefulset-1023 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  4 13:20:22.406: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  4 13:20:22.406: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  4 13:20:22.406: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May  4 13:20:22.415: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    May  4 13:20:32.423: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    May  4 13:20:32.423: INFO: Waiting for statefulset status.replicas updated to 0
    May  4 13:20:32.452: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
    May  4 13:20:32.452: INFO: ss-0  ip-10-0-1-224.us-west-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:20:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:20:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:20:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:20:12 +0000 UTC  }]
    May  4 13:20:32.453: INFO: ss-1                                            Pending         []
    May  4 13:20:32.453: INFO: 
    May  4 13:20:32.453: INFO: StatefulSet ss has not reached scale 3, at 2
    May  4 13:20:33.460: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.987601394s
    May  4 13:20:34.469: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.979837451s
    May  4 13:20:35.475: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.970978547s
    May  4 13:20:36.480: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.965420414s
    May  4 13:20:37.486: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.960103131s
    May  4 13:20:38.494: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.954188321s
    May  4 13:20:39.503: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.94523565s
    May  4 13:20:40.509: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.936457824s
    May  4 13:20:41.521: INFO: Verifying statefulset ss doesn't scale past 3 for another 930.139709ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1023 05/04/23 13:20:42.522
    May  4 13:20:42.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=statefulset-1023 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  4 13:20:42.713: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May  4 13:20:42.713: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May  4 13:20:42.713: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May  4 13:20:42.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=statefulset-1023 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  4 13:20:42.894: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    May  4 13:20:42.894: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May  4 13:20:42.894: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May  4 13:20:42.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=statefulset-1023 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  4 13:20:43.087: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    May  4 13:20:43.087: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May  4 13:20:43.087: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May  4 13:20:43.095: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    May  4 13:20:53.101: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    May  4 13:20:53.101: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    May  4 13:20:53.101: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 05/04/23 13:20:53.101
    May  4 13:20:53.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=statefulset-1023 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  4 13:20:53.251: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  4 13:20:53.251: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  4 13:20:53.251: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May  4 13:20:53.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=statefulset-1023 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  4 13:20:53.449: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  4 13:20:53.449: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  4 13:20:53.449: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May  4 13:20:53.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=statefulset-1023 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  4 13:20:53.664: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  4 13:20:53.664: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  4 13:20:53.664: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May  4 13:20:53.664: INFO: Waiting for statefulset status.replicas updated to 0
    May  4 13:20:53.669: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    May  4 13:21:03.680: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    May  4 13:21:03.681: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    May  4 13:21:03.681: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    May  4 13:21:03.709: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
    May  4 13:21:03.709: INFO: ss-0  ip-10-0-1-224.us-west-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:20:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:20:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:20:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:20:12 +0000 UTC  }]
    May  4 13:21:03.709: INFO: ss-1  ip-10-0-1-216.us-west-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:20:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:20:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:20:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:20:32 +0000 UTC  }]
    May  4 13:21:03.709: INFO: ss-2  ip-10-0-1-232.us-west-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:20:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:20:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:20:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:20:32 +0000 UTC  }]
    May  4 13:21:03.709: INFO: 
    May  4 13:21:03.709: INFO: StatefulSet ss has not reached scale 0, at 3
    May  4 13:21:04.715: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
    May  4 13:21:04.715: INFO: ss-1  ip-10-0-1-216.us-west-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:20:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:20:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:20:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:20:32 +0000 UTC  }]
    May  4 13:21:04.715: INFO: ss-2  ip-10-0-1-232.us-west-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:20:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:20:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:20:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 13:20:32 +0000 UTC  }]
    May  4 13:21:04.715: INFO: 
    May  4 13:21:04.715: INFO: StatefulSet ss has not reached scale 0, at 2
    May  4 13:21:05.720: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.97940872s
    May  4 13:21:06.724: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.974760677s
    May  4 13:21:07.730: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.970074997s
    May  4 13:21:08.734: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.964729211s
    May  4 13:21:09.741: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.959805916s
    May  4 13:21:10.746: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.953239164s
    May  4 13:21:11.753: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.947624342s
    May  4 13:21:12.758: INFO: Verifying statefulset ss doesn't scale past 0 for another 940.808631ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1023 05/04/23 13:21:13.759
    May  4 13:21:13.768: INFO: Scaling statefulset ss to 0
    May  4 13:21:13.783: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    May  4 13:21:13.789: INFO: Deleting all statefulset in ns statefulset-1023
    May  4 13:21:13.793: INFO: Scaling statefulset ss to 0
    May  4 13:21:13.815: INFO: Waiting for statefulset status.replicas updated to 0
    May  4 13:21:13.819: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    May  4 13:21:13.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-1023" for this suite. 05/04/23 13:21:13.856
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:21:13.885
May  4 13:21:13.885: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename downward-api 05/04/23 13:21:13.886
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:21:13.913
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:21:13.917
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
STEP: Creating the pod 05/04/23 13:21:13.922
May  4 13:21:13.942: INFO: Waiting up to 5m0s for pod "labelsupdate18f44c5e-a73b-4db8-9d40-e115201387cc" in namespace "downward-api-5742" to be "running and ready"
May  4 13:21:13.951: INFO: Pod "labelsupdate18f44c5e-a73b-4db8-9d40-e115201387cc": Phase="Pending", Reason="", readiness=false. Elapsed: 8.895311ms
May  4 13:21:13.951: INFO: The phase of Pod labelsupdate18f44c5e-a73b-4db8-9d40-e115201387cc is Pending, waiting for it to be Running (with Ready = true)
May  4 13:21:15.956: INFO: Pod "labelsupdate18f44c5e-a73b-4db8-9d40-e115201387cc": Phase="Running", Reason="", readiness=true. Elapsed: 2.013605338s
May  4 13:21:15.956: INFO: The phase of Pod labelsupdate18f44c5e-a73b-4db8-9d40-e115201387cc is Running (Ready = true)
May  4 13:21:15.956: INFO: Pod "labelsupdate18f44c5e-a73b-4db8-9d40-e115201387cc" satisfied condition "running and ready"
May  4 13:21:16.488: INFO: Successfully updated pod "labelsupdate18f44c5e-a73b-4db8-9d40-e115201387cc"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
May  4 13:21:18.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5742" for this suite. 05/04/23 13:21:18.523
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","completed":355,"skipped":6453,"failed":0}
------------------------------
• [4.648 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:21:13.885
    May  4 13:21:13.885: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename downward-api 05/04/23 13:21:13.886
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:21:13.913
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:21:13.917
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:129
    STEP: Creating the pod 05/04/23 13:21:13.922
    May  4 13:21:13.942: INFO: Waiting up to 5m0s for pod "labelsupdate18f44c5e-a73b-4db8-9d40-e115201387cc" in namespace "downward-api-5742" to be "running and ready"
    May  4 13:21:13.951: INFO: Pod "labelsupdate18f44c5e-a73b-4db8-9d40-e115201387cc": Phase="Pending", Reason="", readiness=false. Elapsed: 8.895311ms
    May  4 13:21:13.951: INFO: The phase of Pod labelsupdate18f44c5e-a73b-4db8-9d40-e115201387cc is Pending, waiting for it to be Running (with Ready = true)
    May  4 13:21:15.956: INFO: Pod "labelsupdate18f44c5e-a73b-4db8-9d40-e115201387cc": Phase="Running", Reason="", readiness=true. Elapsed: 2.013605338s
    May  4 13:21:15.956: INFO: The phase of Pod labelsupdate18f44c5e-a73b-4db8-9d40-e115201387cc is Running (Ready = true)
    May  4 13:21:15.956: INFO: Pod "labelsupdate18f44c5e-a73b-4db8-9d40-e115201387cc" satisfied condition "running and ready"
    May  4 13:21:16.488: INFO: Successfully updated pod "labelsupdate18f44c5e-a73b-4db8-9d40-e115201387cc"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    May  4 13:21:18.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5742" for this suite. 05/04/23 13:21:18.523
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:21:18.535
May  4 13:21:18.535: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename custom-resource-definition 05/04/23 13:21:18.536
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:21:18.555
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:21:18.563
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
May  4 13:21:18.567: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  4 13:21:25.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1995" for this suite. 05/04/23 13:21:25.008
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","completed":356,"skipped":6487,"failed":0}
------------------------------
• [SLOW TEST] [6.483 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:21:18.535
    May  4 13:21:18.535: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename custom-resource-definition 05/04/23 13:21:18.536
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:21:18.555
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:21:18.563
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    May  4 13:21:18.567: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  4 13:21:25.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-1995" for this suite. 05/04/23 13:21:25.008
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:21:25.02
May  4 13:21:25.020: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename kubectl 05/04/23 13:21:25.021
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:21:25.044
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:21:25.049
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
STEP: validating api versions 05/04/23 13:21:25.052
May  4 13:21:25.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-8581 api-versions'
May  4 13:21:25.108: INFO: stderr: ""
May  4 13:21:25.108: INFO: stdout: "admissionregistration.k8s.io/v1\nagent.pf9.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  4 13:21:25.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8581" for this suite. 05/04/23 13:21:25.117
{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","completed":357,"skipped":6529,"failed":0}
------------------------------
• [0.105 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:816
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:21:25.02
    May  4 13:21:25.020: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename kubectl 05/04/23 13:21:25.021
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:21:25.044
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:21:25.049
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:822
    STEP: validating api versions 05/04/23 13:21:25.052
    May  4 13:21:25.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=kubectl-8581 api-versions'
    May  4 13:21:25.108: INFO: stderr: ""
    May  4 13:21:25.108: INFO: stdout: "admissionregistration.k8s.io/v1\nagent.pf9.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  4 13:21:25.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8581" for this suite. 05/04/23 13:21:25.117
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:21:25.127
May  4 13:21:25.127: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename hostport 05/04/23 13:21:25.128
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:21:25.143
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:21:25.147
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 05/04/23 13:21:25.157
May  4 13:21:25.167: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-3717" to be "running and ready"
May  4 13:21:25.171: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.689297ms
May  4 13:21:25.171: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May  4 13:21:27.180: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.013159372s
May  4 13:21:27.180: INFO: The phase of Pod pod1 is Running (Ready = true)
May  4 13:21:27.180: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.0.1.189 on the node which pod1 resides and expect scheduled 05/04/23 13:21:27.18
May  4 13:21:27.191: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-3717" to be "running and ready"
May  4 13:21:27.195: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.141345ms
May  4 13:21:27.195: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May  4 13:21:29.201: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.00944392s
May  4 13:21:29.201: INFO: The phase of Pod pod2 is Running (Ready = false)
May  4 13:21:31.200: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.009233885s
May  4 13:21:31.200: INFO: The phase of Pod pod2 is Running (Ready = true)
May  4 13:21:31.200: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.0.1.189 but use UDP protocol on the node which pod2 resides 05/04/23 13:21:31.2
May  4 13:21:31.208: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-3717" to be "running and ready"
May  4 13:21:31.213: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.740035ms
May  4 13:21:31.213: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
May  4 13:21:33.219: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.010955432s
May  4 13:21:33.219: INFO: The phase of Pod pod3 is Running (Ready = true)
May  4 13:21:33.219: INFO: Pod "pod3" satisfied condition "running and ready"
May  4 13:21:33.229: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-3717" to be "running and ready"
May  4 13:21:33.237: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 7.427536ms
May  4 13:21:33.237: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
May  4 13:21:35.244: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.014299936s
May  4 13:21:35.244: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
May  4 13:21:35.244: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 05/04/23 13:21:35.248
May  4 13:21:35.248: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.1.189 http://127.0.0.1:54323/hostname] Namespace:hostport-3717 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  4 13:21:35.248: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
May  4 13:21:35.249: INFO: ExecWithOptions: Clientset creation
May  4 13:21:35.249: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/hostport-3717/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.0.1.189+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.1.189, port: 54323 05/04/23 13:21:35.345
May  4 13:21:35.345: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.1.189:54323/hostname] Namespace:hostport-3717 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  4 13:21:35.345: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
May  4 13:21:35.345: INFO: ExecWithOptions: Clientset creation
May  4 13:21:35.345: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/hostport-3717/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.0.1.189%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.1.189, port: 54323 UDP 05/04/23 13:21:35.45
May  4 13:21:35.450: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.0.1.189 54323] Namespace:hostport-3717 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  4 13:21:35.450: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
May  4 13:21:35.451: INFO: ExecWithOptions: Clientset creation
May  4 13:21:35.451: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/hostport-3717/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.0.1.189+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
May  4 13:21:40.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-3717" for this suite. 05/04/23 13:21:40.564
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","completed":358,"skipped":6561,"failed":0}
------------------------------
• [SLOW TEST] [15.447 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:21:25.127
    May  4 13:21:25.127: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename hostport 05/04/23 13:21:25.128
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:21:25.143
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:21:25.147
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 05/04/23 13:21:25.157
    May  4 13:21:25.167: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-3717" to be "running and ready"
    May  4 13:21:25.171: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.689297ms
    May  4 13:21:25.171: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    May  4 13:21:27.180: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.013159372s
    May  4 13:21:27.180: INFO: The phase of Pod pod1 is Running (Ready = true)
    May  4 13:21:27.180: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.0.1.189 on the node which pod1 resides and expect scheduled 05/04/23 13:21:27.18
    May  4 13:21:27.191: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-3717" to be "running and ready"
    May  4 13:21:27.195: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.141345ms
    May  4 13:21:27.195: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    May  4 13:21:29.201: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.00944392s
    May  4 13:21:29.201: INFO: The phase of Pod pod2 is Running (Ready = false)
    May  4 13:21:31.200: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.009233885s
    May  4 13:21:31.200: INFO: The phase of Pod pod2 is Running (Ready = true)
    May  4 13:21:31.200: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.0.1.189 but use UDP protocol on the node which pod2 resides 05/04/23 13:21:31.2
    May  4 13:21:31.208: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-3717" to be "running and ready"
    May  4 13:21:31.213: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.740035ms
    May  4 13:21:31.213: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    May  4 13:21:33.219: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.010955432s
    May  4 13:21:33.219: INFO: The phase of Pod pod3 is Running (Ready = true)
    May  4 13:21:33.219: INFO: Pod "pod3" satisfied condition "running and ready"
    May  4 13:21:33.229: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-3717" to be "running and ready"
    May  4 13:21:33.237: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 7.427536ms
    May  4 13:21:33.237: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    May  4 13:21:35.244: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.014299936s
    May  4 13:21:35.244: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    May  4 13:21:35.244: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 05/04/23 13:21:35.248
    May  4 13:21:35.248: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.1.189 http://127.0.0.1:54323/hostname] Namespace:hostport-3717 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  4 13:21:35.248: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    May  4 13:21:35.249: INFO: ExecWithOptions: Clientset creation
    May  4 13:21:35.249: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/hostport-3717/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.0.1.189+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.1.189, port: 54323 05/04/23 13:21:35.345
    May  4 13:21:35.345: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.1.189:54323/hostname] Namespace:hostport-3717 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  4 13:21:35.345: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    May  4 13:21:35.345: INFO: ExecWithOptions: Clientset creation
    May  4 13:21:35.345: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/hostport-3717/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.0.1.189%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.1.189, port: 54323 UDP 05/04/23 13:21:35.45
    May  4 13:21:35.450: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.0.1.189 54323] Namespace:hostport-3717 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  4 13:21:35.450: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    May  4 13:21:35.451: INFO: ExecWithOptions: Clientset creation
    May  4 13:21:35.451: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/hostport-3717/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.0.1.189+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/framework.go:187
    May  4 13:21:40.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "hostport-3717" for this suite. 05/04/23 13:21:40.564
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:21:40.576
May  4 13:21:40.576: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename discovery 05/04/23 13:21:40.577
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:21:40.609
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:21:40.612
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 05/04/23 13:21:40.616
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
May  4 13:21:41.040: INFO: Checking APIGroup: apiregistration.k8s.io
May  4 13:21:41.041: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
May  4 13:21:41.041: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
May  4 13:21:41.041: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
May  4 13:21:41.041: INFO: Checking APIGroup: apps
May  4 13:21:41.042: INFO: PreferredVersion.GroupVersion: apps/v1
May  4 13:21:41.042: INFO: Versions found [{apps/v1 v1}]
May  4 13:21:41.042: INFO: apps/v1 matches apps/v1
May  4 13:21:41.042: INFO: Checking APIGroup: events.k8s.io
May  4 13:21:41.043: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
May  4 13:21:41.043: INFO: Versions found [{events.k8s.io/v1 v1}]
May  4 13:21:41.043: INFO: events.k8s.io/v1 matches events.k8s.io/v1
May  4 13:21:41.043: INFO: Checking APIGroup: authentication.k8s.io
May  4 13:21:41.044: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
May  4 13:21:41.044: INFO: Versions found [{authentication.k8s.io/v1 v1}]
May  4 13:21:41.044: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
May  4 13:21:41.044: INFO: Checking APIGroup: authorization.k8s.io
May  4 13:21:41.044: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
May  4 13:21:41.044: INFO: Versions found [{authorization.k8s.io/v1 v1}]
May  4 13:21:41.044: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
May  4 13:21:41.044: INFO: Checking APIGroup: autoscaling
May  4 13:21:41.046: INFO: PreferredVersion.GroupVersion: autoscaling/v2
May  4 13:21:41.046: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
May  4 13:21:41.046: INFO: autoscaling/v2 matches autoscaling/v2
May  4 13:21:41.046: INFO: Checking APIGroup: batch
May  4 13:21:41.046: INFO: PreferredVersion.GroupVersion: batch/v1
May  4 13:21:41.046: INFO: Versions found [{batch/v1 v1}]
May  4 13:21:41.046: INFO: batch/v1 matches batch/v1
May  4 13:21:41.046: INFO: Checking APIGroup: certificates.k8s.io
May  4 13:21:41.047: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
May  4 13:21:41.047: INFO: Versions found [{certificates.k8s.io/v1 v1}]
May  4 13:21:41.047: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
May  4 13:21:41.047: INFO: Checking APIGroup: networking.k8s.io
May  4 13:21:41.047: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
May  4 13:21:41.047: INFO: Versions found [{networking.k8s.io/v1 v1}]
May  4 13:21:41.047: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
May  4 13:21:41.047: INFO: Checking APIGroup: policy
May  4 13:21:41.048: INFO: PreferredVersion.GroupVersion: policy/v1
May  4 13:21:41.048: INFO: Versions found [{policy/v1 v1}]
May  4 13:21:41.048: INFO: policy/v1 matches policy/v1
May  4 13:21:41.048: INFO: Checking APIGroup: rbac.authorization.k8s.io
May  4 13:21:41.049: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
May  4 13:21:41.049: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
May  4 13:21:41.049: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
May  4 13:21:41.049: INFO: Checking APIGroup: storage.k8s.io
May  4 13:21:41.050: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
May  4 13:21:41.050: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
May  4 13:21:41.050: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
May  4 13:21:41.050: INFO: Checking APIGroup: admissionregistration.k8s.io
May  4 13:21:41.050: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
May  4 13:21:41.050: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
May  4 13:21:41.050: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
May  4 13:21:41.050: INFO: Checking APIGroup: apiextensions.k8s.io
May  4 13:21:41.051: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
May  4 13:21:41.051: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
May  4 13:21:41.051: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
May  4 13:21:41.051: INFO: Checking APIGroup: scheduling.k8s.io
May  4 13:21:41.052: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
May  4 13:21:41.052: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
May  4 13:21:41.052: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
May  4 13:21:41.052: INFO: Checking APIGroup: coordination.k8s.io
May  4 13:21:41.053: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
May  4 13:21:41.053: INFO: Versions found [{coordination.k8s.io/v1 v1}]
May  4 13:21:41.053: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
May  4 13:21:41.053: INFO: Checking APIGroup: node.k8s.io
May  4 13:21:41.053: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
May  4 13:21:41.053: INFO: Versions found [{node.k8s.io/v1 v1}]
May  4 13:21:41.053: INFO: node.k8s.io/v1 matches node.k8s.io/v1
May  4 13:21:41.053: INFO: Checking APIGroup: discovery.k8s.io
May  4 13:21:41.054: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
May  4 13:21:41.054: INFO: Versions found [{discovery.k8s.io/v1 v1}]
May  4 13:21:41.054: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
May  4 13:21:41.054: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
May  4 13:21:41.055: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
May  4 13:21:41.055: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
May  4 13:21:41.055: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
May  4 13:21:41.055: INFO: Checking APIGroup: agent.pf9.io
May  4 13:21:41.056: INFO: PreferredVersion.GroupVersion: agent.pf9.io/v1
May  4 13:21:41.056: INFO: Versions found [{agent.pf9.io/v1 v1}]
May  4 13:21:41.056: INFO: agent.pf9.io/v1 matches agent.pf9.io/v1
May  4 13:21:41.056: INFO: Checking APIGroup: crd.projectcalico.org
May  4 13:21:41.057: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
May  4 13:21:41.057: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
May  4 13:21:41.057: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
May  4 13:21:41.057: INFO: Checking APIGroup: monitoring.coreos.com
May  4 13:21:41.058: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
May  4 13:21:41.058: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
May  4 13:21:41.058: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
May  4 13:21:41.058: INFO: Checking APIGroup: metrics.k8s.io
May  4 13:21:41.058: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
May  4 13:21:41.058: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
May  4 13:21:41.058: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
May  4 13:21:41.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-2552" for this suite. 05/04/23 13:21:41.066
{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","completed":359,"skipped":6578,"failed":0}
------------------------------
• [0.499 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:21:40.576
    May  4 13:21:40.576: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename discovery 05/04/23 13:21:40.577
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:21:40.609
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:21:40.612
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 05/04/23 13:21:40.616
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    May  4 13:21:41.040: INFO: Checking APIGroup: apiregistration.k8s.io
    May  4 13:21:41.041: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    May  4 13:21:41.041: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    May  4 13:21:41.041: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    May  4 13:21:41.041: INFO: Checking APIGroup: apps
    May  4 13:21:41.042: INFO: PreferredVersion.GroupVersion: apps/v1
    May  4 13:21:41.042: INFO: Versions found [{apps/v1 v1}]
    May  4 13:21:41.042: INFO: apps/v1 matches apps/v1
    May  4 13:21:41.042: INFO: Checking APIGroup: events.k8s.io
    May  4 13:21:41.043: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    May  4 13:21:41.043: INFO: Versions found [{events.k8s.io/v1 v1}]
    May  4 13:21:41.043: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    May  4 13:21:41.043: INFO: Checking APIGroup: authentication.k8s.io
    May  4 13:21:41.044: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    May  4 13:21:41.044: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    May  4 13:21:41.044: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    May  4 13:21:41.044: INFO: Checking APIGroup: authorization.k8s.io
    May  4 13:21:41.044: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    May  4 13:21:41.044: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    May  4 13:21:41.044: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    May  4 13:21:41.044: INFO: Checking APIGroup: autoscaling
    May  4 13:21:41.046: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    May  4 13:21:41.046: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
    May  4 13:21:41.046: INFO: autoscaling/v2 matches autoscaling/v2
    May  4 13:21:41.046: INFO: Checking APIGroup: batch
    May  4 13:21:41.046: INFO: PreferredVersion.GroupVersion: batch/v1
    May  4 13:21:41.046: INFO: Versions found [{batch/v1 v1}]
    May  4 13:21:41.046: INFO: batch/v1 matches batch/v1
    May  4 13:21:41.046: INFO: Checking APIGroup: certificates.k8s.io
    May  4 13:21:41.047: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    May  4 13:21:41.047: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    May  4 13:21:41.047: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    May  4 13:21:41.047: INFO: Checking APIGroup: networking.k8s.io
    May  4 13:21:41.047: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    May  4 13:21:41.047: INFO: Versions found [{networking.k8s.io/v1 v1}]
    May  4 13:21:41.047: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    May  4 13:21:41.047: INFO: Checking APIGroup: policy
    May  4 13:21:41.048: INFO: PreferredVersion.GroupVersion: policy/v1
    May  4 13:21:41.048: INFO: Versions found [{policy/v1 v1}]
    May  4 13:21:41.048: INFO: policy/v1 matches policy/v1
    May  4 13:21:41.048: INFO: Checking APIGroup: rbac.authorization.k8s.io
    May  4 13:21:41.049: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    May  4 13:21:41.049: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    May  4 13:21:41.049: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    May  4 13:21:41.049: INFO: Checking APIGroup: storage.k8s.io
    May  4 13:21:41.050: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    May  4 13:21:41.050: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    May  4 13:21:41.050: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    May  4 13:21:41.050: INFO: Checking APIGroup: admissionregistration.k8s.io
    May  4 13:21:41.050: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    May  4 13:21:41.050: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    May  4 13:21:41.050: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    May  4 13:21:41.050: INFO: Checking APIGroup: apiextensions.k8s.io
    May  4 13:21:41.051: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    May  4 13:21:41.051: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    May  4 13:21:41.051: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    May  4 13:21:41.051: INFO: Checking APIGroup: scheduling.k8s.io
    May  4 13:21:41.052: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    May  4 13:21:41.052: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    May  4 13:21:41.052: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    May  4 13:21:41.052: INFO: Checking APIGroup: coordination.k8s.io
    May  4 13:21:41.053: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    May  4 13:21:41.053: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    May  4 13:21:41.053: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    May  4 13:21:41.053: INFO: Checking APIGroup: node.k8s.io
    May  4 13:21:41.053: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    May  4 13:21:41.053: INFO: Versions found [{node.k8s.io/v1 v1}]
    May  4 13:21:41.053: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    May  4 13:21:41.053: INFO: Checking APIGroup: discovery.k8s.io
    May  4 13:21:41.054: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    May  4 13:21:41.054: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    May  4 13:21:41.054: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    May  4 13:21:41.054: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    May  4 13:21:41.055: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
    May  4 13:21:41.055: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
    May  4 13:21:41.055: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
    May  4 13:21:41.055: INFO: Checking APIGroup: agent.pf9.io
    May  4 13:21:41.056: INFO: PreferredVersion.GroupVersion: agent.pf9.io/v1
    May  4 13:21:41.056: INFO: Versions found [{agent.pf9.io/v1 v1}]
    May  4 13:21:41.056: INFO: agent.pf9.io/v1 matches agent.pf9.io/v1
    May  4 13:21:41.056: INFO: Checking APIGroup: crd.projectcalico.org
    May  4 13:21:41.057: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    May  4 13:21:41.057: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    May  4 13:21:41.057: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    May  4 13:21:41.057: INFO: Checking APIGroup: monitoring.coreos.com
    May  4 13:21:41.058: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
    May  4 13:21:41.058: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
    May  4 13:21:41.058: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
    May  4 13:21:41.058: INFO: Checking APIGroup: metrics.k8s.io
    May  4 13:21:41.058: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    May  4 13:21:41.058: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    May  4 13:21:41.058: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:187
    May  4 13:21:41.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "discovery-2552" for this suite. 05/04/23 13:21:41.066
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:21:41.079
May  4 13:21:41.079: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename security-context-test 05/04/23 13:21:41.08
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:21:41.115
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:21:41.119
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
May  4 13:21:41.145: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-c8e3420f-48c5-4e1f-8708-32ebd8a6b770" in namespace "security-context-test-2536" to be "Succeeded or Failed"
May  4 13:21:41.152: INFO: Pod "alpine-nnp-false-c8e3420f-48c5-4e1f-8708-32ebd8a6b770": Phase="Pending", Reason="", readiness=false. Elapsed: 6.492916ms
May  4 13:21:43.159: INFO: Pod "alpine-nnp-false-c8e3420f-48c5-4e1f-8708-32ebd8a6b770": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013390574s
May  4 13:21:45.158: INFO: Pod "alpine-nnp-false-c8e3420f-48c5-4e1f-8708-32ebd8a6b770": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012884917s
May  4 13:21:47.157: INFO: Pod "alpine-nnp-false-c8e3420f-48c5-4e1f-8708-32ebd8a6b770": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012148111s
May  4 13:21:47.157: INFO: Pod "alpine-nnp-false-c8e3420f-48c5-4e1f-8708-32ebd8a6b770" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
May  4 13:21:47.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2536" for this suite. 05/04/23 13:21:47.175
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","completed":360,"skipped":6644,"failed":0}
------------------------------
• [SLOW TEST] [6.105 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:554
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:608

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:21:41.079
    May  4 13:21:41.079: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename security-context-test 05/04/23 13:21:41.08
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:21:41.115
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:21:41.119
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:608
    May  4 13:21:41.145: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-c8e3420f-48c5-4e1f-8708-32ebd8a6b770" in namespace "security-context-test-2536" to be "Succeeded or Failed"
    May  4 13:21:41.152: INFO: Pod "alpine-nnp-false-c8e3420f-48c5-4e1f-8708-32ebd8a6b770": Phase="Pending", Reason="", readiness=false. Elapsed: 6.492916ms
    May  4 13:21:43.159: INFO: Pod "alpine-nnp-false-c8e3420f-48c5-4e1f-8708-32ebd8a6b770": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013390574s
    May  4 13:21:45.158: INFO: Pod "alpine-nnp-false-c8e3420f-48c5-4e1f-8708-32ebd8a6b770": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012884917s
    May  4 13:21:47.157: INFO: Pod "alpine-nnp-false-c8e3420f-48c5-4e1f-8708-32ebd8a6b770": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012148111s
    May  4 13:21:47.157: INFO: Pod "alpine-nnp-false-c8e3420f-48c5-4e1f-8708-32ebd8a6b770" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    May  4 13:21:47.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-2536" for this suite. 05/04/23 13:21:47.175
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:21:47.186
May  4 13:21:47.186: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename downward-api 05/04/23 13:21:47.187
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:21:47.213
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:21:47.216
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
STEP: Creating a pod to test downward API volume plugin 05/04/23 13:21:47.217
May  4 13:21:47.230: INFO: Waiting up to 5m0s for pod "downwardapi-volume-41b6a4c9-8f0a-4865-9dfb-f02b38facc4b" in namespace "downward-api-5915" to be "Succeeded or Failed"
May  4 13:21:47.234: INFO: Pod "downwardapi-volume-41b6a4c9-8f0a-4865-9dfb-f02b38facc4b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.919719ms
May  4 13:21:49.240: INFO: Pod "downwardapi-volume-41b6a4c9-8f0a-4865-9dfb-f02b38facc4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01017693s
May  4 13:21:51.240: INFO: Pod "downwardapi-volume-41b6a4c9-8f0a-4865-9dfb-f02b38facc4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009970319s
STEP: Saw pod success 05/04/23 13:21:51.24
May  4 13:21:51.240: INFO: Pod "downwardapi-volume-41b6a4c9-8f0a-4865-9dfb-f02b38facc4b" satisfied condition "Succeeded or Failed"
May  4 13:21:51.246: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod downwardapi-volume-41b6a4c9-8f0a-4865-9dfb-f02b38facc4b container client-container: <nil>
STEP: delete the pod 05/04/23 13:21:51.26
May  4 13:21:51.283: INFO: Waiting for pod downwardapi-volume-41b6a4c9-8f0a-4865-9dfb-f02b38facc4b to disappear
May  4 13:21:51.288: INFO: Pod downwardapi-volume-41b6a4c9-8f0a-4865-9dfb-f02b38facc4b no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
May  4 13:21:51.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5915" for this suite. 05/04/23 13:21:51.301
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","completed":361,"skipped":6695,"failed":0}
------------------------------
• [4.126 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:21:47.186
    May  4 13:21:47.186: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename downward-api 05/04/23 13:21:47.187
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:21:47.213
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:21:47.216
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:234
    STEP: Creating a pod to test downward API volume plugin 05/04/23 13:21:47.217
    May  4 13:21:47.230: INFO: Waiting up to 5m0s for pod "downwardapi-volume-41b6a4c9-8f0a-4865-9dfb-f02b38facc4b" in namespace "downward-api-5915" to be "Succeeded or Failed"
    May  4 13:21:47.234: INFO: Pod "downwardapi-volume-41b6a4c9-8f0a-4865-9dfb-f02b38facc4b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.919719ms
    May  4 13:21:49.240: INFO: Pod "downwardapi-volume-41b6a4c9-8f0a-4865-9dfb-f02b38facc4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01017693s
    May  4 13:21:51.240: INFO: Pod "downwardapi-volume-41b6a4c9-8f0a-4865-9dfb-f02b38facc4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009970319s
    STEP: Saw pod success 05/04/23 13:21:51.24
    May  4 13:21:51.240: INFO: Pod "downwardapi-volume-41b6a4c9-8f0a-4865-9dfb-f02b38facc4b" satisfied condition "Succeeded or Failed"
    May  4 13:21:51.246: INFO: Trying to get logs from node ip-10-0-1-224.us-west-2.compute.internal pod downwardapi-volume-41b6a4c9-8f0a-4865-9dfb-f02b38facc4b container client-container: <nil>
    STEP: delete the pod 05/04/23 13:21:51.26
    May  4 13:21:51.283: INFO: Waiting for pod downwardapi-volume-41b6a4c9-8f0a-4865-9dfb-f02b38facc4b to disappear
    May  4 13:21:51.288: INFO: Pod downwardapi-volume-41b6a4c9-8f0a-4865-9dfb-f02b38facc4b no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    May  4 13:21:51.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5915" for this suite. 05/04/23 13:21:51.301
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/04/23 13:21:51.312
May  4 13:21:51.312: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
STEP: Building a namespace api object, basename services 05/04/23 13:21:51.313
STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:21:51.339
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:21:51.342
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
STEP: creating service in namespace services-7605 05/04/23 13:21:51.345
May  4 13:21:51.357: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-7605" to be "running and ready"
May  4 13:21:51.362: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 5.153733ms
May  4 13:21:51.363: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
May  4 13:21:53.367: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.009307336s
May  4 13:21:53.367: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
May  4 13:21:53.367: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
May  4 13:21:53.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-7605 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
May  4 13:21:53.523: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
May  4 13:21:53.523: INFO: stdout: "ipvs"
May  4 13:21:53.523: INFO: proxyMode: ipvs
May  4 13:21:53.538: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May  4 13:21:53.543: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-7605 05/04/23 13:21:53.543
STEP: creating replication controller affinity-nodeport-timeout in namespace services-7605 05/04/23 13:21:53.567
I0504 13:21:53.589549      21 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-7605, replica count: 3
I0504 13:21:56.640400      21 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  4 13:21:56.656: INFO: Creating new exec pod
May  4 13:21:56.665: INFO: Waiting up to 5m0s for pod "execpod-affinity6vcjj" in namespace "services-7605" to be "running"
May  4 13:21:56.669: INFO: Pod "execpod-affinity6vcjj": Phase="Pending", Reason="", readiness=false. Elapsed: 4.45891ms
May  4 13:21:58.677: INFO: Pod "execpod-affinity6vcjj": Phase="Running", Reason="", readiness=true. Elapsed: 2.012463286s
May  4 13:21:58.677: INFO: Pod "execpod-affinity6vcjj" satisfied condition "running"
May  4 13:21:59.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-7605 exec execpod-affinity6vcjj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
May  4 13:21:59.867: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
May  4 13:21:59.867: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  4 13:21:59.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-7605 exec execpod-affinity6vcjj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.21.243.239 80'
May  4 13:22:00.035: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.21.243.239 80\nConnection to 10.21.243.239 80 port [tcp/http] succeeded!\n"
May  4 13:22:00.035: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  4 13:22:00.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-7605 exec execpod-affinity6vcjj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.1.232 32424'
May  4 13:22:00.190: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.1.232 32424\nConnection to 10.0.1.232 32424 port [tcp/*] succeeded!\n"
May  4 13:22:00.190: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  4 13:22:00.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-7605 exec execpod-affinity6vcjj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.1.216 32424'
May  4 13:22:00.342: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.1.216 32424\nConnection to 10.0.1.216 32424 port [tcp/*] succeeded!\n"
May  4 13:22:00.342: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  4 13:22:00.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-7605 exec execpod-affinity6vcjj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.1.189:32424/ ; done'
May  4 13:22:00.590: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:32424/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:32424/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:32424/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:32424/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:32424/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:32424/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:32424/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:32424/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:32424/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:32424/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:32424/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:32424/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:32424/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:32424/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:32424/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:32424/\n"
May  4 13:22:00.590: INFO: stdout: "\naffinity-nodeport-timeout-q5xx8\naffinity-nodeport-timeout-q5xx8\naffinity-nodeport-timeout-q5xx8\naffinity-nodeport-timeout-q5xx8\naffinity-nodeport-timeout-q5xx8\naffinity-nodeport-timeout-q5xx8\naffinity-nodeport-timeout-q5xx8\naffinity-nodeport-timeout-q5xx8\naffinity-nodeport-timeout-q5xx8\naffinity-nodeport-timeout-q5xx8\naffinity-nodeport-timeout-q5xx8\naffinity-nodeport-timeout-q5xx8\naffinity-nodeport-timeout-q5xx8\naffinity-nodeport-timeout-q5xx8\naffinity-nodeport-timeout-q5xx8\naffinity-nodeport-timeout-q5xx8"
May  4 13:22:00.590: INFO: Received response from host: affinity-nodeport-timeout-q5xx8
May  4 13:22:00.590: INFO: Received response from host: affinity-nodeport-timeout-q5xx8
May  4 13:22:00.590: INFO: Received response from host: affinity-nodeport-timeout-q5xx8
May  4 13:22:00.590: INFO: Received response from host: affinity-nodeport-timeout-q5xx8
May  4 13:22:00.590: INFO: Received response from host: affinity-nodeport-timeout-q5xx8
May  4 13:22:00.590: INFO: Received response from host: affinity-nodeport-timeout-q5xx8
May  4 13:22:00.590: INFO: Received response from host: affinity-nodeport-timeout-q5xx8
May  4 13:22:00.590: INFO: Received response from host: affinity-nodeport-timeout-q5xx8
May  4 13:22:00.590: INFO: Received response from host: affinity-nodeport-timeout-q5xx8
May  4 13:22:00.590: INFO: Received response from host: affinity-nodeport-timeout-q5xx8
May  4 13:22:00.590: INFO: Received response from host: affinity-nodeport-timeout-q5xx8
May  4 13:22:00.590: INFO: Received response from host: affinity-nodeport-timeout-q5xx8
May  4 13:22:00.590: INFO: Received response from host: affinity-nodeport-timeout-q5xx8
May  4 13:22:00.590: INFO: Received response from host: affinity-nodeport-timeout-q5xx8
May  4 13:22:00.590: INFO: Received response from host: affinity-nodeport-timeout-q5xx8
May  4 13:22:00.590: INFO: Received response from host: affinity-nodeport-timeout-q5xx8
May  4 13:22:00.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-7605 exec execpod-affinity6vcjj -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.1.189:32424/'
May  4 13:22:00.749: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.1.189:32424/\n"
May  4 13:22:00.749: INFO: stdout: "affinity-nodeport-timeout-q5xx8"
May  4 13:24:10.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-7605 exec execpod-affinity6vcjj -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.1.189:32424/'
May  4 13:24:10.914: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.1.189:32424/\n"
May  4 13:24:10.914: INFO: stdout: "affinity-nodeport-timeout-s6tfx"
May  4 13:24:10.914: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-7605, will wait for the garbage collector to delete the pods 05/04/23 13:24:10.939
May  4 13:24:11.004: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 9.771337ms
May  4 13:24:11.109: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 105.216262ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  4 13:24:13.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7605" for this suite. 05/04/23 13:24:13.07
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","completed":362,"skipped":6700,"failed":0}
------------------------------
• [SLOW TEST] [141.768 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/04/23 13:21:51.312
    May  4 13:21:51.312: INFO: >>> kubeConfig: /tmp/kubeconfig-2337427429
    STEP: Building a namespace api object, basename services 05/04/23 13:21:51.313
    STEP: Waiting for a default service account to be provisioned in namespace 05/04/23 13:21:51.339
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/04/23 13:21:51.342
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2221
    STEP: creating service in namespace services-7605 05/04/23 13:21:51.345
    May  4 13:21:51.357: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-7605" to be "running and ready"
    May  4 13:21:51.362: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 5.153733ms
    May  4 13:21:51.363: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    May  4 13:21:53.367: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.009307336s
    May  4 13:21:53.367: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    May  4 13:21:53.367: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    May  4 13:21:53.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-7605 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    May  4 13:21:53.523: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    May  4 13:21:53.523: INFO: stdout: "ipvs"
    May  4 13:21:53.523: INFO: proxyMode: ipvs
    May  4 13:21:53.538: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    May  4 13:21:53.543: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-nodeport-timeout in namespace services-7605 05/04/23 13:21:53.543
    STEP: creating replication controller affinity-nodeport-timeout in namespace services-7605 05/04/23 13:21:53.567
    I0504 13:21:53.589549      21 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-7605, replica count: 3
    I0504 13:21:56.640400      21 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  4 13:21:56.656: INFO: Creating new exec pod
    May  4 13:21:56.665: INFO: Waiting up to 5m0s for pod "execpod-affinity6vcjj" in namespace "services-7605" to be "running"
    May  4 13:21:56.669: INFO: Pod "execpod-affinity6vcjj": Phase="Pending", Reason="", readiness=false. Elapsed: 4.45891ms
    May  4 13:21:58.677: INFO: Pod "execpod-affinity6vcjj": Phase="Running", Reason="", readiness=true. Elapsed: 2.012463286s
    May  4 13:21:58.677: INFO: Pod "execpod-affinity6vcjj" satisfied condition "running"
    May  4 13:21:59.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-7605 exec execpod-affinity6vcjj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
    May  4 13:21:59.867: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
    May  4 13:21:59.867: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  4 13:21:59.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-7605 exec execpod-affinity6vcjj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.21.243.239 80'
    May  4 13:22:00.035: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.21.243.239 80\nConnection to 10.21.243.239 80 port [tcp/http] succeeded!\n"
    May  4 13:22:00.035: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  4 13:22:00.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-7605 exec execpod-affinity6vcjj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.1.232 32424'
    May  4 13:22:00.190: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.1.232 32424\nConnection to 10.0.1.232 32424 port [tcp/*] succeeded!\n"
    May  4 13:22:00.190: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  4 13:22:00.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-7605 exec execpod-affinity6vcjj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.1.216 32424'
    May  4 13:22:00.342: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.1.216 32424\nConnection to 10.0.1.216 32424 port [tcp/*] succeeded!\n"
    May  4 13:22:00.342: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  4 13:22:00.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-7605 exec execpod-affinity6vcjj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.1.189:32424/ ; done'
    May  4 13:22:00.590: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:32424/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:32424/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:32424/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:32424/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:32424/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:32424/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:32424/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:32424/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:32424/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:32424/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:32424/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:32424/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:32424/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:32424/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:32424/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.189:32424/\n"
    May  4 13:22:00.590: INFO: stdout: "\naffinity-nodeport-timeout-q5xx8\naffinity-nodeport-timeout-q5xx8\naffinity-nodeport-timeout-q5xx8\naffinity-nodeport-timeout-q5xx8\naffinity-nodeport-timeout-q5xx8\naffinity-nodeport-timeout-q5xx8\naffinity-nodeport-timeout-q5xx8\naffinity-nodeport-timeout-q5xx8\naffinity-nodeport-timeout-q5xx8\naffinity-nodeport-timeout-q5xx8\naffinity-nodeport-timeout-q5xx8\naffinity-nodeport-timeout-q5xx8\naffinity-nodeport-timeout-q5xx8\naffinity-nodeport-timeout-q5xx8\naffinity-nodeport-timeout-q5xx8\naffinity-nodeport-timeout-q5xx8"
    May  4 13:22:00.590: INFO: Received response from host: affinity-nodeport-timeout-q5xx8
    May  4 13:22:00.590: INFO: Received response from host: affinity-nodeport-timeout-q5xx8
    May  4 13:22:00.590: INFO: Received response from host: affinity-nodeport-timeout-q5xx8
    May  4 13:22:00.590: INFO: Received response from host: affinity-nodeport-timeout-q5xx8
    May  4 13:22:00.590: INFO: Received response from host: affinity-nodeport-timeout-q5xx8
    May  4 13:22:00.590: INFO: Received response from host: affinity-nodeport-timeout-q5xx8
    May  4 13:22:00.590: INFO: Received response from host: affinity-nodeport-timeout-q5xx8
    May  4 13:22:00.590: INFO: Received response from host: affinity-nodeport-timeout-q5xx8
    May  4 13:22:00.590: INFO: Received response from host: affinity-nodeport-timeout-q5xx8
    May  4 13:22:00.590: INFO: Received response from host: affinity-nodeport-timeout-q5xx8
    May  4 13:22:00.590: INFO: Received response from host: affinity-nodeport-timeout-q5xx8
    May  4 13:22:00.590: INFO: Received response from host: affinity-nodeport-timeout-q5xx8
    May  4 13:22:00.590: INFO: Received response from host: affinity-nodeport-timeout-q5xx8
    May  4 13:22:00.590: INFO: Received response from host: affinity-nodeport-timeout-q5xx8
    May  4 13:22:00.590: INFO: Received response from host: affinity-nodeport-timeout-q5xx8
    May  4 13:22:00.590: INFO: Received response from host: affinity-nodeport-timeout-q5xx8
    May  4 13:22:00.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-7605 exec execpod-affinity6vcjj -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.1.189:32424/'
    May  4 13:22:00.749: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.1.189:32424/\n"
    May  4 13:22:00.749: INFO: stdout: "affinity-nodeport-timeout-q5xx8"
    May  4 13:24:10.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2337427429 --namespace=services-7605 exec execpod-affinity6vcjj -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.1.189:32424/'
    May  4 13:24:10.914: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.1.189:32424/\n"
    May  4 13:24:10.914: INFO: stdout: "affinity-nodeport-timeout-s6tfx"
    May  4 13:24:10.914: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-7605, will wait for the garbage collector to delete the pods 05/04/23 13:24:10.939
    May  4 13:24:11.004: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 9.771337ms
    May  4 13:24:11.109: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 105.216262ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  4 13:24:13.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7605" for this suite. 05/04/23 13:24:13.07
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
{"msg":"Test Suite completed","completed":362,"skipped":6704,"failed":0}
May  4 13:24:13.082: INFO: Running AfterSuite actions on all nodes
May  4 13:24:13.082: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
May  4 13:24:13.082: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
May  4 13:24:13.082: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
May  4 13:24:13.082: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
May  4 13:24:13.082: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
May  4 13:24:13.082: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
May  4 13:24:13.082: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
May  4 13:24:13.082: INFO: Running AfterSuite actions on node 1
May  4 13:24:13.082: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    May  4 13:24:13.082: INFO: Running AfterSuite actions on all nodes
    May  4 13:24:13.082: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
    May  4 13:24:13.082: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
    May  4 13:24:13.082: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
    May  4 13:24:13.082: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
    May  4 13:24:13.082: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
    May  4 13:24:13.082: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
    May  4 13:24:13.082: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    May  4 13:24:13.082: INFO: Running AfterSuite actions on node 1
    May  4 13:24:13.082: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:146
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:146
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:559
------------------------------
[ReportAfterSuite] PASSED [0.066 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:559
  << End Captured GinkgoWriter Output
------------------------------

Ran 362 of 7066 Specs in 5985.203 seconds
SUCCESS! -- 362 Passed | 0 Failed | 0 Pending | 6704 Skipped
PASS

Ginkgo ran 1 suite in 1h39m45.806575095s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.1.6[0m

