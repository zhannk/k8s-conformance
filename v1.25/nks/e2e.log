I0119 04:19:49.461793      18 e2e.go:116] Starting e2e run "00430eb5-5905-4abe-8db7-178d376ca2a7" on Ginkgo node 1
Jan 19 04:19:49.476: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1674101989 - will randomize all specs

Will run 362 of 7067 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
{"msg":"Test Suite starting","completed":0,"skipped":0,"failed":0}
Jan 19 04:19:49.599: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
Jan 19 04:19:49.601: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
E0119 04:19:49.623898      18 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
E0119 04:19:49.623898      18 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Jan 19 04:19:49.631: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jan 19 04:19:49.650: INFO: 13 / 13 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jan 19 04:19:49.650: INFO: expected 8 pod replicas in namespace 'kube-system', 8 are Running and Ready.
Jan 19 04:19:49.650: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jan 19 04:19:49.654: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'csi-cinder-nodeplugin' (0 seconds elapsed)
Jan 19 04:19:49.654: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'npd' (0 seconds elapsed)
Jan 19 04:19:49.654: INFO: e2e test version: v1.25.4
Jan 19 04:19:49.656: INFO: kube-apiserver version: v1.25.4
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
Jan 19 04:19:49.656: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
Jan 19 04:19:49.659: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.063 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Jan 19 04:19:49.599: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    Jan 19 04:19:49.601: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    E0119 04:19:49.623898      18 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    Jan 19 04:19:49.631: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Jan 19 04:19:49.650: INFO: 13 / 13 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Jan 19 04:19:49.650: INFO: expected 8 pod replicas in namespace 'kube-system', 8 are Running and Ready.
    Jan 19 04:19:49.650: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Jan 19 04:19:49.654: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'csi-cinder-nodeplugin' (0 seconds elapsed)
    Jan 19 04:19:49.654: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'npd' (0 seconds elapsed)
    Jan 19 04:19:49.654: INFO: e2e test version: v1.25.4
    Jan 19 04:19:49.656: INFO: kube-apiserver version: v1.25.4
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Jan 19 04:19:49.656: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    Jan 19 04:19:49.659: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:19:49.679
Jan 19 04:19:49.679: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename statefulset 01/19/23 04:19:49.68
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:19:49.704
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:19:49.707
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-8702 01/19/23 04:19:49.709
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
Jan 19 04:19:49.732: INFO: Found 0 stateful pods, waiting for 1
Jan 19 04:19:59.737: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Jan 19 04:20:09.736: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 01/19/23 04:20:09.742
W0119 04:20:09.752904      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 19 04:20:09.760: INFO: Found 1 stateful pods, waiting for 2
Jan 19 04:20:19.766: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 04:20:19.766: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 01/19/23 04:20:19.771
STEP: Delete all of the StatefulSets 01/19/23 04:20:19.773
STEP: Verify that StatefulSets have been deleted 01/19/23 04:20:19.781
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 19 04:20:19.784: INFO: Deleting all statefulset in ns statefulset-8702
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 19 04:20:19.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8702" for this suite. 01/19/23 04:20:19.793
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","completed":1,"skipped":1,"failed":0}
------------------------------
â€¢ [SLOW TEST] [30.119 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:906

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:19:49.679
    Jan 19 04:19:49.679: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename statefulset 01/19/23 04:19:49.68
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:19:49.704
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:19:49.707
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-8702 01/19/23 04:19:49.709
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:906
    Jan 19 04:19:49.732: INFO: Found 0 stateful pods, waiting for 1
    Jan 19 04:19:59.737: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Pending - Ready=false
    Jan 19 04:20:09.736: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 01/19/23 04:20:09.742
    W0119 04:20:09.752904      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 19 04:20:09.760: INFO: Found 1 stateful pods, waiting for 2
    Jan 19 04:20:19.766: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 19 04:20:19.766: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 01/19/23 04:20:19.771
    STEP: Delete all of the StatefulSets 01/19/23 04:20:19.773
    STEP: Verify that StatefulSets have been deleted 01/19/23 04:20:19.781
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 19 04:20:19.784: INFO: Deleting all statefulset in ns statefulset-8702
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 19 04:20:19.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-8702" for this suite. 01/19/23 04:20:19.793
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:20:19.8
Jan 19 04:20:19.800: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename secrets 01/19/23 04:20:19.8
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:20:19.832
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:20:19.834
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
STEP: Creating secret with name secret-test-dcb0a914-6a97-4ab6-a030-3b10ae7d3900 01/19/23 04:20:19.839
STEP: Creating a pod to test consume secrets 01/19/23 04:20:19.844
Jan 19 04:20:19.853: INFO: Waiting up to 5m0s for pod "pod-secrets-e7dc1c34-4b3f-41b6-91d9-f89d1f4ac714" in namespace "secrets-450" to be "Succeeded or Failed"
Jan 19 04:20:19.857: INFO: Pod "pod-secrets-e7dc1c34-4b3f-41b6-91d9-f89d1f4ac714": Phase="Pending", Reason="", readiness=false. Elapsed: 3.813815ms
Jan 19 04:20:21.861: INFO: Pod "pod-secrets-e7dc1c34-4b3f-41b6-91d9-f89d1f4ac714": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007984175s
Jan 19 04:20:23.861: INFO: Pod "pod-secrets-e7dc1c34-4b3f-41b6-91d9-f89d1f4ac714": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008069339s
Jan 19 04:20:25.861: INFO: Pod "pod-secrets-e7dc1c34-4b3f-41b6-91d9-f89d1f4ac714": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008122828s
Jan 19 04:20:27.861: INFO: Pod "pod-secrets-e7dc1c34-4b3f-41b6-91d9-f89d1f4ac714": Phase="Pending", Reason="", readiness=false. Elapsed: 8.00812525s
Jan 19 04:20:29.861: INFO: Pod "pod-secrets-e7dc1c34-4b3f-41b6-91d9-f89d1f4ac714": Phase="Running", Reason="", readiness=false. Elapsed: 10.007992886s
Jan 19 04:20:31.861: INFO: Pod "pod-secrets-e7dc1c34-4b3f-41b6-91d9-f89d1f4ac714": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.007474278s
STEP: Saw pod success 01/19/23 04:20:31.861
Jan 19 04:20:31.861: INFO: Pod "pod-secrets-e7dc1c34-4b3f-41b6-91d9-f89d1f4ac714" satisfied condition "Succeeded or Failed"
Jan 19 04:20:31.864: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-secrets-e7dc1c34-4b3f-41b6-91d9-f89d1f4ac714 container secret-volume-test: <nil>
STEP: delete the pod 01/19/23 04:20:31.913
Jan 19 04:20:31.923: INFO: Waiting for pod pod-secrets-e7dc1c34-4b3f-41b6-91d9-f89d1f4ac714 to disappear
Jan 19 04:20:31.926: INFO: Pod pod-secrets-e7dc1c34-4b3f-41b6-91d9-f89d1f4ac714 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 19 04:20:31.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-450" for this suite. 01/19/23 04:20:31.929
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":2,"skipped":20,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.135 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:20:19.8
    Jan 19 04:20:19.800: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename secrets 01/19/23 04:20:19.8
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:20:19.832
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:20:19.834
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:67
    STEP: Creating secret with name secret-test-dcb0a914-6a97-4ab6-a030-3b10ae7d3900 01/19/23 04:20:19.839
    STEP: Creating a pod to test consume secrets 01/19/23 04:20:19.844
    Jan 19 04:20:19.853: INFO: Waiting up to 5m0s for pod "pod-secrets-e7dc1c34-4b3f-41b6-91d9-f89d1f4ac714" in namespace "secrets-450" to be "Succeeded or Failed"
    Jan 19 04:20:19.857: INFO: Pod "pod-secrets-e7dc1c34-4b3f-41b6-91d9-f89d1f4ac714": Phase="Pending", Reason="", readiness=false. Elapsed: 3.813815ms
    Jan 19 04:20:21.861: INFO: Pod "pod-secrets-e7dc1c34-4b3f-41b6-91d9-f89d1f4ac714": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007984175s
    Jan 19 04:20:23.861: INFO: Pod "pod-secrets-e7dc1c34-4b3f-41b6-91d9-f89d1f4ac714": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008069339s
    Jan 19 04:20:25.861: INFO: Pod "pod-secrets-e7dc1c34-4b3f-41b6-91d9-f89d1f4ac714": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008122828s
    Jan 19 04:20:27.861: INFO: Pod "pod-secrets-e7dc1c34-4b3f-41b6-91d9-f89d1f4ac714": Phase="Pending", Reason="", readiness=false. Elapsed: 8.00812525s
    Jan 19 04:20:29.861: INFO: Pod "pod-secrets-e7dc1c34-4b3f-41b6-91d9-f89d1f4ac714": Phase="Running", Reason="", readiness=false. Elapsed: 10.007992886s
    Jan 19 04:20:31.861: INFO: Pod "pod-secrets-e7dc1c34-4b3f-41b6-91d9-f89d1f4ac714": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.007474278s
    STEP: Saw pod success 01/19/23 04:20:31.861
    Jan 19 04:20:31.861: INFO: Pod "pod-secrets-e7dc1c34-4b3f-41b6-91d9-f89d1f4ac714" satisfied condition "Succeeded or Failed"
    Jan 19 04:20:31.864: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-secrets-e7dc1c34-4b3f-41b6-91d9-f89d1f4ac714 container secret-volume-test: <nil>
    STEP: delete the pod 01/19/23 04:20:31.913
    Jan 19 04:20:31.923: INFO: Waiting for pod pod-secrets-e7dc1c34-4b3f-41b6-91d9-f89d1f4ac714 to disappear
    Jan 19 04:20:31.926: INFO: Pod pod-secrets-e7dc1c34-4b3f-41b6-91d9-f89d1f4ac714 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 19 04:20:31.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-450" for this suite. 01/19/23 04:20:31.929
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:20:31.936
Jan 19 04:20:31.936: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename projected 01/19/23 04:20:31.936
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:20:31.955
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:20:31.959
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
STEP: Creating configMap with name configmap-projected-all-test-volume-b03e47ca-ae18-4edf-abc1-6797655235c8 01/19/23 04:20:31.961
STEP: Creating secret with name secret-projected-all-test-volume-84d63a31-26cf-4153-b17b-9573760499aa 01/19/23 04:20:31.965
STEP: Creating a pod to test Check all projections for projected volume plugin 01/19/23 04:20:31.969
Jan 19 04:20:31.978: INFO: Waiting up to 5m0s for pod "projected-volume-66c9debe-7c66-4992-a681-b09d5b7463fd" in namespace "projected-7847" to be "Succeeded or Failed"
Jan 19 04:20:31.984: INFO: Pod "projected-volume-66c9debe-7c66-4992-a681-b09d5b7463fd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.928503ms
Jan 19 04:20:33.988: INFO: Pod "projected-volume-66c9debe-7c66-4992-a681-b09d5b7463fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010197487s
Jan 19 04:20:35.987: INFO: Pod "projected-volume-66c9debe-7c66-4992-a681-b09d5b7463fd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009058849s
Jan 19 04:20:37.989: INFO: Pod "projected-volume-66c9debe-7c66-4992-a681-b09d5b7463fd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010305686s
Jan 19 04:20:39.989: INFO: Pod "projected-volume-66c9debe-7c66-4992-a681-b09d5b7463fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.011103564s
STEP: Saw pod success 01/19/23 04:20:39.989
Jan 19 04:20:39.989: INFO: Pod "projected-volume-66c9debe-7c66-4992-a681-b09d5b7463fd" satisfied condition "Succeeded or Failed"
Jan 19 04:20:39.993: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod projected-volume-66c9debe-7c66-4992-a681-b09d5b7463fd container projected-all-volume-test: <nil>
STEP: delete the pod 01/19/23 04:20:40.003
Jan 19 04:20:40.015: INFO: Waiting for pod projected-volume-66c9debe-7c66-4992-a681-b09d5b7463fd to disappear
Jan 19 04:20:40.017: INFO: Pod projected-volume-66c9debe-7c66-4992-a681-b09d5b7463fd no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
Jan 19 04:20:40.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7847" for this suite. 01/19/23 04:20:40.02
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","completed":3,"skipped":50,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.091 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:20:31.936
    Jan 19 04:20:31.936: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename projected 01/19/23 04:20:31.936
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:20:31.955
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:20:31.959
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:43
    STEP: Creating configMap with name configmap-projected-all-test-volume-b03e47ca-ae18-4edf-abc1-6797655235c8 01/19/23 04:20:31.961
    STEP: Creating secret with name secret-projected-all-test-volume-84d63a31-26cf-4153-b17b-9573760499aa 01/19/23 04:20:31.965
    STEP: Creating a pod to test Check all projections for projected volume plugin 01/19/23 04:20:31.969
    Jan 19 04:20:31.978: INFO: Waiting up to 5m0s for pod "projected-volume-66c9debe-7c66-4992-a681-b09d5b7463fd" in namespace "projected-7847" to be "Succeeded or Failed"
    Jan 19 04:20:31.984: INFO: Pod "projected-volume-66c9debe-7c66-4992-a681-b09d5b7463fd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.928503ms
    Jan 19 04:20:33.988: INFO: Pod "projected-volume-66c9debe-7c66-4992-a681-b09d5b7463fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010197487s
    Jan 19 04:20:35.987: INFO: Pod "projected-volume-66c9debe-7c66-4992-a681-b09d5b7463fd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009058849s
    Jan 19 04:20:37.989: INFO: Pod "projected-volume-66c9debe-7c66-4992-a681-b09d5b7463fd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010305686s
    Jan 19 04:20:39.989: INFO: Pod "projected-volume-66c9debe-7c66-4992-a681-b09d5b7463fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.011103564s
    STEP: Saw pod success 01/19/23 04:20:39.989
    Jan 19 04:20:39.989: INFO: Pod "projected-volume-66c9debe-7c66-4992-a681-b09d5b7463fd" satisfied condition "Succeeded or Failed"
    Jan 19 04:20:39.993: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod projected-volume-66c9debe-7c66-4992-a681-b09d5b7463fd container projected-all-volume-test: <nil>
    STEP: delete the pod 01/19/23 04:20:40.003
    Jan 19 04:20:40.015: INFO: Waiting for pod projected-volume-66c9debe-7c66-4992-a681-b09d5b7463fd to disappear
    Jan 19 04:20:40.017: INFO: Pod projected-volume-66c9debe-7c66-4992-a681-b09d5b7463fd no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:187
    Jan 19 04:20:40.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7847" for this suite. 01/19/23 04:20:40.02
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:20:40.027
Jan 19 04:20:40.027: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename runtimeclass 01/19/23 04:20:40.028
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:20:40.039
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:20:40.041
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 01/19/23 04:20:40.043
STEP: getting /apis/node.k8s.io 01/19/23 04:20:40.044
STEP: getting /apis/node.k8s.io/v1 01/19/23 04:20:40.045
STEP: creating 01/19/23 04:20:40.046
STEP: watching 01/19/23 04:20:40.059
Jan 19 04:20:40.059: INFO: starting watch
STEP: getting 01/19/23 04:20:40.064
STEP: listing 01/19/23 04:20:40.066
STEP: patching 01/19/23 04:20:40.068
STEP: updating 01/19/23 04:20:40.072
Jan 19 04:20:40.076: INFO: waiting for watch events with expected annotations
STEP: deleting 01/19/23 04:20:40.076
STEP: deleting a collection 01/19/23 04:20:40.086
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan 19 04:20:40.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-4364" for this suite. 01/19/23 04:20:40.1
{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","completed":4,"skipped":56,"failed":0}
------------------------------
â€¢ [0.079 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:20:40.027
    Jan 19 04:20:40.027: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename runtimeclass 01/19/23 04:20:40.028
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:20:40.039
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:20:40.041
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 01/19/23 04:20:40.043
    STEP: getting /apis/node.k8s.io 01/19/23 04:20:40.044
    STEP: getting /apis/node.k8s.io/v1 01/19/23 04:20:40.045
    STEP: creating 01/19/23 04:20:40.046
    STEP: watching 01/19/23 04:20:40.059
    Jan 19 04:20:40.059: INFO: starting watch
    STEP: getting 01/19/23 04:20:40.064
    STEP: listing 01/19/23 04:20:40.066
    STEP: patching 01/19/23 04:20:40.068
    STEP: updating 01/19/23 04:20:40.072
    Jan 19 04:20:40.076: INFO: waiting for watch events with expected annotations
    STEP: deleting 01/19/23 04:20:40.076
    STEP: deleting a collection 01/19/23 04:20:40.086
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan 19 04:20:40.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-4364" for this suite. 01/19/23 04:20:40.1
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:20:40.106
Jan 19 04:20:40.107: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename endpointslice 01/19/23 04:20:40.107
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:20:40.119
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:20:40.121
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jan 19 04:20:44.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-644" for this suite. 01/19/23 04:20:44.184
{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","completed":5,"skipped":59,"failed":0}
------------------------------
â€¢ [4.084 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:20:40.106
    Jan 19 04:20:40.107: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename endpointslice 01/19/23 04:20:40.107
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:20:40.119
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:20:40.121
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:101
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jan 19 04:20:44.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-644" for this suite. 01/19/23 04:20:44.184
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:20:44.191
Jan 19 04:20:44.191: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename disruption 01/19/23 04:20:44.191
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:20:44.204
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:20:44.207
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
STEP: creating the pdb 01/19/23 04:20:44.209
STEP: Waiting for the pdb to be processed 01/19/23 04:20:44.215
STEP: updating the pdb 01/19/23 04:20:46.221
STEP: Waiting for the pdb to be processed 01/19/23 04:20:46.237
STEP: patching the pdb 01/19/23 04:20:46.242
STEP: Waiting for the pdb to be processed 01/19/23 04:20:46.252
STEP: Waiting for the pdb to be deleted 01/19/23 04:20:48.269
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan 19 04:20:48.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9794" for this suite. 01/19/23 04:20:48.275
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","completed":6,"skipped":68,"failed":0}
------------------------------
â€¢ [4.090 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:20:44.191
    Jan 19 04:20:44.191: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename disruption 01/19/23 04:20:44.191
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:20:44.204
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:20:44.207
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:107
    STEP: creating the pdb 01/19/23 04:20:44.209
    STEP: Waiting for the pdb to be processed 01/19/23 04:20:44.215
    STEP: updating the pdb 01/19/23 04:20:46.221
    STEP: Waiting for the pdb to be processed 01/19/23 04:20:46.237
    STEP: patching the pdb 01/19/23 04:20:46.242
    STEP: Waiting for the pdb to be processed 01/19/23 04:20:46.252
    STEP: Waiting for the pdb to be deleted 01/19/23 04:20:48.269
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan 19 04:20:48.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-9794" for this suite. 01/19/23 04:20:48.275
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:20:48.281
Jan 19 04:20:48.281: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename lease-test 01/19/23 04:20:48.282
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:20:48.307
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:20:48.309
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
Jan 19 04:20:48.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-4685" for this suite. 01/19/23 04:20:48.364
{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","completed":7,"skipped":72,"failed":0}
------------------------------
â€¢ [0.088 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:20:48.281
    Jan 19 04:20:48.281: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename lease-test 01/19/23 04:20:48.282
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:20:48.307
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:20:48.309
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/framework.go:187
    Jan 19 04:20:48.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "lease-test-4685" for this suite. 01/19/23 04:20:48.364
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:20:48.37
Jan 19 04:20:48.370: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename downward-api 01/19/23 04:20:48.371
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:20:48.385
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:20:48.387
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
STEP: Creating a pod to test downward api env vars 01/19/23 04:20:48.389
Jan 19 04:20:48.399: INFO: Waiting up to 5m0s for pod "downward-api-e96293db-8c6a-4e2e-8430-8ebf7785027b" in namespace "downward-api-8334" to be "Succeeded or Failed"
Jan 19 04:20:48.409: INFO: Pod "downward-api-e96293db-8c6a-4e2e-8430-8ebf7785027b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.457475ms
Jan 19 04:20:50.414: INFO: Pod "downward-api-e96293db-8c6a-4e2e-8430-8ebf7785027b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015102684s
Jan 19 04:20:52.415: INFO: Pod "downward-api-e96293db-8c6a-4e2e-8430-8ebf7785027b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016022411s
STEP: Saw pod success 01/19/23 04:20:52.415
Jan 19 04:20:52.415: INFO: Pod "downward-api-e96293db-8c6a-4e2e-8430-8ebf7785027b" satisfied condition "Succeeded or Failed"
Jan 19 04:20:52.418: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod downward-api-e96293db-8c6a-4e2e-8430-8ebf7785027b container dapi-container: <nil>
STEP: delete the pod 01/19/23 04:20:52.424
Jan 19 04:20:52.434: INFO: Waiting for pod downward-api-e96293db-8c6a-4e2e-8430-8ebf7785027b to disappear
Jan 19 04:20:52.436: INFO: Pod downward-api-e96293db-8c6a-4e2e-8430-8ebf7785027b no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan 19 04:20:52.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8334" for this suite. 01/19/23 04:20:52.439
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","completed":8,"skipped":84,"failed":0}
------------------------------
â€¢ [4.074 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:20:48.37
    Jan 19 04:20:48.370: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename downward-api 01/19/23 04:20:48.371
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:20:48.385
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:20:48.387
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:43
    STEP: Creating a pod to test downward api env vars 01/19/23 04:20:48.389
    Jan 19 04:20:48.399: INFO: Waiting up to 5m0s for pod "downward-api-e96293db-8c6a-4e2e-8430-8ebf7785027b" in namespace "downward-api-8334" to be "Succeeded or Failed"
    Jan 19 04:20:48.409: INFO: Pod "downward-api-e96293db-8c6a-4e2e-8430-8ebf7785027b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.457475ms
    Jan 19 04:20:50.414: INFO: Pod "downward-api-e96293db-8c6a-4e2e-8430-8ebf7785027b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015102684s
    Jan 19 04:20:52.415: INFO: Pod "downward-api-e96293db-8c6a-4e2e-8430-8ebf7785027b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016022411s
    STEP: Saw pod success 01/19/23 04:20:52.415
    Jan 19 04:20:52.415: INFO: Pod "downward-api-e96293db-8c6a-4e2e-8430-8ebf7785027b" satisfied condition "Succeeded or Failed"
    Jan 19 04:20:52.418: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod downward-api-e96293db-8c6a-4e2e-8430-8ebf7785027b container dapi-container: <nil>
    STEP: delete the pod 01/19/23 04:20:52.424
    Jan 19 04:20:52.434: INFO: Waiting for pod downward-api-e96293db-8c6a-4e2e-8430-8ebf7785027b to disappear
    Jan 19 04:20:52.436: INFO: Pod downward-api-e96293db-8c6a-4e2e-8430-8ebf7785027b no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan 19 04:20:52.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8334" for this suite. 01/19/23 04:20:52.439
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:20:52.444
Jan 19 04:20:52.444: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename container-probe 01/19/23 04:20:52.445
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:20:52.46
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:20:52.462
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
STEP: Creating pod busybox-e9afc08d-1c61-4276-8100-7bafe1bb01e8 in namespace container-probe-3021 01/19/23 04:20:52.465
Jan 19 04:20:52.475: INFO: Waiting up to 5m0s for pod "busybox-e9afc08d-1c61-4276-8100-7bafe1bb01e8" in namespace "container-probe-3021" to be "not pending"
Jan 19 04:20:52.478: INFO: Pod "busybox-e9afc08d-1c61-4276-8100-7bafe1bb01e8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.231817ms
Jan 19 04:20:54.483: INFO: Pod "busybox-e9afc08d-1c61-4276-8100-7bafe1bb01e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007800257s
Jan 19 04:20:56.482: INFO: Pod "busybox-e9afc08d-1c61-4276-8100-7bafe1bb01e8": Phase="Running", Reason="", readiness=true. Elapsed: 4.006914569s
Jan 19 04:20:56.482: INFO: Pod "busybox-e9afc08d-1c61-4276-8100-7bafe1bb01e8" satisfied condition "not pending"
Jan 19 04:20:56.482: INFO: Started pod busybox-e9afc08d-1c61-4276-8100-7bafe1bb01e8 in namespace container-probe-3021
STEP: checking the pod's current state and verifying that restartCount is present 01/19/23 04:20:56.482
Jan 19 04:20:56.485: INFO: Initial restart count of pod busybox-e9afc08d-1c61-4276-8100-7bafe1bb01e8 is 0
Jan 19 04:21:44.597: INFO: Restart count of pod container-probe-3021/busybox-e9afc08d-1c61-4276-8100-7bafe1bb01e8 is now 1 (48.111987751s elapsed)
STEP: deleting the pod 01/19/23 04:21:44.597
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 19 04:21:44.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3021" for this suite. 01/19/23 04:21:44.615
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":9,"skipped":87,"failed":0}
------------------------------
â€¢ [SLOW TEST] [52.177 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:20:52.444
    Jan 19 04:20:52.444: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename container-probe 01/19/23 04:20:52.445
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:20:52.46
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:20:52.462
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:131
    STEP: Creating pod busybox-e9afc08d-1c61-4276-8100-7bafe1bb01e8 in namespace container-probe-3021 01/19/23 04:20:52.465
    Jan 19 04:20:52.475: INFO: Waiting up to 5m0s for pod "busybox-e9afc08d-1c61-4276-8100-7bafe1bb01e8" in namespace "container-probe-3021" to be "not pending"
    Jan 19 04:20:52.478: INFO: Pod "busybox-e9afc08d-1c61-4276-8100-7bafe1bb01e8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.231817ms
    Jan 19 04:20:54.483: INFO: Pod "busybox-e9afc08d-1c61-4276-8100-7bafe1bb01e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007800257s
    Jan 19 04:20:56.482: INFO: Pod "busybox-e9afc08d-1c61-4276-8100-7bafe1bb01e8": Phase="Running", Reason="", readiness=true. Elapsed: 4.006914569s
    Jan 19 04:20:56.482: INFO: Pod "busybox-e9afc08d-1c61-4276-8100-7bafe1bb01e8" satisfied condition "not pending"
    Jan 19 04:20:56.482: INFO: Started pod busybox-e9afc08d-1c61-4276-8100-7bafe1bb01e8 in namespace container-probe-3021
    STEP: checking the pod's current state and verifying that restartCount is present 01/19/23 04:20:56.482
    Jan 19 04:20:56.485: INFO: Initial restart count of pod busybox-e9afc08d-1c61-4276-8100-7bafe1bb01e8 is 0
    Jan 19 04:21:44.597: INFO: Restart count of pod container-probe-3021/busybox-e9afc08d-1c61-4276-8100-7bafe1bb01e8 is now 1 (48.111987751s elapsed)
    STEP: deleting the pod 01/19/23 04:21:44.597
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 19 04:21:44.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-3021" for this suite. 01/19/23 04:21:44.615
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:21:44.621
Jan 19 04:21:44.622: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename kubectl 01/19/23 04:21:44.622
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:21:44.636
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:21:44.638
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
STEP: starting the proxy server 01/19/23 04:21:44.64
Jan 19 04:21:44.640: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-2331 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 01/19/23 04:21:44.69
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 19 04:21:44.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2331" for this suite. 01/19/23 04:21:44.719
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","completed":10,"skipped":89,"failed":0}
------------------------------
â€¢ [0.103 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1785

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:21:44.621
    Jan 19 04:21:44.622: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename kubectl 01/19/23 04:21:44.622
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:21:44.636
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:21:44.638
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1785
    STEP: starting the proxy server 01/19/23 04:21:44.64
    Jan 19 04:21:44.640: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-2331 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 01/19/23 04:21:44.69
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 19 04:21:44.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2331" for this suite. 01/19/23 04:21:44.719
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:21:44.725
Jan 19 04:21:44.725: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename watch 01/19/23 04:21:44.726
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:21:44.74
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:21:44.741
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 01/19/23 04:21:44.743
STEP: modifying the configmap once 01/19/23 04:21:44.747
STEP: modifying the configmap a second time 01/19/23 04:21:44.756
STEP: deleting the configmap 01/19/23 04:21:44.765
STEP: creating a watch on configmaps from the resource version returned by the first update 01/19/23 04:21:44.77
STEP: Expecting to observe notifications for all changes to the configmap after the first update 01/19/23 04:21:44.77
Jan 19 04:21:44.770: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9839  466b79ac-3287-4631-ac4c-69597c0da633 4175 0 2023-01-19 04:21:44 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-19 04:21:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 19 04:21:44.771: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9839  466b79ac-3287-4631-ac4c-69597c0da633 4176 0 2023-01-19 04:21:44 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-19 04:21:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan 19 04:21:44.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9839" for this suite. 01/19/23 04:21:44.774
{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","completed":11,"skipped":91,"failed":0}
------------------------------
â€¢ [0.053 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:21:44.725
    Jan 19 04:21:44.725: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename watch 01/19/23 04:21:44.726
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:21:44.74
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:21:44.741
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 01/19/23 04:21:44.743
    STEP: modifying the configmap once 01/19/23 04:21:44.747
    STEP: modifying the configmap a second time 01/19/23 04:21:44.756
    STEP: deleting the configmap 01/19/23 04:21:44.765
    STEP: creating a watch on configmaps from the resource version returned by the first update 01/19/23 04:21:44.77
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 01/19/23 04:21:44.77
    Jan 19 04:21:44.770: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9839  466b79ac-3287-4631-ac4c-69597c0da633 4175 0 2023-01-19 04:21:44 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-19 04:21:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 19 04:21:44.771: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9839  466b79ac-3287-4631-ac4c-69597c0da633 4176 0 2023-01-19 04:21:44 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-19 04:21:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan 19 04:21:44.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-9839" for this suite. 01/19/23 04:21:44.774
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:21:44.779
Jan 19 04:21:44.779: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename configmap 01/19/23 04:21:44.78
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:21:44.794
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:21:44.796
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 19 04:21:44.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7377" for this suite. 01/19/23 04:21:44.827
{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","completed":12,"skipped":105,"failed":0}
------------------------------
â€¢ [0.054 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:21:44.779
    Jan 19 04:21:44.779: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename configmap 01/19/23 04:21:44.78
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:21:44.794
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:21:44.796
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:503
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 19 04:21:44.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7377" for this suite. 01/19/23 04:21:44.827
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:21:44.834
Jan 19 04:21:44.834: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename ephemeral-containers-test 01/19/23 04:21:44.835
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:21:44.847
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:21:44.849
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 01/19/23 04:21:44.851
Jan 19 04:21:44.858: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-5809" to be "running and ready"
Jan 19 04:21:44.860: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.207522ms
Jan 19 04:21:44.860: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 19 04:21:46.865: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007295977s
Jan 19 04:21:46.865: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 19 04:21:48.864: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.00660857s
Jan 19 04:21:48.864: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Jan 19 04:21:48.864: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 01/19/23 04:21:48.867
Jan 19 04:21:48.879: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-5809" to be "container debugger running"
Jan 19 04:21:48.884: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 5.221173ms
Jan 19 04:21:50.892: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.01256044s
Jan 19 04:21:50.892: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 01/19/23 04:21:50.892
Jan 19 04:21:50.892: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-5809 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 04:21:50.892: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
Jan 19 04:21:50.892: INFO: ExecWithOptions: Clientset creation
Jan 19 04:21:50.892: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/ephemeral-containers-test-5809/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Jan 19 04:21:51.050: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 19 04:21:51.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ephemeral-containers-test-5809" for this suite. 01/19/23 04:21:51.058
{"msg":"PASSED [sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]","completed":13,"skipped":147,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.230 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:21:44.834
    Jan 19 04:21:44.834: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename ephemeral-containers-test 01/19/23 04:21:44.835
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:21:44.847
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:21:44.849
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 01/19/23 04:21:44.851
    Jan 19 04:21:44.858: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-5809" to be "running and ready"
    Jan 19 04:21:44.860: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.207522ms
    Jan 19 04:21:44.860: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 04:21:46.865: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007295977s
    Jan 19 04:21:46.865: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 04:21:48.864: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.00660857s
    Jan 19 04:21:48.864: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Jan 19 04:21:48.864: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 01/19/23 04:21:48.867
    Jan 19 04:21:48.879: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-5809" to be "container debugger running"
    Jan 19 04:21:48.884: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 5.221173ms
    Jan 19 04:21:50.892: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.01256044s
    Jan 19 04:21:50.892: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 01/19/23 04:21:50.892
    Jan 19 04:21:50.892: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-5809 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 04:21:50.892: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    Jan 19 04:21:50.892: INFO: ExecWithOptions: Clientset creation
    Jan 19 04:21:50.892: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/ephemeral-containers-test-5809/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Jan 19 04:21:51.050: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 19 04:21:51.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ephemeral-containers-test-5809" for this suite. 01/19/23 04:21:51.058
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:21:51.065
Jan 19 04:21:51.065: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename statefulset 01/19/23 04:21:51.065
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:21:51.079
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:21:51.081
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-9807 01/19/23 04:21:51.084
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
STEP: Creating statefulset ss in namespace statefulset-9807 01/19/23 04:21:51.09
Jan 19 04:21:51.098: INFO: Found 0 stateful pods, waiting for 1
Jan 19 04:22:01.103: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 01/19/23 04:22:01.108
STEP: updating a scale subresource 01/19/23 04:22:01.111
STEP: verifying the statefulset Spec.Replicas was modified 01/19/23 04:22:01.117
STEP: Patch a scale subresource 01/19/23 04:22:01.119
STEP: verifying the statefulset Spec.Replicas was modified 01/19/23 04:22:01.133
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 19 04:22:01.137: INFO: Deleting all statefulset in ns statefulset-9807
Jan 19 04:22:01.144: INFO: Scaling statefulset ss to 0
Jan 19 04:22:11.179: INFO: Waiting for statefulset status.replicas updated to 0
Jan 19 04:22:11.182: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 19 04:22:11.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9807" for this suite. 01/19/23 04:22:11.201
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","completed":14,"skipped":163,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.145 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:846

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:21:51.065
    Jan 19 04:21:51.065: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename statefulset 01/19/23 04:21:51.065
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:21:51.079
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:21:51.081
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-9807 01/19/23 04:21:51.084
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:846
    STEP: Creating statefulset ss in namespace statefulset-9807 01/19/23 04:21:51.09
    Jan 19 04:21:51.098: INFO: Found 0 stateful pods, waiting for 1
    Jan 19 04:22:01.103: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 01/19/23 04:22:01.108
    STEP: updating a scale subresource 01/19/23 04:22:01.111
    STEP: verifying the statefulset Spec.Replicas was modified 01/19/23 04:22:01.117
    STEP: Patch a scale subresource 01/19/23 04:22:01.119
    STEP: verifying the statefulset Spec.Replicas was modified 01/19/23 04:22:01.133
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 19 04:22:01.137: INFO: Deleting all statefulset in ns statefulset-9807
    Jan 19 04:22:01.144: INFO: Scaling statefulset ss to 0
    Jan 19 04:22:11.179: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 19 04:22:11.182: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 19 04:22:11.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-9807" for this suite. 01/19/23 04:22:11.201
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:22:11.211
Jan 19 04:22:11.211: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename svc-latency 01/19/23 04:22:11.211
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:22:11.226
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:22:11.228
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Jan 19 04:22:11.230: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: creating replication controller svc-latency-rc in namespace svc-latency-5586 01/19/23 04:22:11.231
I0119 04:22:11.237946      18 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5586, replica count: 1
I0119 04:22:12.288700      18 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0119 04:22:13.288967      18 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0119 04:22:14.289095      18 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0119 04:22:15.289237      18 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 19 04:22:15.402: INFO: Created: latency-svc-6qwq9
Jan 19 04:22:15.411: INFO: Got endpoints: latency-svc-6qwq9 [22.022905ms]
Jan 19 04:22:15.432: INFO: Created: latency-svc-c9kxc
Jan 19 04:22:15.438: INFO: Got endpoints: latency-svc-c9kxc [26.459752ms]
Jan 19 04:22:15.443: INFO: Created: latency-svc-fzk2v
Jan 19 04:22:15.446: INFO: Got endpoints: latency-svc-fzk2v [34.047036ms]
Jan 19 04:22:15.456: INFO: Created: latency-svc-jxfdw
Jan 19 04:22:15.461: INFO: Got endpoints: latency-svc-jxfdw [49.138845ms]
Jan 19 04:22:15.469: INFO: Created: latency-svc-2zsfc
Jan 19 04:22:15.469: INFO: Got endpoints: latency-svc-2zsfc [56.97089ms]
Jan 19 04:22:15.476: INFO: Created: latency-svc-dtwcw
Jan 19 04:22:15.480: INFO: Got endpoints: latency-svc-dtwcw [68.400646ms]
Jan 19 04:22:15.487: INFO: Created: latency-svc-k2tmw
Jan 19 04:22:15.490: INFO: Got endpoints: latency-svc-k2tmw [78.295579ms]
Jan 19 04:22:15.495: INFO: Created: latency-svc-sd2n6
Jan 19 04:22:15.501: INFO: Got endpoints: latency-svc-sd2n6 [88.715013ms]
Jan 19 04:22:15.505: INFO: Created: latency-svc-57cbt
Jan 19 04:22:15.509: INFO: Got endpoints: latency-svc-57cbt [97.126127ms]
Jan 19 04:22:15.513: INFO: Created: latency-svc-w4zcb
Jan 19 04:22:15.516: INFO: Got endpoints: latency-svc-w4zcb [104.298099ms]
Jan 19 04:22:15.533: INFO: Created: latency-svc-h2th6
Jan 19 04:22:15.533: INFO: Got endpoints: latency-svc-h2th6 [120.659667ms]
Jan 19 04:22:15.545: INFO: Created: latency-svc-m5dzx
Jan 19 04:22:15.548: INFO: Got endpoints: latency-svc-m5dzx [135.589796ms]
Jan 19 04:22:15.553: INFO: Created: latency-svc-nvnw9
Jan 19 04:22:15.555: INFO: Got endpoints: latency-svc-nvnw9 [142.946229ms]
Jan 19 04:22:15.561: INFO: Created: latency-svc-l8fz2
Jan 19 04:22:15.567: INFO: Got endpoints: latency-svc-l8fz2 [155.013945ms]
Jan 19 04:22:15.568: INFO: Created: latency-svc-k8g5q
Jan 19 04:22:15.571: INFO: Got endpoints: latency-svc-k8g5q [159.437784ms]
Jan 19 04:22:15.576: INFO: Created: latency-svc-slj2b
Jan 19 04:22:15.581: INFO: Got endpoints: latency-svc-slj2b [168.780192ms]
Jan 19 04:22:15.585: INFO: Created: latency-svc-xrjrz
Jan 19 04:22:15.590: INFO: Got endpoints: latency-svc-xrjrz [151.804402ms]
Jan 19 04:22:15.593: INFO: Created: latency-svc-5qh5q
Jan 19 04:22:15.607: INFO: Got endpoints: latency-svc-5qh5q [161.486512ms]
Jan 19 04:22:15.620: INFO: Created: latency-svc-gn22n
Jan 19 04:22:15.643: INFO: Got endpoints: latency-svc-gn22n [182.112701ms]
Jan 19 04:22:15.647: INFO: Created: latency-svc-w7wcb
Jan 19 04:22:15.650: INFO: Got endpoints: latency-svc-w7wcb [181.113488ms]
Jan 19 04:22:15.659: INFO: Created: latency-svc-qgc9r
Jan 19 04:22:15.663: INFO: Got endpoints: latency-svc-qgc9r [183.302358ms]
Jan 19 04:22:15.670: INFO: Created: latency-svc-hpvv7
Jan 19 04:22:15.674: INFO: Got endpoints: latency-svc-hpvv7 [183.893978ms]
Jan 19 04:22:15.680: INFO: Created: latency-svc-hfmk8
Jan 19 04:22:15.686: INFO: Got endpoints: latency-svc-hfmk8 [185.273592ms]
Jan 19 04:22:15.693: INFO: Created: latency-svc-zsxnh
Jan 19 04:22:15.695: INFO: Got endpoints: latency-svc-zsxnh [186.459192ms]
Jan 19 04:22:15.702: INFO: Created: latency-svc-7r4t2
Jan 19 04:22:15.706: INFO: Got endpoints: latency-svc-7r4t2 [189.401467ms]
Jan 19 04:22:15.709: INFO: Created: latency-svc-j92p9
Jan 19 04:22:15.713: INFO: Got endpoints: latency-svc-j92p9 [180.634799ms]
Jan 19 04:22:15.717: INFO: Created: latency-svc-mgqmn
Jan 19 04:22:15.723: INFO: Got endpoints: latency-svc-mgqmn [175.828226ms]
Jan 19 04:22:15.731: INFO: Created: latency-svc-tshgl
Jan 19 04:22:15.745: INFO: Created: latency-svc-kcn8h
Jan 19 04:22:15.753: INFO: Got endpoints: latency-svc-kcn8h [186.224444ms]
Jan 19 04:22:15.753: INFO: Got endpoints: latency-svc-tshgl [198.194648ms]
Jan 19 04:22:15.756: INFO: Created: latency-svc-p7dfz
Jan 19 04:22:15.762: INFO: Got endpoints: latency-svc-p7dfz [190.865769ms]
Jan 19 04:22:15.766: INFO: Created: latency-svc-pq8jg
Jan 19 04:22:15.770: INFO: Got endpoints: latency-svc-pq8jg [188.915988ms]
Jan 19 04:22:15.774: INFO: Created: latency-svc-qhp4m
Jan 19 04:22:15.780: INFO: Got endpoints: latency-svc-qhp4m [190.696283ms]
Jan 19 04:22:15.784: INFO: Created: latency-svc-7zx4l
Jan 19 04:22:15.791: INFO: Got endpoints: latency-svc-7zx4l [183.925076ms]
Jan 19 04:22:15.794: INFO: Created: latency-svc-m5gw2
Jan 19 04:22:15.801: INFO: Got endpoints: latency-svc-m5gw2 [158.480093ms]
Jan 19 04:22:15.805: INFO: Created: latency-svc-9fwb2
Jan 19 04:22:15.807: INFO: Got endpoints: latency-svc-9fwb2 [156.536109ms]
Jan 19 04:22:15.810: INFO: Created: latency-svc-52qk7
Jan 19 04:22:15.814: INFO: Got endpoints: latency-svc-52qk7 [150.393049ms]
Jan 19 04:22:15.821: INFO: Created: latency-svc-n2745
Jan 19 04:22:15.831: INFO: Created: latency-svc-j5swq
Jan 19 04:22:15.832: INFO: Got endpoints: latency-svc-n2745 [157.51855ms]
Jan 19 04:22:15.838: INFO: Got endpoints: latency-svc-j5swq [151.927606ms]
Jan 19 04:22:15.843: INFO: Created: latency-svc-98dmh
Jan 19 04:22:15.851: INFO: Created: latency-svc-dpprz
Jan 19 04:22:15.882: INFO: Got endpoints: latency-svc-98dmh [186.136041ms]
Jan 19 04:22:15.886: INFO: Created: latency-svc-bmwbf
Jan 19 04:22:15.890: INFO: Created: latency-svc-vmctn
Jan 19 04:22:15.898: INFO: Created: latency-svc-zmx58
Jan 19 04:22:15.908: INFO: Created: latency-svc-6ngg5
Jan 19 04:22:15.909: INFO: Got endpoints: latency-svc-dpprz [203.562152ms]
Jan 19 04:22:15.915: INFO: Created: latency-svc-glg4k
Jan 19 04:22:15.922: INFO: Created: latency-svc-2bc7x
Jan 19 04:22:15.927: INFO: Created: latency-svc-jxnl5
Jan 19 04:22:15.934: INFO: Created: latency-svc-9k25c
Jan 19 04:22:15.950: INFO: Created: latency-svc-jwtc7
Jan 19 04:22:15.958: INFO: Created: latency-svc-5k2xt
Jan 19 04:22:15.962: INFO: Got endpoints: latency-svc-bmwbf [248.763409ms]
Jan 19 04:22:15.966: INFO: Created: latency-svc-67x75
Jan 19 04:22:15.976: INFO: Created: latency-svc-q8l25
Jan 19 04:22:15.980: INFO: Created: latency-svc-ckmnv
Jan 19 04:22:15.991: INFO: Created: latency-svc-cxfcg
Jan 19 04:22:15.999: INFO: Created: latency-svc-d5kxv
Jan 19 04:22:16.008: INFO: Created: latency-svc-m4t2h
Jan 19 04:22:16.010: INFO: Got endpoints: latency-svc-vmctn [287.088984ms]
Jan 19 04:22:16.020: INFO: Created: latency-svc-85l62
Jan 19 04:22:16.060: INFO: Got endpoints: latency-svc-zmx58 [306.692606ms]
Jan 19 04:22:16.069: INFO: Created: latency-svc-b728v
Jan 19 04:22:16.109: INFO: Got endpoints: latency-svc-6ngg5 [355.483223ms]
Jan 19 04:22:16.121: INFO: Created: latency-svc-x5sjj
Jan 19 04:22:16.161: INFO: Got endpoints: latency-svc-glg4k [398.774368ms]
Jan 19 04:22:16.171: INFO: Created: latency-svc-z5t6g
Jan 19 04:22:16.209: INFO: Got endpoints: latency-svc-2bc7x [438.997455ms]
Jan 19 04:22:16.220: INFO: Created: latency-svc-x9q2b
Jan 19 04:22:16.264: INFO: Got endpoints: latency-svc-jxnl5 [483.256273ms]
Jan 19 04:22:16.274: INFO: Created: latency-svc-6s2sn
Jan 19 04:22:16.310: INFO: Got endpoints: latency-svc-9k25c [518.41007ms]
Jan 19 04:22:16.322: INFO: Created: latency-svc-5rzft
Jan 19 04:22:16.361: INFO: Got endpoints: latency-svc-jwtc7 [559.169474ms]
Jan 19 04:22:16.374: INFO: Created: latency-svc-gn77n
Jan 19 04:22:16.425: INFO: Got endpoints: latency-svc-5k2xt [618.174702ms]
Jan 19 04:22:16.442: INFO: Created: latency-svc-6dpn8
Jan 19 04:22:16.460: INFO: Got endpoints: latency-svc-67x75 [646.378406ms]
Jan 19 04:22:16.474: INFO: Created: latency-svc-rmh8l
Jan 19 04:22:16.512: INFO: Got endpoints: latency-svc-q8l25 [680.143975ms]
Jan 19 04:22:16.521: INFO: Created: latency-svc-8hkgb
Jan 19 04:22:16.559: INFO: Got endpoints: latency-svc-ckmnv [720.994732ms]
Jan 19 04:22:16.569: INFO: Created: latency-svc-q7jrc
Jan 19 04:22:16.612: INFO: Got endpoints: latency-svc-cxfcg [730.654156ms]
Jan 19 04:22:16.623: INFO: Created: latency-svc-tcm9w
Jan 19 04:22:16.658: INFO: Got endpoints: latency-svc-d5kxv [748.594613ms]
Jan 19 04:22:16.669: INFO: Created: latency-svc-2czf9
Jan 19 04:22:16.710: INFO: Got endpoints: latency-svc-m4t2h [747.979322ms]
Jan 19 04:22:16.721: INFO: Created: latency-svc-f2qll
Jan 19 04:22:16.759: INFO: Got endpoints: latency-svc-85l62 [748.753813ms]
Jan 19 04:22:16.769: INFO: Created: latency-svc-l4xx2
Jan 19 04:22:16.813: INFO: Got endpoints: latency-svc-b728v [752.947845ms]
Jan 19 04:22:16.826: INFO: Created: latency-svc-pjxgh
Jan 19 04:22:16.859: INFO: Got endpoints: latency-svc-x5sjj [750.179842ms]
Jan 19 04:22:16.869: INFO: Created: latency-svc-5qbtf
Jan 19 04:22:16.908: INFO: Got endpoints: latency-svc-z5t6g [747.546794ms]
Jan 19 04:22:16.918: INFO: Created: latency-svc-jtlts
Jan 19 04:22:16.960: INFO: Got endpoints: latency-svc-x9q2b [751.039862ms]
Jan 19 04:22:16.969: INFO: Created: latency-svc-7pd2m
Jan 19 04:22:17.009: INFO: Got endpoints: latency-svc-6s2sn [744.977614ms]
Jan 19 04:22:17.031: INFO: Created: latency-svc-hcwd5
Jan 19 04:22:17.059: INFO: Got endpoints: latency-svc-5rzft [749.135887ms]
Jan 19 04:22:17.068: INFO: Created: latency-svc-n72sj
Jan 19 04:22:17.109: INFO: Got endpoints: latency-svc-gn77n [747.923303ms]
Jan 19 04:22:17.120: INFO: Created: latency-svc-zr889
Jan 19 04:22:17.210: INFO: Got endpoints: latency-svc-6dpn8 [784.815161ms]
Jan 19 04:22:17.222: INFO: Created: latency-svc-h29wq
Jan 19 04:22:17.259: INFO: Got endpoints: latency-svc-rmh8l [799.109776ms]
Jan 19 04:22:17.268: INFO: Created: latency-svc-rf2d9
Jan 19 04:22:17.311: INFO: Got endpoints: latency-svc-8hkgb [798.823626ms]
Jan 19 04:22:17.328: INFO: Created: latency-svc-2pl27
Jan 19 04:22:17.360: INFO: Got endpoints: latency-svc-q7jrc [800.972541ms]
Jan 19 04:22:17.369: INFO: Created: latency-svc-tsgdt
Jan 19 04:22:17.409: INFO: Got endpoints: latency-svc-tcm9w [796.896535ms]
Jan 19 04:22:17.422: INFO: Created: latency-svc-sqhjk
Jan 19 04:22:17.458: INFO: Got endpoints: latency-svc-2czf9 [800.582752ms]
Jan 19 04:22:17.470: INFO: Created: latency-svc-2xh72
Jan 19 04:22:17.509: INFO: Got endpoints: latency-svc-f2qll [799.307008ms]
Jan 19 04:22:17.541: INFO: Created: latency-svc-ggckb
Jan 19 04:22:17.561: INFO: Got endpoints: latency-svc-l4xx2 [801.733622ms]
Jan 19 04:22:17.571: INFO: Created: latency-svc-td8t6
Jan 19 04:22:17.611: INFO: Got endpoints: latency-svc-pjxgh [798.765141ms]
Jan 19 04:22:17.622: INFO: Created: latency-svc-6qbxc
Jan 19 04:22:17.661: INFO: Got endpoints: latency-svc-5qbtf [801.866786ms]
Jan 19 04:22:17.671: INFO: Created: latency-svc-gzz9b
Jan 19 04:22:17.709: INFO: Got endpoints: latency-svc-jtlts [800.619241ms]
Jan 19 04:22:17.719: INFO: Created: latency-svc-shktk
Jan 19 04:22:17.760: INFO: Got endpoints: latency-svc-7pd2m [799.904183ms]
Jan 19 04:22:17.769: INFO: Created: latency-svc-dcq5r
Jan 19 04:22:17.810: INFO: Got endpoints: latency-svc-hcwd5 [801.206287ms]
Jan 19 04:22:17.819: INFO: Created: latency-svc-qq4pm
Jan 19 04:22:17.859: INFO: Got endpoints: latency-svc-n72sj [800.558623ms]
Jan 19 04:22:17.869: INFO: Created: latency-svc-d6prz
Jan 19 04:22:17.910: INFO: Got endpoints: latency-svc-zr889 [800.937103ms]
Jan 19 04:22:17.920: INFO: Created: latency-svc-xmgs4
Jan 19 04:22:17.959: INFO: Got endpoints: latency-svc-h29wq [749.69063ms]
Jan 19 04:22:17.970: INFO: Created: latency-svc-748gm
Jan 19 04:22:18.010: INFO: Got endpoints: latency-svc-rf2d9 [750.133914ms]
Jan 19 04:22:18.020: INFO: Created: latency-svc-cd4c9
Jan 19 04:22:18.060: INFO: Got endpoints: latency-svc-2pl27 [749.015461ms]
Jan 19 04:22:18.070: INFO: Created: latency-svc-jdlpd
Jan 19 04:22:18.109: INFO: Got endpoints: latency-svc-tsgdt [749.386822ms]
Jan 19 04:22:18.119: INFO: Created: latency-svc-j5tsd
Jan 19 04:22:18.159: INFO: Got endpoints: latency-svc-sqhjk [749.553309ms]
Jan 19 04:22:18.171: INFO: Created: latency-svc-fjgxx
Jan 19 04:22:18.209: INFO: Got endpoints: latency-svc-2xh72 [750.638141ms]
Jan 19 04:22:18.220: INFO: Created: latency-svc-pp9bb
Jan 19 04:22:18.276: INFO: Got endpoints: latency-svc-ggckb [766.469723ms]
Jan 19 04:22:18.288: INFO: Created: latency-svc-w9nnb
Jan 19 04:22:18.309: INFO: Got endpoints: latency-svc-td8t6 [747.770458ms]
Jan 19 04:22:18.319: INFO: Created: latency-svc-2hd4c
Jan 19 04:22:18.358: INFO: Got endpoints: latency-svc-6qbxc [747.023077ms]
Jan 19 04:22:18.370: INFO: Created: latency-svc-rpvj8
Jan 19 04:22:18.408: INFO: Got endpoints: latency-svc-gzz9b [747.283441ms]
Jan 19 04:22:18.419: INFO: Created: latency-svc-t22mh
Jan 19 04:22:18.460: INFO: Got endpoints: latency-svc-shktk [750.767839ms]
Jan 19 04:22:18.470: INFO: Created: latency-svc-ltn55
Jan 19 04:22:18.508: INFO: Got endpoints: latency-svc-dcq5r [748.536396ms]
Jan 19 04:22:18.528: INFO: Created: latency-svc-zftrb
Jan 19 04:22:18.561: INFO: Got endpoints: latency-svc-qq4pm [751.509891ms]
Jan 19 04:22:18.572: INFO: Created: latency-svc-bwwn8
Jan 19 04:22:18.611: INFO: Got endpoints: latency-svc-d6prz [751.536729ms]
Jan 19 04:22:18.621: INFO: Created: latency-svc-mrjlj
Jan 19 04:22:18.663: INFO: Got endpoints: latency-svc-xmgs4 [753.714507ms]
Jan 19 04:22:18.674: INFO: Created: latency-svc-s8nbq
Jan 19 04:22:18.708: INFO: Got endpoints: latency-svc-748gm [748.373076ms]
Jan 19 04:22:18.719: INFO: Created: latency-svc-86ghg
Jan 19 04:22:18.759: INFO: Got endpoints: latency-svc-cd4c9 [749.385363ms]
Jan 19 04:22:18.769: INFO: Created: latency-svc-tkqg9
Jan 19 04:22:18.809: INFO: Got endpoints: latency-svc-jdlpd [749.289475ms]
Jan 19 04:22:18.819: INFO: Created: latency-svc-hcrsh
Jan 19 04:22:18.861: INFO: Got endpoints: latency-svc-j5tsd [752.188098ms]
Jan 19 04:22:18.872: INFO: Created: latency-svc-8ggcw
Jan 19 04:22:18.911: INFO: Got endpoints: latency-svc-fjgxx [752.173231ms]
Jan 19 04:22:18.921: INFO: Created: latency-svc-64vww
Jan 19 04:22:18.958: INFO: Got endpoints: latency-svc-pp9bb [749.326545ms]
Jan 19 04:22:18.971: INFO: Created: latency-svc-8nhcf
Jan 19 04:22:19.010: INFO: Got endpoints: latency-svc-w9nnb [734.356229ms]
Jan 19 04:22:19.021: INFO: Created: latency-svc-86kzl
Jan 19 04:22:19.058: INFO: Got endpoints: latency-svc-2hd4c [749.431378ms]
Jan 19 04:22:19.068: INFO: Created: latency-svc-lrrzp
Jan 19 04:22:19.109: INFO: Got endpoints: latency-svc-rpvj8 [750.89196ms]
Jan 19 04:22:19.120: INFO: Created: latency-svc-9pmdz
Jan 19 04:22:19.159: INFO: Got endpoints: latency-svc-t22mh [751.151804ms]
Jan 19 04:22:19.175: INFO: Created: latency-svc-9mhrl
Jan 19 04:22:19.209: INFO: Got endpoints: latency-svc-ltn55 [749.688671ms]
Jan 19 04:22:19.221: INFO: Created: latency-svc-tpcsb
Jan 19 04:22:19.260: INFO: Got endpoints: latency-svc-zftrb [751.336106ms]
Jan 19 04:22:19.270: INFO: Created: latency-svc-v9lzd
Jan 19 04:22:19.309: INFO: Got endpoints: latency-svc-bwwn8 [747.83441ms]
Jan 19 04:22:19.321: INFO: Created: latency-svc-xh4dx
Jan 19 04:22:19.361: INFO: Got endpoints: latency-svc-mrjlj [750.292682ms]
Jan 19 04:22:19.371: INFO: Created: latency-svc-vtzp8
Jan 19 04:22:19.411: INFO: Got endpoints: latency-svc-s8nbq [747.758948ms]
Jan 19 04:22:19.423: INFO: Created: latency-svc-7n4gk
Jan 19 04:22:19.460: INFO: Got endpoints: latency-svc-86ghg [752.208243ms]
Jan 19 04:22:19.474: INFO: Created: latency-svc-h5vb9
Jan 19 04:22:19.510: INFO: Got endpoints: latency-svc-tkqg9 [750.802409ms]
Jan 19 04:22:19.522: INFO: Created: latency-svc-vg9d5
Jan 19 04:22:19.562: INFO: Got endpoints: latency-svc-hcrsh [752.629865ms]
Jan 19 04:22:19.572: INFO: Created: latency-svc-8lk2h
Jan 19 04:22:19.620: INFO: Got endpoints: latency-svc-8ggcw [758.176969ms]
Jan 19 04:22:19.629: INFO: Created: latency-svc-2lfnk
Jan 19 04:22:19.660: INFO: Got endpoints: latency-svc-64vww [749.329747ms]
Jan 19 04:22:19.671: INFO: Created: latency-svc-986r9
Jan 19 04:22:19.709: INFO: Got endpoints: latency-svc-8nhcf [750.644902ms]
Jan 19 04:22:19.725: INFO: Created: latency-svc-cn7dg
Jan 19 04:22:19.760: INFO: Got endpoints: latency-svc-86kzl [749.4218ms]
Jan 19 04:22:19.771: INFO: Created: latency-svc-p6vrb
Jan 19 04:22:19.810: INFO: Got endpoints: latency-svc-lrrzp [751.644229ms]
Jan 19 04:22:19.821: INFO: Created: latency-svc-pt57t
Jan 19 04:22:19.861: INFO: Got endpoints: latency-svc-9pmdz [751.549423ms]
Jan 19 04:22:19.871: INFO: Created: latency-svc-ldqfn
Jan 19 04:22:19.911: INFO: Got endpoints: latency-svc-9mhrl [751.610793ms]
Jan 19 04:22:19.922: INFO: Created: latency-svc-hgjz5
Jan 19 04:22:19.961: INFO: Got endpoints: latency-svc-tpcsb [751.434726ms]
Jan 19 04:22:19.971: INFO: Created: latency-svc-6nzpf
Jan 19 04:22:20.011: INFO: Got endpoints: latency-svc-v9lzd [751.289306ms]
Jan 19 04:22:20.021: INFO: Created: latency-svc-nmzl6
Jan 19 04:22:20.064: INFO: Got endpoints: latency-svc-xh4dx [754.548177ms]
Jan 19 04:22:20.081: INFO: Created: latency-svc-l45pv
Jan 19 04:22:20.111: INFO: Got endpoints: latency-svc-vtzp8 [750.132227ms]
Jan 19 04:22:20.122: INFO: Created: latency-svc-hf5l9
Jan 19 04:22:20.159: INFO: Got endpoints: latency-svc-7n4gk [747.542725ms]
Jan 19 04:22:20.170: INFO: Created: latency-svc-d4hxn
Jan 19 04:22:20.210: INFO: Got endpoints: latency-svc-h5vb9 [749.90068ms]
Jan 19 04:22:20.221: INFO: Created: latency-svc-fxn8x
Jan 19 04:22:20.262: INFO: Got endpoints: latency-svc-vg9d5 [751.954695ms]
Jan 19 04:22:20.270: INFO: Created: latency-svc-h5bwj
Jan 19 04:22:20.310: INFO: Got endpoints: latency-svc-8lk2h [748.537258ms]
Jan 19 04:22:20.321: INFO: Created: latency-svc-jfd4h
Jan 19 04:22:20.360: INFO: Got endpoints: latency-svc-2lfnk [740.158032ms]
Jan 19 04:22:20.371: INFO: Created: latency-svc-fz7tg
Jan 19 04:22:20.411: INFO: Got endpoints: latency-svc-986r9 [750.250209ms]
Jan 19 04:22:20.423: INFO: Created: latency-svc-kjmrc
Jan 19 04:22:20.462: INFO: Got endpoints: latency-svc-cn7dg [752.669975ms]
Jan 19 04:22:20.475: INFO: Created: latency-svc-whnh7
Jan 19 04:22:20.515: INFO: Got endpoints: latency-svc-p6vrb [754.843575ms]
Jan 19 04:22:20.527: INFO: Created: latency-svc-gsl2b
Jan 19 04:22:20.561: INFO: Got endpoints: latency-svc-pt57t [751.263988ms]
Jan 19 04:22:20.572: INFO: Created: latency-svc-2484l
Jan 19 04:22:20.612: INFO: Got endpoints: latency-svc-ldqfn [750.550522ms]
Jan 19 04:22:20.627: INFO: Created: latency-svc-vbvxh
Jan 19 04:22:20.660: INFO: Got endpoints: latency-svc-hgjz5 [749.596542ms]
Jan 19 04:22:20.672: INFO: Created: latency-svc-fjmh2
Jan 19 04:22:20.712: INFO: Got endpoints: latency-svc-6nzpf [751.123404ms]
Jan 19 04:22:20.724: INFO: Created: latency-svc-wnqs7
Jan 19 04:22:20.759: INFO: Got endpoints: latency-svc-nmzl6 [747.864535ms]
Jan 19 04:22:20.771: INFO: Created: latency-svc-vdh27
Jan 19 04:22:20.813: INFO: Got endpoints: latency-svc-l45pv [749.191442ms]
Jan 19 04:22:20.823: INFO: Created: latency-svc-vhh2n
Jan 19 04:22:20.861: INFO: Got endpoints: latency-svc-hf5l9 [749.557388ms]
Jan 19 04:22:20.871: INFO: Created: latency-svc-mtmzp
Jan 19 04:22:20.910: INFO: Got endpoints: latency-svc-d4hxn [751.476867ms]
Jan 19 04:22:20.923: INFO: Created: latency-svc-dg2ft
Jan 19 04:22:20.958: INFO: Got endpoints: latency-svc-fxn8x [748.303016ms]
Jan 19 04:22:20.971: INFO: Created: latency-svc-kt842
Jan 19 04:22:21.009: INFO: Got endpoints: latency-svc-h5bwj [747.611797ms]
Jan 19 04:22:21.021: INFO: Created: latency-svc-s9k7k
Jan 19 04:22:21.059: INFO: Got endpoints: latency-svc-jfd4h [749.308508ms]
Jan 19 04:22:21.069: INFO: Created: latency-svc-mbvmg
Jan 19 04:22:21.109: INFO: Got endpoints: latency-svc-fz7tg [749.254017ms]
Jan 19 04:22:21.125: INFO: Created: latency-svc-rcdgf
Jan 19 04:22:21.161: INFO: Got endpoints: latency-svc-kjmrc [750.19431ms]
Jan 19 04:22:21.170: INFO: Created: latency-svc-mhqmc
Jan 19 04:22:21.209: INFO: Got endpoints: latency-svc-whnh7 [747.354605ms]
Jan 19 04:22:21.229: INFO: Created: latency-svc-n8mt7
Jan 19 04:22:21.260: INFO: Got endpoints: latency-svc-gsl2b [745.340609ms]
Jan 19 04:22:21.271: INFO: Created: latency-svc-9nkcr
Jan 19 04:22:21.309: INFO: Got endpoints: latency-svc-2484l [747.57844ms]
Jan 19 04:22:21.326: INFO: Created: latency-svc-hll29
Jan 19 04:22:21.360: INFO: Got endpoints: latency-svc-vbvxh [748.342736ms]
Jan 19 04:22:21.370: INFO: Created: latency-svc-2cqc8
Jan 19 04:22:21.408: INFO: Got endpoints: latency-svc-fjmh2 [747.722157ms]
Jan 19 04:22:21.422: INFO: Created: latency-svc-sl27b
Jan 19 04:22:21.462: INFO: Got endpoints: latency-svc-wnqs7 [749.65505ms]
Jan 19 04:22:21.472: INFO: Created: latency-svc-bltxq
Jan 19 04:22:21.518: INFO: Got endpoints: latency-svc-vdh27 [758.702185ms]
Jan 19 04:22:21.528: INFO: Created: latency-svc-vnt6q
Jan 19 04:22:21.566: INFO: Got endpoints: latency-svc-vhh2n [753.326306ms]
Jan 19 04:22:21.590: INFO: Created: latency-svc-j7zck
Jan 19 04:22:21.632: INFO: Got endpoints: latency-svc-mtmzp [770.986841ms]
Jan 19 04:22:21.650: INFO: Created: latency-svc-74lts
Jan 19 04:22:21.660: INFO: Got endpoints: latency-svc-dg2ft [749.246919ms]
Jan 19 04:22:21.671: INFO: Created: latency-svc-k55l5
Jan 19 04:22:21.708: INFO: Got endpoints: latency-svc-kt842 [749.391656ms]
Jan 19 04:22:21.718: INFO: Created: latency-svc-bctv7
Jan 19 04:22:21.759: INFO: Got endpoints: latency-svc-s9k7k [749.946882ms]
Jan 19 04:22:21.770: INFO: Created: latency-svc-f25fj
Jan 19 04:22:21.810: INFO: Got endpoints: latency-svc-mbvmg [750.358239ms]
Jan 19 04:22:21.821: INFO: Created: latency-svc-pvxp6
Jan 19 04:22:21.859: INFO: Got endpoints: latency-svc-rcdgf [750.074577ms]
Jan 19 04:22:21.870: INFO: Created: latency-svc-jrhdm
Jan 19 04:22:21.910: INFO: Got endpoints: latency-svc-mhqmc [748.806437ms]
Jan 19 04:22:21.920: INFO: Created: latency-svc-bztkw
Jan 19 04:22:21.960: INFO: Got endpoints: latency-svc-n8mt7 [750.675046ms]
Jan 19 04:22:21.970: INFO: Created: latency-svc-xgfw5
Jan 19 04:22:22.011: INFO: Got endpoints: latency-svc-9nkcr [750.731612ms]
Jan 19 04:22:22.020: INFO: Created: latency-svc-k6x8g
Jan 19 04:22:22.059: INFO: Got endpoints: latency-svc-hll29 [750.110872ms]
Jan 19 04:22:22.069: INFO: Created: latency-svc-4s9ln
Jan 19 04:22:22.109: INFO: Got endpoints: latency-svc-2cqc8 [749.066888ms]
Jan 19 04:22:22.121: INFO: Created: latency-svc-bdv7z
Jan 19 04:22:22.160: INFO: Got endpoints: latency-svc-sl27b [751.377175ms]
Jan 19 04:22:22.170: INFO: Created: latency-svc-mkbbf
Jan 19 04:22:22.211: INFO: Got endpoints: latency-svc-bltxq [748.742945ms]
Jan 19 04:22:22.221: INFO: Created: latency-svc-8t8bk
Jan 19 04:22:22.261: INFO: Got endpoints: latency-svc-vnt6q [743.051834ms]
Jan 19 04:22:22.271: INFO: Created: latency-svc-w7xgt
Jan 19 04:22:22.311: INFO: Got endpoints: latency-svc-j7zck [744.855562ms]
Jan 19 04:22:22.321: INFO: Created: latency-svc-lhd4n
Jan 19 04:22:22.358: INFO: Got endpoints: latency-svc-74lts [726.353787ms]
Jan 19 04:22:22.369: INFO: Created: latency-svc-xkspc
Jan 19 04:22:22.409: INFO: Got endpoints: latency-svc-k55l5 [749.400946ms]
Jan 19 04:22:22.419: INFO: Created: latency-svc-pzv5g
Jan 19 04:22:22.462: INFO: Got endpoints: latency-svc-bctv7 [753.899992ms]
Jan 19 04:22:22.473: INFO: Created: latency-svc-mxnmr
Jan 19 04:22:22.511: INFO: Got endpoints: latency-svc-f25fj [751.733088ms]
Jan 19 04:22:22.525: INFO: Created: latency-svc-wkdvw
Jan 19 04:22:22.562: INFO: Got endpoints: latency-svc-pvxp6 [752.413956ms]
Jan 19 04:22:22.573: INFO: Created: latency-svc-s5rjv
Jan 19 04:22:22.610: INFO: Got endpoints: latency-svc-jrhdm [750.592513ms]
Jan 19 04:22:22.622: INFO: Created: latency-svc-9xn9t
Jan 19 04:22:22.671: INFO: Got endpoints: latency-svc-bztkw [761.12411ms]
Jan 19 04:22:22.686: INFO: Created: latency-svc-xr6tz
Jan 19 04:22:22.708: INFO: Got endpoints: latency-svc-xgfw5 [747.824536ms]
Jan 19 04:22:22.718: INFO: Created: latency-svc-86cm6
Jan 19 04:22:22.760: INFO: Got endpoints: latency-svc-k6x8g [748.973689ms]
Jan 19 04:22:22.770: INFO: Created: latency-svc-78bb6
Jan 19 04:22:22.810: INFO: Got endpoints: latency-svc-4s9ln [750.946158ms]
Jan 19 04:22:22.819: INFO: Created: latency-svc-v6rrc
Jan 19 04:22:22.860: INFO: Got endpoints: latency-svc-bdv7z [750.572144ms]
Jan 19 04:22:22.870: INFO: Created: latency-svc-s6sb4
Jan 19 04:22:22.909: INFO: Got endpoints: latency-svc-mkbbf [749.638789ms]
Jan 19 04:22:22.919: INFO: Created: latency-svc-22jjj
Jan 19 04:22:22.960: INFO: Got endpoints: latency-svc-8t8bk [749.186011ms]
Jan 19 04:22:22.970: INFO: Created: latency-svc-56ptd
Jan 19 04:22:23.011: INFO: Got endpoints: latency-svc-w7xgt [750.193504ms]
Jan 19 04:22:23.021: INFO: Created: latency-svc-vr65n
Jan 19 04:22:23.059: INFO: Got endpoints: latency-svc-lhd4n [747.252267ms]
Jan 19 04:22:23.081: INFO: Created: latency-svc-shfgw
Jan 19 04:22:23.109: INFO: Got endpoints: latency-svc-xkspc [750.932345ms]
Jan 19 04:22:23.120: INFO: Created: latency-svc-sg7f4
Jan 19 04:22:23.160: INFO: Got endpoints: latency-svc-pzv5g [751.115626ms]
Jan 19 04:22:23.183: INFO: Created: latency-svc-8ctt9
Jan 19 04:22:23.210: INFO: Got endpoints: latency-svc-mxnmr [747.781731ms]
Jan 19 04:22:23.219: INFO: Created: latency-svc-968sn
Jan 19 04:22:23.261: INFO: Got endpoints: latency-svc-wkdvw [749.707372ms]
Jan 19 04:22:23.270: INFO: Created: latency-svc-thhxz
Jan 19 04:22:23.310: INFO: Got endpoints: latency-svc-s5rjv [747.493311ms]
Jan 19 04:22:23.360: INFO: Got endpoints: latency-svc-9xn9t [750.640645ms]
Jan 19 04:22:23.409: INFO: Got endpoints: latency-svc-xr6tz [737.866698ms]
Jan 19 04:22:23.463: INFO: Got endpoints: latency-svc-86cm6 [754.819429ms]
Jan 19 04:22:23.509: INFO: Got endpoints: latency-svc-78bb6 [749.712223ms]
Jan 19 04:22:23.559: INFO: Got endpoints: latency-svc-v6rrc [748.712443ms]
Jan 19 04:22:23.610: INFO: Got endpoints: latency-svc-s6sb4 [750.070331ms]
Jan 19 04:22:23.659: INFO: Got endpoints: latency-svc-22jjj [749.639712ms]
Jan 19 04:22:23.710: INFO: Got endpoints: latency-svc-56ptd [749.844689ms]
Jan 19 04:22:23.759: INFO: Got endpoints: latency-svc-vr65n [748.275511ms]
Jan 19 04:22:23.814: INFO: Got endpoints: latency-svc-shfgw [755.16679ms]
Jan 19 04:22:23.860: INFO: Got endpoints: latency-svc-sg7f4 [751.007364ms]
Jan 19 04:22:23.912: INFO: Got endpoints: latency-svc-8ctt9 [752.282488ms]
Jan 19 04:22:23.961: INFO: Got endpoints: latency-svc-968sn [751.017106ms]
Jan 19 04:22:24.009: INFO: Got endpoints: latency-svc-thhxz [748.168564ms]
Jan 19 04:22:24.009: INFO: Latencies: [26.459752ms 34.047036ms 49.138845ms 56.97089ms 68.400646ms 78.295579ms 88.715013ms 97.126127ms 104.298099ms 120.659667ms 135.589796ms 142.946229ms 150.393049ms 151.804402ms 151.927606ms 155.013945ms 156.536109ms 157.51855ms 158.480093ms 159.437784ms 161.486512ms 168.780192ms 175.828226ms 180.634799ms 181.113488ms 182.112701ms 183.302358ms 183.893978ms 183.925076ms 185.273592ms 186.136041ms 186.224444ms 186.459192ms 188.915988ms 189.401467ms 190.696283ms 190.865769ms 198.194648ms 203.562152ms 248.763409ms 287.088984ms 306.692606ms 355.483223ms 398.774368ms 438.997455ms 483.256273ms 518.41007ms 559.169474ms 618.174702ms 646.378406ms 680.143975ms 720.994732ms 726.353787ms 730.654156ms 734.356229ms 737.866698ms 740.158032ms 743.051834ms 744.855562ms 744.977614ms 745.340609ms 747.023077ms 747.252267ms 747.283441ms 747.354605ms 747.493311ms 747.542725ms 747.546794ms 747.57844ms 747.611797ms 747.722157ms 747.758948ms 747.770458ms 747.781731ms 747.824536ms 747.83441ms 747.864535ms 747.923303ms 747.979322ms 748.168564ms 748.275511ms 748.303016ms 748.342736ms 748.373076ms 748.536396ms 748.537258ms 748.594613ms 748.712443ms 748.742945ms 748.753813ms 748.806437ms 748.973689ms 749.015461ms 749.066888ms 749.135887ms 749.186011ms 749.191442ms 749.246919ms 749.254017ms 749.289475ms 749.308508ms 749.326545ms 749.329747ms 749.385363ms 749.386822ms 749.391656ms 749.400946ms 749.4218ms 749.431378ms 749.553309ms 749.557388ms 749.596542ms 749.638789ms 749.639712ms 749.65505ms 749.688671ms 749.69063ms 749.707372ms 749.712223ms 749.844689ms 749.90068ms 749.946882ms 750.070331ms 750.074577ms 750.110872ms 750.132227ms 750.133914ms 750.179842ms 750.193504ms 750.19431ms 750.250209ms 750.292682ms 750.358239ms 750.550522ms 750.572144ms 750.592513ms 750.638141ms 750.640645ms 750.644902ms 750.675046ms 750.731612ms 750.767839ms 750.802409ms 750.89196ms 750.932345ms 750.946158ms 751.007364ms 751.017106ms 751.039862ms 751.115626ms 751.123404ms 751.151804ms 751.263988ms 751.289306ms 751.336106ms 751.377175ms 751.434726ms 751.476867ms 751.509891ms 751.536729ms 751.549423ms 751.610793ms 751.644229ms 751.733088ms 751.954695ms 752.173231ms 752.188098ms 752.208243ms 752.282488ms 752.413956ms 752.629865ms 752.669975ms 752.947845ms 753.326306ms 753.714507ms 753.899992ms 754.548177ms 754.819429ms 754.843575ms 755.16679ms 758.176969ms 758.702185ms 761.12411ms 766.469723ms 770.986841ms 784.815161ms 796.896535ms 798.765141ms 798.823626ms 799.109776ms 799.307008ms 799.904183ms 800.558623ms 800.582752ms 800.619241ms 800.937103ms 800.972541ms 801.206287ms 801.733622ms 801.866786ms]
Jan 19 04:22:24.009: INFO: 50 %ile: 749.308508ms
Jan 19 04:22:24.009: INFO: 90 %ile: 758.176969ms
Jan 19 04:22:24.009: INFO: 99 %ile: 801.733622ms
Jan 19 04:22:24.009: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
Jan 19 04:22:24.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-5586" for this suite. 01/19/23 04:22:24.014
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","completed":15,"skipped":202,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.810 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:22:11.211
    Jan 19 04:22:11.211: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename svc-latency 01/19/23 04:22:11.211
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:22:11.226
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:22:11.228
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Jan 19 04:22:11.230: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-5586 01/19/23 04:22:11.231
    I0119 04:22:11.237946      18 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5586, replica count: 1
    I0119 04:22:12.288700      18 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0119 04:22:13.288967      18 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0119 04:22:14.289095      18 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0119 04:22:15.289237      18 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 19 04:22:15.402: INFO: Created: latency-svc-6qwq9
    Jan 19 04:22:15.411: INFO: Got endpoints: latency-svc-6qwq9 [22.022905ms]
    Jan 19 04:22:15.432: INFO: Created: latency-svc-c9kxc
    Jan 19 04:22:15.438: INFO: Got endpoints: latency-svc-c9kxc [26.459752ms]
    Jan 19 04:22:15.443: INFO: Created: latency-svc-fzk2v
    Jan 19 04:22:15.446: INFO: Got endpoints: latency-svc-fzk2v [34.047036ms]
    Jan 19 04:22:15.456: INFO: Created: latency-svc-jxfdw
    Jan 19 04:22:15.461: INFO: Got endpoints: latency-svc-jxfdw [49.138845ms]
    Jan 19 04:22:15.469: INFO: Created: latency-svc-2zsfc
    Jan 19 04:22:15.469: INFO: Got endpoints: latency-svc-2zsfc [56.97089ms]
    Jan 19 04:22:15.476: INFO: Created: latency-svc-dtwcw
    Jan 19 04:22:15.480: INFO: Got endpoints: latency-svc-dtwcw [68.400646ms]
    Jan 19 04:22:15.487: INFO: Created: latency-svc-k2tmw
    Jan 19 04:22:15.490: INFO: Got endpoints: latency-svc-k2tmw [78.295579ms]
    Jan 19 04:22:15.495: INFO: Created: latency-svc-sd2n6
    Jan 19 04:22:15.501: INFO: Got endpoints: latency-svc-sd2n6 [88.715013ms]
    Jan 19 04:22:15.505: INFO: Created: latency-svc-57cbt
    Jan 19 04:22:15.509: INFO: Got endpoints: latency-svc-57cbt [97.126127ms]
    Jan 19 04:22:15.513: INFO: Created: latency-svc-w4zcb
    Jan 19 04:22:15.516: INFO: Got endpoints: latency-svc-w4zcb [104.298099ms]
    Jan 19 04:22:15.533: INFO: Created: latency-svc-h2th6
    Jan 19 04:22:15.533: INFO: Got endpoints: latency-svc-h2th6 [120.659667ms]
    Jan 19 04:22:15.545: INFO: Created: latency-svc-m5dzx
    Jan 19 04:22:15.548: INFO: Got endpoints: latency-svc-m5dzx [135.589796ms]
    Jan 19 04:22:15.553: INFO: Created: latency-svc-nvnw9
    Jan 19 04:22:15.555: INFO: Got endpoints: latency-svc-nvnw9 [142.946229ms]
    Jan 19 04:22:15.561: INFO: Created: latency-svc-l8fz2
    Jan 19 04:22:15.567: INFO: Got endpoints: latency-svc-l8fz2 [155.013945ms]
    Jan 19 04:22:15.568: INFO: Created: latency-svc-k8g5q
    Jan 19 04:22:15.571: INFO: Got endpoints: latency-svc-k8g5q [159.437784ms]
    Jan 19 04:22:15.576: INFO: Created: latency-svc-slj2b
    Jan 19 04:22:15.581: INFO: Got endpoints: latency-svc-slj2b [168.780192ms]
    Jan 19 04:22:15.585: INFO: Created: latency-svc-xrjrz
    Jan 19 04:22:15.590: INFO: Got endpoints: latency-svc-xrjrz [151.804402ms]
    Jan 19 04:22:15.593: INFO: Created: latency-svc-5qh5q
    Jan 19 04:22:15.607: INFO: Got endpoints: latency-svc-5qh5q [161.486512ms]
    Jan 19 04:22:15.620: INFO: Created: latency-svc-gn22n
    Jan 19 04:22:15.643: INFO: Got endpoints: latency-svc-gn22n [182.112701ms]
    Jan 19 04:22:15.647: INFO: Created: latency-svc-w7wcb
    Jan 19 04:22:15.650: INFO: Got endpoints: latency-svc-w7wcb [181.113488ms]
    Jan 19 04:22:15.659: INFO: Created: latency-svc-qgc9r
    Jan 19 04:22:15.663: INFO: Got endpoints: latency-svc-qgc9r [183.302358ms]
    Jan 19 04:22:15.670: INFO: Created: latency-svc-hpvv7
    Jan 19 04:22:15.674: INFO: Got endpoints: latency-svc-hpvv7 [183.893978ms]
    Jan 19 04:22:15.680: INFO: Created: latency-svc-hfmk8
    Jan 19 04:22:15.686: INFO: Got endpoints: latency-svc-hfmk8 [185.273592ms]
    Jan 19 04:22:15.693: INFO: Created: latency-svc-zsxnh
    Jan 19 04:22:15.695: INFO: Got endpoints: latency-svc-zsxnh [186.459192ms]
    Jan 19 04:22:15.702: INFO: Created: latency-svc-7r4t2
    Jan 19 04:22:15.706: INFO: Got endpoints: latency-svc-7r4t2 [189.401467ms]
    Jan 19 04:22:15.709: INFO: Created: latency-svc-j92p9
    Jan 19 04:22:15.713: INFO: Got endpoints: latency-svc-j92p9 [180.634799ms]
    Jan 19 04:22:15.717: INFO: Created: latency-svc-mgqmn
    Jan 19 04:22:15.723: INFO: Got endpoints: latency-svc-mgqmn [175.828226ms]
    Jan 19 04:22:15.731: INFO: Created: latency-svc-tshgl
    Jan 19 04:22:15.745: INFO: Created: latency-svc-kcn8h
    Jan 19 04:22:15.753: INFO: Got endpoints: latency-svc-kcn8h [186.224444ms]
    Jan 19 04:22:15.753: INFO: Got endpoints: latency-svc-tshgl [198.194648ms]
    Jan 19 04:22:15.756: INFO: Created: latency-svc-p7dfz
    Jan 19 04:22:15.762: INFO: Got endpoints: latency-svc-p7dfz [190.865769ms]
    Jan 19 04:22:15.766: INFO: Created: latency-svc-pq8jg
    Jan 19 04:22:15.770: INFO: Got endpoints: latency-svc-pq8jg [188.915988ms]
    Jan 19 04:22:15.774: INFO: Created: latency-svc-qhp4m
    Jan 19 04:22:15.780: INFO: Got endpoints: latency-svc-qhp4m [190.696283ms]
    Jan 19 04:22:15.784: INFO: Created: latency-svc-7zx4l
    Jan 19 04:22:15.791: INFO: Got endpoints: latency-svc-7zx4l [183.925076ms]
    Jan 19 04:22:15.794: INFO: Created: latency-svc-m5gw2
    Jan 19 04:22:15.801: INFO: Got endpoints: latency-svc-m5gw2 [158.480093ms]
    Jan 19 04:22:15.805: INFO: Created: latency-svc-9fwb2
    Jan 19 04:22:15.807: INFO: Got endpoints: latency-svc-9fwb2 [156.536109ms]
    Jan 19 04:22:15.810: INFO: Created: latency-svc-52qk7
    Jan 19 04:22:15.814: INFO: Got endpoints: latency-svc-52qk7 [150.393049ms]
    Jan 19 04:22:15.821: INFO: Created: latency-svc-n2745
    Jan 19 04:22:15.831: INFO: Created: latency-svc-j5swq
    Jan 19 04:22:15.832: INFO: Got endpoints: latency-svc-n2745 [157.51855ms]
    Jan 19 04:22:15.838: INFO: Got endpoints: latency-svc-j5swq [151.927606ms]
    Jan 19 04:22:15.843: INFO: Created: latency-svc-98dmh
    Jan 19 04:22:15.851: INFO: Created: latency-svc-dpprz
    Jan 19 04:22:15.882: INFO: Got endpoints: latency-svc-98dmh [186.136041ms]
    Jan 19 04:22:15.886: INFO: Created: latency-svc-bmwbf
    Jan 19 04:22:15.890: INFO: Created: latency-svc-vmctn
    Jan 19 04:22:15.898: INFO: Created: latency-svc-zmx58
    Jan 19 04:22:15.908: INFO: Created: latency-svc-6ngg5
    Jan 19 04:22:15.909: INFO: Got endpoints: latency-svc-dpprz [203.562152ms]
    Jan 19 04:22:15.915: INFO: Created: latency-svc-glg4k
    Jan 19 04:22:15.922: INFO: Created: latency-svc-2bc7x
    Jan 19 04:22:15.927: INFO: Created: latency-svc-jxnl5
    Jan 19 04:22:15.934: INFO: Created: latency-svc-9k25c
    Jan 19 04:22:15.950: INFO: Created: latency-svc-jwtc7
    Jan 19 04:22:15.958: INFO: Created: latency-svc-5k2xt
    Jan 19 04:22:15.962: INFO: Got endpoints: latency-svc-bmwbf [248.763409ms]
    Jan 19 04:22:15.966: INFO: Created: latency-svc-67x75
    Jan 19 04:22:15.976: INFO: Created: latency-svc-q8l25
    Jan 19 04:22:15.980: INFO: Created: latency-svc-ckmnv
    Jan 19 04:22:15.991: INFO: Created: latency-svc-cxfcg
    Jan 19 04:22:15.999: INFO: Created: latency-svc-d5kxv
    Jan 19 04:22:16.008: INFO: Created: latency-svc-m4t2h
    Jan 19 04:22:16.010: INFO: Got endpoints: latency-svc-vmctn [287.088984ms]
    Jan 19 04:22:16.020: INFO: Created: latency-svc-85l62
    Jan 19 04:22:16.060: INFO: Got endpoints: latency-svc-zmx58 [306.692606ms]
    Jan 19 04:22:16.069: INFO: Created: latency-svc-b728v
    Jan 19 04:22:16.109: INFO: Got endpoints: latency-svc-6ngg5 [355.483223ms]
    Jan 19 04:22:16.121: INFO: Created: latency-svc-x5sjj
    Jan 19 04:22:16.161: INFO: Got endpoints: latency-svc-glg4k [398.774368ms]
    Jan 19 04:22:16.171: INFO: Created: latency-svc-z5t6g
    Jan 19 04:22:16.209: INFO: Got endpoints: latency-svc-2bc7x [438.997455ms]
    Jan 19 04:22:16.220: INFO: Created: latency-svc-x9q2b
    Jan 19 04:22:16.264: INFO: Got endpoints: latency-svc-jxnl5 [483.256273ms]
    Jan 19 04:22:16.274: INFO: Created: latency-svc-6s2sn
    Jan 19 04:22:16.310: INFO: Got endpoints: latency-svc-9k25c [518.41007ms]
    Jan 19 04:22:16.322: INFO: Created: latency-svc-5rzft
    Jan 19 04:22:16.361: INFO: Got endpoints: latency-svc-jwtc7 [559.169474ms]
    Jan 19 04:22:16.374: INFO: Created: latency-svc-gn77n
    Jan 19 04:22:16.425: INFO: Got endpoints: latency-svc-5k2xt [618.174702ms]
    Jan 19 04:22:16.442: INFO: Created: latency-svc-6dpn8
    Jan 19 04:22:16.460: INFO: Got endpoints: latency-svc-67x75 [646.378406ms]
    Jan 19 04:22:16.474: INFO: Created: latency-svc-rmh8l
    Jan 19 04:22:16.512: INFO: Got endpoints: latency-svc-q8l25 [680.143975ms]
    Jan 19 04:22:16.521: INFO: Created: latency-svc-8hkgb
    Jan 19 04:22:16.559: INFO: Got endpoints: latency-svc-ckmnv [720.994732ms]
    Jan 19 04:22:16.569: INFO: Created: latency-svc-q7jrc
    Jan 19 04:22:16.612: INFO: Got endpoints: latency-svc-cxfcg [730.654156ms]
    Jan 19 04:22:16.623: INFO: Created: latency-svc-tcm9w
    Jan 19 04:22:16.658: INFO: Got endpoints: latency-svc-d5kxv [748.594613ms]
    Jan 19 04:22:16.669: INFO: Created: latency-svc-2czf9
    Jan 19 04:22:16.710: INFO: Got endpoints: latency-svc-m4t2h [747.979322ms]
    Jan 19 04:22:16.721: INFO: Created: latency-svc-f2qll
    Jan 19 04:22:16.759: INFO: Got endpoints: latency-svc-85l62 [748.753813ms]
    Jan 19 04:22:16.769: INFO: Created: latency-svc-l4xx2
    Jan 19 04:22:16.813: INFO: Got endpoints: latency-svc-b728v [752.947845ms]
    Jan 19 04:22:16.826: INFO: Created: latency-svc-pjxgh
    Jan 19 04:22:16.859: INFO: Got endpoints: latency-svc-x5sjj [750.179842ms]
    Jan 19 04:22:16.869: INFO: Created: latency-svc-5qbtf
    Jan 19 04:22:16.908: INFO: Got endpoints: latency-svc-z5t6g [747.546794ms]
    Jan 19 04:22:16.918: INFO: Created: latency-svc-jtlts
    Jan 19 04:22:16.960: INFO: Got endpoints: latency-svc-x9q2b [751.039862ms]
    Jan 19 04:22:16.969: INFO: Created: latency-svc-7pd2m
    Jan 19 04:22:17.009: INFO: Got endpoints: latency-svc-6s2sn [744.977614ms]
    Jan 19 04:22:17.031: INFO: Created: latency-svc-hcwd5
    Jan 19 04:22:17.059: INFO: Got endpoints: latency-svc-5rzft [749.135887ms]
    Jan 19 04:22:17.068: INFO: Created: latency-svc-n72sj
    Jan 19 04:22:17.109: INFO: Got endpoints: latency-svc-gn77n [747.923303ms]
    Jan 19 04:22:17.120: INFO: Created: latency-svc-zr889
    Jan 19 04:22:17.210: INFO: Got endpoints: latency-svc-6dpn8 [784.815161ms]
    Jan 19 04:22:17.222: INFO: Created: latency-svc-h29wq
    Jan 19 04:22:17.259: INFO: Got endpoints: latency-svc-rmh8l [799.109776ms]
    Jan 19 04:22:17.268: INFO: Created: latency-svc-rf2d9
    Jan 19 04:22:17.311: INFO: Got endpoints: latency-svc-8hkgb [798.823626ms]
    Jan 19 04:22:17.328: INFO: Created: latency-svc-2pl27
    Jan 19 04:22:17.360: INFO: Got endpoints: latency-svc-q7jrc [800.972541ms]
    Jan 19 04:22:17.369: INFO: Created: latency-svc-tsgdt
    Jan 19 04:22:17.409: INFO: Got endpoints: latency-svc-tcm9w [796.896535ms]
    Jan 19 04:22:17.422: INFO: Created: latency-svc-sqhjk
    Jan 19 04:22:17.458: INFO: Got endpoints: latency-svc-2czf9 [800.582752ms]
    Jan 19 04:22:17.470: INFO: Created: latency-svc-2xh72
    Jan 19 04:22:17.509: INFO: Got endpoints: latency-svc-f2qll [799.307008ms]
    Jan 19 04:22:17.541: INFO: Created: latency-svc-ggckb
    Jan 19 04:22:17.561: INFO: Got endpoints: latency-svc-l4xx2 [801.733622ms]
    Jan 19 04:22:17.571: INFO: Created: latency-svc-td8t6
    Jan 19 04:22:17.611: INFO: Got endpoints: latency-svc-pjxgh [798.765141ms]
    Jan 19 04:22:17.622: INFO: Created: latency-svc-6qbxc
    Jan 19 04:22:17.661: INFO: Got endpoints: latency-svc-5qbtf [801.866786ms]
    Jan 19 04:22:17.671: INFO: Created: latency-svc-gzz9b
    Jan 19 04:22:17.709: INFO: Got endpoints: latency-svc-jtlts [800.619241ms]
    Jan 19 04:22:17.719: INFO: Created: latency-svc-shktk
    Jan 19 04:22:17.760: INFO: Got endpoints: latency-svc-7pd2m [799.904183ms]
    Jan 19 04:22:17.769: INFO: Created: latency-svc-dcq5r
    Jan 19 04:22:17.810: INFO: Got endpoints: latency-svc-hcwd5 [801.206287ms]
    Jan 19 04:22:17.819: INFO: Created: latency-svc-qq4pm
    Jan 19 04:22:17.859: INFO: Got endpoints: latency-svc-n72sj [800.558623ms]
    Jan 19 04:22:17.869: INFO: Created: latency-svc-d6prz
    Jan 19 04:22:17.910: INFO: Got endpoints: latency-svc-zr889 [800.937103ms]
    Jan 19 04:22:17.920: INFO: Created: latency-svc-xmgs4
    Jan 19 04:22:17.959: INFO: Got endpoints: latency-svc-h29wq [749.69063ms]
    Jan 19 04:22:17.970: INFO: Created: latency-svc-748gm
    Jan 19 04:22:18.010: INFO: Got endpoints: latency-svc-rf2d9 [750.133914ms]
    Jan 19 04:22:18.020: INFO: Created: latency-svc-cd4c9
    Jan 19 04:22:18.060: INFO: Got endpoints: latency-svc-2pl27 [749.015461ms]
    Jan 19 04:22:18.070: INFO: Created: latency-svc-jdlpd
    Jan 19 04:22:18.109: INFO: Got endpoints: latency-svc-tsgdt [749.386822ms]
    Jan 19 04:22:18.119: INFO: Created: latency-svc-j5tsd
    Jan 19 04:22:18.159: INFO: Got endpoints: latency-svc-sqhjk [749.553309ms]
    Jan 19 04:22:18.171: INFO: Created: latency-svc-fjgxx
    Jan 19 04:22:18.209: INFO: Got endpoints: latency-svc-2xh72 [750.638141ms]
    Jan 19 04:22:18.220: INFO: Created: latency-svc-pp9bb
    Jan 19 04:22:18.276: INFO: Got endpoints: latency-svc-ggckb [766.469723ms]
    Jan 19 04:22:18.288: INFO: Created: latency-svc-w9nnb
    Jan 19 04:22:18.309: INFO: Got endpoints: latency-svc-td8t6 [747.770458ms]
    Jan 19 04:22:18.319: INFO: Created: latency-svc-2hd4c
    Jan 19 04:22:18.358: INFO: Got endpoints: latency-svc-6qbxc [747.023077ms]
    Jan 19 04:22:18.370: INFO: Created: latency-svc-rpvj8
    Jan 19 04:22:18.408: INFO: Got endpoints: latency-svc-gzz9b [747.283441ms]
    Jan 19 04:22:18.419: INFO: Created: latency-svc-t22mh
    Jan 19 04:22:18.460: INFO: Got endpoints: latency-svc-shktk [750.767839ms]
    Jan 19 04:22:18.470: INFO: Created: latency-svc-ltn55
    Jan 19 04:22:18.508: INFO: Got endpoints: latency-svc-dcq5r [748.536396ms]
    Jan 19 04:22:18.528: INFO: Created: latency-svc-zftrb
    Jan 19 04:22:18.561: INFO: Got endpoints: latency-svc-qq4pm [751.509891ms]
    Jan 19 04:22:18.572: INFO: Created: latency-svc-bwwn8
    Jan 19 04:22:18.611: INFO: Got endpoints: latency-svc-d6prz [751.536729ms]
    Jan 19 04:22:18.621: INFO: Created: latency-svc-mrjlj
    Jan 19 04:22:18.663: INFO: Got endpoints: latency-svc-xmgs4 [753.714507ms]
    Jan 19 04:22:18.674: INFO: Created: latency-svc-s8nbq
    Jan 19 04:22:18.708: INFO: Got endpoints: latency-svc-748gm [748.373076ms]
    Jan 19 04:22:18.719: INFO: Created: latency-svc-86ghg
    Jan 19 04:22:18.759: INFO: Got endpoints: latency-svc-cd4c9 [749.385363ms]
    Jan 19 04:22:18.769: INFO: Created: latency-svc-tkqg9
    Jan 19 04:22:18.809: INFO: Got endpoints: latency-svc-jdlpd [749.289475ms]
    Jan 19 04:22:18.819: INFO: Created: latency-svc-hcrsh
    Jan 19 04:22:18.861: INFO: Got endpoints: latency-svc-j5tsd [752.188098ms]
    Jan 19 04:22:18.872: INFO: Created: latency-svc-8ggcw
    Jan 19 04:22:18.911: INFO: Got endpoints: latency-svc-fjgxx [752.173231ms]
    Jan 19 04:22:18.921: INFO: Created: latency-svc-64vww
    Jan 19 04:22:18.958: INFO: Got endpoints: latency-svc-pp9bb [749.326545ms]
    Jan 19 04:22:18.971: INFO: Created: latency-svc-8nhcf
    Jan 19 04:22:19.010: INFO: Got endpoints: latency-svc-w9nnb [734.356229ms]
    Jan 19 04:22:19.021: INFO: Created: latency-svc-86kzl
    Jan 19 04:22:19.058: INFO: Got endpoints: latency-svc-2hd4c [749.431378ms]
    Jan 19 04:22:19.068: INFO: Created: latency-svc-lrrzp
    Jan 19 04:22:19.109: INFO: Got endpoints: latency-svc-rpvj8 [750.89196ms]
    Jan 19 04:22:19.120: INFO: Created: latency-svc-9pmdz
    Jan 19 04:22:19.159: INFO: Got endpoints: latency-svc-t22mh [751.151804ms]
    Jan 19 04:22:19.175: INFO: Created: latency-svc-9mhrl
    Jan 19 04:22:19.209: INFO: Got endpoints: latency-svc-ltn55 [749.688671ms]
    Jan 19 04:22:19.221: INFO: Created: latency-svc-tpcsb
    Jan 19 04:22:19.260: INFO: Got endpoints: latency-svc-zftrb [751.336106ms]
    Jan 19 04:22:19.270: INFO: Created: latency-svc-v9lzd
    Jan 19 04:22:19.309: INFO: Got endpoints: latency-svc-bwwn8 [747.83441ms]
    Jan 19 04:22:19.321: INFO: Created: latency-svc-xh4dx
    Jan 19 04:22:19.361: INFO: Got endpoints: latency-svc-mrjlj [750.292682ms]
    Jan 19 04:22:19.371: INFO: Created: latency-svc-vtzp8
    Jan 19 04:22:19.411: INFO: Got endpoints: latency-svc-s8nbq [747.758948ms]
    Jan 19 04:22:19.423: INFO: Created: latency-svc-7n4gk
    Jan 19 04:22:19.460: INFO: Got endpoints: latency-svc-86ghg [752.208243ms]
    Jan 19 04:22:19.474: INFO: Created: latency-svc-h5vb9
    Jan 19 04:22:19.510: INFO: Got endpoints: latency-svc-tkqg9 [750.802409ms]
    Jan 19 04:22:19.522: INFO: Created: latency-svc-vg9d5
    Jan 19 04:22:19.562: INFO: Got endpoints: latency-svc-hcrsh [752.629865ms]
    Jan 19 04:22:19.572: INFO: Created: latency-svc-8lk2h
    Jan 19 04:22:19.620: INFO: Got endpoints: latency-svc-8ggcw [758.176969ms]
    Jan 19 04:22:19.629: INFO: Created: latency-svc-2lfnk
    Jan 19 04:22:19.660: INFO: Got endpoints: latency-svc-64vww [749.329747ms]
    Jan 19 04:22:19.671: INFO: Created: latency-svc-986r9
    Jan 19 04:22:19.709: INFO: Got endpoints: latency-svc-8nhcf [750.644902ms]
    Jan 19 04:22:19.725: INFO: Created: latency-svc-cn7dg
    Jan 19 04:22:19.760: INFO: Got endpoints: latency-svc-86kzl [749.4218ms]
    Jan 19 04:22:19.771: INFO: Created: latency-svc-p6vrb
    Jan 19 04:22:19.810: INFO: Got endpoints: latency-svc-lrrzp [751.644229ms]
    Jan 19 04:22:19.821: INFO: Created: latency-svc-pt57t
    Jan 19 04:22:19.861: INFO: Got endpoints: latency-svc-9pmdz [751.549423ms]
    Jan 19 04:22:19.871: INFO: Created: latency-svc-ldqfn
    Jan 19 04:22:19.911: INFO: Got endpoints: latency-svc-9mhrl [751.610793ms]
    Jan 19 04:22:19.922: INFO: Created: latency-svc-hgjz5
    Jan 19 04:22:19.961: INFO: Got endpoints: latency-svc-tpcsb [751.434726ms]
    Jan 19 04:22:19.971: INFO: Created: latency-svc-6nzpf
    Jan 19 04:22:20.011: INFO: Got endpoints: latency-svc-v9lzd [751.289306ms]
    Jan 19 04:22:20.021: INFO: Created: latency-svc-nmzl6
    Jan 19 04:22:20.064: INFO: Got endpoints: latency-svc-xh4dx [754.548177ms]
    Jan 19 04:22:20.081: INFO: Created: latency-svc-l45pv
    Jan 19 04:22:20.111: INFO: Got endpoints: latency-svc-vtzp8 [750.132227ms]
    Jan 19 04:22:20.122: INFO: Created: latency-svc-hf5l9
    Jan 19 04:22:20.159: INFO: Got endpoints: latency-svc-7n4gk [747.542725ms]
    Jan 19 04:22:20.170: INFO: Created: latency-svc-d4hxn
    Jan 19 04:22:20.210: INFO: Got endpoints: latency-svc-h5vb9 [749.90068ms]
    Jan 19 04:22:20.221: INFO: Created: latency-svc-fxn8x
    Jan 19 04:22:20.262: INFO: Got endpoints: latency-svc-vg9d5 [751.954695ms]
    Jan 19 04:22:20.270: INFO: Created: latency-svc-h5bwj
    Jan 19 04:22:20.310: INFO: Got endpoints: latency-svc-8lk2h [748.537258ms]
    Jan 19 04:22:20.321: INFO: Created: latency-svc-jfd4h
    Jan 19 04:22:20.360: INFO: Got endpoints: latency-svc-2lfnk [740.158032ms]
    Jan 19 04:22:20.371: INFO: Created: latency-svc-fz7tg
    Jan 19 04:22:20.411: INFO: Got endpoints: latency-svc-986r9 [750.250209ms]
    Jan 19 04:22:20.423: INFO: Created: latency-svc-kjmrc
    Jan 19 04:22:20.462: INFO: Got endpoints: latency-svc-cn7dg [752.669975ms]
    Jan 19 04:22:20.475: INFO: Created: latency-svc-whnh7
    Jan 19 04:22:20.515: INFO: Got endpoints: latency-svc-p6vrb [754.843575ms]
    Jan 19 04:22:20.527: INFO: Created: latency-svc-gsl2b
    Jan 19 04:22:20.561: INFO: Got endpoints: latency-svc-pt57t [751.263988ms]
    Jan 19 04:22:20.572: INFO: Created: latency-svc-2484l
    Jan 19 04:22:20.612: INFO: Got endpoints: latency-svc-ldqfn [750.550522ms]
    Jan 19 04:22:20.627: INFO: Created: latency-svc-vbvxh
    Jan 19 04:22:20.660: INFO: Got endpoints: latency-svc-hgjz5 [749.596542ms]
    Jan 19 04:22:20.672: INFO: Created: latency-svc-fjmh2
    Jan 19 04:22:20.712: INFO: Got endpoints: latency-svc-6nzpf [751.123404ms]
    Jan 19 04:22:20.724: INFO: Created: latency-svc-wnqs7
    Jan 19 04:22:20.759: INFO: Got endpoints: latency-svc-nmzl6 [747.864535ms]
    Jan 19 04:22:20.771: INFO: Created: latency-svc-vdh27
    Jan 19 04:22:20.813: INFO: Got endpoints: latency-svc-l45pv [749.191442ms]
    Jan 19 04:22:20.823: INFO: Created: latency-svc-vhh2n
    Jan 19 04:22:20.861: INFO: Got endpoints: latency-svc-hf5l9 [749.557388ms]
    Jan 19 04:22:20.871: INFO: Created: latency-svc-mtmzp
    Jan 19 04:22:20.910: INFO: Got endpoints: latency-svc-d4hxn [751.476867ms]
    Jan 19 04:22:20.923: INFO: Created: latency-svc-dg2ft
    Jan 19 04:22:20.958: INFO: Got endpoints: latency-svc-fxn8x [748.303016ms]
    Jan 19 04:22:20.971: INFO: Created: latency-svc-kt842
    Jan 19 04:22:21.009: INFO: Got endpoints: latency-svc-h5bwj [747.611797ms]
    Jan 19 04:22:21.021: INFO: Created: latency-svc-s9k7k
    Jan 19 04:22:21.059: INFO: Got endpoints: latency-svc-jfd4h [749.308508ms]
    Jan 19 04:22:21.069: INFO: Created: latency-svc-mbvmg
    Jan 19 04:22:21.109: INFO: Got endpoints: latency-svc-fz7tg [749.254017ms]
    Jan 19 04:22:21.125: INFO: Created: latency-svc-rcdgf
    Jan 19 04:22:21.161: INFO: Got endpoints: latency-svc-kjmrc [750.19431ms]
    Jan 19 04:22:21.170: INFO: Created: latency-svc-mhqmc
    Jan 19 04:22:21.209: INFO: Got endpoints: latency-svc-whnh7 [747.354605ms]
    Jan 19 04:22:21.229: INFO: Created: latency-svc-n8mt7
    Jan 19 04:22:21.260: INFO: Got endpoints: latency-svc-gsl2b [745.340609ms]
    Jan 19 04:22:21.271: INFO: Created: latency-svc-9nkcr
    Jan 19 04:22:21.309: INFO: Got endpoints: latency-svc-2484l [747.57844ms]
    Jan 19 04:22:21.326: INFO: Created: latency-svc-hll29
    Jan 19 04:22:21.360: INFO: Got endpoints: latency-svc-vbvxh [748.342736ms]
    Jan 19 04:22:21.370: INFO: Created: latency-svc-2cqc8
    Jan 19 04:22:21.408: INFO: Got endpoints: latency-svc-fjmh2 [747.722157ms]
    Jan 19 04:22:21.422: INFO: Created: latency-svc-sl27b
    Jan 19 04:22:21.462: INFO: Got endpoints: latency-svc-wnqs7 [749.65505ms]
    Jan 19 04:22:21.472: INFO: Created: latency-svc-bltxq
    Jan 19 04:22:21.518: INFO: Got endpoints: latency-svc-vdh27 [758.702185ms]
    Jan 19 04:22:21.528: INFO: Created: latency-svc-vnt6q
    Jan 19 04:22:21.566: INFO: Got endpoints: latency-svc-vhh2n [753.326306ms]
    Jan 19 04:22:21.590: INFO: Created: latency-svc-j7zck
    Jan 19 04:22:21.632: INFO: Got endpoints: latency-svc-mtmzp [770.986841ms]
    Jan 19 04:22:21.650: INFO: Created: latency-svc-74lts
    Jan 19 04:22:21.660: INFO: Got endpoints: latency-svc-dg2ft [749.246919ms]
    Jan 19 04:22:21.671: INFO: Created: latency-svc-k55l5
    Jan 19 04:22:21.708: INFO: Got endpoints: latency-svc-kt842 [749.391656ms]
    Jan 19 04:22:21.718: INFO: Created: latency-svc-bctv7
    Jan 19 04:22:21.759: INFO: Got endpoints: latency-svc-s9k7k [749.946882ms]
    Jan 19 04:22:21.770: INFO: Created: latency-svc-f25fj
    Jan 19 04:22:21.810: INFO: Got endpoints: latency-svc-mbvmg [750.358239ms]
    Jan 19 04:22:21.821: INFO: Created: latency-svc-pvxp6
    Jan 19 04:22:21.859: INFO: Got endpoints: latency-svc-rcdgf [750.074577ms]
    Jan 19 04:22:21.870: INFO: Created: latency-svc-jrhdm
    Jan 19 04:22:21.910: INFO: Got endpoints: latency-svc-mhqmc [748.806437ms]
    Jan 19 04:22:21.920: INFO: Created: latency-svc-bztkw
    Jan 19 04:22:21.960: INFO: Got endpoints: latency-svc-n8mt7 [750.675046ms]
    Jan 19 04:22:21.970: INFO: Created: latency-svc-xgfw5
    Jan 19 04:22:22.011: INFO: Got endpoints: latency-svc-9nkcr [750.731612ms]
    Jan 19 04:22:22.020: INFO: Created: latency-svc-k6x8g
    Jan 19 04:22:22.059: INFO: Got endpoints: latency-svc-hll29 [750.110872ms]
    Jan 19 04:22:22.069: INFO: Created: latency-svc-4s9ln
    Jan 19 04:22:22.109: INFO: Got endpoints: latency-svc-2cqc8 [749.066888ms]
    Jan 19 04:22:22.121: INFO: Created: latency-svc-bdv7z
    Jan 19 04:22:22.160: INFO: Got endpoints: latency-svc-sl27b [751.377175ms]
    Jan 19 04:22:22.170: INFO: Created: latency-svc-mkbbf
    Jan 19 04:22:22.211: INFO: Got endpoints: latency-svc-bltxq [748.742945ms]
    Jan 19 04:22:22.221: INFO: Created: latency-svc-8t8bk
    Jan 19 04:22:22.261: INFO: Got endpoints: latency-svc-vnt6q [743.051834ms]
    Jan 19 04:22:22.271: INFO: Created: latency-svc-w7xgt
    Jan 19 04:22:22.311: INFO: Got endpoints: latency-svc-j7zck [744.855562ms]
    Jan 19 04:22:22.321: INFO: Created: latency-svc-lhd4n
    Jan 19 04:22:22.358: INFO: Got endpoints: latency-svc-74lts [726.353787ms]
    Jan 19 04:22:22.369: INFO: Created: latency-svc-xkspc
    Jan 19 04:22:22.409: INFO: Got endpoints: latency-svc-k55l5 [749.400946ms]
    Jan 19 04:22:22.419: INFO: Created: latency-svc-pzv5g
    Jan 19 04:22:22.462: INFO: Got endpoints: latency-svc-bctv7 [753.899992ms]
    Jan 19 04:22:22.473: INFO: Created: latency-svc-mxnmr
    Jan 19 04:22:22.511: INFO: Got endpoints: latency-svc-f25fj [751.733088ms]
    Jan 19 04:22:22.525: INFO: Created: latency-svc-wkdvw
    Jan 19 04:22:22.562: INFO: Got endpoints: latency-svc-pvxp6 [752.413956ms]
    Jan 19 04:22:22.573: INFO: Created: latency-svc-s5rjv
    Jan 19 04:22:22.610: INFO: Got endpoints: latency-svc-jrhdm [750.592513ms]
    Jan 19 04:22:22.622: INFO: Created: latency-svc-9xn9t
    Jan 19 04:22:22.671: INFO: Got endpoints: latency-svc-bztkw [761.12411ms]
    Jan 19 04:22:22.686: INFO: Created: latency-svc-xr6tz
    Jan 19 04:22:22.708: INFO: Got endpoints: latency-svc-xgfw5 [747.824536ms]
    Jan 19 04:22:22.718: INFO: Created: latency-svc-86cm6
    Jan 19 04:22:22.760: INFO: Got endpoints: latency-svc-k6x8g [748.973689ms]
    Jan 19 04:22:22.770: INFO: Created: latency-svc-78bb6
    Jan 19 04:22:22.810: INFO: Got endpoints: latency-svc-4s9ln [750.946158ms]
    Jan 19 04:22:22.819: INFO: Created: latency-svc-v6rrc
    Jan 19 04:22:22.860: INFO: Got endpoints: latency-svc-bdv7z [750.572144ms]
    Jan 19 04:22:22.870: INFO: Created: latency-svc-s6sb4
    Jan 19 04:22:22.909: INFO: Got endpoints: latency-svc-mkbbf [749.638789ms]
    Jan 19 04:22:22.919: INFO: Created: latency-svc-22jjj
    Jan 19 04:22:22.960: INFO: Got endpoints: latency-svc-8t8bk [749.186011ms]
    Jan 19 04:22:22.970: INFO: Created: latency-svc-56ptd
    Jan 19 04:22:23.011: INFO: Got endpoints: latency-svc-w7xgt [750.193504ms]
    Jan 19 04:22:23.021: INFO: Created: latency-svc-vr65n
    Jan 19 04:22:23.059: INFO: Got endpoints: latency-svc-lhd4n [747.252267ms]
    Jan 19 04:22:23.081: INFO: Created: latency-svc-shfgw
    Jan 19 04:22:23.109: INFO: Got endpoints: latency-svc-xkspc [750.932345ms]
    Jan 19 04:22:23.120: INFO: Created: latency-svc-sg7f4
    Jan 19 04:22:23.160: INFO: Got endpoints: latency-svc-pzv5g [751.115626ms]
    Jan 19 04:22:23.183: INFO: Created: latency-svc-8ctt9
    Jan 19 04:22:23.210: INFO: Got endpoints: latency-svc-mxnmr [747.781731ms]
    Jan 19 04:22:23.219: INFO: Created: latency-svc-968sn
    Jan 19 04:22:23.261: INFO: Got endpoints: latency-svc-wkdvw [749.707372ms]
    Jan 19 04:22:23.270: INFO: Created: latency-svc-thhxz
    Jan 19 04:22:23.310: INFO: Got endpoints: latency-svc-s5rjv [747.493311ms]
    Jan 19 04:22:23.360: INFO: Got endpoints: latency-svc-9xn9t [750.640645ms]
    Jan 19 04:22:23.409: INFO: Got endpoints: latency-svc-xr6tz [737.866698ms]
    Jan 19 04:22:23.463: INFO: Got endpoints: latency-svc-86cm6 [754.819429ms]
    Jan 19 04:22:23.509: INFO: Got endpoints: latency-svc-78bb6 [749.712223ms]
    Jan 19 04:22:23.559: INFO: Got endpoints: latency-svc-v6rrc [748.712443ms]
    Jan 19 04:22:23.610: INFO: Got endpoints: latency-svc-s6sb4 [750.070331ms]
    Jan 19 04:22:23.659: INFO: Got endpoints: latency-svc-22jjj [749.639712ms]
    Jan 19 04:22:23.710: INFO: Got endpoints: latency-svc-56ptd [749.844689ms]
    Jan 19 04:22:23.759: INFO: Got endpoints: latency-svc-vr65n [748.275511ms]
    Jan 19 04:22:23.814: INFO: Got endpoints: latency-svc-shfgw [755.16679ms]
    Jan 19 04:22:23.860: INFO: Got endpoints: latency-svc-sg7f4 [751.007364ms]
    Jan 19 04:22:23.912: INFO: Got endpoints: latency-svc-8ctt9 [752.282488ms]
    Jan 19 04:22:23.961: INFO: Got endpoints: latency-svc-968sn [751.017106ms]
    Jan 19 04:22:24.009: INFO: Got endpoints: latency-svc-thhxz [748.168564ms]
    Jan 19 04:22:24.009: INFO: Latencies: [26.459752ms 34.047036ms 49.138845ms 56.97089ms 68.400646ms 78.295579ms 88.715013ms 97.126127ms 104.298099ms 120.659667ms 135.589796ms 142.946229ms 150.393049ms 151.804402ms 151.927606ms 155.013945ms 156.536109ms 157.51855ms 158.480093ms 159.437784ms 161.486512ms 168.780192ms 175.828226ms 180.634799ms 181.113488ms 182.112701ms 183.302358ms 183.893978ms 183.925076ms 185.273592ms 186.136041ms 186.224444ms 186.459192ms 188.915988ms 189.401467ms 190.696283ms 190.865769ms 198.194648ms 203.562152ms 248.763409ms 287.088984ms 306.692606ms 355.483223ms 398.774368ms 438.997455ms 483.256273ms 518.41007ms 559.169474ms 618.174702ms 646.378406ms 680.143975ms 720.994732ms 726.353787ms 730.654156ms 734.356229ms 737.866698ms 740.158032ms 743.051834ms 744.855562ms 744.977614ms 745.340609ms 747.023077ms 747.252267ms 747.283441ms 747.354605ms 747.493311ms 747.542725ms 747.546794ms 747.57844ms 747.611797ms 747.722157ms 747.758948ms 747.770458ms 747.781731ms 747.824536ms 747.83441ms 747.864535ms 747.923303ms 747.979322ms 748.168564ms 748.275511ms 748.303016ms 748.342736ms 748.373076ms 748.536396ms 748.537258ms 748.594613ms 748.712443ms 748.742945ms 748.753813ms 748.806437ms 748.973689ms 749.015461ms 749.066888ms 749.135887ms 749.186011ms 749.191442ms 749.246919ms 749.254017ms 749.289475ms 749.308508ms 749.326545ms 749.329747ms 749.385363ms 749.386822ms 749.391656ms 749.400946ms 749.4218ms 749.431378ms 749.553309ms 749.557388ms 749.596542ms 749.638789ms 749.639712ms 749.65505ms 749.688671ms 749.69063ms 749.707372ms 749.712223ms 749.844689ms 749.90068ms 749.946882ms 750.070331ms 750.074577ms 750.110872ms 750.132227ms 750.133914ms 750.179842ms 750.193504ms 750.19431ms 750.250209ms 750.292682ms 750.358239ms 750.550522ms 750.572144ms 750.592513ms 750.638141ms 750.640645ms 750.644902ms 750.675046ms 750.731612ms 750.767839ms 750.802409ms 750.89196ms 750.932345ms 750.946158ms 751.007364ms 751.017106ms 751.039862ms 751.115626ms 751.123404ms 751.151804ms 751.263988ms 751.289306ms 751.336106ms 751.377175ms 751.434726ms 751.476867ms 751.509891ms 751.536729ms 751.549423ms 751.610793ms 751.644229ms 751.733088ms 751.954695ms 752.173231ms 752.188098ms 752.208243ms 752.282488ms 752.413956ms 752.629865ms 752.669975ms 752.947845ms 753.326306ms 753.714507ms 753.899992ms 754.548177ms 754.819429ms 754.843575ms 755.16679ms 758.176969ms 758.702185ms 761.12411ms 766.469723ms 770.986841ms 784.815161ms 796.896535ms 798.765141ms 798.823626ms 799.109776ms 799.307008ms 799.904183ms 800.558623ms 800.582752ms 800.619241ms 800.937103ms 800.972541ms 801.206287ms 801.733622ms 801.866786ms]
    Jan 19 04:22:24.009: INFO: 50 %ile: 749.308508ms
    Jan 19 04:22:24.009: INFO: 90 %ile: 758.176969ms
    Jan 19 04:22:24.009: INFO: 99 %ile: 801.733622ms
    Jan 19 04:22:24.009: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:187
    Jan 19 04:22:24.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svc-latency-5586" for this suite. 01/19/23 04:22:24.014
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:22:24.022
Jan 19 04:22:24.022: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename configmap 01/19/23 04:22:24.023
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:22:24.037
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:22:24.039
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
STEP: Creating configMap with name configmap-test-volume-map-d3a2ce60-3b60-40ac-9ecf-43ade0c5a9ff 01/19/23 04:22:24.041
STEP: Creating a pod to test consume configMaps 01/19/23 04:22:24.046
Jan 19 04:22:24.055: INFO: Waiting up to 5m0s for pod "pod-configmaps-e1526695-9360-4f3c-9855-0f21dc6c6e9b" in namespace "configmap-7959" to be "Succeeded or Failed"
Jan 19 04:22:24.058: INFO: Pod "pod-configmaps-e1526695-9360-4f3c-9855-0f21dc6c6e9b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.122039ms
Jan 19 04:22:26.062: INFO: Pod "pod-configmaps-e1526695-9360-4f3c-9855-0f21dc6c6e9b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007280309s
Jan 19 04:22:28.062: INFO: Pod "pod-configmaps-e1526695-9360-4f3c-9855-0f21dc6c6e9b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007111683s
Jan 19 04:22:30.062: INFO: Pod "pod-configmaps-e1526695-9360-4f3c-9855-0f21dc6c6e9b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007472317s
STEP: Saw pod success 01/19/23 04:22:30.062
Jan 19 04:22:30.062: INFO: Pod "pod-configmaps-e1526695-9360-4f3c-9855-0f21dc6c6e9b" satisfied condition "Succeeded or Failed"
Jan 19 04:22:30.067: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-configmaps-e1526695-9360-4f3c-9855-0f21dc6c6e9b container agnhost-container: <nil>
STEP: delete the pod 01/19/23 04:22:30.073
Jan 19 04:22:30.089: INFO: Waiting for pod pod-configmaps-e1526695-9360-4f3c-9855-0f21dc6c6e9b to disappear
Jan 19 04:22:30.094: INFO: Pod pod-configmaps-e1526695-9360-4f3c-9855-0f21dc6c6e9b no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 19 04:22:30.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7959" for this suite. 01/19/23 04:22:30.098
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":16,"skipped":257,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.083 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:22:24.022
    Jan 19 04:22:24.022: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename configmap 01/19/23 04:22:24.023
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:22:24.037
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:22:24.039
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:88
    STEP: Creating configMap with name configmap-test-volume-map-d3a2ce60-3b60-40ac-9ecf-43ade0c5a9ff 01/19/23 04:22:24.041
    STEP: Creating a pod to test consume configMaps 01/19/23 04:22:24.046
    Jan 19 04:22:24.055: INFO: Waiting up to 5m0s for pod "pod-configmaps-e1526695-9360-4f3c-9855-0f21dc6c6e9b" in namespace "configmap-7959" to be "Succeeded or Failed"
    Jan 19 04:22:24.058: INFO: Pod "pod-configmaps-e1526695-9360-4f3c-9855-0f21dc6c6e9b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.122039ms
    Jan 19 04:22:26.062: INFO: Pod "pod-configmaps-e1526695-9360-4f3c-9855-0f21dc6c6e9b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007280309s
    Jan 19 04:22:28.062: INFO: Pod "pod-configmaps-e1526695-9360-4f3c-9855-0f21dc6c6e9b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007111683s
    Jan 19 04:22:30.062: INFO: Pod "pod-configmaps-e1526695-9360-4f3c-9855-0f21dc6c6e9b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007472317s
    STEP: Saw pod success 01/19/23 04:22:30.062
    Jan 19 04:22:30.062: INFO: Pod "pod-configmaps-e1526695-9360-4f3c-9855-0f21dc6c6e9b" satisfied condition "Succeeded or Failed"
    Jan 19 04:22:30.067: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-configmaps-e1526695-9360-4f3c-9855-0f21dc6c6e9b container agnhost-container: <nil>
    STEP: delete the pod 01/19/23 04:22:30.073
    Jan 19 04:22:30.089: INFO: Waiting for pod pod-configmaps-e1526695-9360-4f3c-9855-0f21dc6c6e9b to disappear
    Jan 19 04:22:30.094: INFO: Pod pod-configmaps-e1526695-9360-4f3c-9855-0f21dc6c6e9b no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 19 04:22:30.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7959" for this suite. 01/19/23 04:22:30.098
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:22:30.106
Jan 19 04:22:30.106: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename configmap 01/19/23 04:22:30.107
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:22:30.12
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:22:30.122
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
STEP: Creating configMap with name configmap-test-volume-a16ef58c-4e74-46f6-bb66-745601d9f48c 01/19/23 04:22:30.124
STEP: Creating a pod to test consume configMaps 01/19/23 04:22:30.137
Jan 19 04:22:30.148: INFO: Waiting up to 5m0s for pod "pod-configmaps-8d2bcef8-cdbf-4617-986f-198506b19971" in namespace "configmap-494" to be "Succeeded or Failed"
Jan 19 04:22:30.152: INFO: Pod "pod-configmaps-8d2bcef8-cdbf-4617-986f-198506b19971": Phase="Pending", Reason="", readiness=false. Elapsed: 4.668796ms
Jan 19 04:22:32.158: INFO: Pod "pod-configmaps-8d2bcef8-cdbf-4617-986f-198506b19971": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010286411s
Jan 19 04:22:34.158: INFO: Pod "pod-configmaps-8d2bcef8-cdbf-4617-986f-198506b19971": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010293547s
Jan 19 04:22:36.157: INFO: Pod "pod-configmaps-8d2bcef8-cdbf-4617-986f-198506b19971": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009031064s
STEP: Saw pod success 01/19/23 04:22:36.157
Jan 19 04:22:36.157: INFO: Pod "pod-configmaps-8d2bcef8-cdbf-4617-986f-198506b19971" satisfied condition "Succeeded or Failed"
Jan 19 04:22:36.160: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-configmaps-8d2bcef8-cdbf-4617-986f-198506b19971 container agnhost-container: <nil>
STEP: delete the pod 01/19/23 04:22:36.166
Jan 19 04:22:36.179: INFO: Waiting for pod pod-configmaps-8d2bcef8-cdbf-4617-986f-198506b19971 to disappear
Jan 19 04:22:36.181: INFO: Pod pod-configmaps-8d2bcef8-cdbf-4617-986f-198506b19971 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 19 04:22:36.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-494" for this suite. 01/19/23 04:22:36.184
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":17,"skipped":276,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.085 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:22:30.106
    Jan 19 04:22:30.106: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename configmap 01/19/23 04:22:30.107
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:22:30.12
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:22:30.122
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:56
    STEP: Creating configMap with name configmap-test-volume-a16ef58c-4e74-46f6-bb66-745601d9f48c 01/19/23 04:22:30.124
    STEP: Creating a pod to test consume configMaps 01/19/23 04:22:30.137
    Jan 19 04:22:30.148: INFO: Waiting up to 5m0s for pod "pod-configmaps-8d2bcef8-cdbf-4617-986f-198506b19971" in namespace "configmap-494" to be "Succeeded or Failed"
    Jan 19 04:22:30.152: INFO: Pod "pod-configmaps-8d2bcef8-cdbf-4617-986f-198506b19971": Phase="Pending", Reason="", readiness=false. Elapsed: 4.668796ms
    Jan 19 04:22:32.158: INFO: Pod "pod-configmaps-8d2bcef8-cdbf-4617-986f-198506b19971": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010286411s
    Jan 19 04:22:34.158: INFO: Pod "pod-configmaps-8d2bcef8-cdbf-4617-986f-198506b19971": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010293547s
    Jan 19 04:22:36.157: INFO: Pod "pod-configmaps-8d2bcef8-cdbf-4617-986f-198506b19971": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009031064s
    STEP: Saw pod success 01/19/23 04:22:36.157
    Jan 19 04:22:36.157: INFO: Pod "pod-configmaps-8d2bcef8-cdbf-4617-986f-198506b19971" satisfied condition "Succeeded or Failed"
    Jan 19 04:22:36.160: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-configmaps-8d2bcef8-cdbf-4617-986f-198506b19971 container agnhost-container: <nil>
    STEP: delete the pod 01/19/23 04:22:36.166
    Jan 19 04:22:36.179: INFO: Waiting for pod pod-configmaps-8d2bcef8-cdbf-4617-986f-198506b19971 to disappear
    Jan 19 04:22:36.181: INFO: Pod pod-configmaps-8d2bcef8-cdbf-4617-986f-198506b19971 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 19 04:22:36.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-494" for this suite. 01/19/23 04:22:36.184
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:22:36.193
Jan 19 04:22:36.193: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename sched-pred 01/19/23 04:22:36.194
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:22:36.208
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:22:36.211
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 19 04:22:36.213: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 19 04:22:36.218: INFO: Waiting for terminating namespaces to be deleted...
Jan 19 04:22:36.220: INFO: 
Logging pods the apiserver thinks is on node ckcp-nks-default-worker-node-0 before test
Jan 19 04:22:36.225: INFO: kube-flannel-ds-amd64-djm45 from kube-flannel started at 2023-01-19 04:04:25 +0000 UTC (1 container statuses recorded)
Jan 19 04:22:36.225: INFO: 	Container kube-flannel ready: true, restart count 0
Jan 19 04:22:36.225: INFO: coredns-557b76dc76-cg7b6 from kube-system started at 2023-01-19 04:04:25 +0000 UTC (1 container statuses recorded)
Jan 19 04:22:36.225: INFO: 	Container coredns ready: true, restart count 0
Jan 19 04:22:36.225: INFO: coredns-557b76dc76-ncmw9 from kube-system started at 2023-01-19 04:04:25 +0000 UTC (1 container statuses recorded)
Jan 19 04:22:36.225: INFO: 	Container coredns ready: true, restart count 0
Jan 19 04:22:36.225: INFO: csi-cinder-controllerplugin-0 from kube-system started at 2023-01-19 04:04:35 +0000 UTC (5 container statuses recorded)
Jan 19 04:22:36.225: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 19 04:22:36.225: INFO: 	Container csi-attacher ready: true, restart count 0
Jan 19 04:22:36.225: INFO: 	Container csi-provisioner ready: true, restart count 0
Jan 19 04:22:36.225: INFO: 	Container csi-resizer ready: true, restart count 0
Jan 19 04:22:36.225: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jan 19 04:22:36.225: INFO: csi-cinder-nodeplugin-r2d6v from kube-system started at 2023-01-19 04:04:35 +0000 UTC (2 container statuses recorded)
Jan 19 04:22:36.225: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 19 04:22:36.225: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 19 04:22:36.225: INFO: dashboard-metrics-scraper-7cc7856cfb-p9hnn from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
Jan 19 04:22:36.225: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jan 19 04:22:36.225: INFO: kube-dns-autoscaler-7986c68b66-q7lq5 from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
Jan 19 04:22:36.225: INFO: 	Container autoscaler ready: true, restart count 0
Jan 19 04:22:36.225: INFO: kubernetes-dashboard-d7b78f849-zfz2z from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
Jan 19 04:22:36.225: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jan 19 04:22:36.225: INFO: metrics-server-c494c94fc-lkqhw from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
Jan 19 04:22:36.225: INFO: 	Container metrics-server ready: true, restart count 0
Jan 19 04:22:36.225: INFO: npd-428hd from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
Jan 19 04:22:36.225: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 19 04:22:36.225: INFO: snapshot-controller-6bd8bc5484-94b94 from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
Jan 19 04:22:36.225: INFO: 	Container snapshot-controller ready: true, restart count 0
Jan 19 04:22:36.225: INFO: snapshot-controller-6bd8bc5484-nh4pv from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
Jan 19 04:22:36.225: INFO: 	Container snapshot-controller ready: true, restart count 0
Jan 19 04:22:36.225: INFO: sonobuoy-systemd-logs-daemon-set-dbd8990eaea742d4-9s2t2 from sonobuoy started at 2023-01-19 04:19:31 +0000 UTC (2 container statuses recorded)
Jan 19 04:22:36.225: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 04:22:36.225: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 19 04:22:36.225: INFO: 
Logging pods the apiserver thinks is on node ckcp-nks-default-worker-node-1 before test
Jan 19 04:22:36.229: INFO: kube-flannel-ds-amd64-jqq8g from kube-flannel started at 2023-01-19 04:04:25 +0000 UTC (1 container statuses recorded)
Jan 19 04:22:36.229: INFO: 	Container kube-flannel ready: true, restart count 0
Jan 19 04:22:36.229: INFO: csi-cinder-nodeplugin-nrrr6 from kube-system started at 2023-01-19 04:04:35 +0000 UTC (2 container statuses recorded)
Jan 19 04:22:36.229: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 19 04:22:36.229: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 19 04:22:36.229: INFO: npd-wz6bt from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
Jan 19 04:22:36.229: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 19 04:22:36.229: INFO: sonobuoy from sonobuoy started at 2023-01-19 04:19:22 +0000 UTC (1 container statuses recorded)
Jan 19 04:22:36.229: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 19 04:22:36.229: INFO: sonobuoy-e2e-job-35712e4fe0b540a2 from sonobuoy started at 2023-01-19 04:19:31 +0000 UTC (2 container statuses recorded)
Jan 19 04:22:36.229: INFO: 	Container e2e ready: true, restart count 0
Jan 19 04:22:36.229: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 04:22:36.229: INFO: sonobuoy-systemd-logs-daemon-set-dbd8990eaea742d4-7w8p4 from sonobuoy started at 2023-01-19 04:19:31 +0000 UTC (2 container statuses recorded)
Jan 19 04:22:36.229: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 04:22:36.229: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/19/23 04:22:36.229
Jan 19 04:22:36.240: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-5043" to be "running"
Jan 19 04:22:36.245: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.947346ms
Jan 19 04:22:38.249: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009157779s
Jan 19 04:22:40.250: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.010313001s
Jan 19 04:22:40.250: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/19/23 04:22:40.253
STEP: Trying to apply a random label on the found node. 01/19/23 04:22:40.264
STEP: verifying the node has the label kubernetes.io/e2e-f36ffacc-34e5-4edd-8ade-3604b2136e68 42 01/19/23 04:22:40.276
STEP: Trying to relaunch the pod, now with labels. 01/19/23 04:22:40.279
Jan 19 04:22:40.286: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-5043" to be "not pending"
Jan 19 04:22:40.292: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 6.086539ms
Jan 19 04:22:42.296: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.00992614s
Jan 19 04:22:42.296: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-f36ffacc-34e5-4edd-8ade-3604b2136e68 off the node ckcp-nks-default-worker-node-1 01/19/23 04:22:42.298
STEP: verifying the node doesn't have the label kubernetes.io/e2e-f36ffacc-34e5-4edd-8ade-3604b2136e68 01/19/23 04:22:42.309
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jan 19 04:22:42.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5043" for this suite. 01/19/23 04:22:42.316
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","completed":18,"skipped":334,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.130 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:22:36.193
    Jan 19 04:22:36.193: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename sched-pred 01/19/23 04:22:36.194
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:22:36.208
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:22:36.211
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jan 19 04:22:36.213: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 19 04:22:36.218: INFO: Waiting for terminating namespaces to be deleted...
    Jan 19 04:22:36.220: INFO: 
    Logging pods the apiserver thinks is on node ckcp-nks-default-worker-node-0 before test
    Jan 19 04:22:36.225: INFO: kube-flannel-ds-amd64-djm45 from kube-flannel started at 2023-01-19 04:04:25 +0000 UTC (1 container statuses recorded)
    Jan 19 04:22:36.225: INFO: 	Container kube-flannel ready: true, restart count 0
    Jan 19 04:22:36.225: INFO: coredns-557b76dc76-cg7b6 from kube-system started at 2023-01-19 04:04:25 +0000 UTC (1 container statuses recorded)
    Jan 19 04:22:36.225: INFO: 	Container coredns ready: true, restart count 0
    Jan 19 04:22:36.225: INFO: coredns-557b76dc76-ncmw9 from kube-system started at 2023-01-19 04:04:25 +0000 UTC (1 container statuses recorded)
    Jan 19 04:22:36.225: INFO: 	Container coredns ready: true, restart count 0
    Jan 19 04:22:36.225: INFO: csi-cinder-controllerplugin-0 from kube-system started at 2023-01-19 04:04:35 +0000 UTC (5 container statuses recorded)
    Jan 19 04:22:36.225: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Jan 19 04:22:36.225: INFO: 	Container csi-attacher ready: true, restart count 0
    Jan 19 04:22:36.225: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jan 19 04:22:36.225: INFO: 	Container csi-resizer ready: true, restart count 0
    Jan 19 04:22:36.225: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jan 19 04:22:36.225: INFO: csi-cinder-nodeplugin-r2d6v from kube-system started at 2023-01-19 04:04:35 +0000 UTC (2 container statuses recorded)
    Jan 19 04:22:36.225: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Jan 19 04:22:36.225: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jan 19 04:22:36.225: INFO: dashboard-metrics-scraper-7cc7856cfb-p9hnn from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
    Jan 19 04:22:36.225: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    Jan 19 04:22:36.225: INFO: kube-dns-autoscaler-7986c68b66-q7lq5 from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
    Jan 19 04:22:36.225: INFO: 	Container autoscaler ready: true, restart count 0
    Jan 19 04:22:36.225: INFO: kubernetes-dashboard-d7b78f849-zfz2z from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
    Jan 19 04:22:36.225: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    Jan 19 04:22:36.225: INFO: metrics-server-c494c94fc-lkqhw from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
    Jan 19 04:22:36.225: INFO: 	Container metrics-server ready: true, restart count 0
    Jan 19 04:22:36.225: INFO: npd-428hd from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
    Jan 19 04:22:36.225: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan 19 04:22:36.225: INFO: snapshot-controller-6bd8bc5484-94b94 from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
    Jan 19 04:22:36.225: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jan 19 04:22:36.225: INFO: snapshot-controller-6bd8bc5484-nh4pv from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
    Jan 19 04:22:36.225: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jan 19 04:22:36.225: INFO: sonobuoy-systemd-logs-daemon-set-dbd8990eaea742d4-9s2t2 from sonobuoy started at 2023-01-19 04:19:31 +0000 UTC (2 container statuses recorded)
    Jan 19 04:22:36.225: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 19 04:22:36.225: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 19 04:22:36.225: INFO: 
    Logging pods the apiserver thinks is on node ckcp-nks-default-worker-node-1 before test
    Jan 19 04:22:36.229: INFO: kube-flannel-ds-amd64-jqq8g from kube-flannel started at 2023-01-19 04:04:25 +0000 UTC (1 container statuses recorded)
    Jan 19 04:22:36.229: INFO: 	Container kube-flannel ready: true, restart count 0
    Jan 19 04:22:36.229: INFO: csi-cinder-nodeplugin-nrrr6 from kube-system started at 2023-01-19 04:04:35 +0000 UTC (2 container statuses recorded)
    Jan 19 04:22:36.229: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Jan 19 04:22:36.229: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jan 19 04:22:36.229: INFO: npd-wz6bt from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
    Jan 19 04:22:36.229: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan 19 04:22:36.229: INFO: sonobuoy from sonobuoy started at 2023-01-19 04:19:22 +0000 UTC (1 container statuses recorded)
    Jan 19 04:22:36.229: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 19 04:22:36.229: INFO: sonobuoy-e2e-job-35712e4fe0b540a2 from sonobuoy started at 2023-01-19 04:19:31 +0000 UTC (2 container statuses recorded)
    Jan 19 04:22:36.229: INFO: 	Container e2e ready: true, restart count 0
    Jan 19 04:22:36.229: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 19 04:22:36.229: INFO: sonobuoy-systemd-logs-daemon-set-dbd8990eaea742d4-7w8p4 from sonobuoy started at 2023-01-19 04:19:31 +0000 UTC (2 container statuses recorded)
    Jan 19 04:22:36.229: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 19 04:22:36.229: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:461
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/19/23 04:22:36.229
    Jan 19 04:22:36.240: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-5043" to be "running"
    Jan 19 04:22:36.245: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.947346ms
    Jan 19 04:22:38.249: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009157779s
    Jan 19 04:22:40.250: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.010313001s
    Jan 19 04:22:40.250: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/19/23 04:22:40.253
    STEP: Trying to apply a random label on the found node. 01/19/23 04:22:40.264
    STEP: verifying the node has the label kubernetes.io/e2e-f36ffacc-34e5-4edd-8ade-3604b2136e68 42 01/19/23 04:22:40.276
    STEP: Trying to relaunch the pod, now with labels. 01/19/23 04:22:40.279
    Jan 19 04:22:40.286: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-5043" to be "not pending"
    Jan 19 04:22:40.292: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 6.086539ms
    Jan 19 04:22:42.296: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.00992614s
    Jan 19 04:22:42.296: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-f36ffacc-34e5-4edd-8ade-3604b2136e68 off the node ckcp-nks-default-worker-node-1 01/19/23 04:22:42.298
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-f36ffacc-34e5-4edd-8ade-3604b2136e68 01/19/23 04:22:42.309
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jan 19 04:22:42.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-5043" for this suite. 01/19/23 04:22:42.316
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:22:42.323
Jan 19 04:22:42.323: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename projected 01/19/23 04:22:42.324
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:22:42.365
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:22:42.368
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
STEP: Creating projection with secret that has name projected-secret-test-map-7f2cfade-38af-4ede-9e83-5b1a9d09ffe6 01/19/23 04:22:42.371
STEP: Creating a pod to test consume secrets 01/19/23 04:22:42.376
Jan 19 04:22:42.385: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-97c95bb3-84bf-48b0-82b5-b908ed2c60f2" in namespace "projected-1317" to be "Succeeded or Failed"
Jan 19 04:22:42.392: INFO: Pod "pod-projected-secrets-97c95bb3-84bf-48b0-82b5-b908ed2c60f2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.823173ms
Jan 19 04:22:44.395: INFO: Pod "pod-projected-secrets-97c95bb3-84bf-48b0-82b5-b908ed2c60f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010596053s
Jan 19 04:22:46.396: INFO: Pod "pod-projected-secrets-97c95bb3-84bf-48b0-82b5-b908ed2c60f2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011257082s
Jan 19 04:22:48.397: INFO: Pod "pod-projected-secrets-97c95bb3-84bf-48b0-82b5-b908ed2c60f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011836418s
STEP: Saw pod success 01/19/23 04:22:48.397
Jan 19 04:22:48.397: INFO: Pod "pod-projected-secrets-97c95bb3-84bf-48b0-82b5-b908ed2c60f2" satisfied condition "Succeeded or Failed"
Jan 19 04:22:48.401: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-projected-secrets-97c95bb3-84bf-48b0-82b5-b908ed2c60f2 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/19/23 04:22:48.414
Jan 19 04:22:48.425: INFO: Waiting for pod pod-projected-secrets-97c95bb3-84bf-48b0-82b5-b908ed2c60f2 to disappear
Jan 19 04:22:48.429: INFO: Pod pod-projected-secrets-97c95bb3-84bf-48b0-82b5-b908ed2c60f2 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 19 04:22:48.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1317" for this suite. 01/19/23 04:22:48.434
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":19,"skipped":339,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.116 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:22:42.323
    Jan 19 04:22:42.323: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename projected 01/19/23 04:22:42.324
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:22:42.365
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:22:42.368
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:87
    STEP: Creating projection with secret that has name projected-secret-test-map-7f2cfade-38af-4ede-9e83-5b1a9d09ffe6 01/19/23 04:22:42.371
    STEP: Creating a pod to test consume secrets 01/19/23 04:22:42.376
    Jan 19 04:22:42.385: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-97c95bb3-84bf-48b0-82b5-b908ed2c60f2" in namespace "projected-1317" to be "Succeeded or Failed"
    Jan 19 04:22:42.392: INFO: Pod "pod-projected-secrets-97c95bb3-84bf-48b0-82b5-b908ed2c60f2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.823173ms
    Jan 19 04:22:44.395: INFO: Pod "pod-projected-secrets-97c95bb3-84bf-48b0-82b5-b908ed2c60f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010596053s
    Jan 19 04:22:46.396: INFO: Pod "pod-projected-secrets-97c95bb3-84bf-48b0-82b5-b908ed2c60f2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011257082s
    Jan 19 04:22:48.397: INFO: Pod "pod-projected-secrets-97c95bb3-84bf-48b0-82b5-b908ed2c60f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011836418s
    STEP: Saw pod success 01/19/23 04:22:48.397
    Jan 19 04:22:48.397: INFO: Pod "pod-projected-secrets-97c95bb3-84bf-48b0-82b5-b908ed2c60f2" satisfied condition "Succeeded or Failed"
    Jan 19 04:22:48.401: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-projected-secrets-97c95bb3-84bf-48b0-82b5-b908ed2c60f2 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/19/23 04:22:48.414
    Jan 19 04:22:48.425: INFO: Waiting for pod pod-projected-secrets-97c95bb3-84bf-48b0-82b5-b908ed2c60f2 to disappear
    Jan 19 04:22:48.429: INFO: Pod pod-projected-secrets-97c95bb3-84bf-48b0-82b5-b908ed2c60f2 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 19 04:22:48.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1317" for this suite. 01/19/23 04:22:48.434
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:22:48.439
Jan 19 04:22:48.439: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename projected 01/19/23 04:22:48.44
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:22:48.454
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:22:48.456
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
STEP: Creating a pod to test downward API volume plugin 01/19/23 04:22:48.459
Jan 19 04:22:48.466: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8ad8d275-75fd-4f77-881b-e03c43c98d60" in namespace "projected-1932" to be "Succeeded or Failed"
Jan 19 04:22:48.469: INFO: Pod "downwardapi-volume-8ad8d275-75fd-4f77-881b-e03c43c98d60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.689011ms
Jan 19 04:22:50.474: INFO: Pod "downwardapi-volume-8ad8d275-75fd-4f77-881b-e03c43c98d60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007499752s
Jan 19 04:22:52.474: INFO: Pod "downwardapi-volume-8ad8d275-75fd-4f77-881b-e03c43c98d60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007674618s
STEP: Saw pod success 01/19/23 04:22:52.474
Jan 19 04:22:52.474: INFO: Pod "downwardapi-volume-8ad8d275-75fd-4f77-881b-e03c43c98d60" satisfied condition "Succeeded or Failed"
Jan 19 04:22:52.477: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod downwardapi-volume-8ad8d275-75fd-4f77-881b-e03c43c98d60 container client-container: <nil>
STEP: delete the pod 01/19/23 04:22:52.482
Jan 19 04:22:52.496: INFO: Waiting for pod downwardapi-volume-8ad8d275-75fd-4f77-881b-e03c43c98d60 to disappear
Jan 19 04:22:52.499: INFO: Pod downwardapi-volume-8ad8d275-75fd-4f77-881b-e03c43c98d60 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 19 04:22:52.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1932" for this suite. 01/19/23 04:22:52.501
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","completed":20,"skipped":342,"failed":0}
------------------------------
â€¢ [4.070 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:22:48.439
    Jan 19 04:22:48.439: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename projected 01/19/23 04:22:48.44
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:22:48.454
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:22:48.456
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:206
    STEP: Creating a pod to test downward API volume plugin 01/19/23 04:22:48.459
    Jan 19 04:22:48.466: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8ad8d275-75fd-4f77-881b-e03c43c98d60" in namespace "projected-1932" to be "Succeeded or Failed"
    Jan 19 04:22:48.469: INFO: Pod "downwardapi-volume-8ad8d275-75fd-4f77-881b-e03c43c98d60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.689011ms
    Jan 19 04:22:50.474: INFO: Pod "downwardapi-volume-8ad8d275-75fd-4f77-881b-e03c43c98d60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007499752s
    Jan 19 04:22:52.474: INFO: Pod "downwardapi-volume-8ad8d275-75fd-4f77-881b-e03c43c98d60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007674618s
    STEP: Saw pod success 01/19/23 04:22:52.474
    Jan 19 04:22:52.474: INFO: Pod "downwardapi-volume-8ad8d275-75fd-4f77-881b-e03c43c98d60" satisfied condition "Succeeded or Failed"
    Jan 19 04:22:52.477: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod downwardapi-volume-8ad8d275-75fd-4f77-881b-e03c43c98d60 container client-container: <nil>
    STEP: delete the pod 01/19/23 04:22:52.482
    Jan 19 04:22:52.496: INFO: Waiting for pod downwardapi-volume-8ad8d275-75fd-4f77-881b-e03c43c98d60 to disappear
    Jan 19 04:22:52.499: INFO: Pod downwardapi-volume-8ad8d275-75fd-4f77-881b-e03c43c98d60 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 19 04:22:52.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1932" for this suite. 01/19/23 04:22:52.501
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:22:52.509
Jan 19 04:22:52.509: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename deployment 01/19/23 04:22:52.51
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:22:52.529
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:22:52.531
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Jan 19 04:22:52.533: INFO: Creating deployment "test-recreate-deployment"
Jan 19 04:22:52.539: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jan 19 04:22:52.544: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jan 19 04:22:54.552: INFO: Waiting deployment "test-recreate-deployment" to complete
Jan 19 04:22:54.556: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jan 19 04:22:54.564: INFO: Updating deployment test-recreate-deployment
Jan 19 04:22:54.564: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 19 04:22:54.653: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-3679  37970fc9-0844-4b34-b238-3cc3d70a1183 6306 2 2023-01-19 04:22:52 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-19 04:22:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 04:22:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037870c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-19 04:22:54 +0000 UTC,LastTransitionTime:2023-01-19 04:22:54 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-01-19 04:22:54 +0000 UTC,LastTransitionTime:2023-01-19 04:22:52 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jan 19 04:22:54.656: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-3679  c0d77f38-6fe4-4559-b63e-d56718ee0eb0 6305 1 2023-01-19 04:22:54 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 37970fc9-0844-4b34-b238-3cc3d70a1183 0xc0038181e0 0xc0038181e1}] [] [{kube-controller-manager Update apps/v1 2023-01-19 04:22:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37970fc9-0844-4b34-b238-3cc3d70a1183\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 04:22:54 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003818278 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 19 04:22:54.656: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jan 19 04:22:54.656: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-3679  d92a7fbd-d211-475a-8142-d0e7d2b8fe2e 6294 2 2023-01-19 04:22:52 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 37970fc9-0844-4b34-b238-3cc3d70a1183 0xc0038180d7 0xc0038180d8}] [] [{kube-controller-manager Update apps/v1 2023-01-19 04:22:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37970fc9-0844-4b34-b238-3cc3d70a1183\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 04:22:54 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003818188 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 19 04:22:54.659: INFO: Pod "test-recreate-deployment-9d58999df-m72hg" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-9d58999df-m72hg test-recreate-deployment-9d58999df- deployment-3679  bef6d88c-df83-4535-9471-0a13c95d61c8 6303 0 2023-01-19 04:22:54 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df c0d77f38-6fe4-4559-b63e-d56718ee0eb0 0xc003787470 0xc003787471}] [] [{kube-controller-manager Update v1 2023-01-19 04:22:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c0d77f38-6fe4-4559-b63e-d56718ee0eb0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 04:22:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b2m89,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b2m89,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:22:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:22:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:22:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:22:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.78,PodIP:,StartTime:2023-01-19 04:22:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 19 04:22:54.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3679" for this suite. 01/19/23 04:22:54.662
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","completed":21,"skipped":351,"failed":0}
------------------------------
â€¢ [2.160 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:22:52.509
    Jan 19 04:22:52.509: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename deployment 01/19/23 04:22:52.51
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:22:52.529
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:22:52.531
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Jan 19 04:22:52.533: INFO: Creating deployment "test-recreate-deployment"
    Jan 19 04:22:52.539: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Jan 19 04:22:52.544: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Jan 19 04:22:54.552: INFO: Waiting deployment "test-recreate-deployment" to complete
    Jan 19 04:22:54.556: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Jan 19 04:22:54.564: INFO: Updating deployment test-recreate-deployment
    Jan 19 04:22:54.564: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 19 04:22:54.653: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-3679  37970fc9-0844-4b34-b238-3cc3d70a1183 6306 2 2023-01-19 04:22:52 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-19 04:22:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 04:22:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037870c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-19 04:22:54 +0000 UTC,LastTransitionTime:2023-01-19 04:22:54 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-01-19 04:22:54 +0000 UTC,LastTransitionTime:2023-01-19 04:22:52 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Jan 19 04:22:54.656: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-3679  c0d77f38-6fe4-4559-b63e-d56718ee0eb0 6305 1 2023-01-19 04:22:54 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 37970fc9-0844-4b34-b238-3cc3d70a1183 0xc0038181e0 0xc0038181e1}] [] [{kube-controller-manager Update apps/v1 2023-01-19 04:22:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37970fc9-0844-4b34-b238-3cc3d70a1183\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 04:22:54 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003818278 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 19 04:22:54.656: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Jan 19 04:22:54.656: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-3679  d92a7fbd-d211-475a-8142-d0e7d2b8fe2e 6294 2 2023-01-19 04:22:52 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 37970fc9-0844-4b34-b238-3cc3d70a1183 0xc0038180d7 0xc0038180d8}] [] [{kube-controller-manager Update apps/v1 2023-01-19 04:22:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37970fc9-0844-4b34-b238-3cc3d70a1183\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 04:22:54 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003818188 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 19 04:22:54.659: INFO: Pod "test-recreate-deployment-9d58999df-m72hg" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-9d58999df-m72hg test-recreate-deployment-9d58999df- deployment-3679  bef6d88c-df83-4535-9471-0a13c95d61c8 6303 0 2023-01-19 04:22:54 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df c0d77f38-6fe4-4559-b63e-d56718ee0eb0 0xc003787470 0xc003787471}] [] [{kube-controller-manager Update v1 2023-01-19 04:22:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c0d77f38-6fe4-4559-b63e-d56718ee0eb0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 04:22:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b2m89,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b2m89,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:22:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:22:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:22:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:22:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.78,PodIP:,StartTime:2023-01-19 04:22:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 19 04:22:54.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-3679" for this suite. 01/19/23 04:22:54.662
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:22:54.67
Jan 19 04:22:54.670: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename init-container 01/19/23 04:22:54.671
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:22:54.683
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:22:54.687
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
STEP: creating the pod 01/19/23 04:22:54.69
Jan 19 04:22:54.690: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 19 04:22:59.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9738" for this suite. 01/19/23 04:22:59.45
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","completed":22,"skipped":371,"failed":0}
------------------------------
â€¢ [4.787 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:22:54.67
    Jan 19 04:22:54.670: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename init-container 01/19/23 04:22:54.671
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:22:54.683
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:22:54.687
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:254
    STEP: creating the pod 01/19/23 04:22:54.69
    Jan 19 04:22:54.690: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 19 04:22:59.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-9738" for this suite. 01/19/23 04:22:59.45
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:22:59.459
Jan 19 04:22:59.459: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename gc 01/19/23 04:22:59.459
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:22:59.474
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:22:59.476
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 01/19/23 04:22:59.484
STEP: delete the rc 01/19/23 04:23:04.496
STEP: wait for the rc to be deleted 01/19/23 04:23:04.504
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 01/19/23 04:23:09.51
STEP: Gathering metrics 01/19/23 04:23:39.524
W0119 04:23:39.536640      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 19 04:23:39.536: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan 19 04:23:39.536: INFO: Deleting pod "simpletest.rc-2xlbh" in namespace "gc-6463"
Jan 19 04:23:39.549: INFO: Deleting pod "simpletest.rc-2zzz5" in namespace "gc-6463"
Jan 19 04:23:39.562: INFO: Deleting pod "simpletest.rc-46vkx" in namespace "gc-6463"
Jan 19 04:23:39.574: INFO: Deleting pod "simpletest.rc-4btrq" in namespace "gc-6463"
Jan 19 04:23:39.586: INFO: Deleting pod "simpletest.rc-4dpjb" in namespace "gc-6463"
Jan 19 04:23:39.598: INFO: Deleting pod "simpletest.rc-4glhf" in namespace "gc-6463"
Jan 19 04:23:39.619: INFO: Deleting pod "simpletest.rc-4mc5p" in namespace "gc-6463"
Jan 19 04:23:39.629: INFO: Deleting pod "simpletest.rc-4sdd9" in namespace "gc-6463"
Jan 19 04:23:39.642: INFO: Deleting pod "simpletest.rc-56d4m" in namespace "gc-6463"
Jan 19 04:23:39.657: INFO: Deleting pod "simpletest.rc-59rvp" in namespace "gc-6463"
Jan 19 04:23:39.686: INFO: Deleting pod "simpletest.rc-5jts6" in namespace "gc-6463"
Jan 19 04:23:39.704: INFO: Deleting pod "simpletest.rc-5vmg5" in namespace "gc-6463"
Jan 19 04:23:39.718: INFO: Deleting pod "simpletest.rc-5wt7j" in namespace "gc-6463"
Jan 19 04:23:39.728: INFO: Deleting pod "simpletest.rc-5z8s5" in namespace "gc-6463"
Jan 19 04:23:39.745: INFO: Deleting pod "simpletest.rc-69gsx" in namespace "gc-6463"
Jan 19 04:23:39.760: INFO: Deleting pod "simpletest.rc-6k6jc" in namespace "gc-6463"
Jan 19 04:23:39.769: INFO: Deleting pod "simpletest.rc-6kbwx" in namespace "gc-6463"
Jan 19 04:23:39.782: INFO: Deleting pod "simpletest.rc-6mccd" in namespace "gc-6463"
Jan 19 04:23:39.800: INFO: Deleting pod "simpletest.rc-6plpw" in namespace "gc-6463"
Jan 19 04:23:39.813: INFO: Deleting pod "simpletest.rc-756lp" in namespace "gc-6463"
Jan 19 04:23:39.828: INFO: Deleting pod "simpletest.rc-7f586" in namespace "gc-6463"
Jan 19 04:23:39.840: INFO: Deleting pod "simpletest.rc-7jd6w" in namespace "gc-6463"
Jan 19 04:23:39.854: INFO: Deleting pod "simpletest.rc-7mnwz" in namespace "gc-6463"
Jan 19 04:23:39.869: INFO: Deleting pod "simpletest.rc-87dkr" in namespace "gc-6463"
Jan 19 04:23:39.883: INFO: Deleting pod "simpletest.rc-882mq" in namespace "gc-6463"
Jan 19 04:23:39.896: INFO: Deleting pod "simpletest.rc-8msrx" in namespace "gc-6463"
Jan 19 04:23:39.909: INFO: Deleting pod "simpletest.rc-8r42c" in namespace "gc-6463"
Jan 19 04:23:39.918: INFO: Deleting pod "simpletest.rc-8vtpc" in namespace "gc-6463"
Jan 19 04:23:39.929: INFO: Deleting pod "simpletest.rc-8zhfv" in namespace "gc-6463"
Jan 19 04:23:39.944: INFO: Deleting pod "simpletest.rc-974tm" in namespace "gc-6463"
Jan 19 04:23:39.958: INFO: Deleting pod "simpletest.rc-9dlz2" in namespace "gc-6463"
Jan 19 04:23:39.968: INFO: Deleting pod "simpletest.rc-bjd9s" in namespace "gc-6463"
Jan 19 04:23:39.979: INFO: Deleting pod "simpletest.rc-btx5n" in namespace "gc-6463"
Jan 19 04:23:39.990: INFO: Deleting pod "simpletest.rc-cg2lt" in namespace "gc-6463"
Jan 19 04:23:40.002: INFO: Deleting pod "simpletest.rc-cr5rh" in namespace "gc-6463"
Jan 19 04:23:40.027: INFO: Deleting pod "simpletest.rc-ct647" in namespace "gc-6463"
Jan 19 04:23:40.038: INFO: Deleting pod "simpletest.rc-cwvb6" in namespace "gc-6463"
Jan 19 04:23:40.053: INFO: Deleting pod "simpletest.rc-dfztx" in namespace "gc-6463"
Jan 19 04:23:40.064: INFO: Deleting pod "simpletest.rc-f4t2t" in namespace "gc-6463"
Jan 19 04:23:40.076: INFO: Deleting pod "simpletest.rc-f5dkw" in namespace "gc-6463"
Jan 19 04:23:40.088: INFO: Deleting pod "simpletest.rc-f5ms8" in namespace "gc-6463"
Jan 19 04:23:40.098: INFO: Deleting pod "simpletest.rc-fkr4f" in namespace "gc-6463"
Jan 19 04:23:40.112: INFO: Deleting pod "simpletest.rc-fq7bn" in namespace "gc-6463"
Jan 19 04:23:40.123: INFO: Deleting pod "simpletest.rc-fqj6r" in namespace "gc-6463"
Jan 19 04:23:40.134: INFO: Deleting pod "simpletest.rc-g9b68" in namespace "gc-6463"
Jan 19 04:23:40.143: INFO: Deleting pod "simpletest.rc-gd255" in namespace "gc-6463"
Jan 19 04:23:40.161: INFO: Deleting pod "simpletest.rc-gfsxm" in namespace "gc-6463"
Jan 19 04:23:40.172: INFO: Deleting pod "simpletest.rc-gg5jk" in namespace "gc-6463"
Jan 19 04:23:40.182: INFO: Deleting pod "simpletest.rc-gxjgv" in namespace "gc-6463"
Jan 19 04:23:40.196: INFO: Deleting pod "simpletest.rc-h2jz7" in namespace "gc-6463"
Jan 19 04:23:40.209: INFO: Deleting pod "simpletest.rc-h7brg" in namespace "gc-6463"
Jan 19 04:23:40.221: INFO: Deleting pod "simpletest.rc-h9ncn" in namespace "gc-6463"
Jan 19 04:23:40.237: INFO: Deleting pod "simpletest.rc-hqxz6" in namespace "gc-6463"
Jan 19 04:23:40.248: INFO: Deleting pod "simpletest.rc-hwcn6" in namespace "gc-6463"
Jan 19 04:23:40.260: INFO: Deleting pod "simpletest.rc-hxfdn" in namespace "gc-6463"
Jan 19 04:23:40.271: INFO: Deleting pod "simpletest.rc-j4hg6" in namespace "gc-6463"
Jan 19 04:23:40.281: INFO: Deleting pod "simpletest.rc-j9d9w" in namespace "gc-6463"
Jan 19 04:23:40.290: INFO: Deleting pod "simpletest.rc-jwjbf" in namespace "gc-6463"
Jan 19 04:23:40.300: INFO: Deleting pod "simpletest.rc-kwpg5" in namespace "gc-6463"
Jan 19 04:23:40.311: INFO: Deleting pod "simpletest.rc-kx85b" in namespace "gc-6463"
Jan 19 04:23:40.323: INFO: Deleting pod "simpletest.rc-lb2j6" in namespace "gc-6463"
Jan 19 04:23:40.338: INFO: Deleting pod "simpletest.rc-lx2gv" in namespace "gc-6463"
Jan 19 04:23:40.352: INFO: Deleting pod "simpletest.rc-m2tzk" in namespace "gc-6463"
Jan 19 04:23:40.362: INFO: Deleting pod "simpletest.rc-mbhz6" in namespace "gc-6463"
Jan 19 04:23:40.373: INFO: Deleting pod "simpletest.rc-mpxfh" in namespace "gc-6463"
Jan 19 04:23:40.385: INFO: Deleting pod "simpletest.rc-msplp" in namespace "gc-6463"
Jan 19 04:23:40.421: INFO: Deleting pod "simpletest.rc-mz9nb" in namespace "gc-6463"
Jan 19 04:23:40.475: INFO: Deleting pod "simpletest.rc-nwpnm" in namespace "gc-6463"
Jan 19 04:23:40.525: INFO: Deleting pod "simpletest.rc-p2q6s" in namespace "gc-6463"
Jan 19 04:23:40.576: INFO: Deleting pod "simpletest.rc-pk2jb" in namespace "gc-6463"
Jan 19 04:23:40.626: INFO: Deleting pod "simpletest.rc-psp6r" in namespace "gc-6463"
Jan 19 04:23:40.673: INFO: Deleting pod "simpletest.rc-qvszb" in namespace "gc-6463"
Jan 19 04:23:40.723: INFO: Deleting pod "simpletest.rc-rck6x" in namespace "gc-6463"
Jan 19 04:23:40.776: INFO: Deleting pod "simpletest.rc-rg492" in namespace "gc-6463"
Jan 19 04:23:40.830: INFO: Deleting pod "simpletest.rc-rmw8n" in namespace "gc-6463"
Jan 19 04:23:40.873: INFO: Deleting pod "simpletest.rc-rwzh6" in namespace "gc-6463"
Jan 19 04:23:40.929: INFO: Deleting pod "simpletest.rc-s48sb" in namespace "gc-6463"
Jan 19 04:23:40.975: INFO: Deleting pod "simpletest.rc-s7wdk" in namespace "gc-6463"
Jan 19 04:23:41.026: INFO: Deleting pod "simpletest.rc-snvsc" in namespace "gc-6463"
Jan 19 04:23:41.077: INFO: Deleting pod "simpletest.rc-strk6" in namespace "gc-6463"
Jan 19 04:23:41.129: INFO: Deleting pod "simpletest.rc-tt785" in namespace "gc-6463"
Jan 19 04:23:41.176: INFO: Deleting pod "simpletest.rc-v9hl4" in namespace "gc-6463"
Jan 19 04:23:41.227: INFO: Deleting pod "simpletest.rc-v9qzd" in namespace "gc-6463"
Jan 19 04:23:41.275: INFO: Deleting pod "simpletest.rc-vgx6j" in namespace "gc-6463"
Jan 19 04:23:41.331: INFO: Deleting pod "simpletest.rc-vw62x" in namespace "gc-6463"
Jan 19 04:23:41.374: INFO: Deleting pod "simpletest.rc-wl64q" in namespace "gc-6463"
Jan 19 04:23:41.429: INFO: Deleting pod "simpletest.rc-wppbx" in namespace "gc-6463"
Jan 19 04:23:41.475: INFO: Deleting pod "simpletest.rc-wwsq2" in namespace "gc-6463"
Jan 19 04:23:41.546: INFO: Deleting pod "simpletest.rc-x5m46" in namespace "gc-6463"
Jan 19 04:23:41.579: INFO: Deleting pod "simpletest.rc-xk6qv" in namespace "gc-6463"
Jan 19 04:23:41.623: INFO: Deleting pod "simpletest.rc-xqsb4" in namespace "gc-6463"
Jan 19 04:23:41.675: INFO: Deleting pod "simpletest.rc-xt9vq" in namespace "gc-6463"
Jan 19 04:23:41.724: INFO: Deleting pod "simpletest.rc-xz4js" in namespace "gc-6463"
Jan 19 04:23:41.777: INFO: Deleting pod "simpletest.rc-zbm5t" in namespace "gc-6463"
Jan 19 04:23:41.829: INFO: Deleting pod "simpletest.rc-zbwl5" in namespace "gc-6463"
Jan 19 04:23:41.873: INFO: Deleting pod "simpletest.rc-zfjpm" in namespace "gc-6463"
Jan 19 04:23:41.923: INFO: Deleting pod "simpletest.rc-zgc57" in namespace "gc-6463"
Jan 19 04:23:41.978: INFO: Deleting pod "simpletest.rc-zjrml" in namespace "gc-6463"
Jan 19 04:23:42.029: INFO: Deleting pod "simpletest.rc-zrnqh" in namespace "gc-6463"
Jan 19 04:23:42.074: INFO: Deleting pod "simpletest.rc-zrscb" in namespace "gc-6463"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 19 04:23:42.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6463" for this suite. 01/19/23 04:23:42.168
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","completed":23,"skipped":419,"failed":0}
------------------------------
â€¢ [SLOW TEST] [42.760 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:22:59.459
    Jan 19 04:22:59.459: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename gc 01/19/23 04:22:59.459
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:22:59.474
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:22:59.476
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 01/19/23 04:22:59.484
    STEP: delete the rc 01/19/23 04:23:04.496
    STEP: wait for the rc to be deleted 01/19/23 04:23:04.504
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 01/19/23 04:23:09.51
    STEP: Gathering metrics 01/19/23 04:23:39.524
    W0119 04:23:39.536640      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 19 04:23:39.536: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jan 19 04:23:39.536: INFO: Deleting pod "simpletest.rc-2xlbh" in namespace "gc-6463"
    Jan 19 04:23:39.549: INFO: Deleting pod "simpletest.rc-2zzz5" in namespace "gc-6463"
    Jan 19 04:23:39.562: INFO: Deleting pod "simpletest.rc-46vkx" in namespace "gc-6463"
    Jan 19 04:23:39.574: INFO: Deleting pod "simpletest.rc-4btrq" in namespace "gc-6463"
    Jan 19 04:23:39.586: INFO: Deleting pod "simpletest.rc-4dpjb" in namespace "gc-6463"
    Jan 19 04:23:39.598: INFO: Deleting pod "simpletest.rc-4glhf" in namespace "gc-6463"
    Jan 19 04:23:39.619: INFO: Deleting pod "simpletest.rc-4mc5p" in namespace "gc-6463"
    Jan 19 04:23:39.629: INFO: Deleting pod "simpletest.rc-4sdd9" in namespace "gc-6463"
    Jan 19 04:23:39.642: INFO: Deleting pod "simpletest.rc-56d4m" in namespace "gc-6463"
    Jan 19 04:23:39.657: INFO: Deleting pod "simpletest.rc-59rvp" in namespace "gc-6463"
    Jan 19 04:23:39.686: INFO: Deleting pod "simpletest.rc-5jts6" in namespace "gc-6463"
    Jan 19 04:23:39.704: INFO: Deleting pod "simpletest.rc-5vmg5" in namespace "gc-6463"
    Jan 19 04:23:39.718: INFO: Deleting pod "simpletest.rc-5wt7j" in namespace "gc-6463"
    Jan 19 04:23:39.728: INFO: Deleting pod "simpletest.rc-5z8s5" in namespace "gc-6463"
    Jan 19 04:23:39.745: INFO: Deleting pod "simpletest.rc-69gsx" in namespace "gc-6463"
    Jan 19 04:23:39.760: INFO: Deleting pod "simpletest.rc-6k6jc" in namespace "gc-6463"
    Jan 19 04:23:39.769: INFO: Deleting pod "simpletest.rc-6kbwx" in namespace "gc-6463"
    Jan 19 04:23:39.782: INFO: Deleting pod "simpletest.rc-6mccd" in namespace "gc-6463"
    Jan 19 04:23:39.800: INFO: Deleting pod "simpletest.rc-6plpw" in namespace "gc-6463"
    Jan 19 04:23:39.813: INFO: Deleting pod "simpletest.rc-756lp" in namespace "gc-6463"
    Jan 19 04:23:39.828: INFO: Deleting pod "simpletest.rc-7f586" in namespace "gc-6463"
    Jan 19 04:23:39.840: INFO: Deleting pod "simpletest.rc-7jd6w" in namespace "gc-6463"
    Jan 19 04:23:39.854: INFO: Deleting pod "simpletest.rc-7mnwz" in namespace "gc-6463"
    Jan 19 04:23:39.869: INFO: Deleting pod "simpletest.rc-87dkr" in namespace "gc-6463"
    Jan 19 04:23:39.883: INFO: Deleting pod "simpletest.rc-882mq" in namespace "gc-6463"
    Jan 19 04:23:39.896: INFO: Deleting pod "simpletest.rc-8msrx" in namespace "gc-6463"
    Jan 19 04:23:39.909: INFO: Deleting pod "simpletest.rc-8r42c" in namespace "gc-6463"
    Jan 19 04:23:39.918: INFO: Deleting pod "simpletest.rc-8vtpc" in namespace "gc-6463"
    Jan 19 04:23:39.929: INFO: Deleting pod "simpletest.rc-8zhfv" in namespace "gc-6463"
    Jan 19 04:23:39.944: INFO: Deleting pod "simpletest.rc-974tm" in namespace "gc-6463"
    Jan 19 04:23:39.958: INFO: Deleting pod "simpletest.rc-9dlz2" in namespace "gc-6463"
    Jan 19 04:23:39.968: INFO: Deleting pod "simpletest.rc-bjd9s" in namespace "gc-6463"
    Jan 19 04:23:39.979: INFO: Deleting pod "simpletest.rc-btx5n" in namespace "gc-6463"
    Jan 19 04:23:39.990: INFO: Deleting pod "simpletest.rc-cg2lt" in namespace "gc-6463"
    Jan 19 04:23:40.002: INFO: Deleting pod "simpletest.rc-cr5rh" in namespace "gc-6463"
    Jan 19 04:23:40.027: INFO: Deleting pod "simpletest.rc-ct647" in namespace "gc-6463"
    Jan 19 04:23:40.038: INFO: Deleting pod "simpletest.rc-cwvb6" in namespace "gc-6463"
    Jan 19 04:23:40.053: INFO: Deleting pod "simpletest.rc-dfztx" in namespace "gc-6463"
    Jan 19 04:23:40.064: INFO: Deleting pod "simpletest.rc-f4t2t" in namespace "gc-6463"
    Jan 19 04:23:40.076: INFO: Deleting pod "simpletest.rc-f5dkw" in namespace "gc-6463"
    Jan 19 04:23:40.088: INFO: Deleting pod "simpletest.rc-f5ms8" in namespace "gc-6463"
    Jan 19 04:23:40.098: INFO: Deleting pod "simpletest.rc-fkr4f" in namespace "gc-6463"
    Jan 19 04:23:40.112: INFO: Deleting pod "simpletest.rc-fq7bn" in namespace "gc-6463"
    Jan 19 04:23:40.123: INFO: Deleting pod "simpletest.rc-fqj6r" in namespace "gc-6463"
    Jan 19 04:23:40.134: INFO: Deleting pod "simpletest.rc-g9b68" in namespace "gc-6463"
    Jan 19 04:23:40.143: INFO: Deleting pod "simpletest.rc-gd255" in namespace "gc-6463"
    Jan 19 04:23:40.161: INFO: Deleting pod "simpletest.rc-gfsxm" in namespace "gc-6463"
    Jan 19 04:23:40.172: INFO: Deleting pod "simpletest.rc-gg5jk" in namespace "gc-6463"
    Jan 19 04:23:40.182: INFO: Deleting pod "simpletest.rc-gxjgv" in namespace "gc-6463"
    Jan 19 04:23:40.196: INFO: Deleting pod "simpletest.rc-h2jz7" in namespace "gc-6463"
    Jan 19 04:23:40.209: INFO: Deleting pod "simpletest.rc-h7brg" in namespace "gc-6463"
    Jan 19 04:23:40.221: INFO: Deleting pod "simpletest.rc-h9ncn" in namespace "gc-6463"
    Jan 19 04:23:40.237: INFO: Deleting pod "simpletest.rc-hqxz6" in namespace "gc-6463"
    Jan 19 04:23:40.248: INFO: Deleting pod "simpletest.rc-hwcn6" in namespace "gc-6463"
    Jan 19 04:23:40.260: INFO: Deleting pod "simpletest.rc-hxfdn" in namespace "gc-6463"
    Jan 19 04:23:40.271: INFO: Deleting pod "simpletest.rc-j4hg6" in namespace "gc-6463"
    Jan 19 04:23:40.281: INFO: Deleting pod "simpletest.rc-j9d9w" in namespace "gc-6463"
    Jan 19 04:23:40.290: INFO: Deleting pod "simpletest.rc-jwjbf" in namespace "gc-6463"
    Jan 19 04:23:40.300: INFO: Deleting pod "simpletest.rc-kwpg5" in namespace "gc-6463"
    Jan 19 04:23:40.311: INFO: Deleting pod "simpletest.rc-kx85b" in namespace "gc-6463"
    Jan 19 04:23:40.323: INFO: Deleting pod "simpletest.rc-lb2j6" in namespace "gc-6463"
    Jan 19 04:23:40.338: INFO: Deleting pod "simpletest.rc-lx2gv" in namespace "gc-6463"
    Jan 19 04:23:40.352: INFO: Deleting pod "simpletest.rc-m2tzk" in namespace "gc-6463"
    Jan 19 04:23:40.362: INFO: Deleting pod "simpletest.rc-mbhz6" in namespace "gc-6463"
    Jan 19 04:23:40.373: INFO: Deleting pod "simpletest.rc-mpxfh" in namespace "gc-6463"
    Jan 19 04:23:40.385: INFO: Deleting pod "simpletest.rc-msplp" in namespace "gc-6463"
    Jan 19 04:23:40.421: INFO: Deleting pod "simpletest.rc-mz9nb" in namespace "gc-6463"
    Jan 19 04:23:40.475: INFO: Deleting pod "simpletest.rc-nwpnm" in namespace "gc-6463"
    Jan 19 04:23:40.525: INFO: Deleting pod "simpletest.rc-p2q6s" in namespace "gc-6463"
    Jan 19 04:23:40.576: INFO: Deleting pod "simpletest.rc-pk2jb" in namespace "gc-6463"
    Jan 19 04:23:40.626: INFO: Deleting pod "simpletest.rc-psp6r" in namespace "gc-6463"
    Jan 19 04:23:40.673: INFO: Deleting pod "simpletest.rc-qvszb" in namespace "gc-6463"
    Jan 19 04:23:40.723: INFO: Deleting pod "simpletest.rc-rck6x" in namespace "gc-6463"
    Jan 19 04:23:40.776: INFO: Deleting pod "simpletest.rc-rg492" in namespace "gc-6463"
    Jan 19 04:23:40.830: INFO: Deleting pod "simpletest.rc-rmw8n" in namespace "gc-6463"
    Jan 19 04:23:40.873: INFO: Deleting pod "simpletest.rc-rwzh6" in namespace "gc-6463"
    Jan 19 04:23:40.929: INFO: Deleting pod "simpletest.rc-s48sb" in namespace "gc-6463"
    Jan 19 04:23:40.975: INFO: Deleting pod "simpletest.rc-s7wdk" in namespace "gc-6463"
    Jan 19 04:23:41.026: INFO: Deleting pod "simpletest.rc-snvsc" in namespace "gc-6463"
    Jan 19 04:23:41.077: INFO: Deleting pod "simpletest.rc-strk6" in namespace "gc-6463"
    Jan 19 04:23:41.129: INFO: Deleting pod "simpletest.rc-tt785" in namespace "gc-6463"
    Jan 19 04:23:41.176: INFO: Deleting pod "simpletest.rc-v9hl4" in namespace "gc-6463"
    Jan 19 04:23:41.227: INFO: Deleting pod "simpletest.rc-v9qzd" in namespace "gc-6463"
    Jan 19 04:23:41.275: INFO: Deleting pod "simpletest.rc-vgx6j" in namespace "gc-6463"
    Jan 19 04:23:41.331: INFO: Deleting pod "simpletest.rc-vw62x" in namespace "gc-6463"
    Jan 19 04:23:41.374: INFO: Deleting pod "simpletest.rc-wl64q" in namespace "gc-6463"
    Jan 19 04:23:41.429: INFO: Deleting pod "simpletest.rc-wppbx" in namespace "gc-6463"
    Jan 19 04:23:41.475: INFO: Deleting pod "simpletest.rc-wwsq2" in namespace "gc-6463"
    Jan 19 04:23:41.546: INFO: Deleting pod "simpletest.rc-x5m46" in namespace "gc-6463"
    Jan 19 04:23:41.579: INFO: Deleting pod "simpletest.rc-xk6qv" in namespace "gc-6463"
    Jan 19 04:23:41.623: INFO: Deleting pod "simpletest.rc-xqsb4" in namespace "gc-6463"
    Jan 19 04:23:41.675: INFO: Deleting pod "simpletest.rc-xt9vq" in namespace "gc-6463"
    Jan 19 04:23:41.724: INFO: Deleting pod "simpletest.rc-xz4js" in namespace "gc-6463"
    Jan 19 04:23:41.777: INFO: Deleting pod "simpletest.rc-zbm5t" in namespace "gc-6463"
    Jan 19 04:23:41.829: INFO: Deleting pod "simpletest.rc-zbwl5" in namespace "gc-6463"
    Jan 19 04:23:41.873: INFO: Deleting pod "simpletest.rc-zfjpm" in namespace "gc-6463"
    Jan 19 04:23:41.923: INFO: Deleting pod "simpletest.rc-zgc57" in namespace "gc-6463"
    Jan 19 04:23:41.978: INFO: Deleting pod "simpletest.rc-zjrml" in namespace "gc-6463"
    Jan 19 04:23:42.029: INFO: Deleting pod "simpletest.rc-zrnqh" in namespace "gc-6463"
    Jan 19 04:23:42.074: INFO: Deleting pod "simpletest.rc-zrscb" in namespace "gc-6463"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 19 04:23:42.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-6463" for this suite. 01/19/23 04:23:42.168
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:23:42.221
Jan 19 04:23:42.221: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename emptydir 01/19/23 04:23:42.222
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:23:42.237
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:23:42.24
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
STEP: Creating a pod to test emptydir 0777 on tmpfs 01/19/23 04:23:42.245
Jan 19 04:23:42.267: INFO: Waiting up to 5m0s for pod "pod-04d71ca3-a293-4964-b434-269625dbf0c9" in namespace "emptydir-1900" to be "Succeeded or Failed"
Jan 19 04:23:42.272: INFO: Pod "pod-04d71ca3-a293-4964-b434-269625dbf0c9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.272702ms
Jan 19 04:23:44.277: INFO: Pod "pod-04d71ca3-a293-4964-b434-269625dbf0c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009374877s
Jan 19 04:23:46.279: INFO: Pod "pod-04d71ca3-a293-4964-b434-269625dbf0c9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011342776s
Jan 19 04:23:48.276: INFO: Pod "pod-04d71ca3-a293-4964-b434-269625dbf0c9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008766657s
Jan 19 04:23:50.276: INFO: Pod "pod-04d71ca3-a293-4964-b434-269625dbf0c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008792463s
STEP: Saw pod success 01/19/23 04:23:50.276
Jan 19 04:23:50.276: INFO: Pod "pod-04d71ca3-a293-4964-b434-269625dbf0c9" satisfied condition "Succeeded or Failed"
Jan 19 04:23:50.279: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-04d71ca3-a293-4964-b434-269625dbf0c9 container test-container: <nil>
STEP: delete the pod 01/19/23 04:23:50.284
Jan 19 04:23:50.297: INFO: Waiting for pod pod-04d71ca3-a293-4964-b434-269625dbf0c9 to disappear
Jan 19 04:23:50.300: INFO: Pod pod-04d71ca3-a293-4964-b434-269625dbf0c9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 19 04:23:50.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1900" for this suite. 01/19/23 04:23:50.303
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":24,"skipped":481,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.088 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:23:42.221
    Jan 19 04:23:42.221: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename emptydir 01/19/23 04:23:42.222
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:23:42.237
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:23:42.24
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:146
    STEP: Creating a pod to test emptydir 0777 on tmpfs 01/19/23 04:23:42.245
    Jan 19 04:23:42.267: INFO: Waiting up to 5m0s for pod "pod-04d71ca3-a293-4964-b434-269625dbf0c9" in namespace "emptydir-1900" to be "Succeeded or Failed"
    Jan 19 04:23:42.272: INFO: Pod "pod-04d71ca3-a293-4964-b434-269625dbf0c9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.272702ms
    Jan 19 04:23:44.277: INFO: Pod "pod-04d71ca3-a293-4964-b434-269625dbf0c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009374877s
    Jan 19 04:23:46.279: INFO: Pod "pod-04d71ca3-a293-4964-b434-269625dbf0c9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011342776s
    Jan 19 04:23:48.276: INFO: Pod "pod-04d71ca3-a293-4964-b434-269625dbf0c9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008766657s
    Jan 19 04:23:50.276: INFO: Pod "pod-04d71ca3-a293-4964-b434-269625dbf0c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008792463s
    STEP: Saw pod success 01/19/23 04:23:50.276
    Jan 19 04:23:50.276: INFO: Pod "pod-04d71ca3-a293-4964-b434-269625dbf0c9" satisfied condition "Succeeded or Failed"
    Jan 19 04:23:50.279: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-04d71ca3-a293-4964-b434-269625dbf0c9 container test-container: <nil>
    STEP: delete the pod 01/19/23 04:23:50.284
    Jan 19 04:23:50.297: INFO: Waiting for pod pod-04d71ca3-a293-4964-b434-269625dbf0c9 to disappear
    Jan 19 04:23:50.300: INFO: Pod pod-04d71ca3-a293-4964-b434-269625dbf0c9 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 19 04:23:50.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1900" for this suite. 01/19/23 04:23:50.303
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:23:50.309
Jan 19 04:23:50.309: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename services 01/19/23 04:23:50.31
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:23:50.324
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:23:50.327
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
STEP: creating a Service 01/19/23 04:23:50.333
STEP: watching for the Service to be added 01/19/23 04:23:50.343
Jan 19 04:23:50.345: INFO: Found Service test-service-w9nc6 in namespace services-8431 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Jan 19 04:23:50.345: INFO: Service test-service-w9nc6 created
STEP: Getting /status 01/19/23 04:23:50.345
Jan 19 04:23:50.349: INFO: Service test-service-w9nc6 has LoadBalancer: {[]}
STEP: patching the ServiceStatus 01/19/23 04:23:50.349
STEP: watching for the Service to be patched 01/19/23 04:23:50.357
Jan 19 04:23:50.358: INFO: observed Service test-service-w9nc6 in namespace services-8431 with annotations: map[] & LoadBalancer: {[]}
Jan 19 04:23:50.358: INFO: Found Service test-service-w9nc6 in namespace services-8431 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Jan 19 04:23:50.358: INFO: Service test-service-w9nc6 has service status patched
STEP: updating the ServiceStatus 01/19/23 04:23:50.358
Jan 19 04:23:50.369: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 01/19/23 04:23:50.369
Jan 19 04:23:50.370: INFO: Observed Service test-service-w9nc6 in namespace services-8431 with annotations: map[] & Conditions: {[]}
Jan 19 04:23:50.370: INFO: Observed event: &Service{ObjectMeta:{test-service-w9nc6  services-8431  17e07874-09de-4222-8f6b-a0d65e4215b7 8230 0 2023-01-19 04:23:50 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-01-19 04:23:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-01-19 04:23:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.254.109.49,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.254.109.49],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Jan 19 04:23:50.370: INFO: Found Service test-service-w9nc6 in namespace services-8431 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 19 04:23:50.370: INFO: Service test-service-w9nc6 has service status updated
STEP: patching the service 01/19/23 04:23:50.37
STEP: watching for the Service to be patched 01/19/23 04:23:50.384
Jan 19 04:23:50.386: INFO: observed Service test-service-w9nc6 in namespace services-8431 with labels: map[test-service-static:true]
Jan 19 04:23:50.386: INFO: observed Service test-service-w9nc6 in namespace services-8431 with labels: map[test-service-static:true]
Jan 19 04:23:50.386: INFO: observed Service test-service-w9nc6 in namespace services-8431 with labels: map[test-service-static:true]
Jan 19 04:23:50.386: INFO: Found Service test-service-w9nc6 in namespace services-8431 with labels: map[test-service:patched test-service-static:true]
Jan 19 04:23:50.386: INFO: Service test-service-w9nc6 patched
STEP: deleting the service 01/19/23 04:23:50.386
STEP: watching for the Service to be deleted 01/19/23 04:23:50.737
Jan 19 04:23:50.739: INFO: Observed event: ADDED
Jan 19 04:23:50.739: INFO: Observed event: MODIFIED
Jan 19 04:23:50.739: INFO: Observed event: MODIFIED
Jan 19 04:23:50.739: INFO: Observed event: MODIFIED
Jan 19 04:23:50.739: INFO: Found Service test-service-w9nc6 in namespace services-8431 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Jan 19 04:23:50.739: INFO: Service test-service-w9nc6 deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 19 04:23:50.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8431" for this suite. 01/19/23 04:23:50.742
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","completed":25,"skipped":500,"failed":0}
------------------------------
â€¢ [0.441 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:23:50.309
    Jan 19 04:23:50.309: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename services 01/19/23 04:23:50.31
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:23:50.324
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:23:50.327
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3415
    STEP: creating a Service 01/19/23 04:23:50.333
    STEP: watching for the Service to be added 01/19/23 04:23:50.343
    Jan 19 04:23:50.345: INFO: Found Service test-service-w9nc6 in namespace services-8431 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Jan 19 04:23:50.345: INFO: Service test-service-w9nc6 created
    STEP: Getting /status 01/19/23 04:23:50.345
    Jan 19 04:23:50.349: INFO: Service test-service-w9nc6 has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 01/19/23 04:23:50.349
    STEP: watching for the Service to be patched 01/19/23 04:23:50.357
    Jan 19 04:23:50.358: INFO: observed Service test-service-w9nc6 in namespace services-8431 with annotations: map[] & LoadBalancer: {[]}
    Jan 19 04:23:50.358: INFO: Found Service test-service-w9nc6 in namespace services-8431 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Jan 19 04:23:50.358: INFO: Service test-service-w9nc6 has service status patched
    STEP: updating the ServiceStatus 01/19/23 04:23:50.358
    Jan 19 04:23:50.369: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 01/19/23 04:23:50.369
    Jan 19 04:23:50.370: INFO: Observed Service test-service-w9nc6 in namespace services-8431 with annotations: map[] & Conditions: {[]}
    Jan 19 04:23:50.370: INFO: Observed event: &Service{ObjectMeta:{test-service-w9nc6  services-8431  17e07874-09de-4222-8f6b-a0d65e4215b7 8230 0 2023-01-19 04:23:50 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-01-19 04:23:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-01-19 04:23:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.254.109.49,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.254.109.49],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Jan 19 04:23:50.370: INFO: Found Service test-service-w9nc6 in namespace services-8431 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 19 04:23:50.370: INFO: Service test-service-w9nc6 has service status updated
    STEP: patching the service 01/19/23 04:23:50.37
    STEP: watching for the Service to be patched 01/19/23 04:23:50.384
    Jan 19 04:23:50.386: INFO: observed Service test-service-w9nc6 in namespace services-8431 with labels: map[test-service-static:true]
    Jan 19 04:23:50.386: INFO: observed Service test-service-w9nc6 in namespace services-8431 with labels: map[test-service-static:true]
    Jan 19 04:23:50.386: INFO: observed Service test-service-w9nc6 in namespace services-8431 with labels: map[test-service-static:true]
    Jan 19 04:23:50.386: INFO: Found Service test-service-w9nc6 in namespace services-8431 with labels: map[test-service:patched test-service-static:true]
    Jan 19 04:23:50.386: INFO: Service test-service-w9nc6 patched
    STEP: deleting the service 01/19/23 04:23:50.386
    STEP: watching for the Service to be deleted 01/19/23 04:23:50.737
    Jan 19 04:23:50.739: INFO: Observed event: ADDED
    Jan 19 04:23:50.739: INFO: Observed event: MODIFIED
    Jan 19 04:23:50.739: INFO: Observed event: MODIFIED
    Jan 19 04:23:50.739: INFO: Observed event: MODIFIED
    Jan 19 04:23:50.739: INFO: Found Service test-service-w9nc6 in namespace services-8431 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Jan 19 04:23:50.739: INFO: Service test-service-w9nc6 deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 19 04:23:50.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8431" for this suite. 01/19/23 04:23:50.742
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:23:50.751
Jan 19 04:23:50.751: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename sysctl 01/19/23 04:23:50.751
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:23:50.765
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:23:50.766
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 01/19/23 04:23:50.768
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 19 04:23:50.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-1762" for this suite. 01/19/23 04:23:50.776
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":26,"skipped":508,"failed":0}
------------------------------
â€¢ [0.030 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:23:50.751
    Jan 19 04:23:50.751: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename sysctl 01/19/23 04:23:50.751
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:23:50.765
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:23:50.766
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 01/19/23 04:23:50.768
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 19 04:23:50.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-1762" for this suite. 01/19/23 04:23:50.776
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:23:50.782
Jan 19 04:23:50.782: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename cronjob 01/19/23 04:23:50.782
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:23:50.844
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:23:50.846
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 01/19/23 04:23:50.849
STEP: Ensuring more than one job is running at a time 01/19/23 04:23:50.914
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 01/19/23 04:25:00.92
STEP: Removing cronjob 01/19/23 04:25:00.926
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan 19 04:25:00.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6215" for this suite. 01/19/23 04:25:00.937
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","completed":27,"skipped":522,"failed":0}
------------------------------
â€¢ [SLOW TEST] [70.162 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:23:50.782
    Jan 19 04:23:50.782: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename cronjob 01/19/23 04:23:50.782
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:23:50.844
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:23:50.846
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 01/19/23 04:23:50.849
    STEP: Ensuring more than one job is running at a time 01/19/23 04:23:50.914
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 01/19/23 04:25:00.92
    STEP: Removing cronjob 01/19/23 04:25:00.926
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan 19 04:25:00.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-6215" for this suite. 01/19/23 04:25:00.937
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:25:00.944
Jan 19 04:25:00.944: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename kubectl 01/19/23 04:25:00.945
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:25:00.978
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:25:00.981
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1492
STEP: creating the pod 01/19/23 04:25:00.984
Jan 19 04:25:00.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-2120 create -f -'
Jan 19 04:25:01.983: INFO: stderr: ""
Jan 19 04:25:01.983: INFO: stdout: "pod/pause created\n"
Jan 19 04:25:01.983: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jan 19 04:25:01.983: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-2120" to be "running and ready"
Jan 19 04:25:01.990: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 7.28428ms
Jan 19 04:25:01.990: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'ckcp-nks-default-worker-node-1' to be 'Running' but was 'Pending'
Jan 19 04:25:03.995: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011778517s
Jan 19 04:25:03.995: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'ckcp-nks-default-worker-node-1' to be 'Running' but was 'Pending'
Jan 19 04:25:06.009: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.026547697s
Jan 19 04:25:06.009: INFO: Pod "pause" satisfied condition "running and ready"
Jan 19 04:25:06.009: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
STEP: adding the label testing-label with value testing-label-value to a pod 01/19/23 04:25:06.009
Jan 19 04:25:06.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-2120 label pods pause testing-label=testing-label-value'
Jan 19 04:25:06.111: INFO: stderr: ""
Jan 19 04:25:06.111: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 01/19/23 04:25:06.111
Jan 19 04:25:06.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-2120 get pod pause -L testing-label'
Jan 19 04:25:06.190: INFO: stderr: ""
Jan 19 04:25:06.190: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
STEP: removing the label testing-label of a pod 01/19/23 04:25:06.19
Jan 19 04:25:06.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-2120 label pods pause testing-label-'
Jan 19 04:25:06.275: INFO: stderr: ""
Jan 19 04:25:06.275: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 01/19/23 04:25:06.275
Jan 19 04:25:06.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-2120 get pod pause -L testing-label'
Jan 19 04:25:06.353: INFO: stderr: ""
Jan 19 04:25:06.353: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1498
STEP: using delete to clean up resources 01/19/23 04:25:06.353
Jan 19 04:25:06.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-2120 delete --grace-period=0 --force -f -'
Jan 19 04:25:06.437: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 19 04:25:06.437: INFO: stdout: "pod \"pause\" force deleted\n"
Jan 19 04:25:06.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-2120 get rc,svc -l name=pause --no-headers'
Jan 19 04:25:06.526: INFO: stderr: "No resources found in kubectl-2120 namespace.\n"
Jan 19 04:25:06.526: INFO: stdout: ""
Jan 19 04:25:06.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-2120 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 19 04:25:06.603: INFO: stderr: ""
Jan 19 04:25:06.603: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 19 04:25:06.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2120" for this suite. 01/19/23 04:25:06.607
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","completed":28,"skipped":544,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.670 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1490
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:25:00.944
    Jan 19 04:25:00.944: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename kubectl 01/19/23 04:25:00.945
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:25:00.978
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:25:00.981
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1492
    STEP: creating the pod 01/19/23 04:25:00.984
    Jan 19 04:25:00.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-2120 create -f -'
    Jan 19 04:25:01.983: INFO: stderr: ""
    Jan 19 04:25:01.983: INFO: stdout: "pod/pause created\n"
    Jan 19 04:25:01.983: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Jan 19 04:25:01.983: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-2120" to be "running and ready"
    Jan 19 04:25:01.990: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 7.28428ms
    Jan 19 04:25:01.990: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'ckcp-nks-default-worker-node-1' to be 'Running' but was 'Pending'
    Jan 19 04:25:03.995: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011778517s
    Jan 19 04:25:03.995: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'ckcp-nks-default-worker-node-1' to be 'Running' but was 'Pending'
    Jan 19 04:25:06.009: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.026547697s
    Jan 19 04:25:06.009: INFO: Pod "pause" satisfied condition "running and ready"
    Jan 19 04:25:06.009: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1507
    STEP: adding the label testing-label with value testing-label-value to a pod 01/19/23 04:25:06.009
    Jan 19 04:25:06.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-2120 label pods pause testing-label=testing-label-value'
    Jan 19 04:25:06.111: INFO: stderr: ""
    Jan 19 04:25:06.111: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 01/19/23 04:25:06.111
    Jan 19 04:25:06.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-2120 get pod pause -L testing-label'
    Jan 19 04:25:06.190: INFO: stderr: ""
    Jan 19 04:25:06.190: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 01/19/23 04:25:06.19
    Jan 19 04:25:06.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-2120 label pods pause testing-label-'
    Jan 19 04:25:06.275: INFO: stderr: ""
    Jan 19 04:25:06.275: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 01/19/23 04:25:06.275
    Jan 19 04:25:06.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-2120 get pod pause -L testing-label'
    Jan 19 04:25:06.353: INFO: stderr: ""
    Jan 19 04:25:06.353: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1498
    STEP: using delete to clean up resources 01/19/23 04:25:06.353
    Jan 19 04:25:06.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-2120 delete --grace-period=0 --force -f -'
    Jan 19 04:25:06.437: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 19 04:25:06.437: INFO: stdout: "pod \"pause\" force deleted\n"
    Jan 19 04:25:06.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-2120 get rc,svc -l name=pause --no-headers'
    Jan 19 04:25:06.526: INFO: stderr: "No resources found in kubectl-2120 namespace.\n"
    Jan 19 04:25:06.526: INFO: stdout: ""
    Jan 19 04:25:06.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-2120 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 19 04:25:06.603: INFO: stderr: ""
    Jan 19 04:25:06.603: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 19 04:25:06.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2120" for this suite. 01/19/23 04:25:06.607
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:25:06.615
Jan 19 04:25:06.615: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename pod-network-test 01/19/23 04:25:06.616
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:25:06.631
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:25:06.634
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-2916 01/19/23 04:25:06.636
STEP: creating a selector 01/19/23 04:25:06.69
STEP: Creating the service pods in kubernetes 01/19/23 04:25:06.69
Jan 19 04:25:06.690: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 19 04:25:06.716: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2916" to be "running and ready"
Jan 19 04:25:06.721: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.538829ms
Jan 19 04:25:06.721: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 04:25:08.726: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010129425s
Jan 19 04:25:08.726: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 04:25:10.726: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010326491s
Jan 19 04:25:10.726: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 04:25:12.726: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009946436s
Jan 19 04:25:12.726: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 04:25:14.725: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.009497349s
Jan 19 04:25:14.725: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 04:25:16.726: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.010464444s
Jan 19 04:25:16.726: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 04:25:18.726: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.010415831s
Jan 19 04:25:18.726: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 04:25:20.725: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.009741189s
Jan 19 04:25:20.725: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 04:25:22.726: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.01010023s
Jan 19 04:25:22.726: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 04:25:24.726: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.009943584s
Jan 19 04:25:24.726: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 04:25:26.726: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.00989283s
Jan 19 04:25:26.726: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 04:25:28.726: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.009911799s
Jan 19 04:25:28.726: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 19 04:25:28.726: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 19 04:25:28.729: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2916" to be "running and ready"
Jan 19 04:25:28.732: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.612433ms
Jan 19 04:25:28.732: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 19 04:25:28.732: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 01/19/23 04:25:28.734
Jan 19 04:25:28.750: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2916" to be "running"
Jan 19 04:25:28.756: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.944647ms
Jan 19 04:25:30.760: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010228514s
Jan 19 04:25:32.761: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.010916563s
Jan 19 04:25:32.761: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 19 04:25:32.764: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-2916" to be "running"
Jan 19 04:25:32.767: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.332539ms
Jan 19 04:25:32.767: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jan 19 04:25:32.770: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 19 04:25:32.770: INFO: Going to poll 10.100.24.62 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Jan 19 04:25:32.772: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.100.24.62 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2916 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 04:25:32.772: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
Jan 19 04:25:32.772: INFO: ExecWithOptions: Clientset creation
Jan 19 04:25:32.772: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-2916/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.100.24.62+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 19 04:25:33.888: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 19 04:25:33.888: INFO: Going to poll 10.100.70.78 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Jan 19 04:25:33.892: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.100.70.78 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2916 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 04:25:33.892: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
Jan 19 04:25:33.892: INFO: ExecWithOptions: Clientset creation
Jan 19 04:25:33.892: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-2916/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.100.70.78+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 19 04:25:34.975: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jan 19 04:25:34.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2916" for this suite. 01/19/23 04:25:34.978
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","completed":29,"skipped":586,"failed":0}
------------------------------
â€¢ [SLOW TEST] [28.369 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:25:06.615
    Jan 19 04:25:06.615: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename pod-network-test 01/19/23 04:25:06.616
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:25:06.631
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:25:06.634
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-2916 01/19/23 04:25:06.636
    STEP: creating a selector 01/19/23 04:25:06.69
    STEP: Creating the service pods in kubernetes 01/19/23 04:25:06.69
    Jan 19 04:25:06.690: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 19 04:25:06.716: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2916" to be "running and ready"
    Jan 19 04:25:06.721: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.538829ms
    Jan 19 04:25:06.721: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 04:25:08.726: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010129425s
    Jan 19 04:25:08.726: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 04:25:10.726: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010326491s
    Jan 19 04:25:10.726: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 04:25:12.726: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009946436s
    Jan 19 04:25:12.726: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 04:25:14.725: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.009497349s
    Jan 19 04:25:14.725: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 04:25:16.726: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.010464444s
    Jan 19 04:25:16.726: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 04:25:18.726: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.010415831s
    Jan 19 04:25:18.726: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 04:25:20.725: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.009741189s
    Jan 19 04:25:20.725: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 04:25:22.726: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.01010023s
    Jan 19 04:25:22.726: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 04:25:24.726: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.009943584s
    Jan 19 04:25:24.726: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 04:25:26.726: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.00989283s
    Jan 19 04:25:26.726: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 04:25:28.726: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.009911799s
    Jan 19 04:25:28.726: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 19 04:25:28.726: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 19 04:25:28.729: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2916" to be "running and ready"
    Jan 19 04:25:28.732: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.612433ms
    Jan 19 04:25:28.732: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 19 04:25:28.732: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 01/19/23 04:25:28.734
    Jan 19 04:25:28.750: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2916" to be "running"
    Jan 19 04:25:28.756: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.944647ms
    Jan 19 04:25:30.760: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010228514s
    Jan 19 04:25:32.761: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.010916563s
    Jan 19 04:25:32.761: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 19 04:25:32.764: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-2916" to be "running"
    Jan 19 04:25:32.767: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.332539ms
    Jan 19 04:25:32.767: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jan 19 04:25:32.770: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jan 19 04:25:32.770: INFO: Going to poll 10.100.24.62 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Jan 19 04:25:32.772: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.100.24.62 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2916 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 04:25:32.772: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    Jan 19 04:25:32.772: INFO: ExecWithOptions: Clientset creation
    Jan 19 04:25:32.772: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-2916/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.100.24.62+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 19 04:25:33.888: INFO: Found all 1 expected endpoints: [netserver-0]
    Jan 19 04:25:33.888: INFO: Going to poll 10.100.70.78 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Jan 19 04:25:33.892: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.100.70.78 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2916 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 04:25:33.892: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    Jan 19 04:25:33.892: INFO: ExecWithOptions: Clientset creation
    Jan 19 04:25:33.892: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-2916/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.100.70.78+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 19 04:25:34.975: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jan 19 04:25:34.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-2916" for this suite. 01/19/23 04:25:34.978
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:25:34.985
Jan 19 04:25:34.985: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename hostport 01/19/23 04:25:34.986
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:25:35.001
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:25:35.002
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 01/19/23 04:25:35.008
Jan 19 04:25:35.018: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-1225" to be "running and ready"
Jan 19 04:25:35.026: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.989299ms
Jan 19 04:25:35.026: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 04:25:37.031: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013484456s
Jan 19 04:25:37.031: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 04:25:39.030: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.012766201s
Jan 19 04:25:39.031: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 19 04:25:39.031: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.0.78 on the node which pod1 resides and expect scheduled 01/19/23 04:25:39.031
Jan 19 04:25:39.038: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-1225" to be "running and ready"
Jan 19 04:25:39.042: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.866857ms
Jan 19 04:25:39.042: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 04:25:41.048: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01077376s
Jan 19 04:25:41.048: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 04:25:43.046: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.008075162s
Jan 19 04:25:43.046: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 19 04:25:43.046: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.0.78 but use UDP protocol on the node which pod2 resides 01/19/23 04:25:43.046
Jan 19 04:25:43.054: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-1225" to be "running and ready"
Jan 19 04:25:43.059: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.040286ms
Jan 19 04:25:43.059: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 04:25:45.067: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012379548s
Jan 19 04:25:45.067: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 04:25:47.064: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.00936901s
Jan 19 04:25:47.064: INFO: The phase of Pod pod3 is Running (Ready = true)
Jan 19 04:25:47.064: INFO: Pod "pod3" satisfied condition "running and ready"
Jan 19 04:25:47.070: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-1225" to be "running and ready"
Jan 19 04:25:47.074: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 3.926059ms
Jan 19 04:25:47.074: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jan 19 04:25:49.079: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.008725314s
Jan 19 04:25:49.079: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Jan 19 04:25:49.079: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 01/19/23 04:25:49.082
Jan 19 04:25:49.082: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.0.78 http://127.0.0.1:54323/hostname] Namespace:hostport-1225 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 04:25:49.082: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
Jan 19 04:25:49.082: INFO: ExecWithOptions: Clientset creation
Jan 19 04:25:49.082: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/hostport-1225/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.0.78+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.0.78, port: 54323 01/19/23 04:25:49.164
Jan 19 04:25:49.164: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.0.78:54323/hostname] Namespace:hostport-1225 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 04:25:49.164: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
Jan 19 04:25:49.164: INFO: ExecWithOptions: Clientset creation
Jan 19 04:25:49.164: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/hostport-1225/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.0.78%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.0.78, port: 54323 UDP 01/19/23 04:25:49.241
Jan 19 04:25:49.241: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.0.78 54323] Namespace:hostport-1225 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 04:25:49.241: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
Jan 19 04:25:49.241: INFO: ExecWithOptions: Clientset creation
Jan 19 04:25:49.242: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/hostport-1225/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.0.78+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
Jan 19 04:25:54.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-1225" for this suite. 01/19/23 04:25:54.324
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","completed":30,"skipped":615,"failed":0}
------------------------------
â€¢ [SLOW TEST] [19.343 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:25:34.985
    Jan 19 04:25:34.985: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename hostport 01/19/23 04:25:34.986
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:25:35.001
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:25:35.002
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 01/19/23 04:25:35.008
    Jan 19 04:25:35.018: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-1225" to be "running and ready"
    Jan 19 04:25:35.026: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.989299ms
    Jan 19 04:25:35.026: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 04:25:37.031: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013484456s
    Jan 19 04:25:37.031: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 04:25:39.030: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.012766201s
    Jan 19 04:25:39.031: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 19 04:25:39.031: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.0.78 on the node which pod1 resides and expect scheduled 01/19/23 04:25:39.031
    Jan 19 04:25:39.038: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-1225" to be "running and ready"
    Jan 19 04:25:39.042: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.866857ms
    Jan 19 04:25:39.042: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 04:25:41.048: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01077376s
    Jan 19 04:25:41.048: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 04:25:43.046: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.008075162s
    Jan 19 04:25:43.046: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 19 04:25:43.046: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.0.78 but use UDP protocol on the node which pod2 resides 01/19/23 04:25:43.046
    Jan 19 04:25:43.054: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-1225" to be "running and ready"
    Jan 19 04:25:43.059: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.040286ms
    Jan 19 04:25:43.059: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 04:25:45.067: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012379548s
    Jan 19 04:25:45.067: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 04:25:47.064: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.00936901s
    Jan 19 04:25:47.064: INFO: The phase of Pod pod3 is Running (Ready = true)
    Jan 19 04:25:47.064: INFO: Pod "pod3" satisfied condition "running and ready"
    Jan 19 04:25:47.070: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-1225" to be "running and ready"
    Jan 19 04:25:47.074: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 3.926059ms
    Jan 19 04:25:47.074: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 04:25:49.079: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.008725314s
    Jan 19 04:25:49.079: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Jan 19 04:25:49.079: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 01/19/23 04:25:49.082
    Jan 19 04:25:49.082: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.0.78 http://127.0.0.1:54323/hostname] Namespace:hostport-1225 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 04:25:49.082: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    Jan 19 04:25:49.082: INFO: ExecWithOptions: Clientset creation
    Jan 19 04:25:49.082: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/hostport-1225/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.0.78+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.0.78, port: 54323 01/19/23 04:25:49.164
    Jan 19 04:25:49.164: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.0.78:54323/hostname] Namespace:hostport-1225 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 04:25:49.164: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    Jan 19 04:25:49.164: INFO: ExecWithOptions: Clientset creation
    Jan 19 04:25:49.164: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/hostport-1225/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.0.78%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.0.78, port: 54323 UDP 01/19/23 04:25:49.241
    Jan 19 04:25:49.241: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.0.78 54323] Namespace:hostport-1225 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 04:25:49.241: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    Jan 19 04:25:49.241: INFO: ExecWithOptions: Clientset creation
    Jan 19 04:25:49.242: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/hostport-1225/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.0.78+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/framework.go:187
    Jan 19 04:25:54.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "hostport-1225" for this suite. 01/19/23 04:25:54.324
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:25:54.329
Jan 19 04:25:54.329: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename webhook 01/19/23 04:25:54.33
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:25:54.342
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:25:54.344
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/19/23 04:25:54.359
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 04:25:54.941
STEP: Deploying the webhook pod 01/19/23 04:25:54.949
STEP: Wait for the deployment to be ready 01/19/23 04:25:54.961
Jan 19 04:25:54.980: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 19 04:25:56.991: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 4, 25, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 25, 54, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 4, 25, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 25, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/19/23 04:25:58.996
STEP: Verifying the service has paired with the endpoint 01/19/23 04:25:59.009
Jan 19 04:26:00.010: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 01/19/23 04:26:00.014
STEP: create a pod that should be updated by the webhook 01/19/23 04:26:00.029
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 04:26:00.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4673" for this suite. 01/19/23 04:26:00.054
STEP: Destroying namespace "webhook-4673-markers" for this suite. 01/19/23 04:26:00.064
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","completed":31,"skipped":616,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.799 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:25:54.329
    Jan 19 04:25:54.329: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename webhook 01/19/23 04:25:54.33
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:25:54.342
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:25:54.344
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/19/23 04:25:54.359
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 04:25:54.941
    STEP: Deploying the webhook pod 01/19/23 04:25:54.949
    STEP: Wait for the deployment to be ready 01/19/23 04:25:54.961
    Jan 19 04:25:54.980: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 19 04:25:56.991: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 4, 25, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 25, 54, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 4, 25, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 25, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/19/23 04:25:58.996
    STEP: Verifying the service has paired with the endpoint 01/19/23 04:25:59.009
    Jan 19 04:26:00.010: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:263
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 01/19/23 04:26:00.014
    STEP: create a pod that should be updated by the webhook 01/19/23 04:26:00.029
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 04:26:00.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4673" for this suite. 01/19/23 04:26:00.054
    STEP: Destroying namespace "webhook-4673-markers" for this suite. 01/19/23 04:26:00.064
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:26:00.128
Jan 19 04:26:00.128: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename services 01/19/23 04:26:00.129
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:26:00.193
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:26:00.196
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
STEP: creating service multi-endpoint-test in namespace services-337 01/19/23 04:26:00.197
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-337 to expose endpoints map[] 01/19/23 04:26:00.209
Jan 19 04:26:00.217: INFO: successfully validated that service multi-endpoint-test in namespace services-337 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-337 01/19/23 04:26:00.217
Jan 19 04:26:00.223: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-337" to be "running and ready"
Jan 19 04:26:00.226: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.259783ms
Jan 19 04:26:00.226: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 04:26:02.231: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008546505s
Jan 19 04:26:02.231: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 04:26:04.231: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.007847735s
Jan 19 04:26:04.231: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 19 04:26:04.231: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-337 to expose endpoints map[pod1:[100]] 01/19/23 04:26:04.233
Jan 19 04:26:04.243: INFO: successfully validated that service multi-endpoint-test in namespace services-337 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-337 01/19/23 04:26:04.243
Jan 19 04:26:04.249: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-337" to be "running and ready"
Jan 19 04:26:04.252: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.981998ms
Jan 19 04:26:04.252: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 04:26:06.257: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007819525s
Jan 19 04:26:06.257: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 04:26:08.257: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.007957879s
Jan 19 04:26:08.257: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 19 04:26:08.257: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-337 to expose endpoints map[pod1:[100] pod2:[101]] 01/19/23 04:26:08.26
Jan 19 04:26:08.271: INFO: successfully validated that service multi-endpoint-test in namespace services-337 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 01/19/23 04:26:08.271
Jan 19 04:26:08.271: INFO: Creating new exec pod
Jan 19 04:26:08.277: INFO: Waiting up to 5m0s for pod "execpodwfmjg" in namespace "services-337" to be "running"
Jan 19 04:26:08.280: INFO: Pod "execpodwfmjg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.767965ms
Jan 19 04:26:10.285: INFO: Pod "execpodwfmjg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00815189s
Jan 19 04:26:12.284: INFO: Pod "execpodwfmjg": Phase="Running", Reason="", readiness=true. Elapsed: 4.006743199s
Jan 19 04:26:12.284: INFO: Pod "execpodwfmjg" satisfied condition "running"
Jan 19 04:26:13.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-337 exec execpodwfmjg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Jan 19 04:26:13.485: INFO: stderr: "+ nc -v -t -w 2 multi-endpoint-test 80\n+ echo hostName\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Jan 19 04:26:13.485: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 04:26:13.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-337 exec execpodwfmjg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.66.194 80'
Jan 19 04:26:13.642: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.66.194 80\nConnection to 10.254.66.194 80 port [tcp/http] succeeded!\n"
Jan 19 04:26:13.642: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 04:26:13.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-337 exec execpodwfmjg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Jan 19 04:26:13.804: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Jan 19 04:26:13.804: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 04:26:13.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-337 exec execpodwfmjg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.66.194 81'
Jan 19 04:26:13.956: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.66.194 81\nConnection to 10.254.66.194 81 port [tcp/*] succeeded!\n"
Jan 19 04:26:13.956: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-337 01/19/23 04:26:13.956
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-337 to expose endpoints map[pod2:[101]] 01/19/23 04:26:13.972
Jan 19 04:26:13.983: INFO: successfully validated that service multi-endpoint-test in namespace services-337 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-337 01/19/23 04:26:13.983
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-337 to expose endpoints map[] 01/19/23 04:26:13.995
Jan 19 04:26:14.005: INFO: successfully validated that service multi-endpoint-test in namespace services-337 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 19 04:26:14.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-337" for this suite. 01/19/23 04:26:14.03
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","completed":32,"skipped":633,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.913 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:26:00.128
    Jan 19 04:26:00.128: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename services 01/19/23 04:26:00.129
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:26:00.193
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:26:00.196
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:852
    STEP: creating service multi-endpoint-test in namespace services-337 01/19/23 04:26:00.197
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-337 to expose endpoints map[] 01/19/23 04:26:00.209
    Jan 19 04:26:00.217: INFO: successfully validated that service multi-endpoint-test in namespace services-337 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-337 01/19/23 04:26:00.217
    Jan 19 04:26:00.223: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-337" to be "running and ready"
    Jan 19 04:26:00.226: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.259783ms
    Jan 19 04:26:00.226: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 04:26:02.231: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008546505s
    Jan 19 04:26:02.231: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 04:26:04.231: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.007847735s
    Jan 19 04:26:04.231: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 19 04:26:04.231: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-337 to expose endpoints map[pod1:[100]] 01/19/23 04:26:04.233
    Jan 19 04:26:04.243: INFO: successfully validated that service multi-endpoint-test in namespace services-337 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-337 01/19/23 04:26:04.243
    Jan 19 04:26:04.249: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-337" to be "running and ready"
    Jan 19 04:26:04.252: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.981998ms
    Jan 19 04:26:04.252: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 04:26:06.257: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007819525s
    Jan 19 04:26:06.257: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 04:26:08.257: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.007957879s
    Jan 19 04:26:08.257: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 19 04:26:08.257: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-337 to expose endpoints map[pod1:[100] pod2:[101]] 01/19/23 04:26:08.26
    Jan 19 04:26:08.271: INFO: successfully validated that service multi-endpoint-test in namespace services-337 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 01/19/23 04:26:08.271
    Jan 19 04:26:08.271: INFO: Creating new exec pod
    Jan 19 04:26:08.277: INFO: Waiting up to 5m0s for pod "execpodwfmjg" in namespace "services-337" to be "running"
    Jan 19 04:26:08.280: INFO: Pod "execpodwfmjg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.767965ms
    Jan 19 04:26:10.285: INFO: Pod "execpodwfmjg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00815189s
    Jan 19 04:26:12.284: INFO: Pod "execpodwfmjg": Phase="Running", Reason="", readiness=true. Elapsed: 4.006743199s
    Jan 19 04:26:12.284: INFO: Pod "execpodwfmjg" satisfied condition "running"
    Jan 19 04:26:13.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-337 exec execpodwfmjg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
    Jan 19 04:26:13.485: INFO: stderr: "+ nc -v -t -w 2 multi-endpoint-test 80\n+ echo hostName\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Jan 19 04:26:13.485: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 19 04:26:13.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-337 exec execpodwfmjg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.66.194 80'
    Jan 19 04:26:13.642: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.66.194 80\nConnection to 10.254.66.194 80 port [tcp/http] succeeded!\n"
    Jan 19 04:26:13.642: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 19 04:26:13.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-337 exec execpodwfmjg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
    Jan 19 04:26:13.804: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Jan 19 04:26:13.804: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 19 04:26:13.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-337 exec execpodwfmjg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.66.194 81'
    Jan 19 04:26:13.956: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.66.194 81\nConnection to 10.254.66.194 81 port [tcp/*] succeeded!\n"
    Jan 19 04:26:13.956: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-337 01/19/23 04:26:13.956
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-337 to expose endpoints map[pod2:[101]] 01/19/23 04:26:13.972
    Jan 19 04:26:13.983: INFO: successfully validated that service multi-endpoint-test in namespace services-337 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-337 01/19/23 04:26:13.983
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-337 to expose endpoints map[] 01/19/23 04:26:13.995
    Jan 19 04:26:14.005: INFO: successfully validated that service multi-endpoint-test in namespace services-337 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 19 04:26:14.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-337" for this suite. 01/19/23 04:26:14.03
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:26:14.042
Jan 19 04:26:14.042: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename var-expansion 01/19/23 04:26:14.042
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:26:14.056
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:26:14.059
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
STEP: Creating a pod to test substitution in volume subpath 01/19/23 04:26:14.064
Jan 19 04:26:14.076: INFO: Waiting up to 5m0s for pod "var-expansion-d60707b1-2407-4e3b-9648-48d20f5da8e9" in namespace "var-expansion-7268" to be "Succeeded or Failed"
Jan 19 04:26:14.079: INFO: Pod "var-expansion-d60707b1-2407-4e3b-9648-48d20f5da8e9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.702438ms
Jan 19 04:26:16.084: INFO: Pod "var-expansion-d60707b1-2407-4e3b-9648-48d20f5da8e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008287249s
Jan 19 04:26:18.085: INFO: Pod "var-expansion-d60707b1-2407-4e3b-9648-48d20f5da8e9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008879103s
Jan 19 04:26:20.084: INFO: Pod "var-expansion-d60707b1-2407-4e3b-9648-48d20f5da8e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008322215s
STEP: Saw pod success 01/19/23 04:26:20.084
Jan 19 04:26:20.084: INFO: Pod "var-expansion-d60707b1-2407-4e3b-9648-48d20f5da8e9" satisfied condition "Succeeded or Failed"
Jan 19 04:26:20.088: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod var-expansion-d60707b1-2407-4e3b-9648-48d20f5da8e9 container dapi-container: <nil>
STEP: delete the pod 01/19/23 04:26:20.122
Jan 19 04:26:20.131: INFO: Waiting for pod var-expansion-d60707b1-2407-4e3b-9648-48d20f5da8e9 to disappear
Jan 19 04:26:20.134: INFO: Pod var-expansion-d60707b1-2407-4e3b-9648-48d20f5da8e9 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 19 04:26:20.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7268" for this suite. 01/19/23 04:26:20.137
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","completed":33,"skipped":633,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.100 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:26:14.042
    Jan 19 04:26:14.042: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename var-expansion 01/19/23 04:26:14.042
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:26:14.056
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:26:14.059
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:111
    STEP: Creating a pod to test substitution in volume subpath 01/19/23 04:26:14.064
    Jan 19 04:26:14.076: INFO: Waiting up to 5m0s for pod "var-expansion-d60707b1-2407-4e3b-9648-48d20f5da8e9" in namespace "var-expansion-7268" to be "Succeeded or Failed"
    Jan 19 04:26:14.079: INFO: Pod "var-expansion-d60707b1-2407-4e3b-9648-48d20f5da8e9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.702438ms
    Jan 19 04:26:16.084: INFO: Pod "var-expansion-d60707b1-2407-4e3b-9648-48d20f5da8e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008287249s
    Jan 19 04:26:18.085: INFO: Pod "var-expansion-d60707b1-2407-4e3b-9648-48d20f5da8e9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008879103s
    Jan 19 04:26:20.084: INFO: Pod "var-expansion-d60707b1-2407-4e3b-9648-48d20f5da8e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008322215s
    STEP: Saw pod success 01/19/23 04:26:20.084
    Jan 19 04:26:20.084: INFO: Pod "var-expansion-d60707b1-2407-4e3b-9648-48d20f5da8e9" satisfied condition "Succeeded or Failed"
    Jan 19 04:26:20.088: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod var-expansion-d60707b1-2407-4e3b-9648-48d20f5da8e9 container dapi-container: <nil>
    STEP: delete the pod 01/19/23 04:26:20.122
    Jan 19 04:26:20.131: INFO: Waiting for pod var-expansion-d60707b1-2407-4e3b-9648-48d20f5da8e9 to disappear
    Jan 19 04:26:20.134: INFO: Pod var-expansion-d60707b1-2407-4e3b-9648-48d20f5da8e9 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 19 04:26:20.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-7268" for this suite. 01/19/23 04:26:20.137
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:26:20.143
Jan 19 04:26:20.143: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename crd-watch 01/19/23 04:26:20.143
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:26:20.157
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:26:20.158
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Jan 19 04:26:20.161: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Creating first CR  01/19/23 04:26:22.725
Jan 19 04:26:22.730: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-19T04:26:22Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-19T04:26:22Z]] name:name1 resourceVersion:8962 uid:a3b0a634-24f1-410f-af9d-5f4ee50ec1c0] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 01/19/23 04:26:32.73
Jan 19 04:26:32.739: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-19T04:26:32Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-19T04:26:32Z]] name:name2 resourceVersion:8992 uid:ec654484-454a-4112-955f-16980e61c9a2] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 01/19/23 04:26:42.739
Jan 19 04:26:42.746: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-19T04:26:22Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-19T04:26:42Z]] name:name1 resourceVersion:9014 uid:a3b0a634-24f1-410f-af9d-5f4ee50ec1c0] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 01/19/23 04:26:52.746
Jan 19 04:26:52.752: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-19T04:26:32Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-19T04:26:52Z]] name:name2 resourceVersion:9037 uid:ec654484-454a-4112-955f-16980e61c9a2] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 01/19/23 04:27:02.752
Jan 19 04:27:02.762: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-19T04:26:22Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-19T04:26:42Z]] name:name1 resourceVersion:9058 uid:a3b0a634-24f1-410f-af9d-5f4ee50ec1c0] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 01/19/23 04:27:12.762
Jan 19 04:27:12.770: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-19T04:26:32Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-19T04:26:52Z]] name:name2 resourceVersion:9080 uid:ec654484-454a-4112-955f-16980e61c9a2] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 04:27:23.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-8361" for this suite. 01/19/23 04:27:23.292
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","completed":34,"skipped":646,"failed":0}
------------------------------
â€¢ [SLOW TEST] [63.155 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:26:20.143
    Jan 19 04:26:20.143: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename crd-watch 01/19/23 04:26:20.143
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:26:20.157
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:26:20.158
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Jan 19 04:26:20.161: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Creating first CR  01/19/23 04:26:22.725
    Jan 19 04:26:22.730: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-19T04:26:22Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-19T04:26:22Z]] name:name1 resourceVersion:8962 uid:a3b0a634-24f1-410f-af9d-5f4ee50ec1c0] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 01/19/23 04:26:32.73
    Jan 19 04:26:32.739: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-19T04:26:32Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-19T04:26:32Z]] name:name2 resourceVersion:8992 uid:ec654484-454a-4112-955f-16980e61c9a2] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 01/19/23 04:26:42.739
    Jan 19 04:26:42.746: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-19T04:26:22Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-19T04:26:42Z]] name:name1 resourceVersion:9014 uid:a3b0a634-24f1-410f-af9d-5f4ee50ec1c0] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 01/19/23 04:26:52.746
    Jan 19 04:26:52.752: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-19T04:26:32Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-19T04:26:52Z]] name:name2 resourceVersion:9037 uid:ec654484-454a-4112-955f-16980e61c9a2] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 01/19/23 04:27:02.752
    Jan 19 04:27:02.762: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-19T04:26:22Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-19T04:26:42Z]] name:name1 resourceVersion:9058 uid:a3b0a634-24f1-410f-af9d-5f4ee50ec1c0] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 01/19/23 04:27:12.762
    Jan 19 04:27:12.770: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-19T04:26:32Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-19T04:26:52Z]] name:name2 resourceVersion:9080 uid:ec654484-454a-4112-955f-16980e61c9a2] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 04:27:23.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-watch-8361" for this suite. 01/19/23 04:27:23.292
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:27:23.298
Jan 19 04:27:23.298: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename container-runtime 01/19/23 04:27:23.299
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:27:23.315
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:27:23.317
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
STEP: create the container 01/19/23 04:27:23.319
STEP: wait for the container to reach Succeeded 01/19/23 04:27:23.325
STEP: get the container status 01/19/23 04:27:29.353
STEP: the container should be terminated 01/19/23 04:27:29.356
STEP: the termination message should be set 01/19/23 04:27:29.356
Jan 19 04:27:29.356: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 01/19/23 04:27:29.356
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan 19 04:27:29.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1749" for this suite. 01/19/23 04:27:29.371
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","completed":35,"skipped":664,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.078 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:27:23.298
    Jan 19 04:27:23.298: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename container-runtime 01/19/23 04:27:23.299
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:27:23.315
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:27:23.317
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194
    STEP: create the container 01/19/23 04:27:23.319
    STEP: wait for the container to reach Succeeded 01/19/23 04:27:23.325
    STEP: get the container status 01/19/23 04:27:29.353
    STEP: the container should be terminated 01/19/23 04:27:29.356
    STEP: the termination message should be set 01/19/23 04:27:29.356
    Jan 19 04:27:29.356: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 01/19/23 04:27:29.356
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan 19 04:27:29.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-1749" for this suite. 01/19/23 04:27:29.371
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:27:29.378
Jan 19 04:27:29.378: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename custom-resource-definition 01/19/23 04:27:29.378
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:27:29.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:27:29.394
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 01/19/23 04:27:29.396
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 01/19/23 04:27:29.397
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 01/19/23 04:27:29.397
STEP: fetching the /apis/apiextensions.k8s.io discovery document 01/19/23 04:27:29.397
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 01/19/23 04:27:29.398
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 01/19/23 04:27:29.398
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 01/19/23 04:27:29.398
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 04:27:29.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9642" for this suite. 01/19/23 04:27:29.401
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","completed":36,"skipped":701,"failed":0}
------------------------------
â€¢ [0.029 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:27:29.378
    Jan 19 04:27:29.378: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename custom-resource-definition 01/19/23 04:27:29.378
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:27:29.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:27:29.394
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 01/19/23 04:27:29.396
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 01/19/23 04:27:29.397
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 01/19/23 04:27:29.397
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 01/19/23 04:27:29.397
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 01/19/23 04:27:29.398
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 01/19/23 04:27:29.398
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 01/19/23 04:27:29.398
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 04:27:29.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-9642" for this suite. 01/19/23 04:27:29.401
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:27:29.407
Jan 19 04:27:29.407: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename deployment 01/19/23 04:27:29.407
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:27:29.423
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:27:29.424
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Jan 19 04:27:29.426: INFO: Creating simple deployment test-new-deployment
Jan 19 04:27:29.436: INFO: deployment "test-new-deployment" doesn't have the required revision set
Jan 19 04:27:31.446: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 4, 27, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 27, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 4, 27, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 27, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-845c8977d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: getting scale subresource 01/19/23 04:27:33.453
STEP: updating a scale subresource 01/19/23 04:27:33.456
STEP: verifying the deployment Spec.Replicas was modified 01/19/23 04:27:33.463
STEP: Patch a scale subresource 01/19/23 04:27:33.465
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 19 04:27:33.517: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-983  17de0119-b0fd-467e-8954-a2a3ff666afb 9179 3 2023-01-19 04:27:29 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-01-19 04:27:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 04:27:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0026fe748 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-01-19 04:27:31 +0000 UTC,LastTransitionTime:2023-01-19 04:27:29 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-19 04:27:33 +0000 UTC,LastTransitionTime:2023-01-19 04:27:33 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 19 04:27:33.526: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-983  5e17e679-e9ec-46ba-9958-8ce3e430c14c 9184 2 2023-01-19 04:27:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 17de0119-b0fd-467e-8954-a2a3ff666afb 0xc002393360 0xc002393361}] [] [{kube-controller-manager Update apps/v1 2023-01-19 04:27:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"17de0119-b0fd-467e-8954-a2a3ff666afb\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 04:27:33 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0023933e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 19 04:27:33.532: INFO: Pod "test-new-deployment-845c8977d9-dvhwd" is available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-dvhwd test-new-deployment-845c8977d9- deployment-983  5b0773ec-fd65-497c-9c0c-abd45ad9c793 9167 0 2023-01-19 04:27:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 5e17e679-e9ec-46ba-9958-8ce3e430c14c 0xc0026feba0 0xc0026feba1}] [] [{kube-controller-manager Update v1 2023-01-19 04:27:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e17e679-e9ec-46ba-9958-8ce3e430c14c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 04:27:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.70.89\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4gm6k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4gm6k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:27:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:27:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:27:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:27:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.78,PodIP:10.100.70.89,StartTime:2023-01-19 04:27:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 04:27:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://a4d3c2b6026efd140255c01e98ce9b73ae92582878d16f9843ae320cf0a51ed6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.70.89,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 04:27:33.532: INFO: Pod "test-new-deployment-845c8977d9-w5ds9" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-w5ds9 test-new-deployment-845c8977d9- deployment-983  7ec34604-521f-40a0-8df1-4057bdc17d40 9185 0 2023-01-19 04:27:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 5e17e679-e9ec-46ba-9958-8ce3e430c14c 0xc0026fed70 0xc0026fed71}] [] [{kube-controller-manager Update v1 2023-01-19 04:27:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e17e679-e9ec-46ba-9958-8ce3e430c14c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 04:27:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lx9g6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lx9g6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:27:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:27:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:27:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:27:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.78,PodIP:,StartTime:2023-01-19 04:27:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 19 04:27:33.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-983" for this suite. 01/19/23 04:27:33.538
{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","completed":37,"skipped":715,"failed":0}
------------------------------
â€¢ [4.164 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:27:29.407
    Jan 19 04:27:29.407: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename deployment 01/19/23 04:27:29.407
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:27:29.423
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:27:29.424
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Jan 19 04:27:29.426: INFO: Creating simple deployment test-new-deployment
    Jan 19 04:27:29.436: INFO: deployment "test-new-deployment" doesn't have the required revision set
    Jan 19 04:27:31.446: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 4, 27, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 27, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 4, 27, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 27, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-845c8977d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: getting scale subresource 01/19/23 04:27:33.453
    STEP: updating a scale subresource 01/19/23 04:27:33.456
    STEP: verifying the deployment Spec.Replicas was modified 01/19/23 04:27:33.463
    STEP: Patch a scale subresource 01/19/23 04:27:33.465
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 19 04:27:33.517: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-983  17de0119-b0fd-467e-8954-a2a3ff666afb 9179 3 2023-01-19 04:27:29 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-01-19 04:27:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 04:27:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0026fe748 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-01-19 04:27:31 +0000 UTC,LastTransitionTime:2023-01-19 04:27:29 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-19 04:27:33 +0000 UTC,LastTransitionTime:2023-01-19 04:27:33 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 19 04:27:33.526: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-983  5e17e679-e9ec-46ba-9958-8ce3e430c14c 9184 2 2023-01-19 04:27:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 17de0119-b0fd-467e-8954-a2a3ff666afb 0xc002393360 0xc002393361}] [] [{kube-controller-manager Update apps/v1 2023-01-19 04:27:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"17de0119-b0fd-467e-8954-a2a3ff666afb\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 04:27:33 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0023933e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 19 04:27:33.532: INFO: Pod "test-new-deployment-845c8977d9-dvhwd" is available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-dvhwd test-new-deployment-845c8977d9- deployment-983  5b0773ec-fd65-497c-9c0c-abd45ad9c793 9167 0 2023-01-19 04:27:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 5e17e679-e9ec-46ba-9958-8ce3e430c14c 0xc0026feba0 0xc0026feba1}] [] [{kube-controller-manager Update v1 2023-01-19 04:27:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e17e679-e9ec-46ba-9958-8ce3e430c14c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 04:27:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.70.89\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4gm6k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4gm6k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:27:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:27:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:27:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:27:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.78,PodIP:10.100.70.89,StartTime:2023-01-19 04:27:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 04:27:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://a4d3c2b6026efd140255c01e98ce9b73ae92582878d16f9843ae320cf0a51ed6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.70.89,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 19 04:27:33.532: INFO: Pod "test-new-deployment-845c8977d9-w5ds9" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-w5ds9 test-new-deployment-845c8977d9- deployment-983  7ec34604-521f-40a0-8df1-4057bdc17d40 9185 0 2023-01-19 04:27:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 5e17e679-e9ec-46ba-9958-8ce3e430c14c 0xc0026fed70 0xc0026fed71}] [] [{kube-controller-manager Update v1 2023-01-19 04:27:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e17e679-e9ec-46ba-9958-8ce3e430c14c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 04:27:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lx9g6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lx9g6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:27:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:27:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:27:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:27:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.78,PodIP:,StartTime:2023-01-19 04:27:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 19 04:27:33.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-983" for this suite. 01/19/23 04:27:33.538
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:27:33.571
Jan 19 04:27:33.571: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename container-runtime 01/19/23 04:27:33.572
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:27:33.592
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:27:33.594
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
STEP: create the container 01/19/23 04:27:33.596
STEP: wait for the container to reach Failed 01/19/23 04:27:33.603
STEP: get the container status 01/19/23 04:27:38.626
STEP: the container should be terminated 01/19/23 04:27:38.629
STEP: the termination message should be set 01/19/23 04:27:38.629
Jan 19 04:27:38.630: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 01/19/23 04:27:38.63
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan 19 04:27:38.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4175" for this suite. 01/19/23 04:27:38.644
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":38,"skipped":715,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.081 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:27:33.571
    Jan 19 04:27:33.571: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename container-runtime 01/19/23 04:27:33.572
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:27:33.592
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:27:33.594
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215
    STEP: create the container 01/19/23 04:27:33.596
    STEP: wait for the container to reach Failed 01/19/23 04:27:33.603
    STEP: get the container status 01/19/23 04:27:38.626
    STEP: the container should be terminated 01/19/23 04:27:38.629
    STEP: the termination message should be set 01/19/23 04:27:38.629
    Jan 19 04:27:38.630: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 01/19/23 04:27:38.63
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan 19 04:27:38.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-4175" for this suite. 01/19/23 04:27:38.644
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:27:38.653
Jan 19 04:27:38.653: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename sched-preemption 01/19/23 04:27:38.654
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:27:38.668
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:27:38.669
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 19 04:27:38.683: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 19 04:28:38.701: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
STEP: Create pods that use 4/5 of node resources. 01/19/23 04:28:38.704
Jan 19 04:28:38.723: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan 19 04:28:38.730: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan 19 04:28:38.750: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan 19 04:28:38.756: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 01/19/23 04:28:38.756
Jan 19 04:28:38.756: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-7753" to be "running"
Jan 19 04:28:38.773: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 17.438932ms
Jan 19 04:28:40.778: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022229392s
Jan 19 04:28:42.777: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021119207s
Jan 19 04:28:44.778: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02224731s
Jan 19 04:28:46.778: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.022447664s
Jan 19 04:28:48.778: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.022360209s
Jan 19 04:28:50.777: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 12.021675439s
Jan 19 04:28:52.777: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 14.021051958s
Jan 19 04:28:54.778: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 16.022467389s
Jan 19 04:28:54.778: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jan 19 04:28:54.778: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-7753" to be "running"
Jan 19 04:28:54.782: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.363763ms
Jan 19 04:28:54.782: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 19 04:28:54.782: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-7753" to be "running"
Jan 19 04:28:54.784: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.383246ms
Jan 19 04:28:54.784: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 19 04:28:54.784: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-7753" to be "running"
Jan 19 04:28:54.790: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.961424ms
Jan 19 04:28:54.790: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 01/19/23 04:28:54.79
Jan 19 04:28:54.797: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-7753" to be "running"
Jan 19 04:28:54.799: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.459342ms
Jan 19 04:28:56.804: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007136729s
Jan 19 04:28:58.804: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0066647s
Jan 19 04:29:00.803: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.006545707s
Jan 19 04:29:00.803: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jan 19 04:29:00.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7753" for this suite. 01/19/23 04:29:00.82
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","completed":39,"skipped":741,"failed":0}
------------------------------
â€¢ [SLOW TEST] [82.202 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:27:38.653
    Jan 19 04:27:38.653: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename sched-preemption 01/19/23 04:27:38.654
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:27:38.668
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:27:38.669
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jan 19 04:27:38.683: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 19 04:28:38.701: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:125
    STEP: Create pods that use 4/5 of node resources. 01/19/23 04:28:38.704
    Jan 19 04:28:38.723: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jan 19 04:28:38.730: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jan 19 04:28:38.750: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jan 19 04:28:38.756: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 01/19/23 04:28:38.756
    Jan 19 04:28:38.756: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-7753" to be "running"
    Jan 19 04:28:38.773: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 17.438932ms
    Jan 19 04:28:40.778: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022229392s
    Jan 19 04:28:42.777: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021119207s
    Jan 19 04:28:44.778: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02224731s
    Jan 19 04:28:46.778: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.022447664s
    Jan 19 04:28:48.778: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.022360209s
    Jan 19 04:28:50.777: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 12.021675439s
    Jan 19 04:28:52.777: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 14.021051958s
    Jan 19 04:28:54.778: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 16.022467389s
    Jan 19 04:28:54.778: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jan 19 04:28:54.778: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-7753" to be "running"
    Jan 19 04:28:54.782: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.363763ms
    Jan 19 04:28:54.782: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 19 04:28:54.782: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-7753" to be "running"
    Jan 19 04:28:54.784: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.383246ms
    Jan 19 04:28:54.784: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 19 04:28:54.784: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-7753" to be "running"
    Jan 19 04:28:54.790: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.961424ms
    Jan 19 04:28:54.790: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 01/19/23 04:28:54.79
    Jan 19 04:28:54.797: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-7753" to be "running"
    Jan 19 04:28:54.799: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.459342ms
    Jan 19 04:28:56.804: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007136729s
    Jan 19 04:28:58.804: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0066647s
    Jan 19 04:29:00.803: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.006545707s
    Jan 19 04:29:00.803: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jan 19 04:29:00.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-7753" for this suite. 01/19/23 04:29:00.82
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:29:00.855
Jan 19 04:29:00.855: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename gc 01/19/23 04:29:00.856
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:29:00.869
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:29:00.871
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 01/19/23 04:29:00.876
STEP: create the rc2 01/19/23 04:29:00.881
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 01/19/23 04:29:05.894
STEP: delete the rc simpletest-rc-to-be-deleted 01/19/23 04:29:06.463
STEP: wait for the rc to be deleted 01/19/23 04:29:06.471
Jan 19 04:29:11.486: INFO: 69 pods remaining
Jan 19 04:29:11.486: INFO: 69 pods has nil DeletionTimestamp
Jan 19 04:29:11.486: INFO: 
STEP: Gathering metrics 01/19/23 04:29:16.485
W0119 04:29:16.489643      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 19 04:29:16.489: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan 19 04:29:16.489: INFO: Deleting pod "simpletest-rc-to-be-deleted-2jnhg" in namespace "gc-9850"
Jan 19 04:29:16.500: INFO: Deleting pod "simpletest-rc-to-be-deleted-2swq5" in namespace "gc-9850"
Jan 19 04:29:16.511: INFO: Deleting pod "simpletest-rc-to-be-deleted-47gfs" in namespace "gc-9850"
Jan 19 04:29:16.522: INFO: Deleting pod "simpletest-rc-to-be-deleted-4s9c2" in namespace "gc-9850"
Jan 19 04:29:16.535: INFO: Deleting pod "simpletest-rc-to-be-deleted-4zp5d" in namespace "gc-9850"
Jan 19 04:29:16.548: INFO: Deleting pod "simpletest-rc-to-be-deleted-52qwn" in namespace "gc-9850"
Jan 19 04:29:16.558: INFO: Deleting pod "simpletest-rc-to-be-deleted-54xm5" in namespace "gc-9850"
Jan 19 04:29:16.569: INFO: Deleting pod "simpletest-rc-to-be-deleted-5c9wl" in namespace "gc-9850"
Jan 19 04:29:16.579: INFO: Deleting pod "simpletest-rc-to-be-deleted-5drv2" in namespace "gc-9850"
Jan 19 04:29:16.590: INFO: Deleting pod "simpletest-rc-to-be-deleted-5txpv" in namespace "gc-9850"
Jan 19 04:29:16.601: INFO: Deleting pod "simpletest-rc-to-be-deleted-6kwzp" in namespace "gc-9850"
Jan 19 04:29:16.611: INFO: Deleting pod "simpletest-rc-to-be-deleted-6pdf9" in namespace "gc-9850"
Jan 19 04:29:16.621: INFO: Deleting pod "simpletest-rc-to-be-deleted-6vbmk" in namespace "gc-9850"
Jan 19 04:29:16.631: INFO: Deleting pod "simpletest-rc-to-be-deleted-75vlc" in namespace "gc-9850"
Jan 19 04:29:16.642: INFO: Deleting pod "simpletest-rc-to-be-deleted-79mht" in namespace "gc-9850"
Jan 19 04:29:16.651: INFO: Deleting pod "simpletest-rc-to-be-deleted-7grr8" in namespace "gc-9850"
Jan 19 04:29:16.660: INFO: Deleting pod "simpletest-rc-to-be-deleted-7qw69" in namespace "gc-9850"
Jan 19 04:29:16.673: INFO: Deleting pod "simpletest-rc-to-be-deleted-7r77s" in namespace "gc-9850"
Jan 19 04:29:16.686: INFO: Deleting pod "simpletest-rc-to-be-deleted-7z6s5" in namespace "gc-9850"
Jan 19 04:29:16.695: INFO: Deleting pod "simpletest-rc-to-be-deleted-828zl" in namespace "gc-9850"
Jan 19 04:29:16.706: INFO: Deleting pod "simpletest-rc-to-be-deleted-8cd6w" in namespace "gc-9850"
Jan 19 04:29:16.716: INFO: Deleting pod "simpletest-rc-to-be-deleted-b2wqf" in namespace "gc-9850"
Jan 19 04:29:16.730: INFO: Deleting pod "simpletest-rc-to-be-deleted-b4pvs" in namespace "gc-9850"
Jan 19 04:29:16.740: INFO: Deleting pod "simpletest-rc-to-be-deleted-b5n4p" in namespace "gc-9850"
Jan 19 04:29:16.748: INFO: Deleting pod "simpletest-rc-to-be-deleted-b74k2" in namespace "gc-9850"
Jan 19 04:29:16.757: INFO: Deleting pod "simpletest-rc-to-be-deleted-bmjvt" in namespace "gc-9850"
Jan 19 04:29:16.781: INFO: Deleting pod "simpletest-rc-to-be-deleted-bpcdv" in namespace "gc-9850"
Jan 19 04:29:16.797: INFO: Deleting pod "simpletest-rc-to-be-deleted-c7hlw" in namespace "gc-9850"
Jan 19 04:29:16.815: INFO: Deleting pod "simpletest-rc-to-be-deleted-c8rrc" in namespace "gc-9850"
Jan 19 04:29:16.826: INFO: Deleting pod "simpletest-rc-to-be-deleted-cd6kk" in namespace "gc-9850"
Jan 19 04:29:16.835: INFO: Deleting pod "simpletest-rc-to-be-deleted-cfzwx" in namespace "gc-9850"
Jan 19 04:29:16.846: INFO: Deleting pod "simpletest-rc-to-be-deleted-cgzc5" in namespace "gc-9850"
Jan 19 04:29:16.856: INFO: Deleting pod "simpletest-rc-to-be-deleted-cqtkp" in namespace "gc-9850"
Jan 19 04:29:16.866: INFO: Deleting pod "simpletest-rc-to-be-deleted-dpbp9" in namespace "gc-9850"
Jan 19 04:29:16.877: INFO: Deleting pod "simpletest-rc-to-be-deleted-drkdd" in namespace "gc-9850"
Jan 19 04:29:16.890: INFO: Deleting pod "simpletest-rc-to-be-deleted-f5q26" in namespace "gc-9850"
Jan 19 04:29:16.901: INFO: Deleting pod "simpletest-rc-to-be-deleted-fhb4l" in namespace "gc-9850"
Jan 19 04:29:16.915: INFO: Deleting pod "simpletest-rc-to-be-deleted-fnp6s" in namespace "gc-9850"
Jan 19 04:29:16.926: INFO: Deleting pod "simpletest-rc-to-be-deleted-fqp98" in namespace "gc-9850"
Jan 19 04:29:16.939: INFO: Deleting pod "simpletest-rc-to-be-deleted-fsmhb" in namespace "gc-9850"
Jan 19 04:29:16.949: INFO: Deleting pod "simpletest-rc-to-be-deleted-fzn9r" in namespace "gc-9850"
Jan 19 04:29:16.958: INFO: Deleting pod "simpletest-rc-to-be-deleted-gh976" in namespace "gc-9850"
Jan 19 04:29:16.970: INFO: Deleting pod "simpletest-rc-to-be-deleted-gkb59" in namespace "gc-9850"
Jan 19 04:29:16.983: INFO: Deleting pod "simpletest-rc-to-be-deleted-glrxq" in namespace "gc-9850"
Jan 19 04:29:16.991: INFO: Deleting pod "simpletest-rc-to-be-deleted-gmj7x" in namespace "gc-9850"
Jan 19 04:29:17.001: INFO: Deleting pod "simpletest-rc-to-be-deleted-gth6w" in namespace "gc-9850"
Jan 19 04:29:17.010: INFO: Deleting pod "simpletest-rc-to-be-deleted-gzp9v" in namespace "gc-9850"
Jan 19 04:29:17.019: INFO: Deleting pod "simpletest-rc-to-be-deleted-hdgjj" in namespace "gc-9850"
Jan 19 04:29:17.030: INFO: Deleting pod "simpletest-rc-to-be-deleted-hxh7t" in namespace "gc-9850"
Jan 19 04:29:17.040: INFO: Deleting pod "simpletest-rc-to-be-deleted-j5pjf" in namespace "gc-9850"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 19 04:29:17.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9850" for this suite. 01/19/23 04:29:17.051
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","completed":40,"skipped":748,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.202 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:29:00.855
    Jan 19 04:29:00.855: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename gc 01/19/23 04:29:00.856
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:29:00.869
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:29:00.871
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 01/19/23 04:29:00.876
    STEP: create the rc2 01/19/23 04:29:00.881
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 01/19/23 04:29:05.894
    STEP: delete the rc simpletest-rc-to-be-deleted 01/19/23 04:29:06.463
    STEP: wait for the rc to be deleted 01/19/23 04:29:06.471
    Jan 19 04:29:11.486: INFO: 69 pods remaining
    Jan 19 04:29:11.486: INFO: 69 pods has nil DeletionTimestamp
    Jan 19 04:29:11.486: INFO: 
    STEP: Gathering metrics 01/19/23 04:29:16.485
    W0119 04:29:16.489643      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 19 04:29:16.489: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jan 19 04:29:16.489: INFO: Deleting pod "simpletest-rc-to-be-deleted-2jnhg" in namespace "gc-9850"
    Jan 19 04:29:16.500: INFO: Deleting pod "simpletest-rc-to-be-deleted-2swq5" in namespace "gc-9850"
    Jan 19 04:29:16.511: INFO: Deleting pod "simpletest-rc-to-be-deleted-47gfs" in namespace "gc-9850"
    Jan 19 04:29:16.522: INFO: Deleting pod "simpletest-rc-to-be-deleted-4s9c2" in namespace "gc-9850"
    Jan 19 04:29:16.535: INFO: Deleting pod "simpletest-rc-to-be-deleted-4zp5d" in namespace "gc-9850"
    Jan 19 04:29:16.548: INFO: Deleting pod "simpletest-rc-to-be-deleted-52qwn" in namespace "gc-9850"
    Jan 19 04:29:16.558: INFO: Deleting pod "simpletest-rc-to-be-deleted-54xm5" in namespace "gc-9850"
    Jan 19 04:29:16.569: INFO: Deleting pod "simpletest-rc-to-be-deleted-5c9wl" in namespace "gc-9850"
    Jan 19 04:29:16.579: INFO: Deleting pod "simpletest-rc-to-be-deleted-5drv2" in namespace "gc-9850"
    Jan 19 04:29:16.590: INFO: Deleting pod "simpletest-rc-to-be-deleted-5txpv" in namespace "gc-9850"
    Jan 19 04:29:16.601: INFO: Deleting pod "simpletest-rc-to-be-deleted-6kwzp" in namespace "gc-9850"
    Jan 19 04:29:16.611: INFO: Deleting pod "simpletest-rc-to-be-deleted-6pdf9" in namespace "gc-9850"
    Jan 19 04:29:16.621: INFO: Deleting pod "simpletest-rc-to-be-deleted-6vbmk" in namespace "gc-9850"
    Jan 19 04:29:16.631: INFO: Deleting pod "simpletest-rc-to-be-deleted-75vlc" in namespace "gc-9850"
    Jan 19 04:29:16.642: INFO: Deleting pod "simpletest-rc-to-be-deleted-79mht" in namespace "gc-9850"
    Jan 19 04:29:16.651: INFO: Deleting pod "simpletest-rc-to-be-deleted-7grr8" in namespace "gc-9850"
    Jan 19 04:29:16.660: INFO: Deleting pod "simpletest-rc-to-be-deleted-7qw69" in namespace "gc-9850"
    Jan 19 04:29:16.673: INFO: Deleting pod "simpletest-rc-to-be-deleted-7r77s" in namespace "gc-9850"
    Jan 19 04:29:16.686: INFO: Deleting pod "simpletest-rc-to-be-deleted-7z6s5" in namespace "gc-9850"
    Jan 19 04:29:16.695: INFO: Deleting pod "simpletest-rc-to-be-deleted-828zl" in namespace "gc-9850"
    Jan 19 04:29:16.706: INFO: Deleting pod "simpletest-rc-to-be-deleted-8cd6w" in namespace "gc-9850"
    Jan 19 04:29:16.716: INFO: Deleting pod "simpletest-rc-to-be-deleted-b2wqf" in namespace "gc-9850"
    Jan 19 04:29:16.730: INFO: Deleting pod "simpletest-rc-to-be-deleted-b4pvs" in namespace "gc-9850"
    Jan 19 04:29:16.740: INFO: Deleting pod "simpletest-rc-to-be-deleted-b5n4p" in namespace "gc-9850"
    Jan 19 04:29:16.748: INFO: Deleting pod "simpletest-rc-to-be-deleted-b74k2" in namespace "gc-9850"
    Jan 19 04:29:16.757: INFO: Deleting pod "simpletest-rc-to-be-deleted-bmjvt" in namespace "gc-9850"
    Jan 19 04:29:16.781: INFO: Deleting pod "simpletest-rc-to-be-deleted-bpcdv" in namespace "gc-9850"
    Jan 19 04:29:16.797: INFO: Deleting pod "simpletest-rc-to-be-deleted-c7hlw" in namespace "gc-9850"
    Jan 19 04:29:16.815: INFO: Deleting pod "simpletest-rc-to-be-deleted-c8rrc" in namespace "gc-9850"
    Jan 19 04:29:16.826: INFO: Deleting pod "simpletest-rc-to-be-deleted-cd6kk" in namespace "gc-9850"
    Jan 19 04:29:16.835: INFO: Deleting pod "simpletest-rc-to-be-deleted-cfzwx" in namespace "gc-9850"
    Jan 19 04:29:16.846: INFO: Deleting pod "simpletest-rc-to-be-deleted-cgzc5" in namespace "gc-9850"
    Jan 19 04:29:16.856: INFO: Deleting pod "simpletest-rc-to-be-deleted-cqtkp" in namespace "gc-9850"
    Jan 19 04:29:16.866: INFO: Deleting pod "simpletest-rc-to-be-deleted-dpbp9" in namespace "gc-9850"
    Jan 19 04:29:16.877: INFO: Deleting pod "simpletest-rc-to-be-deleted-drkdd" in namespace "gc-9850"
    Jan 19 04:29:16.890: INFO: Deleting pod "simpletest-rc-to-be-deleted-f5q26" in namespace "gc-9850"
    Jan 19 04:29:16.901: INFO: Deleting pod "simpletest-rc-to-be-deleted-fhb4l" in namespace "gc-9850"
    Jan 19 04:29:16.915: INFO: Deleting pod "simpletest-rc-to-be-deleted-fnp6s" in namespace "gc-9850"
    Jan 19 04:29:16.926: INFO: Deleting pod "simpletest-rc-to-be-deleted-fqp98" in namespace "gc-9850"
    Jan 19 04:29:16.939: INFO: Deleting pod "simpletest-rc-to-be-deleted-fsmhb" in namespace "gc-9850"
    Jan 19 04:29:16.949: INFO: Deleting pod "simpletest-rc-to-be-deleted-fzn9r" in namespace "gc-9850"
    Jan 19 04:29:16.958: INFO: Deleting pod "simpletest-rc-to-be-deleted-gh976" in namespace "gc-9850"
    Jan 19 04:29:16.970: INFO: Deleting pod "simpletest-rc-to-be-deleted-gkb59" in namespace "gc-9850"
    Jan 19 04:29:16.983: INFO: Deleting pod "simpletest-rc-to-be-deleted-glrxq" in namespace "gc-9850"
    Jan 19 04:29:16.991: INFO: Deleting pod "simpletest-rc-to-be-deleted-gmj7x" in namespace "gc-9850"
    Jan 19 04:29:17.001: INFO: Deleting pod "simpletest-rc-to-be-deleted-gth6w" in namespace "gc-9850"
    Jan 19 04:29:17.010: INFO: Deleting pod "simpletest-rc-to-be-deleted-gzp9v" in namespace "gc-9850"
    Jan 19 04:29:17.019: INFO: Deleting pod "simpletest-rc-to-be-deleted-hdgjj" in namespace "gc-9850"
    Jan 19 04:29:17.030: INFO: Deleting pod "simpletest-rc-to-be-deleted-hxh7t" in namespace "gc-9850"
    Jan 19 04:29:17.040: INFO: Deleting pod "simpletest-rc-to-be-deleted-j5pjf" in namespace "gc-9850"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 19 04:29:17.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-9850" for this suite. 01/19/23 04:29:17.051
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:29:17.057
Jan 19 04:29:17.057: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename watch 01/19/23 04:29:17.058
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:29:17.088
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:29:17.092
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 01/19/23 04:29:17.094
STEP: creating a new configmap 01/19/23 04:29:17.097
STEP: modifying the configmap once 01/19/23 04:29:17.102
STEP: changing the label value of the configmap 01/19/23 04:29:17.11
STEP: Expecting to observe a delete notification for the watched object 01/19/23 04:29:17.12
Jan 19 04:29:17.120: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2608  4e5aef49-cbc3-4c34-9de1-cb7fd2ce2a3b 10525 0 2023-01-19 04:29:17 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-19 04:29:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 19 04:29:17.120: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2608  4e5aef49-cbc3-4c34-9de1-cb7fd2ce2a3b 10526 0 2023-01-19 04:29:17 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-19 04:29:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 19 04:29:17.120: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2608  4e5aef49-cbc3-4c34-9de1-cb7fd2ce2a3b 10527 0 2023-01-19 04:29:17 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-19 04:29:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 01/19/23 04:29:17.12
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 01/19/23 04:29:17.127
STEP: changing the label value of the configmap back 01/19/23 04:29:27.127
STEP: modifying the configmap a third time 01/19/23 04:29:27.137
STEP: deleting the configmap 01/19/23 04:29:27.143
STEP: Expecting to observe an add notification for the watched object when the label value was restored 01/19/23 04:29:27.149
Jan 19 04:29:27.149: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2608  4e5aef49-cbc3-4c34-9de1-cb7fd2ce2a3b 10832 0 2023-01-19 04:29:17 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-19 04:29:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 19 04:29:27.149: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2608  4e5aef49-cbc3-4c34-9de1-cb7fd2ce2a3b 10833 0 2023-01-19 04:29:17 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-19 04:29:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 19 04:29:27.149: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2608  4e5aef49-cbc3-4c34-9de1-cb7fd2ce2a3b 10834 0 2023-01-19 04:29:17 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-19 04:29:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan 19 04:29:27.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2608" for this suite. 01/19/23 04:29:27.153
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","completed":41,"skipped":762,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.101 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:29:17.057
    Jan 19 04:29:17.057: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename watch 01/19/23 04:29:17.058
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:29:17.088
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:29:17.092
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 01/19/23 04:29:17.094
    STEP: creating a new configmap 01/19/23 04:29:17.097
    STEP: modifying the configmap once 01/19/23 04:29:17.102
    STEP: changing the label value of the configmap 01/19/23 04:29:17.11
    STEP: Expecting to observe a delete notification for the watched object 01/19/23 04:29:17.12
    Jan 19 04:29:17.120: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2608  4e5aef49-cbc3-4c34-9de1-cb7fd2ce2a3b 10525 0 2023-01-19 04:29:17 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-19 04:29:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 19 04:29:17.120: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2608  4e5aef49-cbc3-4c34-9de1-cb7fd2ce2a3b 10526 0 2023-01-19 04:29:17 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-19 04:29:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 19 04:29:17.120: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2608  4e5aef49-cbc3-4c34-9de1-cb7fd2ce2a3b 10527 0 2023-01-19 04:29:17 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-19 04:29:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 01/19/23 04:29:17.12
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 01/19/23 04:29:17.127
    STEP: changing the label value of the configmap back 01/19/23 04:29:27.127
    STEP: modifying the configmap a third time 01/19/23 04:29:27.137
    STEP: deleting the configmap 01/19/23 04:29:27.143
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 01/19/23 04:29:27.149
    Jan 19 04:29:27.149: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2608  4e5aef49-cbc3-4c34-9de1-cb7fd2ce2a3b 10832 0 2023-01-19 04:29:17 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-19 04:29:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 19 04:29:27.149: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2608  4e5aef49-cbc3-4c34-9de1-cb7fd2ce2a3b 10833 0 2023-01-19 04:29:17 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-19 04:29:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 19 04:29:27.149: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2608  4e5aef49-cbc3-4c34-9de1-cb7fd2ce2a3b 10834 0 2023-01-19 04:29:17 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-19 04:29:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan 19 04:29:27.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-2608" for this suite. 01/19/23 04:29:27.153
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:29:27.159
Jan 19 04:29:27.159: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename csistoragecapacity 01/19/23 04:29:27.159
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:29:27.174
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:29:27.177
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 01/19/23 04:29:27.179
STEP: getting /apis/storage.k8s.io 01/19/23 04:29:27.181
STEP: getting /apis/storage.k8s.io/v1 01/19/23 04:29:27.182
STEP: creating 01/19/23 04:29:27.183
STEP: watching 01/19/23 04:29:27.199
Jan 19 04:29:27.200: INFO: starting watch
STEP: getting 01/19/23 04:29:27.206
STEP: listing in namespace 01/19/23 04:29:27.209
STEP: listing across namespaces 01/19/23 04:29:27.211
STEP: patching 01/19/23 04:29:27.215
STEP: updating 01/19/23 04:29:27.221
Jan 19 04:29:27.227: INFO: waiting for watch events with expected annotations in namespace
Jan 19 04:29:27.227: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 01/19/23 04:29:27.227
STEP: deleting a collection 01/19/23 04:29:27.238
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
Jan 19 04:29:27.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-5138" for this suite. 01/19/23 04:29:27.254
{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","completed":42,"skipped":763,"failed":0}
------------------------------
â€¢ [0.102 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:29:27.159
    Jan 19 04:29:27.159: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename csistoragecapacity 01/19/23 04:29:27.159
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:29:27.174
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:29:27.177
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 01/19/23 04:29:27.179
    STEP: getting /apis/storage.k8s.io 01/19/23 04:29:27.181
    STEP: getting /apis/storage.k8s.io/v1 01/19/23 04:29:27.182
    STEP: creating 01/19/23 04:29:27.183
    STEP: watching 01/19/23 04:29:27.199
    Jan 19 04:29:27.200: INFO: starting watch
    STEP: getting 01/19/23 04:29:27.206
    STEP: listing in namespace 01/19/23 04:29:27.209
    STEP: listing across namespaces 01/19/23 04:29:27.211
    STEP: patching 01/19/23 04:29:27.215
    STEP: updating 01/19/23 04:29:27.221
    Jan 19 04:29:27.227: INFO: waiting for watch events with expected annotations in namespace
    Jan 19 04:29:27.227: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 01/19/23 04:29:27.227
    STEP: deleting a collection 01/19/23 04:29:27.238
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:187
    Jan 19 04:29:27.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "csistoragecapacity-5138" for this suite. 01/19/23 04:29:27.254
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:29:27.261
Jan 19 04:29:27.261: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename pods 01/19/23 04:29:27.262
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:29:27.276
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:29:27.279
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
Jan 19 04:29:27.282: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: creating the pod 01/19/23 04:29:27.282
STEP: submitting the pod to kubernetes 01/19/23 04:29:27.282
Jan 19 04:29:27.291: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-a5fd583f-cdd7-4795-b79e-d8614a5eb9d5" in namespace "pods-440" to be "running and ready"
Jan 19 04:29:27.294: INFO: Pod "pod-logs-websocket-a5fd583f-cdd7-4795-b79e-d8614a5eb9d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.994274ms
Jan 19 04:29:27.294: INFO: The phase of Pod pod-logs-websocket-a5fd583f-cdd7-4795-b79e-d8614a5eb9d5 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 04:29:29.299: INFO: Pod "pod-logs-websocket-a5fd583f-cdd7-4795-b79e-d8614a5eb9d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007331606s
Jan 19 04:29:29.299: INFO: The phase of Pod pod-logs-websocket-a5fd583f-cdd7-4795-b79e-d8614a5eb9d5 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 04:29:31.299: INFO: Pod "pod-logs-websocket-a5fd583f-cdd7-4795-b79e-d8614a5eb9d5": Phase="Running", Reason="", readiness=true. Elapsed: 4.008194837s
Jan 19 04:29:31.300: INFO: The phase of Pod pod-logs-websocket-a5fd583f-cdd7-4795-b79e-d8614a5eb9d5 is Running (Ready = true)
Jan 19 04:29:31.300: INFO: Pod "pod-logs-websocket-a5fd583f-cdd7-4795-b79e-d8614a5eb9d5" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 19 04:29:31.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-440" for this suite. 01/19/23 04:29:31.356
{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","completed":43,"skipped":802,"failed":0}
------------------------------
â€¢ [4.100 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:29:27.261
    Jan 19 04:29:27.261: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename pods 01/19/23 04:29:27.262
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:29:27.276
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:29:27.279
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:617
    Jan 19 04:29:27.282: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: creating the pod 01/19/23 04:29:27.282
    STEP: submitting the pod to kubernetes 01/19/23 04:29:27.282
    Jan 19 04:29:27.291: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-a5fd583f-cdd7-4795-b79e-d8614a5eb9d5" in namespace "pods-440" to be "running and ready"
    Jan 19 04:29:27.294: INFO: Pod "pod-logs-websocket-a5fd583f-cdd7-4795-b79e-d8614a5eb9d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.994274ms
    Jan 19 04:29:27.294: INFO: The phase of Pod pod-logs-websocket-a5fd583f-cdd7-4795-b79e-d8614a5eb9d5 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 04:29:29.299: INFO: Pod "pod-logs-websocket-a5fd583f-cdd7-4795-b79e-d8614a5eb9d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007331606s
    Jan 19 04:29:29.299: INFO: The phase of Pod pod-logs-websocket-a5fd583f-cdd7-4795-b79e-d8614a5eb9d5 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 04:29:31.299: INFO: Pod "pod-logs-websocket-a5fd583f-cdd7-4795-b79e-d8614a5eb9d5": Phase="Running", Reason="", readiness=true. Elapsed: 4.008194837s
    Jan 19 04:29:31.300: INFO: The phase of Pod pod-logs-websocket-a5fd583f-cdd7-4795-b79e-d8614a5eb9d5 is Running (Ready = true)
    Jan 19 04:29:31.300: INFO: Pod "pod-logs-websocket-a5fd583f-cdd7-4795-b79e-d8614a5eb9d5" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 19 04:29:31.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-440" for this suite. 01/19/23 04:29:31.356
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:29:31.362
Jan 19 04:29:31.362: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename deployment 01/19/23 04:29:31.363
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:29:31.378
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:29:31.38
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Jan 19 04:29:31.382: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jan 19 04:29:31.395: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 19 04:29:36.400: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/19/23 04:29:36.4
Jan 19 04:29:36.401: INFO: Creating deployment "test-rolling-update-deployment"
Jan 19 04:29:36.406: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jan 19 04:29:36.412: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jan 19 04:29:38.419: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jan 19 04:29:38.422: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 4, 29, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 29, 36, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 4, 29, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 29, 36, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-78f575d8ff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 04:29:40.426: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 19 04:29:40.434: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-1176  0bd22e46-4003-4c51-8b3c-95cca9b9f024 10932 1 2023-01-19 04:29:36 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-01-19 04:29:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 04:29:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003b047a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-19 04:29:36 +0000 UTC,LastTransitionTime:2023-01-19 04:29:36 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-01-19 04:29:39 +0000 UTC,LastTransitionTime:2023-01-19 04:29:36 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 19 04:29:40.438: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-1176  846df6e8-9656-4179-9ea5-fa66e939858a 10923 1 2023-01-19 04:29:36 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 0bd22e46-4003-4c51-8b3c-95cca9b9f024 0xc003b04cc7 0xc003b04cc8}] [] [{kube-controller-manager Update apps/v1 2023-01-19 04:29:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0bd22e46-4003-4c51-8b3c-95cca9b9f024\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 04:29:39 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003b04d78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 19 04:29:40.438: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jan 19 04:29:40.438: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-1176  6a22a838-6210-4815-b35a-f87982712c21 10931 2 2023-01-19 04:29:31 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 0bd22e46-4003-4c51-8b3c-95cca9b9f024 0xc003b04b97 0xc003b04b98}] [] [{e2e.test Update apps/v1 2023-01-19 04:29:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 04:29:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0bd22e46-4003-4c51-8b3c-95cca9b9f024\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-19 04:29:39 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003b04c58 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 19 04:29:40.441: INFO: Pod "test-rolling-update-deployment-78f575d8ff-2hv6n" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-2hv6n test-rolling-update-deployment-78f575d8ff- deployment-1176  dc19d85e-ca24-4229-aeff-dca133effb7d 10922 0 2023-01-19 04:29:36 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 846df6e8-9656-4179-9ea5-fa66e939858a 0xc002fcade7 0xc002fcade8}] [] [{kube-controller-manager Update v1 2023-01-19 04:29:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"846df6e8-9656-4179-9ea5-fa66e939858a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 04:29:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.70.141\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qw2lg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qw2lg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:29:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:29:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:29:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:29:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.78,PodIP:10.100.70.141,StartTime:2023-01-19 04:29:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 04:29:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://c2b41f9d0275f38af49c5517a8a117aef3097e1831a6187c14702abd02287532,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.70.141,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 19 04:29:40.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1176" for this suite. 01/19/23 04:29:40.445
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","completed":44,"skipped":813,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.090 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:29:31.362
    Jan 19 04:29:31.362: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename deployment 01/19/23 04:29:31.363
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:29:31.378
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:29:31.38
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Jan 19 04:29:31.382: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Jan 19 04:29:31.395: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 19 04:29:36.400: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/19/23 04:29:36.4
    Jan 19 04:29:36.401: INFO: Creating deployment "test-rolling-update-deployment"
    Jan 19 04:29:36.406: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Jan 19 04:29:36.412: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Jan 19 04:29:38.419: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Jan 19 04:29:38.422: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 4, 29, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 29, 36, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 4, 29, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 29, 36, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-78f575d8ff\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 19 04:29:40.426: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 19 04:29:40.434: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-1176  0bd22e46-4003-4c51-8b3c-95cca9b9f024 10932 1 2023-01-19 04:29:36 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-01-19 04:29:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 04:29:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003b047a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-19 04:29:36 +0000 UTC,LastTransitionTime:2023-01-19 04:29:36 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-01-19 04:29:39 +0000 UTC,LastTransitionTime:2023-01-19 04:29:36 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 19 04:29:40.438: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-1176  846df6e8-9656-4179-9ea5-fa66e939858a 10923 1 2023-01-19 04:29:36 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 0bd22e46-4003-4c51-8b3c-95cca9b9f024 0xc003b04cc7 0xc003b04cc8}] [] [{kube-controller-manager Update apps/v1 2023-01-19 04:29:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0bd22e46-4003-4c51-8b3c-95cca9b9f024\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 04:29:39 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003b04d78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 19 04:29:40.438: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Jan 19 04:29:40.438: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-1176  6a22a838-6210-4815-b35a-f87982712c21 10931 2 2023-01-19 04:29:31 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 0bd22e46-4003-4c51-8b3c-95cca9b9f024 0xc003b04b97 0xc003b04b98}] [] [{e2e.test Update apps/v1 2023-01-19 04:29:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 04:29:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0bd22e46-4003-4c51-8b3c-95cca9b9f024\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-19 04:29:39 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003b04c58 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 19 04:29:40.441: INFO: Pod "test-rolling-update-deployment-78f575d8ff-2hv6n" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-2hv6n test-rolling-update-deployment-78f575d8ff- deployment-1176  dc19d85e-ca24-4229-aeff-dca133effb7d 10922 0 2023-01-19 04:29:36 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 846df6e8-9656-4179-9ea5-fa66e939858a 0xc002fcade7 0xc002fcade8}] [] [{kube-controller-manager Update v1 2023-01-19 04:29:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"846df6e8-9656-4179-9ea5-fa66e939858a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 04:29:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.70.141\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qw2lg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qw2lg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:29:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:29:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:29:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:29:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.78,PodIP:10.100.70.141,StartTime:2023-01-19 04:29:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 04:29:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://c2b41f9d0275f38af49c5517a8a117aef3097e1831a6187c14702abd02287532,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.70.141,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 19 04:29:40.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-1176" for this suite. 01/19/23 04:29:40.445
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:29:40.452
Jan 19 04:29:40.453: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename kubectl 01/19/23 04:29:40.453
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:29:40.516
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:29:40.519
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1570
STEP: creating an pod 01/19/23 04:29:40.521
Jan 19 04:29:40.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-6452 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Jan 19 04:29:40.632: INFO: stderr: ""
Jan 19 04:29:40.632: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
STEP: Waiting for log generator to start. 01/19/23 04:29:40.632
Jan 19 04:29:40.632: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jan 19 04:29:40.632: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-6452" to be "running and ready, or succeeded"
Jan 19 04:29:40.638: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014004ms
Jan 19 04:29:40.638: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'ckcp-nks-default-worker-node-1' to be 'Running' but was 'Pending'
Jan 19 04:29:42.643: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010700362s
Jan 19 04:29:42.643: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'ckcp-nks-default-worker-node-1' to be 'Running' but was 'Pending'
Jan 19 04:29:44.643: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.010410049s
Jan 19 04:29:44.643: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jan 19 04:29:44.643: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 01/19/23 04:29:44.643
Jan 19 04:29:44.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-6452 logs logs-generator logs-generator'
Jan 19 04:29:44.725: INFO: stderr: ""
Jan 19 04:29:44.725: INFO: stdout: "I0119 04:29:43.094141       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/25t 554\nI0119 04:29:43.294549       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/dlzh 302\nI0119 04:29:43.495036       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/wk6 412\nI0119 04:29:43.694422       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/n4q 282\nI0119 04:29:43.894768       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/f4n 259\nI0119 04:29:44.095246       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/tk9s 208\nI0119 04:29:44.294758       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/vs57 317\nI0119 04:29:44.495218       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/p24 204\nI0119 04:29:44.694951       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/sp4 565\n"
STEP: limiting log lines 01/19/23 04:29:44.725
Jan 19 04:29:44.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-6452 logs logs-generator logs-generator --tail=1'
Jan 19 04:29:44.833: INFO: stderr: ""
Jan 19 04:29:44.833: INFO: stdout: "I0119 04:29:44.694951       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/sp4 565\n"
Jan 19 04:29:44.833: INFO: got output "I0119 04:29:44.694951       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/sp4 565\n"
STEP: limiting log bytes 01/19/23 04:29:44.833
Jan 19 04:29:44.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-6452 logs logs-generator logs-generator --limit-bytes=1'
Jan 19 04:29:44.917: INFO: stderr: ""
Jan 19 04:29:44.917: INFO: stdout: "I"
Jan 19 04:29:44.917: INFO: got output "I"
STEP: exposing timestamps 01/19/23 04:29:44.917
Jan 19 04:29:44.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-6452 logs logs-generator logs-generator --tail=1 --timestamps'
Jan 19 04:29:45.001: INFO: stderr: ""
Jan 19 04:29:45.001: INFO: stdout: "2023-01-19T13:29:44.894401748+09:00 I0119 04:29:44.894226       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/kgn 549\n"
Jan 19 04:29:45.001: INFO: got output "2023-01-19T13:29:44.894401748+09:00 I0119 04:29:44.894226       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/kgn 549\n"
STEP: restricting to a time range 01/19/23 04:29:45.001
Jan 19 04:29:47.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-6452 logs logs-generator logs-generator --since=1s'
Jan 19 04:29:47.589: INFO: stderr: ""
Jan 19 04:29:47.589: INFO: stdout: "I0119 04:29:46.695022       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/qngx 447\nI0119 04:29:46.894240       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/k6h8 430\nI0119 04:29:47.094571       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/9ckl 318\nI0119 04:29:47.294908       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/mr95 222\nI0119 04:29:47.494220       1 logs_generator.go:76] 22 POST /api/v1/namespaces/ns/pods/7zcd 547\n"
Jan 19 04:29:47.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-6452 logs logs-generator logs-generator --since=24h'
Jan 19 04:29:47.671: INFO: stderr: ""
Jan 19 04:29:47.671: INFO: stdout: "I0119 04:29:43.094141       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/25t 554\nI0119 04:29:43.294549       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/dlzh 302\nI0119 04:29:43.495036       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/wk6 412\nI0119 04:29:43.694422       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/n4q 282\nI0119 04:29:43.894768       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/f4n 259\nI0119 04:29:44.095246       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/tk9s 208\nI0119 04:29:44.294758       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/vs57 317\nI0119 04:29:44.495218       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/p24 204\nI0119 04:29:44.694951       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/sp4 565\nI0119 04:29:44.894226       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/kgn 549\nI0119 04:29:45.094602       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/vpqm 203\nI0119 04:29:45.294988       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/5t8d 495\nI0119 04:29:45.494242       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/default/pods/cq4 555\nI0119 04:29:45.694602       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/47w 576\nI0119 04:29:45.894946       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/zgb 348\nI0119 04:29:46.095031       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/lkt 475\nI0119 04:29:46.294250       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/2bwk 346\nI0119 04:29:46.494695       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/vtf4 533\nI0119 04:29:46.695022       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/qngx 447\nI0119 04:29:46.894240       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/k6h8 430\nI0119 04:29:47.094571       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/9ckl 318\nI0119 04:29:47.294908       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/mr95 222\nI0119 04:29:47.494220       1 logs_generator.go:76] 22 POST /api/v1/namespaces/ns/pods/7zcd 547\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1575
Jan 19 04:29:47.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-6452 delete pod logs-generator'
Jan 19 04:29:48.445: INFO: stderr: ""
Jan 19 04:29:48.445: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 19 04:29:48.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6452" for this suite. 01/19/23 04:29:48.45
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","completed":45,"skipped":818,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.004 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1567
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1590

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:29:40.452
    Jan 19 04:29:40.453: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename kubectl 01/19/23 04:29:40.453
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:29:40.516
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:29:40.519
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1570
    STEP: creating an pod 01/19/23 04:29:40.521
    Jan 19 04:29:40.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-6452 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Jan 19 04:29:40.632: INFO: stderr: ""
    Jan 19 04:29:40.632: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1590
    STEP: Waiting for log generator to start. 01/19/23 04:29:40.632
    Jan 19 04:29:40.632: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Jan 19 04:29:40.632: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-6452" to be "running and ready, or succeeded"
    Jan 19 04:29:40.638: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014004ms
    Jan 19 04:29:40.638: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'ckcp-nks-default-worker-node-1' to be 'Running' but was 'Pending'
    Jan 19 04:29:42.643: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010700362s
    Jan 19 04:29:42.643: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'ckcp-nks-default-worker-node-1' to be 'Running' but was 'Pending'
    Jan 19 04:29:44.643: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.010410049s
    Jan 19 04:29:44.643: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Jan 19 04:29:44.643: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 01/19/23 04:29:44.643
    Jan 19 04:29:44.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-6452 logs logs-generator logs-generator'
    Jan 19 04:29:44.725: INFO: stderr: ""
    Jan 19 04:29:44.725: INFO: stdout: "I0119 04:29:43.094141       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/25t 554\nI0119 04:29:43.294549       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/dlzh 302\nI0119 04:29:43.495036       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/wk6 412\nI0119 04:29:43.694422       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/n4q 282\nI0119 04:29:43.894768       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/f4n 259\nI0119 04:29:44.095246       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/tk9s 208\nI0119 04:29:44.294758       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/vs57 317\nI0119 04:29:44.495218       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/p24 204\nI0119 04:29:44.694951       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/sp4 565\n"
    STEP: limiting log lines 01/19/23 04:29:44.725
    Jan 19 04:29:44.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-6452 logs logs-generator logs-generator --tail=1'
    Jan 19 04:29:44.833: INFO: stderr: ""
    Jan 19 04:29:44.833: INFO: stdout: "I0119 04:29:44.694951       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/sp4 565\n"
    Jan 19 04:29:44.833: INFO: got output "I0119 04:29:44.694951       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/sp4 565\n"
    STEP: limiting log bytes 01/19/23 04:29:44.833
    Jan 19 04:29:44.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-6452 logs logs-generator logs-generator --limit-bytes=1'
    Jan 19 04:29:44.917: INFO: stderr: ""
    Jan 19 04:29:44.917: INFO: stdout: "I"
    Jan 19 04:29:44.917: INFO: got output "I"
    STEP: exposing timestamps 01/19/23 04:29:44.917
    Jan 19 04:29:44.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-6452 logs logs-generator logs-generator --tail=1 --timestamps'
    Jan 19 04:29:45.001: INFO: stderr: ""
    Jan 19 04:29:45.001: INFO: stdout: "2023-01-19T13:29:44.894401748+09:00 I0119 04:29:44.894226       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/kgn 549\n"
    Jan 19 04:29:45.001: INFO: got output "2023-01-19T13:29:44.894401748+09:00 I0119 04:29:44.894226       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/kgn 549\n"
    STEP: restricting to a time range 01/19/23 04:29:45.001
    Jan 19 04:29:47.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-6452 logs logs-generator logs-generator --since=1s'
    Jan 19 04:29:47.589: INFO: stderr: ""
    Jan 19 04:29:47.589: INFO: stdout: "I0119 04:29:46.695022       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/qngx 447\nI0119 04:29:46.894240       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/k6h8 430\nI0119 04:29:47.094571       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/9ckl 318\nI0119 04:29:47.294908       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/mr95 222\nI0119 04:29:47.494220       1 logs_generator.go:76] 22 POST /api/v1/namespaces/ns/pods/7zcd 547\n"
    Jan 19 04:29:47.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-6452 logs logs-generator logs-generator --since=24h'
    Jan 19 04:29:47.671: INFO: stderr: ""
    Jan 19 04:29:47.671: INFO: stdout: "I0119 04:29:43.094141       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/25t 554\nI0119 04:29:43.294549       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/dlzh 302\nI0119 04:29:43.495036       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/wk6 412\nI0119 04:29:43.694422       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/n4q 282\nI0119 04:29:43.894768       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/f4n 259\nI0119 04:29:44.095246       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/tk9s 208\nI0119 04:29:44.294758       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/vs57 317\nI0119 04:29:44.495218       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/p24 204\nI0119 04:29:44.694951       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/sp4 565\nI0119 04:29:44.894226       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/kgn 549\nI0119 04:29:45.094602       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/vpqm 203\nI0119 04:29:45.294988       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/5t8d 495\nI0119 04:29:45.494242       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/default/pods/cq4 555\nI0119 04:29:45.694602       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/47w 576\nI0119 04:29:45.894946       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/zgb 348\nI0119 04:29:46.095031       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/lkt 475\nI0119 04:29:46.294250       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/2bwk 346\nI0119 04:29:46.494695       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/vtf4 533\nI0119 04:29:46.695022       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/qngx 447\nI0119 04:29:46.894240       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/k6h8 430\nI0119 04:29:47.094571       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/9ckl 318\nI0119 04:29:47.294908       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/mr95 222\nI0119 04:29:47.494220       1 logs_generator.go:76] 22 POST /api/v1/namespaces/ns/pods/7zcd 547\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1575
    Jan 19 04:29:47.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-6452 delete pod logs-generator'
    Jan 19 04:29:48.445: INFO: stderr: ""
    Jan 19 04:29:48.445: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 19 04:29:48.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6452" for this suite. 01/19/23 04:29:48.45
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:29:48.458
Jan 19 04:29:48.458: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename downward-api 01/19/23 04:29:48.458
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:29:48.475
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:29:48.477
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
STEP: Creating a pod to test downward API volume plugin 01/19/23 04:29:48.48
Jan 19 04:29:48.489: INFO: Waiting up to 5m0s for pod "downwardapi-volume-206299be-c75c-489d-a848-9092927c4240" in namespace "downward-api-4215" to be "Succeeded or Failed"
Jan 19 04:29:48.495: INFO: Pod "downwardapi-volume-206299be-c75c-489d-a848-9092927c4240": Phase="Pending", Reason="", readiness=false. Elapsed: 5.826846ms
Jan 19 04:29:50.503: INFO: Pod "downwardapi-volume-206299be-c75c-489d-a848-9092927c4240": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01362289s
Jan 19 04:29:52.501: INFO: Pod "downwardapi-volume-206299be-c75c-489d-a848-9092927c4240": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011642879s
Jan 19 04:29:54.499: INFO: Pod "downwardapi-volume-206299be-c75c-489d-a848-9092927c4240": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010193485s
STEP: Saw pod success 01/19/23 04:29:54.499
Jan 19 04:29:54.499: INFO: Pod "downwardapi-volume-206299be-c75c-489d-a848-9092927c4240" satisfied condition "Succeeded or Failed"
Jan 19 04:29:54.503: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod downwardapi-volume-206299be-c75c-489d-a848-9092927c4240 container client-container: <nil>
STEP: delete the pod 01/19/23 04:29:54.509
Jan 19 04:29:54.519: INFO: Waiting for pod downwardapi-volume-206299be-c75c-489d-a848-9092927c4240 to disappear
Jan 19 04:29:54.521: INFO: Pod downwardapi-volume-206299be-c75c-489d-a848-9092927c4240 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 19 04:29:54.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4215" for this suite. 01/19/23 04:29:54.525
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","completed":46,"skipped":843,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.073 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:29:48.458
    Jan 19 04:29:48.458: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename downward-api 01/19/23 04:29:48.458
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:29:48.475
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:29:48.477
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:220
    STEP: Creating a pod to test downward API volume plugin 01/19/23 04:29:48.48
    Jan 19 04:29:48.489: INFO: Waiting up to 5m0s for pod "downwardapi-volume-206299be-c75c-489d-a848-9092927c4240" in namespace "downward-api-4215" to be "Succeeded or Failed"
    Jan 19 04:29:48.495: INFO: Pod "downwardapi-volume-206299be-c75c-489d-a848-9092927c4240": Phase="Pending", Reason="", readiness=false. Elapsed: 5.826846ms
    Jan 19 04:29:50.503: INFO: Pod "downwardapi-volume-206299be-c75c-489d-a848-9092927c4240": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01362289s
    Jan 19 04:29:52.501: INFO: Pod "downwardapi-volume-206299be-c75c-489d-a848-9092927c4240": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011642879s
    Jan 19 04:29:54.499: INFO: Pod "downwardapi-volume-206299be-c75c-489d-a848-9092927c4240": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010193485s
    STEP: Saw pod success 01/19/23 04:29:54.499
    Jan 19 04:29:54.499: INFO: Pod "downwardapi-volume-206299be-c75c-489d-a848-9092927c4240" satisfied condition "Succeeded or Failed"
    Jan 19 04:29:54.503: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod downwardapi-volume-206299be-c75c-489d-a848-9092927c4240 container client-container: <nil>
    STEP: delete the pod 01/19/23 04:29:54.509
    Jan 19 04:29:54.519: INFO: Waiting for pod downwardapi-volume-206299be-c75c-489d-a848-9092927c4240 to disappear
    Jan 19 04:29:54.521: INFO: Pod downwardapi-volume-206299be-c75c-489d-a848-9092927c4240 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 19 04:29:54.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-4215" for this suite. 01/19/23 04:29:54.525
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:29:54.531
Jan 19 04:29:54.531: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename cronjob 01/19/23 04:29:54.532
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:29:54.545
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:29:54.547
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 01/19/23 04:29:54.549
STEP: creating 01/19/23 04:29:54.549
STEP: getting 01/19/23 04:29:54.555
STEP: listing 01/19/23 04:29:54.557
STEP: watching 01/19/23 04:29:54.559
Jan 19 04:29:54.559: INFO: starting watch
STEP: cluster-wide listing 01/19/23 04:29:54.56
STEP: cluster-wide watching 01/19/23 04:29:54.563
Jan 19 04:29:54.563: INFO: starting watch
STEP: patching 01/19/23 04:29:54.564
STEP: updating 01/19/23 04:29:54.57
Jan 19 04:29:54.579: INFO: waiting for watch events with expected annotations
Jan 19 04:29:54.579: INFO: saw patched and updated annotations
STEP: patching /status 01/19/23 04:29:54.579
STEP: updating /status 01/19/23 04:29:54.585
STEP: get /status 01/19/23 04:29:54.592
STEP: deleting 01/19/23 04:29:54.595
STEP: deleting a collection 01/19/23 04:29:54.608
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan 19 04:29:54.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8861" for this suite. 01/19/23 04:29:54.62
{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","completed":47,"skipped":876,"failed":0}
------------------------------
â€¢ [0.096 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:29:54.531
    Jan 19 04:29:54.531: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename cronjob 01/19/23 04:29:54.532
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:29:54.545
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:29:54.547
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 01/19/23 04:29:54.549
    STEP: creating 01/19/23 04:29:54.549
    STEP: getting 01/19/23 04:29:54.555
    STEP: listing 01/19/23 04:29:54.557
    STEP: watching 01/19/23 04:29:54.559
    Jan 19 04:29:54.559: INFO: starting watch
    STEP: cluster-wide listing 01/19/23 04:29:54.56
    STEP: cluster-wide watching 01/19/23 04:29:54.563
    Jan 19 04:29:54.563: INFO: starting watch
    STEP: patching 01/19/23 04:29:54.564
    STEP: updating 01/19/23 04:29:54.57
    Jan 19 04:29:54.579: INFO: waiting for watch events with expected annotations
    Jan 19 04:29:54.579: INFO: saw patched and updated annotations
    STEP: patching /status 01/19/23 04:29:54.579
    STEP: updating /status 01/19/23 04:29:54.585
    STEP: get /status 01/19/23 04:29:54.592
    STEP: deleting 01/19/23 04:29:54.595
    STEP: deleting a collection 01/19/23 04:29:54.608
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan 19 04:29:54.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-8861" for this suite. 01/19/23 04:29:54.62
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:29:54.628
Jan 19 04:29:54.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename pods 01/19/23 04:29:54.629
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:29:54.642
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:29:54.644
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 01/19/23 04:29:54.647
STEP: submitting the pod to kubernetes 01/19/23 04:29:54.647
STEP: verifying QOS class is set on the pod 01/19/23 04:29:54.656
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
Jan 19 04:29:54.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4693" for this suite. 01/19/23 04:29:54.68
{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","completed":48,"skipped":880,"failed":0}
------------------------------
â€¢ [0.057 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:29:54.628
    Jan 19 04:29:54.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename pods 01/19/23 04:29:54.629
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:29:54.642
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:29:54.644
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 01/19/23 04:29:54.647
    STEP: submitting the pod to kubernetes 01/19/23 04:29:54.647
    STEP: verifying QOS class is set on the pod 01/19/23 04:29:54.656
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:187
    Jan 19 04:29:54.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4693" for this suite. 01/19/23 04:29:54.68
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:29:54.686
Jan 19 04:29:54.686: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename projected 01/19/23 04:29:54.686
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:29:54.699
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:29:54.701
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
STEP: Creating a pod to test downward API volume plugin 01/19/23 04:29:54.704
Jan 19 04:29:54.712: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e40c93ac-0d04-4943-ae92-22118f13dc60" in namespace "projected-5658" to be "Succeeded or Failed"
Jan 19 04:29:54.715: INFO: Pod "downwardapi-volume-e40c93ac-0d04-4943-ae92-22118f13dc60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.539367ms
Jan 19 04:29:56.719: INFO: Pod "downwardapi-volume-e40c93ac-0d04-4943-ae92-22118f13dc60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006200676s
Jan 19 04:29:58.720: INFO: Pod "downwardapi-volume-e40c93ac-0d04-4943-ae92-22118f13dc60": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007578072s
Jan 19 04:30:00.719: INFO: Pod "downwardapi-volume-e40c93ac-0d04-4943-ae92-22118f13dc60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006352495s
STEP: Saw pod success 01/19/23 04:30:00.719
Jan 19 04:30:00.719: INFO: Pod "downwardapi-volume-e40c93ac-0d04-4943-ae92-22118f13dc60" satisfied condition "Succeeded or Failed"
Jan 19 04:30:00.722: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod downwardapi-volume-e40c93ac-0d04-4943-ae92-22118f13dc60 container client-container: <nil>
STEP: delete the pod 01/19/23 04:30:00.727
Jan 19 04:30:00.740: INFO: Waiting for pod downwardapi-volume-e40c93ac-0d04-4943-ae92-22118f13dc60 to disappear
Jan 19 04:30:00.743: INFO: Pod downwardapi-volume-e40c93ac-0d04-4943-ae92-22118f13dc60 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 19 04:30:00.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5658" for this suite. 01/19/23 04:30:00.746
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","completed":49,"skipped":882,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.067 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:29:54.686
    Jan 19 04:29:54.686: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename projected 01/19/23 04:29:54.686
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:29:54.699
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:29:54.701
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:234
    STEP: Creating a pod to test downward API volume plugin 01/19/23 04:29:54.704
    Jan 19 04:29:54.712: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e40c93ac-0d04-4943-ae92-22118f13dc60" in namespace "projected-5658" to be "Succeeded or Failed"
    Jan 19 04:29:54.715: INFO: Pod "downwardapi-volume-e40c93ac-0d04-4943-ae92-22118f13dc60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.539367ms
    Jan 19 04:29:56.719: INFO: Pod "downwardapi-volume-e40c93ac-0d04-4943-ae92-22118f13dc60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006200676s
    Jan 19 04:29:58.720: INFO: Pod "downwardapi-volume-e40c93ac-0d04-4943-ae92-22118f13dc60": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007578072s
    Jan 19 04:30:00.719: INFO: Pod "downwardapi-volume-e40c93ac-0d04-4943-ae92-22118f13dc60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006352495s
    STEP: Saw pod success 01/19/23 04:30:00.719
    Jan 19 04:30:00.719: INFO: Pod "downwardapi-volume-e40c93ac-0d04-4943-ae92-22118f13dc60" satisfied condition "Succeeded or Failed"
    Jan 19 04:30:00.722: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod downwardapi-volume-e40c93ac-0d04-4943-ae92-22118f13dc60 container client-container: <nil>
    STEP: delete the pod 01/19/23 04:30:00.727
    Jan 19 04:30:00.740: INFO: Waiting for pod downwardapi-volume-e40c93ac-0d04-4943-ae92-22118f13dc60 to disappear
    Jan 19 04:30:00.743: INFO: Pod downwardapi-volume-e40c93ac-0d04-4943-ae92-22118f13dc60 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 19 04:30:00.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5658" for this suite. 01/19/23 04:30:00.746
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:30:00.753
Jan 19 04:30:00.753: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename downward-api 01/19/23 04:30:00.754
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:30:00.768
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:30:00.77
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
STEP: Creating the pod 01/19/23 04:30:00.772
Jan 19 04:30:00.780: INFO: Waiting up to 5m0s for pod "labelsupdate4ad24470-a3aa-4c78-a634-63363b1ea93b" in namespace "downward-api-4766" to be "running and ready"
Jan 19 04:30:00.785: INFO: Pod "labelsupdate4ad24470-a3aa-4c78-a634-63363b1ea93b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.019985ms
Jan 19 04:30:00.785: INFO: The phase of Pod labelsupdate4ad24470-a3aa-4c78-a634-63363b1ea93b is Pending, waiting for it to be Running (with Ready = true)
Jan 19 04:30:02.789: INFO: Pod "labelsupdate4ad24470-a3aa-4c78-a634-63363b1ea93b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009131822s
Jan 19 04:30:02.789: INFO: The phase of Pod labelsupdate4ad24470-a3aa-4c78-a634-63363b1ea93b is Pending, waiting for it to be Running (with Ready = true)
Jan 19 04:30:04.789: INFO: Pod "labelsupdate4ad24470-a3aa-4c78-a634-63363b1ea93b": Phase="Running", Reason="", readiness=true. Elapsed: 4.00949875s
Jan 19 04:30:04.789: INFO: The phase of Pod labelsupdate4ad24470-a3aa-4c78-a634-63363b1ea93b is Running (Ready = true)
Jan 19 04:30:04.789: INFO: Pod "labelsupdate4ad24470-a3aa-4c78-a634-63363b1ea93b" satisfied condition "running and ready"
Jan 19 04:30:05.313: INFO: Successfully updated pod "labelsupdate4ad24470-a3aa-4c78-a634-63363b1ea93b"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 19 04:30:07.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4766" for this suite. 01/19/23 04:30:07.333
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","completed":50,"skipped":885,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.588 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:30:00.753
    Jan 19 04:30:00.753: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename downward-api 01/19/23 04:30:00.754
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:30:00.768
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:30:00.77
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:129
    STEP: Creating the pod 01/19/23 04:30:00.772
    Jan 19 04:30:00.780: INFO: Waiting up to 5m0s for pod "labelsupdate4ad24470-a3aa-4c78-a634-63363b1ea93b" in namespace "downward-api-4766" to be "running and ready"
    Jan 19 04:30:00.785: INFO: Pod "labelsupdate4ad24470-a3aa-4c78-a634-63363b1ea93b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.019985ms
    Jan 19 04:30:00.785: INFO: The phase of Pod labelsupdate4ad24470-a3aa-4c78-a634-63363b1ea93b is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 04:30:02.789: INFO: Pod "labelsupdate4ad24470-a3aa-4c78-a634-63363b1ea93b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009131822s
    Jan 19 04:30:02.789: INFO: The phase of Pod labelsupdate4ad24470-a3aa-4c78-a634-63363b1ea93b is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 04:30:04.789: INFO: Pod "labelsupdate4ad24470-a3aa-4c78-a634-63363b1ea93b": Phase="Running", Reason="", readiness=true. Elapsed: 4.00949875s
    Jan 19 04:30:04.789: INFO: The phase of Pod labelsupdate4ad24470-a3aa-4c78-a634-63363b1ea93b is Running (Ready = true)
    Jan 19 04:30:04.789: INFO: Pod "labelsupdate4ad24470-a3aa-4c78-a634-63363b1ea93b" satisfied condition "running and ready"
    Jan 19 04:30:05.313: INFO: Successfully updated pod "labelsupdate4ad24470-a3aa-4c78-a634-63363b1ea93b"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 19 04:30:07.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-4766" for this suite. 01/19/23 04:30:07.333
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:30:07.342
Jan 19 04:30:07.342: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename var-expansion 01/19/23 04:30:07.342
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:30:07.357
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:30:07.36
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
STEP: Creating a pod to test env composition 01/19/23 04:30:07.362
Jan 19 04:30:07.369: INFO: Waiting up to 5m0s for pod "var-expansion-61ec9419-481e-4343-9a34-276cf5ff7274" in namespace "var-expansion-1740" to be "Succeeded or Failed"
Jan 19 04:30:07.372: INFO: Pod "var-expansion-61ec9419-481e-4343-9a34-276cf5ff7274": Phase="Pending", Reason="", readiness=false. Elapsed: 3.200181ms
Jan 19 04:30:09.376: INFO: Pod "var-expansion-61ec9419-481e-4343-9a34-276cf5ff7274": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007338703s
Jan 19 04:30:11.375: INFO: Pod "var-expansion-61ec9419-481e-4343-9a34-276cf5ff7274": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006838458s
Jan 19 04:30:13.377: INFO: Pod "var-expansion-61ec9419-481e-4343-9a34-276cf5ff7274": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008649149s
STEP: Saw pod success 01/19/23 04:30:13.377
Jan 19 04:30:13.377: INFO: Pod "var-expansion-61ec9419-481e-4343-9a34-276cf5ff7274" satisfied condition "Succeeded or Failed"
Jan 19 04:30:13.380: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod var-expansion-61ec9419-481e-4343-9a34-276cf5ff7274 container dapi-container: <nil>
STEP: delete the pod 01/19/23 04:30:13.387
Jan 19 04:30:13.401: INFO: Waiting for pod var-expansion-61ec9419-481e-4343-9a34-276cf5ff7274 to disappear
Jan 19 04:30:13.404: INFO: Pod var-expansion-61ec9419-481e-4343-9a34-276cf5ff7274 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 19 04:30:13.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1740" for this suite. 01/19/23 04:30:13.407
{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","completed":51,"skipped":906,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.073 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:30:07.342
    Jan 19 04:30:07.342: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename var-expansion 01/19/23 04:30:07.342
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:30:07.357
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:30:07.36
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:43
    STEP: Creating a pod to test env composition 01/19/23 04:30:07.362
    Jan 19 04:30:07.369: INFO: Waiting up to 5m0s for pod "var-expansion-61ec9419-481e-4343-9a34-276cf5ff7274" in namespace "var-expansion-1740" to be "Succeeded or Failed"
    Jan 19 04:30:07.372: INFO: Pod "var-expansion-61ec9419-481e-4343-9a34-276cf5ff7274": Phase="Pending", Reason="", readiness=false. Elapsed: 3.200181ms
    Jan 19 04:30:09.376: INFO: Pod "var-expansion-61ec9419-481e-4343-9a34-276cf5ff7274": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007338703s
    Jan 19 04:30:11.375: INFO: Pod "var-expansion-61ec9419-481e-4343-9a34-276cf5ff7274": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006838458s
    Jan 19 04:30:13.377: INFO: Pod "var-expansion-61ec9419-481e-4343-9a34-276cf5ff7274": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008649149s
    STEP: Saw pod success 01/19/23 04:30:13.377
    Jan 19 04:30:13.377: INFO: Pod "var-expansion-61ec9419-481e-4343-9a34-276cf5ff7274" satisfied condition "Succeeded or Failed"
    Jan 19 04:30:13.380: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod var-expansion-61ec9419-481e-4343-9a34-276cf5ff7274 container dapi-container: <nil>
    STEP: delete the pod 01/19/23 04:30:13.387
    Jan 19 04:30:13.401: INFO: Waiting for pod var-expansion-61ec9419-481e-4343-9a34-276cf5ff7274 to disappear
    Jan 19 04:30:13.404: INFO: Pod var-expansion-61ec9419-481e-4343-9a34-276cf5ff7274 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 19 04:30:13.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-1740" for this suite. 01/19/23 04:30:13.407
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:30:13.415
Jan 19 04:30:13.415: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename emptydir 01/19/23 04:30:13.416
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:30:13.429
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:30:13.431
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
STEP: Creating a pod to test emptydir 0644 on tmpfs 01/19/23 04:30:13.434
Jan 19 04:30:13.442: INFO: Waiting up to 5m0s for pod "pod-3a43c92d-e1a1-4d7e-bfa2-f1597c57d481" in namespace "emptydir-2518" to be "Succeeded or Failed"
Jan 19 04:30:13.447: INFO: Pod "pod-3a43c92d-e1a1-4d7e-bfa2-f1597c57d481": Phase="Pending", Reason="", readiness=false. Elapsed: 4.228473ms
Jan 19 04:30:15.450: INFO: Pod "pod-3a43c92d-e1a1-4d7e-bfa2-f1597c57d481": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008067577s
Jan 19 04:30:17.452: INFO: Pod "pod-3a43c92d-e1a1-4d7e-bfa2-f1597c57d481": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009234874s
Jan 19 04:30:19.452: INFO: Pod "pod-3a43c92d-e1a1-4d7e-bfa2-f1597c57d481": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00923875s
STEP: Saw pod success 01/19/23 04:30:19.452
Jan 19 04:30:19.452: INFO: Pod "pod-3a43c92d-e1a1-4d7e-bfa2-f1597c57d481" satisfied condition "Succeeded or Failed"
Jan 19 04:30:19.454: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-3a43c92d-e1a1-4d7e-bfa2-f1597c57d481 container test-container: <nil>
STEP: delete the pod 01/19/23 04:30:19.46
Jan 19 04:30:19.475: INFO: Waiting for pod pod-3a43c92d-e1a1-4d7e-bfa2-f1597c57d481 to disappear
Jan 19 04:30:19.478: INFO: Pod pod-3a43c92d-e1a1-4d7e-bfa2-f1597c57d481 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 19 04:30:19.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2518" for this suite. 01/19/23 04:30:19.481
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":52,"skipped":909,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.073 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:30:13.415
    Jan 19 04:30:13.415: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename emptydir 01/19/23 04:30:13.416
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:30:13.429
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:30:13.431
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:96
    STEP: Creating a pod to test emptydir 0644 on tmpfs 01/19/23 04:30:13.434
    Jan 19 04:30:13.442: INFO: Waiting up to 5m0s for pod "pod-3a43c92d-e1a1-4d7e-bfa2-f1597c57d481" in namespace "emptydir-2518" to be "Succeeded or Failed"
    Jan 19 04:30:13.447: INFO: Pod "pod-3a43c92d-e1a1-4d7e-bfa2-f1597c57d481": Phase="Pending", Reason="", readiness=false. Elapsed: 4.228473ms
    Jan 19 04:30:15.450: INFO: Pod "pod-3a43c92d-e1a1-4d7e-bfa2-f1597c57d481": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008067577s
    Jan 19 04:30:17.452: INFO: Pod "pod-3a43c92d-e1a1-4d7e-bfa2-f1597c57d481": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009234874s
    Jan 19 04:30:19.452: INFO: Pod "pod-3a43c92d-e1a1-4d7e-bfa2-f1597c57d481": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00923875s
    STEP: Saw pod success 01/19/23 04:30:19.452
    Jan 19 04:30:19.452: INFO: Pod "pod-3a43c92d-e1a1-4d7e-bfa2-f1597c57d481" satisfied condition "Succeeded or Failed"
    Jan 19 04:30:19.454: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-3a43c92d-e1a1-4d7e-bfa2-f1597c57d481 container test-container: <nil>
    STEP: delete the pod 01/19/23 04:30:19.46
    Jan 19 04:30:19.475: INFO: Waiting for pod pod-3a43c92d-e1a1-4d7e-bfa2-f1597c57d481 to disappear
    Jan 19 04:30:19.478: INFO: Pod pod-3a43c92d-e1a1-4d7e-bfa2-f1597c57d481 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 19 04:30:19.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2518" for this suite. 01/19/23 04:30:19.481
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:30:19.49
Jan 19 04:30:19.490: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename daemonsets 01/19/23 04:30:19.491
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:30:19.506
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:30:19.508
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
STEP: Creating simple DaemonSet "daemon-set" 01/19/23 04:30:19.525
STEP: Check that daemon pods launch on every node of the cluster. 01/19/23 04:30:19.532
Jan 19 04:30:19.540: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 04:30:19.540: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
Jan 19 04:30:20.557: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 04:30:20.557: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
Jan 19 04:30:21.554: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 04:30:21.554: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
Jan 19 04:30:22.547: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 19 04:30:22.547: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
Jan 19 04:30:23.549: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 19 04:30:23.549: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
Jan 19 04:30:24.554: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 19 04:30:24.554: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
Jan 19 04:30:25.556: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 19 04:30:25.556: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
Jan 19 04:30:26.547: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 19 04:30:26.547: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
Jan 19 04:30:27.547: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 19 04:30:27.547: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
Jan 19 04:30:28.548: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 19 04:30:28.548: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
Jan 19 04:30:29.547: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 19 04:30:29.547: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 01/19/23 04:30:29.549
Jan 19 04:30:29.566: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 19 04:30:29.566: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
Jan 19 04:30:30.573: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 19 04:30:30.573: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
Jan 19 04:30:31.573: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 19 04:30:31.573: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
Jan 19 04:30:32.575: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 19 04:30:32.575: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
Jan 19 04:30:33.572: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 19 04:30:33.572: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
Jan 19 04:30:34.575: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 19 04:30:34.575: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/19/23 04:30:34.577
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5855, will wait for the garbage collector to delete the pods 01/19/23 04:30:34.577
Jan 19 04:30:34.637: INFO: Deleting DaemonSet.extensions daemon-set took: 6.533577ms
Jan 19 04:30:34.738: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.770592ms
Jan 19 04:30:36.643: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 04:30:36.643: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 19 04:30:36.646: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"11294"},"items":null}

Jan 19 04:30:36.649: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"11294"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 19 04:30:36.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5855" for this suite. 01/19/23 04:30:36.662
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","completed":53,"skipped":972,"failed":0}
------------------------------
â€¢ [SLOW TEST] [17.179 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:30:19.49
    Jan 19 04:30:19.490: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename daemonsets 01/19/23 04:30:19.491
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:30:19.506
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:30:19.508
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:165
    STEP: Creating simple DaemonSet "daemon-set" 01/19/23 04:30:19.525
    STEP: Check that daemon pods launch on every node of the cluster. 01/19/23 04:30:19.532
    Jan 19 04:30:19.540: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 04:30:19.540: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
    Jan 19 04:30:20.557: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 04:30:20.557: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
    Jan 19 04:30:21.554: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 04:30:21.554: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
    Jan 19 04:30:22.547: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 19 04:30:22.547: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
    Jan 19 04:30:23.549: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 19 04:30:23.549: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
    Jan 19 04:30:24.554: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 19 04:30:24.554: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
    Jan 19 04:30:25.556: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 19 04:30:25.556: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
    Jan 19 04:30:26.547: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 19 04:30:26.547: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
    Jan 19 04:30:27.547: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 19 04:30:27.547: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
    Jan 19 04:30:28.548: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 19 04:30:28.548: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
    Jan 19 04:30:29.547: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 19 04:30:29.547: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 01/19/23 04:30:29.549
    Jan 19 04:30:29.566: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 19 04:30:29.566: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
    Jan 19 04:30:30.573: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 19 04:30:30.573: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
    Jan 19 04:30:31.573: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 19 04:30:31.573: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
    Jan 19 04:30:32.575: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 19 04:30:32.575: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
    Jan 19 04:30:33.572: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 19 04:30:33.572: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
    Jan 19 04:30:34.575: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 19 04:30:34.575: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/19/23 04:30:34.577
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5855, will wait for the garbage collector to delete the pods 01/19/23 04:30:34.577
    Jan 19 04:30:34.637: INFO: Deleting DaemonSet.extensions daemon-set took: 6.533577ms
    Jan 19 04:30:34.738: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.770592ms
    Jan 19 04:30:36.643: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 04:30:36.643: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 19 04:30:36.646: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"11294"},"items":null}

    Jan 19 04:30:36.649: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"11294"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 19 04:30:36.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-5855" for this suite. 01/19/23 04:30:36.662
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:30:36.67
Jan 19 04:30:36.670: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename downward-api 01/19/23 04:30:36.671
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:30:36.684
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:30:36.687
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
STEP: Creating a pod to test downward api env vars 01/19/23 04:30:36.689
Jan 19 04:30:36.699: INFO: Waiting up to 5m0s for pod "downward-api-c2d6c18d-69e3-4d50-bae3-ddb9ee45fb10" in namespace "downward-api-8425" to be "Succeeded or Failed"
Jan 19 04:30:36.703: INFO: Pod "downward-api-c2d6c18d-69e3-4d50-bae3-ddb9ee45fb10": Phase="Pending", Reason="", readiness=false. Elapsed: 4.261083ms
Jan 19 04:30:38.708: INFO: Pod "downward-api-c2d6c18d-69e3-4d50-bae3-ddb9ee45fb10": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008975896s
Jan 19 04:30:40.708: INFO: Pod "downward-api-c2d6c18d-69e3-4d50-bae3-ddb9ee45fb10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008656202s
STEP: Saw pod success 01/19/23 04:30:40.708
Jan 19 04:30:40.708: INFO: Pod "downward-api-c2d6c18d-69e3-4d50-bae3-ddb9ee45fb10" satisfied condition "Succeeded or Failed"
Jan 19 04:30:40.711: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod downward-api-c2d6c18d-69e3-4d50-bae3-ddb9ee45fb10 container dapi-container: <nil>
STEP: delete the pod 01/19/23 04:30:40.717
Jan 19 04:30:40.728: INFO: Waiting for pod downward-api-c2d6c18d-69e3-4d50-bae3-ddb9ee45fb10 to disappear
Jan 19 04:30:40.730: INFO: Pod downward-api-c2d6c18d-69e3-4d50-bae3-ddb9ee45fb10 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan 19 04:30:40.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8425" for this suite. 01/19/23 04:30:40.733
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","completed":54,"skipped":1010,"failed":0}
------------------------------
â€¢ [4.070 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:30:36.67
    Jan 19 04:30:36.670: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename downward-api 01/19/23 04:30:36.671
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:30:36.684
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:30:36.687
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:266
    STEP: Creating a pod to test downward api env vars 01/19/23 04:30:36.689
    Jan 19 04:30:36.699: INFO: Waiting up to 5m0s for pod "downward-api-c2d6c18d-69e3-4d50-bae3-ddb9ee45fb10" in namespace "downward-api-8425" to be "Succeeded or Failed"
    Jan 19 04:30:36.703: INFO: Pod "downward-api-c2d6c18d-69e3-4d50-bae3-ddb9ee45fb10": Phase="Pending", Reason="", readiness=false. Elapsed: 4.261083ms
    Jan 19 04:30:38.708: INFO: Pod "downward-api-c2d6c18d-69e3-4d50-bae3-ddb9ee45fb10": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008975896s
    Jan 19 04:30:40.708: INFO: Pod "downward-api-c2d6c18d-69e3-4d50-bae3-ddb9ee45fb10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008656202s
    STEP: Saw pod success 01/19/23 04:30:40.708
    Jan 19 04:30:40.708: INFO: Pod "downward-api-c2d6c18d-69e3-4d50-bae3-ddb9ee45fb10" satisfied condition "Succeeded or Failed"
    Jan 19 04:30:40.711: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod downward-api-c2d6c18d-69e3-4d50-bae3-ddb9ee45fb10 container dapi-container: <nil>
    STEP: delete the pod 01/19/23 04:30:40.717
    Jan 19 04:30:40.728: INFO: Waiting for pod downward-api-c2d6c18d-69e3-4d50-bae3-ddb9ee45fb10 to disappear
    Jan 19 04:30:40.730: INFO: Pod downward-api-c2d6c18d-69e3-4d50-bae3-ddb9ee45fb10 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan 19 04:30:40.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8425" for this suite. 01/19/23 04:30:40.733
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:30:40.74
Jan 19 04:30:40.740: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename downward-api 01/19/23 04:30:40.741
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:30:40.755
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:30:40.757
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
STEP: Creating a pod to test downward api env vars 01/19/23 04:30:40.759
Jan 19 04:30:40.768: INFO: Waiting up to 5m0s for pod "downward-api-efe5ba36-407e-4a42-9512-05b79ac70184" in namespace "downward-api-5572" to be "Succeeded or Failed"
Jan 19 04:30:40.773: INFO: Pod "downward-api-efe5ba36-407e-4a42-9512-05b79ac70184": Phase="Pending", Reason="", readiness=false. Elapsed: 4.311091ms
Jan 19 04:30:42.777: INFO: Pod "downward-api-efe5ba36-407e-4a42-9512-05b79ac70184": Phase="Running", Reason="", readiness=true. Elapsed: 2.008794051s
Jan 19 04:30:44.777: INFO: Pod "downward-api-efe5ba36-407e-4a42-9512-05b79ac70184": Phase="Running", Reason="", readiness=false. Elapsed: 4.008567233s
Jan 19 04:30:46.778: INFO: Pod "downward-api-efe5ba36-407e-4a42-9512-05b79ac70184": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009643074s
STEP: Saw pod success 01/19/23 04:30:46.778
Jan 19 04:30:46.778: INFO: Pod "downward-api-efe5ba36-407e-4a42-9512-05b79ac70184" satisfied condition "Succeeded or Failed"
Jan 19 04:30:46.781: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod downward-api-efe5ba36-407e-4a42-9512-05b79ac70184 container dapi-container: <nil>
STEP: delete the pod 01/19/23 04:30:46.787
Jan 19 04:30:46.799: INFO: Waiting for pod downward-api-efe5ba36-407e-4a42-9512-05b79ac70184 to disappear
Jan 19 04:30:46.801: INFO: Pod downward-api-efe5ba36-407e-4a42-9512-05b79ac70184 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan 19 04:30:46.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5572" for this suite. 01/19/23 04:30:46.805
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","completed":55,"skipped":1011,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.071 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:30:40.74
    Jan 19 04:30:40.740: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename downward-api 01/19/23 04:30:40.741
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:30:40.755
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:30:40.757
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:165
    STEP: Creating a pod to test downward api env vars 01/19/23 04:30:40.759
    Jan 19 04:30:40.768: INFO: Waiting up to 5m0s for pod "downward-api-efe5ba36-407e-4a42-9512-05b79ac70184" in namespace "downward-api-5572" to be "Succeeded or Failed"
    Jan 19 04:30:40.773: INFO: Pod "downward-api-efe5ba36-407e-4a42-9512-05b79ac70184": Phase="Pending", Reason="", readiness=false. Elapsed: 4.311091ms
    Jan 19 04:30:42.777: INFO: Pod "downward-api-efe5ba36-407e-4a42-9512-05b79ac70184": Phase="Running", Reason="", readiness=true. Elapsed: 2.008794051s
    Jan 19 04:30:44.777: INFO: Pod "downward-api-efe5ba36-407e-4a42-9512-05b79ac70184": Phase="Running", Reason="", readiness=false. Elapsed: 4.008567233s
    Jan 19 04:30:46.778: INFO: Pod "downward-api-efe5ba36-407e-4a42-9512-05b79ac70184": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009643074s
    STEP: Saw pod success 01/19/23 04:30:46.778
    Jan 19 04:30:46.778: INFO: Pod "downward-api-efe5ba36-407e-4a42-9512-05b79ac70184" satisfied condition "Succeeded or Failed"
    Jan 19 04:30:46.781: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod downward-api-efe5ba36-407e-4a42-9512-05b79ac70184 container dapi-container: <nil>
    STEP: delete the pod 01/19/23 04:30:46.787
    Jan 19 04:30:46.799: INFO: Waiting for pod downward-api-efe5ba36-407e-4a42-9512-05b79ac70184 to disappear
    Jan 19 04:30:46.801: INFO: Pod downward-api-efe5ba36-407e-4a42-9512-05b79ac70184 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan 19 04:30:46.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5572" for this suite. 01/19/23 04:30:46.805
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:30:46.812
Jan 19 04:30:46.812: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename daemonsets 01/19/23 04:30:46.813
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:30:46.828
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:30:46.831
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
Jan 19 04:30:46.848: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 01/19/23 04:30:46.854
Jan 19 04:30:46.859: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 04:30:46.859: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 01/19/23 04:30:46.859
Jan 19 04:30:46.878: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 04:30:46.878: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
Jan 19 04:30:47.882: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 04:30:47.882: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
Jan 19 04:30:48.883: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 04:30:48.883: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
Jan 19 04:30:49.883: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 19 04:30:49.883: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 01/19/23 04:30:49.885
Jan 19 04:30:49.915: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 04:30:49.915: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 01/19/23 04:30:49.915
Jan 19 04:30:49.933: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 04:30:49.933: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
Jan 19 04:30:50.939: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 04:30:50.939: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
Jan 19 04:30:51.937: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 04:30:51.937: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
Jan 19 04:30:52.938: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 04:30:52.938: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
Jan 19 04:30:53.937: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 04:30:53.937: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
Jan 19 04:30:54.938: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 04:30:54.938: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
Jan 19 04:30:55.938: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 19 04:30:55.938: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/19/23 04:30:55.945
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7869, will wait for the garbage collector to delete the pods 01/19/23 04:30:55.945
Jan 19 04:30:56.004: INFO: Deleting DaemonSet.extensions daemon-set took: 5.972935ms
Jan 19 04:30:56.105: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.73367ms
Jan 19 04:30:58.708: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 04:30:58.708: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 19 04:30:58.712: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"11466"},"items":null}

Jan 19 04:30:58.714: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"11466"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 19 04:30:58.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7869" for this suite. 01/19/23 04:30:58.734
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","completed":56,"skipped":1050,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.928 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:30:46.812
    Jan 19 04:30:46.812: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename daemonsets 01/19/23 04:30:46.813
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:30:46.828
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:30:46.831
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:193
    Jan 19 04:30:46.848: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 01/19/23 04:30:46.854
    Jan 19 04:30:46.859: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 04:30:46.859: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 01/19/23 04:30:46.859
    Jan 19 04:30:46.878: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 04:30:46.878: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
    Jan 19 04:30:47.882: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 04:30:47.882: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
    Jan 19 04:30:48.883: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 04:30:48.883: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
    Jan 19 04:30:49.883: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 19 04:30:49.883: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 01/19/23 04:30:49.885
    Jan 19 04:30:49.915: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 04:30:49.915: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 01/19/23 04:30:49.915
    Jan 19 04:30:49.933: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 04:30:49.933: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
    Jan 19 04:30:50.939: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 04:30:50.939: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
    Jan 19 04:30:51.937: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 04:30:51.937: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
    Jan 19 04:30:52.938: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 04:30:52.938: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
    Jan 19 04:30:53.937: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 04:30:53.937: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
    Jan 19 04:30:54.938: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 04:30:54.938: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
    Jan 19 04:30:55.938: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 19 04:30:55.938: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/19/23 04:30:55.945
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7869, will wait for the garbage collector to delete the pods 01/19/23 04:30:55.945
    Jan 19 04:30:56.004: INFO: Deleting DaemonSet.extensions daemon-set took: 5.972935ms
    Jan 19 04:30:56.105: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.73367ms
    Jan 19 04:30:58.708: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 04:30:58.708: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 19 04:30:58.712: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"11466"},"items":null}

    Jan 19 04:30:58.714: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"11466"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 19 04:30:58.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-7869" for this suite. 01/19/23 04:30:58.734
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:30:58.74
Jan 19 04:30:58.740: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename daemonsets 01/19/23 04:30:58.741
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:30:58.752
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:30:58.754
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
STEP: Creating a simple DaemonSet "daemon-set" 01/19/23 04:30:58.768
STEP: Check that daemon pods launch on every node of the cluster. 01/19/23 04:30:58.772
Jan 19 04:30:58.778: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 04:30:58.778: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
Jan 19 04:30:59.785: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 19 04:30:59.785: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
Jan 19 04:31:00.786: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 19 04:31:00.786: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
Jan 19 04:31:01.787: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 19 04:31:01.787: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 01/19/23 04:31:01.791
Jan 19 04:31:01.816: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 19 04:31:01.816: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
Jan 19 04:31:02.823: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 19 04:31:02.823: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
Jan 19 04:31:03.823: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 19 04:31:03.823: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 01/19/23 04:31:03.823
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/19/23 04:31:03.828
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2080, will wait for the garbage collector to delete the pods 01/19/23 04:31:03.828
Jan 19 04:31:03.886: INFO: Deleting DaemonSet.extensions daemon-set took: 5.662682ms
Jan 19 04:31:03.987: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.475977ms
Jan 19 04:31:06.690: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 04:31:06.690: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 19 04:31:06.693: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"11561"},"items":null}

Jan 19 04:31:06.695: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"11561"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 19 04:31:06.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2080" for this suite. 01/19/23 04:31:06.705
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","completed":57,"skipped":1052,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.969 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:30:58.74
    Jan 19 04:30:58.740: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename daemonsets 01/19/23 04:30:58.741
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:30:58.752
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:30:58.754
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:293
    STEP: Creating a simple DaemonSet "daemon-set" 01/19/23 04:30:58.768
    STEP: Check that daemon pods launch on every node of the cluster. 01/19/23 04:30:58.772
    Jan 19 04:30:58.778: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 04:30:58.778: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
    Jan 19 04:30:59.785: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 19 04:30:59.785: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
    Jan 19 04:31:00.786: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 19 04:31:00.786: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
    Jan 19 04:31:01.787: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 19 04:31:01.787: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 01/19/23 04:31:01.791
    Jan 19 04:31:01.816: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 19 04:31:01.816: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
    Jan 19 04:31:02.823: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 19 04:31:02.823: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
    Jan 19 04:31:03.823: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 19 04:31:03.823: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 01/19/23 04:31:03.823
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/19/23 04:31:03.828
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2080, will wait for the garbage collector to delete the pods 01/19/23 04:31:03.828
    Jan 19 04:31:03.886: INFO: Deleting DaemonSet.extensions daemon-set took: 5.662682ms
    Jan 19 04:31:03.987: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.475977ms
    Jan 19 04:31:06.690: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 04:31:06.690: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 19 04:31:06.693: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"11561"},"items":null}

    Jan 19 04:31:06.695: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"11561"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 19 04:31:06.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-2080" for this suite. 01/19/23 04:31:06.705
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:31:06.71
Jan 19 04:31:06.710: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename namespaces 01/19/23 04:31:06.711
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:31:06.726
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:31:06.728
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
STEP: creating a Namespace 01/19/23 04:31:06.73
STEP: patching the Namespace 01/19/23 04:31:06.742
STEP: get the Namespace and ensuring it has the label 01/19/23 04:31:06.748
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jan 19 04:31:06.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5864" for this suite. 01/19/23 04:31:06.754
STEP: Destroying namespace "nspatchtest-8bd0fbfb-8753-4643-992e-11017ca00e4d-2473" for this suite. 01/19/23 04:31:06.759
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","completed":58,"skipped":1054,"failed":0}
------------------------------
â€¢ [0.056 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:31:06.71
    Jan 19 04:31:06.710: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename namespaces 01/19/23 04:31:06.711
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:31:06.726
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:31:06.728
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:267
    STEP: creating a Namespace 01/19/23 04:31:06.73
    STEP: patching the Namespace 01/19/23 04:31:06.742
    STEP: get the Namespace and ensuring it has the label 01/19/23 04:31:06.748
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jan 19 04:31:06.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-5864" for this suite. 01/19/23 04:31:06.754
    STEP: Destroying namespace "nspatchtest-8bd0fbfb-8753-4643-992e-11017ca00e4d-2473" for this suite. 01/19/23 04:31:06.759
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:31:06.767
Jan 19 04:31:06.767: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename namespaces 01/19/23 04:31:06.767
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:31:06.781
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:31:06.783
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
STEP: Creating a test namespace 01/19/23 04:31:06.785
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:31:06.798
STEP: Creating a pod in the namespace 01/19/23 04:31:06.799
STEP: Waiting for the pod to have running status 01/19/23 04:31:06.808
Jan 19 04:31:06.808: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-6807" to be "running"
Jan 19 04:31:06.810: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.810528ms
Jan 19 04:31:08.816: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008324206s
Jan 19 04:31:10.815: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.007664528s
Jan 19 04:31:10.815: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 01/19/23 04:31:10.815
STEP: Waiting for the namespace to be removed. 01/19/23 04:31:10.821
STEP: Recreating the namespace 01/19/23 04:31:21.825
STEP: Verifying there are no pods in the namespace 01/19/23 04:31:21.84
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jan 19 04:31:21.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2548" for this suite. 01/19/23 04:31:21.847
STEP: Destroying namespace "nsdeletetest-6807" for this suite. 01/19/23 04:31:21.852
Jan 19 04:31:21.855: INFO: Namespace nsdeletetest-6807 was already deleted
STEP: Destroying namespace "nsdeletetest-6270" for this suite. 01/19/23 04:31:21.855
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","completed":59,"skipped":1068,"failed":0}
------------------------------
â€¢ [SLOW TEST] [15.094 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:31:06.767
    Jan 19 04:31:06.767: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename namespaces 01/19/23 04:31:06.767
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:31:06.781
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:31:06.783
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:242
    STEP: Creating a test namespace 01/19/23 04:31:06.785
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:31:06.798
    STEP: Creating a pod in the namespace 01/19/23 04:31:06.799
    STEP: Waiting for the pod to have running status 01/19/23 04:31:06.808
    Jan 19 04:31:06.808: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-6807" to be "running"
    Jan 19 04:31:06.810: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.810528ms
    Jan 19 04:31:08.816: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008324206s
    Jan 19 04:31:10.815: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.007664528s
    Jan 19 04:31:10.815: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 01/19/23 04:31:10.815
    STEP: Waiting for the namespace to be removed. 01/19/23 04:31:10.821
    STEP: Recreating the namespace 01/19/23 04:31:21.825
    STEP: Verifying there are no pods in the namespace 01/19/23 04:31:21.84
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jan 19 04:31:21.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-2548" for this suite. 01/19/23 04:31:21.847
    STEP: Destroying namespace "nsdeletetest-6807" for this suite. 01/19/23 04:31:21.852
    Jan 19 04:31:21.855: INFO: Namespace nsdeletetest-6807 was already deleted
    STEP: Destroying namespace "nsdeletetest-6270" for this suite. 01/19/23 04:31:21.855
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:31:21.861
Jan 19 04:31:21.861: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename endpointslice 01/19/23 04:31:21.862
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:31:21.876
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:31:21.878
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
STEP: getting /apis 01/19/23 04:31:21.88
STEP: getting /apis/discovery.k8s.io 01/19/23 04:31:21.882
STEP: getting /apis/discovery.k8s.iov1 01/19/23 04:31:21.883
STEP: creating 01/19/23 04:31:21.884
STEP: getting 01/19/23 04:31:21.898
STEP: listing 01/19/23 04:31:21.9
STEP: watching 01/19/23 04:31:21.903
Jan 19 04:31:21.903: INFO: starting watch
STEP: cluster-wide listing 01/19/23 04:31:21.904
STEP: cluster-wide watching 01/19/23 04:31:21.907
Jan 19 04:31:21.907: INFO: starting watch
STEP: patching 01/19/23 04:31:21.908
STEP: updating 01/19/23 04:31:21.913
Jan 19 04:31:21.920: INFO: waiting for watch events with expected annotations
Jan 19 04:31:21.920: INFO: saw patched and updated annotations
STEP: deleting 01/19/23 04:31:21.92
STEP: deleting a collection 01/19/23 04:31:21.93
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jan 19 04:31:21.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-6804" for this suite. 01/19/23 04:31:21.945
{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","completed":60,"skipped":1081,"failed":0}
------------------------------
â€¢ [0.090 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:31:21.861
    Jan 19 04:31:21.861: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename endpointslice 01/19/23 04:31:21.862
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:31:21.876
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:31:21.878
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:352
    STEP: getting /apis 01/19/23 04:31:21.88
    STEP: getting /apis/discovery.k8s.io 01/19/23 04:31:21.882
    STEP: getting /apis/discovery.k8s.iov1 01/19/23 04:31:21.883
    STEP: creating 01/19/23 04:31:21.884
    STEP: getting 01/19/23 04:31:21.898
    STEP: listing 01/19/23 04:31:21.9
    STEP: watching 01/19/23 04:31:21.903
    Jan 19 04:31:21.903: INFO: starting watch
    STEP: cluster-wide listing 01/19/23 04:31:21.904
    STEP: cluster-wide watching 01/19/23 04:31:21.907
    Jan 19 04:31:21.907: INFO: starting watch
    STEP: patching 01/19/23 04:31:21.908
    STEP: updating 01/19/23 04:31:21.913
    Jan 19 04:31:21.920: INFO: waiting for watch events with expected annotations
    Jan 19 04:31:21.920: INFO: saw patched and updated annotations
    STEP: deleting 01/19/23 04:31:21.92
    STEP: deleting a collection 01/19/23 04:31:21.93
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jan 19 04:31:21.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-6804" for this suite. 01/19/23 04:31:21.945
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:31:21.951
Jan 19 04:31:21.951: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename statefulset 01/19/23 04:31:21.952
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:31:21.965
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:31:21.968
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-172 01/19/23 04:31:21.97
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
STEP: Creating statefulset ss in namespace statefulset-172 01/19/23 04:31:21.98
Jan 19 04:31:21.988: INFO: Found 0 stateful pods, waiting for 1
Jan 19 04:31:31.994: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 01/19/23 04:31:32
STEP: Getting /status 01/19/23 04:31:32.007
Jan 19 04:31:32.032: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 01/19/23 04:31:32.032
Jan 19 04:31:32.042: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 01/19/23 04:31:32.042
Jan 19 04:31:32.043: INFO: Observed &StatefulSet event: ADDED
Jan 19 04:31:32.043: INFO: Found Statefulset ss in namespace statefulset-172 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 19 04:31:32.043: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 01/19/23 04:31:32.043
Jan 19 04:31:32.043: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 19 04:31:32.051: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 01/19/23 04:31:32.051
Jan 19 04:31:32.052: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 19 04:31:32.052: INFO: Deleting all statefulset in ns statefulset-172
Jan 19 04:31:32.055: INFO: Scaling statefulset ss to 0
Jan 19 04:31:42.070: INFO: Waiting for statefulset status.replicas updated to 0
Jan 19 04:31:42.072: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 19 04:31:42.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-172" for this suite. 01/19/23 04:31:42.092
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","completed":61,"skipped":1083,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.147 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:975

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:31:21.951
    Jan 19 04:31:21.951: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename statefulset 01/19/23 04:31:21.952
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:31:21.965
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:31:21.968
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-172 01/19/23 04:31:21.97
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:975
    STEP: Creating statefulset ss in namespace statefulset-172 01/19/23 04:31:21.98
    Jan 19 04:31:21.988: INFO: Found 0 stateful pods, waiting for 1
    Jan 19 04:31:31.994: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 01/19/23 04:31:32
    STEP: Getting /status 01/19/23 04:31:32.007
    Jan 19 04:31:32.032: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 01/19/23 04:31:32.032
    Jan 19 04:31:32.042: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 01/19/23 04:31:32.042
    Jan 19 04:31:32.043: INFO: Observed &StatefulSet event: ADDED
    Jan 19 04:31:32.043: INFO: Found Statefulset ss in namespace statefulset-172 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 19 04:31:32.043: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 01/19/23 04:31:32.043
    Jan 19 04:31:32.043: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 19 04:31:32.051: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 01/19/23 04:31:32.051
    Jan 19 04:31:32.052: INFO: Observed &StatefulSet event: ADDED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 19 04:31:32.052: INFO: Deleting all statefulset in ns statefulset-172
    Jan 19 04:31:32.055: INFO: Scaling statefulset ss to 0
    Jan 19 04:31:42.070: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 19 04:31:42.072: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 19 04:31:42.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-172" for this suite. 01/19/23 04:31:42.092
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:31:42.098
Jan 19 04:31:42.099: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename emptydir-wrapper 01/19/23 04:31:42.099
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:31:42.124
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:31:42.126
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Jan 19 04:31:42.144: INFO: Waiting up to 5m0s for pod "pod-secrets-d14cb6a9-a998-468e-b972-e9b88f9a22cb" in namespace "emptydir-wrapper-647" to be "running and ready"
Jan 19 04:31:42.147: INFO: Pod "pod-secrets-d14cb6a9-a998-468e-b972-e9b88f9a22cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.966744ms
Jan 19 04:31:42.147: INFO: The phase of Pod pod-secrets-d14cb6a9-a998-468e-b972-e9b88f9a22cb is Pending, waiting for it to be Running (with Ready = true)
Jan 19 04:31:44.153: INFO: Pod "pod-secrets-d14cb6a9-a998-468e-b972-e9b88f9a22cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008800983s
Jan 19 04:31:44.153: INFO: The phase of Pod pod-secrets-d14cb6a9-a998-468e-b972-e9b88f9a22cb is Pending, waiting for it to be Running (with Ready = true)
Jan 19 04:31:46.151: INFO: Pod "pod-secrets-d14cb6a9-a998-468e-b972-e9b88f9a22cb": Phase="Running", Reason="", readiness=true. Elapsed: 4.007697843s
Jan 19 04:31:46.151: INFO: The phase of Pod pod-secrets-d14cb6a9-a998-468e-b972-e9b88f9a22cb is Running (Ready = true)
Jan 19 04:31:46.151: INFO: Pod "pod-secrets-d14cb6a9-a998-468e-b972-e9b88f9a22cb" satisfied condition "running and ready"
STEP: Cleaning up the secret 01/19/23 04:31:46.154
STEP: Cleaning up the configmap 01/19/23 04:31:46.16
STEP: Cleaning up the pod 01/19/23 04:31:46.166
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Jan 19 04:31:46.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-647" for this suite. 01/19/23 04:31:46.182
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","completed":62,"skipped":1094,"failed":0}
------------------------------
â€¢ [4.090 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:31:42.098
    Jan 19 04:31:42.099: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename emptydir-wrapper 01/19/23 04:31:42.099
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:31:42.124
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:31:42.126
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Jan 19 04:31:42.144: INFO: Waiting up to 5m0s for pod "pod-secrets-d14cb6a9-a998-468e-b972-e9b88f9a22cb" in namespace "emptydir-wrapper-647" to be "running and ready"
    Jan 19 04:31:42.147: INFO: Pod "pod-secrets-d14cb6a9-a998-468e-b972-e9b88f9a22cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.966744ms
    Jan 19 04:31:42.147: INFO: The phase of Pod pod-secrets-d14cb6a9-a998-468e-b972-e9b88f9a22cb is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 04:31:44.153: INFO: Pod "pod-secrets-d14cb6a9-a998-468e-b972-e9b88f9a22cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008800983s
    Jan 19 04:31:44.153: INFO: The phase of Pod pod-secrets-d14cb6a9-a998-468e-b972-e9b88f9a22cb is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 04:31:46.151: INFO: Pod "pod-secrets-d14cb6a9-a998-468e-b972-e9b88f9a22cb": Phase="Running", Reason="", readiness=true. Elapsed: 4.007697843s
    Jan 19 04:31:46.151: INFO: The phase of Pod pod-secrets-d14cb6a9-a998-468e-b972-e9b88f9a22cb is Running (Ready = true)
    Jan 19 04:31:46.151: INFO: Pod "pod-secrets-d14cb6a9-a998-468e-b972-e9b88f9a22cb" satisfied condition "running and ready"
    STEP: Cleaning up the secret 01/19/23 04:31:46.154
    STEP: Cleaning up the configmap 01/19/23 04:31:46.16
    STEP: Cleaning up the pod 01/19/23 04:31:46.166
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Jan 19 04:31:46.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-647" for this suite. 01/19/23 04:31:46.182
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:31:46.189
Jan 19 04:31:46.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename job 01/19/23 04:31:46.19
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:31:46.203
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:31:46.204
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
STEP: Creating a job 01/19/23 04:31:46.206
STEP: Ensure pods equal to paralellism count is attached to the job 01/19/23 04:31:46.212
STEP: patching /status 01/19/23 04:31:50.216
STEP: updating /status 01/19/23 04:31:50.223
STEP: get /status 01/19/23 04:31:50.232
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 19 04:31:50.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6572" for this suite. 01/19/23 04:31:50.239
{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","completed":63,"skipped":1104,"failed":0}
------------------------------
â€¢ [4.056 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:31:46.189
    Jan 19 04:31:46.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename job 01/19/23 04:31:46.19
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:31:46.203
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:31:46.204
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:464
    STEP: Creating a job 01/19/23 04:31:46.206
    STEP: Ensure pods equal to paralellism count is attached to the job 01/19/23 04:31:46.212
    STEP: patching /status 01/19/23 04:31:50.216
    STEP: updating /status 01/19/23 04:31:50.223
    STEP: get /status 01/19/23 04:31:50.232
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 19 04:31:50.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-6572" for this suite. 01/19/23 04:31:50.239
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:31:50.246
Jan 19 04:31:50.246: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename emptydir 01/19/23 04:31:50.246
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:31:50.274
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:31:50.277
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
STEP: Creating a pod to test emptydir 0666 on tmpfs 01/19/23 04:31:50.279
Jan 19 04:31:50.287: INFO: Waiting up to 5m0s for pod "pod-dd77efca-731b-44f2-a3ec-7a52d9225720" in namespace "emptydir-9143" to be "Succeeded or Failed"
Jan 19 04:31:50.291: INFO: Pod "pod-dd77efca-731b-44f2-a3ec-7a52d9225720": Phase="Pending", Reason="", readiness=false. Elapsed: 4.489978ms
Jan 19 04:31:52.296: INFO: Pod "pod-dd77efca-731b-44f2-a3ec-7a52d9225720": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009696897s
Jan 19 04:31:54.296: INFO: Pod "pod-dd77efca-731b-44f2-a3ec-7a52d9225720": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009050759s
Jan 19 04:31:56.296: INFO: Pod "pod-dd77efca-731b-44f2-a3ec-7a52d9225720": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009671549s
STEP: Saw pod success 01/19/23 04:31:56.296
Jan 19 04:31:56.297: INFO: Pod "pod-dd77efca-731b-44f2-a3ec-7a52d9225720" satisfied condition "Succeeded or Failed"
Jan 19 04:31:56.299: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-dd77efca-731b-44f2-a3ec-7a52d9225720 container test-container: <nil>
STEP: delete the pod 01/19/23 04:31:56.307
Jan 19 04:31:56.316: INFO: Waiting for pod pod-dd77efca-731b-44f2-a3ec-7a52d9225720 to disappear
Jan 19 04:31:56.318: INFO: Pod pod-dd77efca-731b-44f2-a3ec-7a52d9225720 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 19 04:31:56.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9143" for this suite. 01/19/23 04:31:56.321
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":64,"skipped":1122,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.081 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:31:50.246
    Jan 19 04:31:50.246: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename emptydir 01/19/23 04:31:50.246
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:31:50.274
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:31:50.277
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:106
    STEP: Creating a pod to test emptydir 0666 on tmpfs 01/19/23 04:31:50.279
    Jan 19 04:31:50.287: INFO: Waiting up to 5m0s for pod "pod-dd77efca-731b-44f2-a3ec-7a52d9225720" in namespace "emptydir-9143" to be "Succeeded or Failed"
    Jan 19 04:31:50.291: INFO: Pod "pod-dd77efca-731b-44f2-a3ec-7a52d9225720": Phase="Pending", Reason="", readiness=false. Elapsed: 4.489978ms
    Jan 19 04:31:52.296: INFO: Pod "pod-dd77efca-731b-44f2-a3ec-7a52d9225720": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009696897s
    Jan 19 04:31:54.296: INFO: Pod "pod-dd77efca-731b-44f2-a3ec-7a52d9225720": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009050759s
    Jan 19 04:31:56.296: INFO: Pod "pod-dd77efca-731b-44f2-a3ec-7a52d9225720": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009671549s
    STEP: Saw pod success 01/19/23 04:31:56.296
    Jan 19 04:31:56.297: INFO: Pod "pod-dd77efca-731b-44f2-a3ec-7a52d9225720" satisfied condition "Succeeded or Failed"
    Jan 19 04:31:56.299: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-dd77efca-731b-44f2-a3ec-7a52d9225720 container test-container: <nil>
    STEP: delete the pod 01/19/23 04:31:56.307
    Jan 19 04:31:56.316: INFO: Waiting for pod pod-dd77efca-731b-44f2-a3ec-7a52d9225720 to disappear
    Jan 19 04:31:56.318: INFO: Pod pod-dd77efca-731b-44f2-a3ec-7a52d9225720 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 19 04:31:56.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9143" for this suite. 01/19/23 04:31:56.321
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:31:56.327
Jan 19 04:31:56.327: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename projected 01/19/23 04:31:56.328
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:31:56.341
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:31:56.342
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
STEP: Creating configMap with name projected-configmap-test-volume-map-ed8921bc-53a9-41d5-952e-556d9c6e941e 01/19/23 04:31:56.344
STEP: Creating a pod to test consume configMaps 01/19/23 04:31:56.35
Jan 19 04:31:56.358: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f0d84ccc-3b02-45da-8df0-bf0613da62f5" in namespace "projected-7813" to be "Succeeded or Failed"
Jan 19 04:31:56.361: INFO: Pod "pod-projected-configmaps-f0d84ccc-3b02-45da-8df0-bf0613da62f5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.158501ms
Jan 19 04:31:58.366: INFO: Pod "pod-projected-configmaps-f0d84ccc-3b02-45da-8df0-bf0613da62f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008504832s
Jan 19 04:32:00.365: INFO: Pod "pod-projected-configmaps-f0d84ccc-3b02-45da-8df0-bf0613da62f5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006923087s
Jan 19 04:32:02.367: INFO: Pod "pod-projected-configmaps-f0d84ccc-3b02-45da-8df0-bf0613da62f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009718975s
STEP: Saw pod success 01/19/23 04:32:02.367
Jan 19 04:32:02.367: INFO: Pod "pod-projected-configmaps-f0d84ccc-3b02-45da-8df0-bf0613da62f5" satisfied condition "Succeeded or Failed"
Jan 19 04:32:02.371: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-projected-configmaps-f0d84ccc-3b02-45da-8df0-bf0613da62f5 container agnhost-container: <nil>
STEP: delete the pod 01/19/23 04:32:02.378
Jan 19 04:32:02.394: INFO: Waiting for pod pod-projected-configmaps-f0d84ccc-3b02-45da-8df0-bf0613da62f5 to disappear
Jan 19 04:32:02.397: INFO: Pod pod-projected-configmaps-f0d84ccc-3b02-45da-8df0-bf0613da62f5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 19 04:32:02.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7813" for this suite. 01/19/23 04:32:02.4
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":65,"skipped":1127,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.080 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:31:56.327
    Jan 19 04:31:56.327: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename projected 01/19/23 04:31:56.328
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:31:56.341
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:31:56.342
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:88
    STEP: Creating configMap with name projected-configmap-test-volume-map-ed8921bc-53a9-41d5-952e-556d9c6e941e 01/19/23 04:31:56.344
    STEP: Creating a pod to test consume configMaps 01/19/23 04:31:56.35
    Jan 19 04:31:56.358: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f0d84ccc-3b02-45da-8df0-bf0613da62f5" in namespace "projected-7813" to be "Succeeded or Failed"
    Jan 19 04:31:56.361: INFO: Pod "pod-projected-configmaps-f0d84ccc-3b02-45da-8df0-bf0613da62f5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.158501ms
    Jan 19 04:31:58.366: INFO: Pod "pod-projected-configmaps-f0d84ccc-3b02-45da-8df0-bf0613da62f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008504832s
    Jan 19 04:32:00.365: INFO: Pod "pod-projected-configmaps-f0d84ccc-3b02-45da-8df0-bf0613da62f5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006923087s
    Jan 19 04:32:02.367: INFO: Pod "pod-projected-configmaps-f0d84ccc-3b02-45da-8df0-bf0613da62f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009718975s
    STEP: Saw pod success 01/19/23 04:32:02.367
    Jan 19 04:32:02.367: INFO: Pod "pod-projected-configmaps-f0d84ccc-3b02-45da-8df0-bf0613da62f5" satisfied condition "Succeeded or Failed"
    Jan 19 04:32:02.371: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-projected-configmaps-f0d84ccc-3b02-45da-8df0-bf0613da62f5 container agnhost-container: <nil>
    STEP: delete the pod 01/19/23 04:32:02.378
    Jan 19 04:32:02.394: INFO: Waiting for pod pod-projected-configmaps-f0d84ccc-3b02-45da-8df0-bf0613da62f5 to disappear
    Jan 19 04:32:02.397: INFO: Pod pod-projected-configmaps-f0d84ccc-3b02-45da-8df0-bf0613da62f5 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 19 04:32:02.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7813" for this suite. 01/19/23 04:32:02.4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:32:02.408
Jan 19 04:32:02.408: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename kubectl 01/19/23 04:32:02.408
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:32:02.476
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:32:02.478
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
STEP: Starting the proxy 01/19/23 04:32:02.479
Jan 19 04:32:02.480: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-2557 proxy --unix-socket=/tmp/kubectl-proxy-unix127750531/test'
STEP: retrieving proxy /api/ output 01/19/23 04:32:02.532
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 19 04:32:02.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2557" for this suite. 01/19/23 04:32:02.536
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","completed":66,"skipped":1147,"failed":0}
------------------------------
â€¢ [0.135 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:32:02.408
    Jan 19 04:32:02.408: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename kubectl 01/19/23 04:32:02.408
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:32:02.476
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:32:02.478
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1810
    STEP: Starting the proxy 01/19/23 04:32:02.479
    Jan 19 04:32:02.480: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-2557 proxy --unix-socket=/tmp/kubectl-proxy-unix127750531/test'
    STEP: retrieving proxy /api/ output 01/19/23 04:32:02.532
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 19 04:32:02.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2557" for this suite. 01/19/23 04:32:02.536
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:32:02.544
Jan 19 04:32:02.544: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename webhook 01/19/23 04:32:02.544
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:32:02.561
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:32:02.563
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/19/23 04:32:02.579
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 04:32:03.229
STEP: Deploying the webhook pod 01/19/23 04:32:03.239
STEP: Wait for the deployment to be ready 01/19/23 04:32:03.251
Jan 19 04:32:03.275: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 19 04:32:05.286: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 4, 32, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 32, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 4, 32, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 32, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/19/23 04:32:07.291
STEP: Verifying the service has paired with the endpoint 01/19/23 04:32:07.304
Jan 19 04:32:08.304: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
Jan 19 04:32:08.308: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Registering the custom resource webhook via the AdmissionRegistration API 01/19/23 04:32:08.82
STEP: Creating a custom resource that should be denied by the webhook 01/19/23 04:32:08.835
STEP: Creating a custom resource whose deletion would be denied by the webhook 01/19/23 04:32:10.885
STEP: Updating the custom resource with disallowed data should be denied 01/19/23 04:32:10.894
STEP: Deleting the custom resource should be denied 01/19/23 04:32:10.902
STEP: Remove the offending key and value from the custom resource data 01/19/23 04:32:10.908
STEP: Deleting the updated custom resource should be successful 01/19/23 04:32:10.916
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 04:32:11.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7476" for this suite. 01/19/23 04:32:11.446
STEP: Destroying namespace "webhook-7476-markers" for this suite. 01/19/23 04:32:11.452
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","completed":67,"skipped":1168,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.975 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:32:02.544
    Jan 19 04:32:02.544: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename webhook 01/19/23 04:32:02.544
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:32:02.561
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:32:02.563
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/19/23 04:32:02.579
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 04:32:03.229
    STEP: Deploying the webhook pod 01/19/23 04:32:03.239
    STEP: Wait for the deployment to be ready 01/19/23 04:32:03.251
    Jan 19 04:32:03.275: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 19 04:32:05.286: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 4, 32, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 32, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 4, 32, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 32, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/19/23 04:32:07.291
    STEP: Verifying the service has paired with the endpoint 01/19/23 04:32:07.304
    Jan 19 04:32:08.304: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:220
    Jan 19 04:32:08.308: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 01/19/23 04:32:08.82
    STEP: Creating a custom resource that should be denied by the webhook 01/19/23 04:32:08.835
    STEP: Creating a custom resource whose deletion would be denied by the webhook 01/19/23 04:32:10.885
    STEP: Updating the custom resource with disallowed data should be denied 01/19/23 04:32:10.894
    STEP: Deleting the custom resource should be denied 01/19/23 04:32:10.902
    STEP: Remove the offending key and value from the custom resource data 01/19/23 04:32:10.908
    STEP: Deleting the updated custom resource should be successful 01/19/23 04:32:10.916
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 04:32:11.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7476" for this suite. 01/19/23 04:32:11.446
    STEP: Destroying namespace "webhook-7476-markers" for this suite. 01/19/23 04:32:11.452
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:32:11.521
Jan 19 04:32:11.521: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename dns 01/19/23 04:32:11.521
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:32:11.538
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:32:11.541
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 01/19/23 04:32:11.545
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3285.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3285.svc.cluster.local; sleep 1; done
 01/19/23 04:32:11.55
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3285.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3285.svc.cluster.local; sleep 1; done
 01/19/23 04:32:11.55
STEP: creating a pod to probe DNS 01/19/23 04:32:11.55
STEP: submitting the pod to kubernetes 01/19/23 04:32:11.55
Jan 19 04:32:11.560: INFO: Waiting up to 15m0s for pod "dns-test-3b01fd97-7d28-4a86-99d7-d5f921a2d8d8" in namespace "dns-3285" to be "running"
Jan 19 04:32:11.566: INFO: Pod "dns-test-3b01fd97-7d28-4a86-99d7-d5f921a2d8d8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.321535ms
Jan 19 04:32:13.570: INFO: Pod "dns-test-3b01fd97-7d28-4a86-99d7-d5f921a2d8d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009920757s
Jan 19 04:32:15.570: INFO: Pod "dns-test-3b01fd97-7d28-4a86-99d7-d5f921a2d8d8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009380259s
Jan 19 04:32:17.571: INFO: Pod "dns-test-3b01fd97-7d28-4a86-99d7-d5f921a2d8d8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010426319s
Jan 19 04:32:19.570: INFO: Pod "dns-test-3b01fd97-7d28-4a86-99d7-d5f921a2d8d8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.00946s
Jan 19 04:32:21.573: INFO: Pod "dns-test-3b01fd97-7d28-4a86-99d7-d5f921a2d8d8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.012129152s
Jan 19 04:32:23.571: INFO: Pod "dns-test-3b01fd97-7d28-4a86-99d7-d5f921a2d8d8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.01032107s
Jan 19 04:32:25.569: INFO: Pod "dns-test-3b01fd97-7d28-4a86-99d7-d5f921a2d8d8": Phase="Running", Reason="", readiness=true. Elapsed: 14.009003968s
Jan 19 04:32:25.569: INFO: Pod "dns-test-3b01fd97-7d28-4a86-99d7-d5f921a2d8d8" satisfied condition "running"
STEP: retrieving the pod 01/19/23 04:32:25.569
STEP: looking for the results for each expected name from probers 01/19/23 04:32:25.572
Jan 19 04:32:25.579: INFO: DNS probes using dns-test-3b01fd97-7d28-4a86-99d7-d5f921a2d8d8 succeeded

STEP: deleting the pod 01/19/23 04:32:25.579
STEP: changing the externalName to bar.example.com 01/19/23 04:32:25.588
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3285.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3285.svc.cluster.local; sleep 1; done
 01/19/23 04:32:25.597
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3285.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3285.svc.cluster.local; sleep 1; done
 01/19/23 04:32:25.597
STEP: creating a second pod to probe DNS 01/19/23 04:32:25.597
STEP: submitting the pod to kubernetes 01/19/23 04:32:25.597
Jan 19 04:32:25.604: INFO: Waiting up to 15m0s for pod "dns-test-3dda8e30-061c-4eb3-be21-27f02a7eded9" in namespace "dns-3285" to be "running"
Jan 19 04:32:25.613: INFO: Pod "dns-test-3dda8e30-061c-4eb3-be21-27f02a7eded9": Phase="Pending", Reason="", readiness=false. Elapsed: 9.178156ms
Jan 19 04:32:27.617: INFO: Pod "dns-test-3dda8e30-061c-4eb3-be21-27f02a7eded9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013901983s
Jan 19 04:32:29.618: INFO: Pod "dns-test-3dda8e30-061c-4eb3-be21-27f02a7eded9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014538348s
Jan 19 04:32:31.616: INFO: Pod "dns-test-3dda8e30-061c-4eb3-be21-27f02a7eded9": Phase="Running", Reason="", readiness=true. Elapsed: 6.012345293s
Jan 19 04:32:31.616: INFO: Pod "dns-test-3dda8e30-061c-4eb3-be21-27f02a7eded9" satisfied condition "running"
STEP: retrieving the pod 01/19/23 04:32:31.616
STEP: looking for the results for each expected name from probers 01/19/23 04:32:31.618
Jan 19 04:32:31.624: INFO: DNS probes using dns-test-3dda8e30-061c-4eb3-be21-27f02a7eded9 succeeded

STEP: deleting the pod 01/19/23 04:32:31.624
STEP: changing the service to type=ClusterIP 01/19/23 04:32:31.637
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3285.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3285.svc.cluster.local; sleep 1; done
 01/19/23 04:32:31.651
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3285.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3285.svc.cluster.local; sleep 1; done
 01/19/23 04:32:31.651
STEP: creating a third pod to probe DNS 01/19/23 04:32:31.651
STEP: submitting the pod to kubernetes 01/19/23 04:32:31.655
Jan 19 04:32:31.664: INFO: Waiting up to 15m0s for pod "dns-test-a6ee153c-669f-41af-b311-05f22f4d256b" in namespace "dns-3285" to be "running"
Jan 19 04:32:31.671: INFO: Pod "dns-test-a6ee153c-669f-41af-b311-05f22f4d256b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.482858ms
Jan 19 04:32:33.675: INFO: Pod "dns-test-a6ee153c-669f-41af-b311-05f22f4d256b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010499693s
Jan 19 04:32:35.676: INFO: Pod "dns-test-a6ee153c-669f-41af-b311-05f22f4d256b": Phase="Running", Reason="", readiness=true. Elapsed: 4.011126118s
Jan 19 04:32:35.676: INFO: Pod "dns-test-a6ee153c-669f-41af-b311-05f22f4d256b" satisfied condition "running"
STEP: retrieving the pod 01/19/23 04:32:35.676
STEP: looking for the results for each expected name from probers 01/19/23 04:32:35.678
Jan 19 04:32:35.684: INFO: DNS probes using dns-test-a6ee153c-669f-41af-b311-05f22f4d256b succeeded

STEP: deleting the pod 01/19/23 04:32:35.684
STEP: deleting the test externalName service 01/19/23 04:32:35.696
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 19 04:32:35.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3285" for this suite. 01/19/23 04:32:35.722
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","completed":68,"skipped":1230,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.208 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:32:11.521
    Jan 19 04:32:11.521: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename dns 01/19/23 04:32:11.521
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:32:11.538
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:32:11.541
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 01/19/23 04:32:11.545
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3285.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3285.svc.cluster.local; sleep 1; done
     01/19/23 04:32:11.55
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3285.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3285.svc.cluster.local; sleep 1; done
     01/19/23 04:32:11.55
    STEP: creating a pod to probe DNS 01/19/23 04:32:11.55
    STEP: submitting the pod to kubernetes 01/19/23 04:32:11.55
    Jan 19 04:32:11.560: INFO: Waiting up to 15m0s for pod "dns-test-3b01fd97-7d28-4a86-99d7-d5f921a2d8d8" in namespace "dns-3285" to be "running"
    Jan 19 04:32:11.566: INFO: Pod "dns-test-3b01fd97-7d28-4a86-99d7-d5f921a2d8d8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.321535ms
    Jan 19 04:32:13.570: INFO: Pod "dns-test-3b01fd97-7d28-4a86-99d7-d5f921a2d8d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009920757s
    Jan 19 04:32:15.570: INFO: Pod "dns-test-3b01fd97-7d28-4a86-99d7-d5f921a2d8d8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009380259s
    Jan 19 04:32:17.571: INFO: Pod "dns-test-3b01fd97-7d28-4a86-99d7-d5f921a2d8d8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010426319s
    Jan 19 04:32:19.570: INFO: Pod "dns-test-3b01fd97-7d28-4a86-99d7-d5f921a2d8d8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.00946s
    Jan 19 04:32:21.573: INFO: Pod "dns-test-3b01fd97-7d28-4a86-99d7-d5f921a2d8d8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.012129152s
    Jan 19 04:32:23.571: INFO: Pod "dns-test-3b01fd97-7d28-4a86-99d7-d5f921a2d8d8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.01032107s
    Jan 19 04:32:25.569: INFO: Pod "dns-test-3b01fd97-7d28-4a86-99d7-d5f921a2d8d8": Phase="Running", Reason="", readiness=true. Elapsed: 14.009003968s
    Jan 19 04:32:25.569: INFO: Pod "dns-test-3b01fd97-7d28-4a86-99d7-d5f921a2d8d8" satisfied condition "running"
    STEP: retrieving the pod 01/19/23 04:32:25.569
    STEP: looking for the results for each expected name from probers 01/19/23 04:32:25.572
    Jan 19 04:32:25.579: INFO: DNS probes using dns-test-3b01fd97-7d28-4a86-99d7-d5f921a2d8d8 succeeded

    STEP: deleting the pod 01/19/23 04:32:25.579
    STEP: changing the externalName to bar.example.com 01/19/23 04:32:25.588
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3285.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3285.svc.cluster.local; sleep 1; done
     01/19/23 04:32:25.597
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3285.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3285.svc.cluster.local; sleep 1; done
     01/19/23 04:32:25.597
    STEP: creating a second pod to probe DNS 01/19/23 04:32:25.597
    STEP: submitting the pod to kubernetes 01/19/23 04:32:25.597
    Jan 19 04:32:25.604: INFO: Waiting up to 15m0s for pod "dns-test-3dda8e30-061c-4eb3-be21-27f02a7eded9" in namespace "dns-3285" to be "running"
    Jan 19 04:32:25.613: INFO: Pod "dns-test-3dda8e30-061c-4eb3-be21-27f02a7eded9": Phase="Pending", Reason="", readiness=false. Elapsed: 9.178156ms
    Jan 19 04:32:27.617: INFO: Pod "dns-test-3dda8e30-061c-4eb3-be21-27f02a7eded9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013901983s
    Jan 19 04:32:29.618: INFO: Pod "dns-test-3dda8e30-061c-4eb3-be21-27f02a7eded9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014538348s
    Jan 19 04:32:31.616: INFO: Pod "dns-test-3dda8e30-061c-4eb3-be21-27f02a7eded9": Phase="Running", Reason="", readiness=true. Elapsed: 6.012345293s
    Jan 19 04:32:31.616: INFO: Pod "dns-test-3dda8e30-061c-4eb3-be21-27f02a7eded9" satisfied condition "running"
    STEP: retrieving the pod 01/19/23 04:32:31.616
    STEP: looking for the results for each expected name from probers 01/19/23 04:32:31.618
    Jan 19 04:32:31.624: INFO: DNS probes using dns-test-3dda8e30-061c-4eb3-be21-27f02a7eded9 succeeded

    STEP: deleting the pod 01/19/23 04:32:31.624
    STEP: changing the service to type=ClusterIP 01/19/23 04:32:31.637
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3285.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3285.svc.cluster.local; sleep 1; done
     01/19/23 04:32:31.651
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3285.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3285.svc.cluster.local; sleep 1; done
     01/19/23 04:32:31.651
    STEP: creating a third pod to probe DNS 01/19/23 04:32:31.651
    STEP: submitting the pod to kubernetes 01/19/23 04:32:31.655
    Jan 19 04:32:31.664: INFO: Waiting up to 15m0s for pod "dns-test-a6ee153c-669f-41af-b311-05f22f4d256b" in namespace "dns-3285" to be "running"
    Jan 19 04:32:31.671: INFO: Pod "dns-test-a6ee153c-669f-41af-b311-05f22f4d256b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.482858ms
    Jan 19 04:32:33.675: INFO: Pod "dns-test-a6ee153c-669f-41af-b311-05f22f4d256b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010499693s
    Jan 19 04:32:35.676: INFO: Pod "dns-test-a6ee153c-669f-41af-b311-05f22f4d256b": Phase="Running", Reason="", readiness=true. Elapsed: 4.011126118s
    Jan 19 04:32:35.676: INFO: Pod "dns-test-a6ee153c-669f-41af-b311-05f22f4d256b" satisfied condition "running"
    STEP: retrieving the pod 01/19/23 04:32:35.676
    STEP: looking for the results for each expected name from probers 01/19/23 04:32:35.678
    Jan 19 04:32:35.684: INFO: DNS probes using dns-test-a6ee153c-669f-41af-b311-05f22f4d256b succeeded

    STEP: deleting the pod 01/19/23 04:32:35.684
    STEP: deleting the test externalName service 01/19/23 04:32:35.696
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 19 04:32:35.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-3285" for this suite. 01/19/23 04:32:35.722
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:32:35.729
Jan 19 04:32:35.730: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename secrets 01/19/23 04:32:35.73
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:32:35.748
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:32:35.75
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
STEP: Creating secret with name secret-test-554d5236-f827-494c-b657-ab6211f6a919 01/19/23 04:32:35.825
STEP: Creating a pod to test consume secrets 01/19/23 04:32:35.828
Jan 19 04:32:35.837: INFO: Waiting up to 5m0s for pod "pod-secrets-f4b81618-f21c-47d0-a70c-018884d29fdb" in namespace "secrets-2529" to be "Succeeded or Failed"
Jan 19 04:32:35.839: INFO: Pod "pod-secrets-f4b81618-f21c-47d0-a70c-018884d29fdb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.364839ms
Jan 19 04:32:37.844: INFO: Pod "pod-secrets-f4b81618-f21c-47d0-a70c-018884d29fdb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00718673s
Jan 19 04:32:39.843: INFO: Pod "pod-secrets-f4b81618-f21c-47d0-a70c-018884d29fdb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006470593s
Jan 19 04:32:41.842: INFO: Pod "pod-secrets-f4b81618-f21c-47d0-a70c-018884d29fdb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005788342s
STEP: Saw pod success 01/19/23 04:32:41.842
Jan 19 04:32:41.843: INFO: Pod "pod-secrets-f4b81618-f21c-47d0-a70c-018884d29fdb" satisfied condition "Succeeded or Failed"
Jan 19 04:32:41.846: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-secrets-f4b81618-f21c-47d0-a70c-018884d29fdb container secret-volume-test: <nil>
STEP: delete the pod 01/19/23 04:32:41.852
Jan 19 04:32:41.864: INFO: Waiting for pod pod-secrets-f4b81618-f21c-47d0-a70c-018884d29fdb to disappear
Jan 19 04:32:41.866: INFO: Pod pod-secrets-f4b81618-f21c-47d0-a70c-018884d29fdb no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 19 04:32:41.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2529" for this suite. 01/19/23 04:32:41.869
STEP: Destroying namespace "secret-namespace-6689" for this suite. 01/19/23 04:32:41.875
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","completed":69,"skipped":1252,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.151 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:32:35.729
    Jan 19 04:32:35.730: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename secrets 01/19/23 04:32:35.73
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:32:35.748
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:32:35.75
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:98
    STEP: Creating secret with name secret-test-554d5236-f827-494c-b657-ab6211f6a919 01/19/23 04:32:35.825
    STEP: Creating a pod to test consume secrets 01/19/23 04:32:35.828
    Jan 19 04:32:35.837: INFO: Waiting up to 5m0s for pod "pod-secrets-f4b81618-f21c-47d0-a70c-018884d29fdb" in namespace "secrets-2529" to be "Succeeded or Failed"
    Jan 19 04:32:35.839: INFO: Pod "pod-secrets-f4b81618-f21c-47d0-a70c-018884d29fdb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.364839ms
    Jan 19 04:32:37.844: INFO: Pod "pod-secrets-f4b81618-f21c-47d0-a70c-018884d29fdb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00718673s
    Jan 19 04:32:39.843: INFO: Pod "pod-secrets-f4b81618-f21c-47d0-a70c-018884d29fdb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006470593s
    Jan 19 04:32:41.842: INFO: Pod "pod-secrets-f4b81618-f21c-47d0-a70c-018884d29fdb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005788342s
    STEP: Saw pod success 01/19/23 04:32:41.842
    Jan 19 04:32:41.843: INFO: Pod "pod-secrets-f4b81618-f21c-47d0-a70c-018884d29fdb" satisfied condition "Succeeded or Failed"
    Jan 19 04:32:41.846: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-secrets-f4b81618-f21c-47d0-a70c-018884d29fdb container secret-volume-test: <nil>
    STEP: delete the pod 01/19/23 04:32:41.852
    Jan 19 04:32:41.864: INFO: Waiting for pod pod-secrets-f4b81618-f21c-47d0-a70c-018884d29fdb to disappear
    Jan 19 04:32:41.866: INFO: Pod pod-secrets-f4b81618-f21c-47d0-a70c-018884d29fdb no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 19 04:32:41.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-2529" for this suite. 01/19/23 04:32:41.869
    STEP: Destroying namespace "secret-namespace-6689" for this suite. 01/19/23 04:32:41.875
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:32:41.881
Jan 19 04:32:41.882: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename proxy 01/19/23 04:32:41.882
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:32:41.896
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:32:41.898
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Jan 19 04:32:41.900: INFO: Creating pod...
Jan 19 04:32:41.908: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6158" to be "running"
Jan 19 04:32:41.910: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.716789ms
Jan 19 04:32:43.914: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006403996s
Jan 19 04:32:45.914: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.006585686s
Jan 19 04:32:45.914: INFO: Pod "agnhost" satisfied condition "running"
Jan 19 04:32:45.914: INFO: Creating service...
Jan 19 04:32:45.924: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6158/pods/agnhost/proxy/some/path/with/DELETE
Jan 19 04:32:45.935: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 19 04:32:45.935: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6158/pods/agnhost/proxy/some/path/with/GET
Jan 19 04:32:45.938: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 19 04:32:45.938: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6158/pods/agnhost/proxy/some/path/with/HEAD
Jan 19 04:32:45.940: INFO: http.Client request:HEAD | StatusCode:200
Jan 19 04:32:45.940: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6158/pods/agnhost/proxy/some/path/with/OPTIONS
Jan 19 04:32:45.943: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 19 04:32:45.943: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6158/pods/agnhost/proxy/some/path/with/PATCH
Jan 19 04:32:45.945: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 19 04:32:45.945: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6158/pods/agnhost/proxy/some/path/with/POST
Jan 19 04:32:45.949: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 19 04:32:45.949: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6158/pods/agnhost/proxy/some/path/with/PUT
Jan 19 04:32:45.952: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 19 04:32:45.952: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6158/services/test-service/proxy/some/path/with/DELETE
Jan 19 04:32:45.958: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 19 04:32:45.958: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6158/services/test-service/proxy/some/path/with/GET
Jan 19 04:32:45.962: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 19 04:32:45.962: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6158/services/test-service/proxy/some/path/with/HEAD
Jan 19 04:32:45.968: INFO: http.Client request:HEAD | StatusCode:200
Jan 19 04:32:45.968: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6158/services/test-service/proxy/some/path/with/OPTIONS
Jan 19 04:32:45.972: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 19 04:32:45.972: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6158/services/test-service/proxy/some/path/with/PATCH
Jan 19 04:32:45.976: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 19 04:32:45.976: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6158/services/test-service/proxy/some/path/with/POST
Jan 19 04:32:45.980: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 19 04:32:45.980: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6158/services/test-service/proxy/some/path/with/PUT
Jan 19 04:32:45.985: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Jan 19 04:32:45.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6158" for this suite. 01/19/23 04:32:45.988
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","completed":70,"skipped":1269,"failed":0}
------------------------------
â€¢ [4.112 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:32:41.881
    Jan 19 04:32:41.882: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename proxy 01/19/23 04:32:41.882
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:32:41.896
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:32:41.898
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Jan 19 04:32:41.900: INFO: Creating pod...
    Jan 19 04:32:41.908: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6158" to be "running"
    Jan 19 04:32:41.910: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.716789ms
    Jan 19 04:32:43.914: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006403996s
    Jan 19 04:32:45.914: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.006585686s
    Jan 19 04:32:45.914: INFO: Pod "agnhost" satisfied condition "running"
    Jan 19 04:32:45.914: INFO: Creating service...
    Jan 19 04:32:45.924: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6158/pods/agnhost/proxy/some/path/with/DELETE
    Jan 19 04:32:45.935: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 19 04:32:45.935: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6158/pods/agnhost/proxy/some/path/with/GET
    Jan 19 04:32:45.938: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jan 19 04:32:45.938: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6158/pods/agnhost/proxy/some/path/with/HEAD
    Jan 19 04:32:45.940: INFO: http.Client request:HEAD | StatusCode:200
    Jan 19 04:32:45.940: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6158/pods/agnhost/proxy/some/path/with/OPTIONS
    Jan 19 04:32:45.943: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 19 04:32:45.943: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6158/pods/agnhost/proxy/some/path/with/PATCH
    Jan 19 04:32:45.945: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 19 04:32:45.945: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6158/pods/agnhost/proxy/some/path/with/POST
    Jan 19 04:32:45.949: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 19 04:32:45.949: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6158/pods/agnhost/proxy/some/path/with/PUT
    Jan 19 04:32:45.952: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 19 04:32:45.952: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6158/services/test-service/proxy/some/path/with/DELETE
    Jan 19 04:32:45.958: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 19 04:32:45.958: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6158/services/test-service/proxy/some/path/with/GET
    Jan 19 04:32:45.962: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jan 19 04:32:45.962: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6158/services/test-service/proxy/some/path/with/HEAD
    Jan 19 04:32:45.968: INFO: http.Client request:HEAD | StatusCode:200
    Jan 19 04:32:45.968: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6158/services/test-service/proxy/some/path/with/OPTIONS
    Jan 19 04:32:45.972: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 19 04:32:45.972: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6158/services/test-service/proxy/some/path/with/PATCH
    Jan 19 04:32:45.976: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 19 04:32:45.976: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6158/services/test-service/proxy/some/path/with/POST
    Jan 19 04:32:45.980: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 19 04:32:45.980: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6158/services/test-service/proxy/some/path/with/PUT
    Jan 19 04:32:45.985: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Jan 19 04:32:45.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-6158" for this suite. 01/19/23 04:32:45.988
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:32:45.994
Jan 19 04:32:45.994: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename container-probe 01/19/23 04:32:45.995
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:32:46.008
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:32:46.011
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
STEP: Creating pod liveness-2524747e-a74e-4835-9036-f2d99dde5609 in namespace container-probe-2910 01/19/23 04:32:46.013
Jan 19 04:32:46.034: INFO: Waiting up to 5m0s for pod "liveness-2524747e-a74e-4835-9036-f2d99dde5609" in namespace "container-probe-2910" to be "not pending"
Jan 19 04:32:46.037: INFO: Pod "liveness-2524747e-a74e-4835-9036-f2d99dde5609": Phase="Pending", Reason="", readiness=false. Elapsed: 2.640272ms
Jan 19 04:32:48.042: INFO: Pod "liveness-2524747e-a74e-4835-9036-f2d99dde5609": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007251553s
Jan 19 04:32:50.042: INFO: Pod "liveness-2524747e-a74e-4835-9036-f2d99dde5609": Phase="Running", Reason="", readiness=true. Elapsed: 4.007107434s
Jan 19 04:32:50.042: INFO: Pod "liveness-2524747e-a74e-4835-9036-f2d99dde5609" satisfied condition "not pending"
Jan 19 04:32:50.042: INFO: Started pod liveness-2524747e-a74e-4835-9036-f2d99dde5609 in namespace container-probe-2910
STEP: checking the pod's current state and verifying that restartCount is present 01/19/23 04:32:50.042
Jan 19 04:32:50.044: INFO: Initial restart count of pod liveness-2524747e-a74e-4835-9036-f2d99dde5609 is 0
STEP: deleting the pod 01/19/23 04:36:50.591
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 19 04:36:50.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2910" for this suite. 01/19/23 04:36:50.617
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","completed":71,"skipped":1299,"failed":0}
------------------------------
â€¢ [SLOW TEST] [244.631 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:32:45.994
    Jan 19 04:32:45.994: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename container-probe 01/19/23 04:32:45.995
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:32:46.008
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:32:46.011
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:180
    STEP: Creating pod liveness-2524747e-a74e-4835-9036-f2d99dde5609 in namespace container-probe-2910 01/19/23 04:32:46.013
    Jan 19 04:32:46.034: INFO: Waiting up to 5m0s for pod "liveness-2524747e-a74e-4835-9036-f2d99dde5609" in namespace "container-probe-2910" to be "not pending"
    Jan 19 04:32:46.037: INFO: Pod "liveness-2524747e-a74e-4835-9036-f2d99dde5609": Phase="Pending", Reason="", readiness=false. Elapsed: 2.640272ms
    Jan 19 04:32:48.042: INFO: Pod "liveness-2524747e-a74e-4835-9036-f2d99dde5609": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007251553s
    Jan 19 04:32:50.042: INFO: Pod "liveness-2524747e-a74e-4835-9036-f2d99dde5609": Phase="Running", Reason="", readiness=true. Elapsed: 4.007107434s
    Jan 19 04:32:50.042: INFO: Pod "liveness-2524747e-a74e-4835-9036-f2d99dde5609" satisfied condition "not pending"
    Jan 19 04:32:50.042: INFO: Started pod liveness-2524747e-a74e-4835-9036-f2d99dde5609 in namespace container-probe-2910
    STEP: checking the pod's current state and verifying that restartCount is present 01/19/23 04:32:50.042
    Jan 19 04:32:50.044: INFO: Initial restart count of pod liveness-2524747e-a74e-4835-9036-f2d99dde5609 is 0
    STEP: deleting the pod 01/19/23 04:36:50.591
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 19 04:36:50.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-2910" for this suite. 01/19/23 04:36:50.617
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:36:50.628
Jan 19 04:36:50.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename projected 01/19/23 04:36:50.629
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:36:50.647
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:36:50.65
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
STEP: Creating projection with secret that has name projected-secret-test-94af3c5e-3e56-4045-8b1d-4478264e8ecb 01/19/23 04:36:50.652
STEP: Creating a pod to test consume secrets 01/19/23 04:36:50.658
Jan 19 04:36:50.668: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-817b0a36-f016-4186-8cec-9d794cf5b2b5" in namespace "projected-5298" to be "Succeeded or Failed"
Jan 19 04:36:50.675: INFO: Pod "pod-projected-secrets-817b0a36-f016-4186-8cec-9d794cf5b2b5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.115246ms
Jan 19 04:36:52.679: INFO: Pod "pod-projected-secrets-817b0a36-f016-4186-8cec-9d794cf5b2b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010878308s
Jan 19 04:36:54.680: INFO: Pod "pod-projected-secrets-817b0a36-f016-4186-8cec-9d794cf5b2b5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012083831s
Jan 19 04:36:56.679: INFO: Pod "pod-projected-secrets-817b0a36-f016-4186-8cec-9d794cf5b2b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010662926s
STEP: Saw pod success 01/19/23 04:36:56.679
Jan 19 04:36:56.679: INFO: Pod "pod-projected-secrets-817b0a36-f016-4186-8cec-9d794cf5b2b5" satisfied condition "Succeeded or Failed"
Jan 19 04:36:56.681: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-projected-secrets-817b0a36-f016-4186-8cec-9d794cf5b2b5 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/19/23 04:36:56.714
Jan 19 04:36:56.741: INFO: Waiting for pod pod-projected-secrets-817b0a36-f016-4186-8cec-9d794cf5b2b5 to disappear
Jan 19 04:36:56.743: INFO: Pod pod-projected-secrets-817b0a36-f016-4186-8cec-9d794cf5b2b5 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 19 04:36:56.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5298" for this suite. 01/19/23 04:36:56.746
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":72,"skipped":1375,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.123 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:36:50.628
    Jan 19 04:36:50.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename projected 01/19/23 04:36:50.629
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:36:50.647
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:36:50.65
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:55
    STEP: Creating projection with secret that has name projected-secret-test-94af3c5e-3e56-4045-8b1d-4478264e8ecb 01/19/23 04:36:50.652
    STEP: Creating a pod to test consume secrets 01/19/23 04:36:50.658
    Jan 19 04:36:50.668: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-817b0a36-f016-4186-8cec-9d794cf5b2b5" in namespace "projected-5298" to be "Succeeded or Failed"
    Jan 19 04:36:50.675: INFO: Pod "pod-projected-secrets-817b0a36-f016-4186-8cec-9d794cf5b2b5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.115246ms
    Jan 19 04:36:52.679: INFO: Pod "pod-projected-secrets-817b0a36-f016-4186-8cec-9d794cf5b2b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010878308s
    Jan 19 04:36:54.680: INFO: Pod "pod-projected-secrets-817b0a36-f016-4186-8cec-9d794cf5b2b5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012083831s
    Jan 19 04:36:56.679: INFO: Pod "pod-projected-secrets-817b0a36-f016-4186-8cec-9d794cf5b2b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010662926s
    STEP: Saw pod success 01/19/23 04:36:56.679
    Jan 19 04:36:56.679: INFO: Pod "pod-projected-secrets-817b0a36-f016-4186-8cec-9d794cf5b2b5" satisfied condition "Succeeded or Failed"
    Jan 19 04:36:56.681: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-projected-secrets-817b0a36-f016-4186-8cec-9d794cf5b2b5 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/19/23 04:36:56.714
    Jan 19 04:36:56.741: INFO: Waiting for pod pod-projected-secrets-817b0a36-f016-4186-8cec-9d794cf5b2b5 to disappear
    Jan 19 04:36:56.743: INFO: Pod pod-projected-secrets-817b0a36-f016-4186-8cec-9d794cf5b2b5 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 19 04:36:56.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5298" for this suite. 01/19/23 04:36:56.746
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:36:56.753
Jan 19 04:36:56.753: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename projected 01/19/23 04:36:56.753
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:36:56.768
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:36:56.77
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
STEP: Creating secret with name projected-secret-test-f5cd76c5-4f44-4e0d-9a4d-a15eb8aed5c2 01/19/23 04:36:56.772
STEP: Creating a pod to test consume secrets 01/19/23 04:36:56.778
Jan 19 04:36:56.787: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-30920787-2375-49a5-b776-998649af7fa3" in namespace "projected-5147" to be "Succeeded or Failed"
Jan 19 04:36:56.791: INFO: Pod "pod-projected-secrets-30920787-2375-49a5-b776-998649af7fa3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.5384ms
Jan 19 04:36:58.794: INFO: Pod "pod-projected-secrets-30920787-2375-49a5-b776-998649af7fa3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007157037s
Jan 19 04:37:00.795: INFO: Pod "pod-projected-secrets-30920787-2375-49a5-b776-998649af7fa3": Phase="Running", Reason="", readiness=false. Elapsed: 4.008374145s
Jan 19 04:37:02.797: INFO: Pod "pod-projected-secrets-30920787-2375-49a5-b776-998649af7fa3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00953306s
STEP: Saw pod success 01/19/23 04:37:02.797
Jan 19 04:37:02.797: INFO: Pod "pod-projected-secrets-30920787-2375-49a5-b776-998649af7fa3" satisfied condition "Succeeded or Failed"
Jan 19 04:37:02.800: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-projected-secrets-30920787-2375-49a5-b776-998649af7fa3 container secret-volume-test: <nil>
STEP: delete the pod 01/19/23 04:37:02.806
Jan 19 04:37:02.819: INFO: Waiting for pod pod-projected-secrets-30920787-2375-49a5-b776-998649af7fa3 to disappear
Jan 19 04:37:02.821: INFO: Pod pod-projected-secrets-30920787-2375-49a5-b776-998649af7fa3 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 19 04:37:02.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5147" for this suite. 01/19/23 04:37:02.824
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":73,"skipped":1417,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.078 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:36:56.753
    Jan 19 04:36:56.753: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename projected 01/19/23 04:36:56.753
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:36:56.768
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:36:56.77
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:118
    STEP: Creating secret with name projected-secret-test-f5cd76c5-4f44-4e0d-9a4d-a15eb8aed5c2 01/19/23 04:36:56.772
    STEP: Creating a pod to test consume secrets 01/19/23 04:36:56.778
    Jan 19 04:36:56.787: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-30920787-2375-49a5-b776-998649af7fa3" in namespace "projected-5147" to be "Succeeded or Failed"
    Jan 19 04:36:56.791: INFO: Pod "pod-projected-secrets-30920787-2375-49a5-b776-998649af7fa3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.5384ms
    Jan 19 04:36:58.794: INFO: Pod "pod-projected-secrets-30920787-2375-49a5-b776-998649af7fa3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007157037s
    Jan 19 04:37:00.795: INFO: Pod "pod-projected-secrets-30920787-2375-49a5-b776-998649af7fa3": Phase="Running", Reason="", readiness=false. Elapsed: 4.008374145s
    Jan 19 04:37:02.797: INFO: Pod "pod-projected-secrets-30920787-2375-49a5-b776-998649af7fa3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00953306s
    STEP: Saw pod success 01/19/23 04:37:02.797
    Jan 19 04:37:02.797: INFO: Pod "pod-projected-secrets-30920787-2375-49a5-b776-998649af7fa3" satisfied condition "Succeeded or Failed"
    Jan 19 04:37:02.800: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-projected-secrets-30920787-2375-49a5-b776-998649af7fa3 container secret-volume-test: <nil>
    STEP: delete the pod 01/19/23 04:37:02.806
    Jan 19 04:37:02.819: INFO: Waiting for pod pod-projected-secrets-30920787-2375-49a5-b776-998649af7fa3 to disappear
    Jan 19 04:37:02.821: INFO: Pod pod-projected-secrets-30920787-2375-49a5-b776-998649af7fa3 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 19 04:37:02.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5147" for this suite. 01/19/23 04:37:02.824
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:37:02.833
Jan 19 04:37:02.833: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename container-probe 01/19/23 04:37:02.834
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:37:02.847
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:37:02.849
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
STEP: Creating pod test-webserver-510da9aa-cbed-4d95-8e1c-115318ddc1d2 in namespace container-probe-1230 01/19/23 04:37:02.852
Jan 19 04:37:02.863: INFO: Waiting up to 5m0s for pod "test-webserver-510da9aa-cbed-4d95-8e1c-115318ddc1d2" in namespace "container-probe-1230" to be "not pending"
Jan 19 04:37:02.871: INFO: Pod "test-webserver-510da9aa-cbed-4d95-8e1c-115318ddc1d2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.206789ms
Jan 19 04:37:04.875: INFO: Pod "test-webserver-510da9aa-cbed-4d95-8e1c-115318ddc1d2": Phase="Running", Reason="", readiness=true. Elapsed: 2.011676275s
Jan 19 04:37:04.875: INFO: Pod "test-webserver-510da9aa-cbed-4d95-8e1c-115318ddc1d2" satisfied condition "not pending"
Jan 19 04:37:04.875: INFO: Started pod test-webserver-510da9aa-cbed-4d95-8e1c-115318ddc1d2 in namespace container-probe-1230
STEP: checking the pod's current state and verifying that restartCount is present 01/19/23 04:37:04.875
Jan 19 04:37:04.878: INFO: Initial restart count of pod test-webserver-510da9aa-cbed-4d95-8e1c-115318ddc1d2 is 0
STEP: deleting the pod 01/19/23 04:41:05.442
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 19 04:41:05.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1230" for this suite. 01/19/23 04:41:05.467
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":74,"skipped":1481,"failed":0}
------------------------------
â€¢ [SLOW TEST] [242.643 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:37:02.833
    Jan 19 04:37:02.833: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename container-probe 01/19/23 04:37:02.834
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:37:02.847
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:37:02.849
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:211
    STEP: Creating pod test-webserver-510da9aa-cbed-4d95-8e1c-115318ddc1d2 in namespace container-probe-1230 01/19/23 04:37:02.852
    Jan 19 04:37:02.863: INFO: Waiting up to 5m0s for pod "test-webserver-510da9aa-cbed-4d95-8e1c-115318ddc1d2" in namespace "container-probe-1230" to be "not pending"
    Jan 19 04:37:02.871: INFO: Pod "test-webserver-510da9aa-cbed-4d95-8e1c-115318ddc1d2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.206789ms
    Jan 19 04:37:04.875: INFO: Pod "test-webserver-510da9aa-cbed-4d95-8e1c-115318ddc1d2": Phase="Running", Reason="", readiness=true. Elapsed: 2.011676275s
    Jan 19 04:37:04.875: INFO: Pod "test-webserver-510da9aa-cbed-4d95-8e1c-115318ddc1d2" satisfied condition "not pending"
    Jan 19 04:37:04.875: INFO: Started pod test-webserver-510da9aa-cbed-4d95-8e1c-115318ddc1d2 in namespace container-probe-1230
    STEP: checking the pod's current state and verifying that restartCount is present 01/19/23 04:37:04.875
    Jan 19 04:37:04.878: INFO: Initial restart count of pod test-webserver-510da9aa-cbed-4d95-8e1c-115318ddc1d2 is 0
    STEP: deleting the pod 01/19/23 04:41:05.442
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 19 04:41:05.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-1230" for this suite. 01/19/23 04:41:05.467
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:41:05.476
Jan 19 04:41:05.476: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename configmap 01/19/23 04:41:05.476
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:41:05.491
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:41:05.493
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
STEP: Creating configMap with name configmap-test-volume-map-b7522ff5-dd64-4556-bfc1-9fdafbba20f7 01/19/23 04:41:05.495
STEP: Creating a pod to test consume configMaps 01/19/23 04:41:05.499
Jan 19 04:41:05.506: INFO: Waiting up to 5m0s for pod "pod-configmaps-9ad28c6a-3a68-43e7-9feb-2667b5ecfacb" in namespace "configmap-6719" to be "Succeeded or Failed"
Jan 19 04:41:05.509: INFO: Pod "pod-configmaps-9ad28c6a-3a68-43e7-9feb-2667b5ecfacb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.473272ms
Jan 19 04:41:07.513: INFO: Pod "pod-configmaps-9ad28c6a-3a68-43e7-9feb-2667b5ecfacb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006744371s
Jan 19 04:41:09.513: INFO: Pod "pod-configmaps-9ad28c6a-3a68-43e7-9feb-2667b5ecfacb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006847182s
Jan 19 04:41:11.513: INFO: Pod "pod-configmaps-9ad28c6a-3a68-43e7-9feb-2667b5ecfacb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007242768s
STEP: Saw pod success 01/19/23 04:41:11.513
Jan 19 04:41:11.514: INFO: Pod "pod-configmaps-9ad28c6a-3a68-43e7-9feb-2667b5ecfacb" satisfied condition "Succeeded or Failed"
Jan 19 04:41:11.517: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-configmaps-9ad28c6a-3a68-43e7-9feb-2667b5ecfacb container agnhost-container: <nil>
STEP: delete the pod 01/19/23 04:41:11.551
Jan 19 04:41:11.564: INFO: Waiting for pod pod-configmaps-9ad28c6a-3a68-43e7-9feb-2667b5ecfacb to disappear
Jan 19 04:41:11.567: INFO: Pod pod-configmaps-9ad28c6a-3a68-43e7-9feb-2667b5ecfacb no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 19 04:41:11.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6719" for this suite. 01/19/23 04:41:11.57
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":75,"skipped":1481,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.100 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:41:05.476
    Jan 19 04:41:05.476: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename configmap 01/19/23 04:41:05.476
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:41:05.491
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:41:05.493
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:108
    STEP: Creating configMap with name configmap-test-volume-map-b7522ff5-dd64-4556-bfc1-9fdafbba20f7 01/19/23 04:41:05.495
    STEP: Creating a pod to test consume configMaps 01/19/23 04:41:05.499
    Jan 19 04:41:05.506: INFO: Waiting up to 5m0s for pod "pod-configmaps-9ad28c6a-3a68-43e7-9feb-2667b5ecfacb" in namespace "configmap-6719" to be "Succeeded or Failed"
    Jan 19 04:41:05.509: INFO: Pod "pod-configmaps-9ad28c6a-3a68-43e7-9feb-2667b5ecfacb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.473272ms
    Jan 19 04:41:07.513: INFO: Pod "pod-configmaps-9ad28c6a-3a68-43e7-9feb-2667b5ecfacb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006744371s
    Jan 19 04:41:09.513: INFO: Pod "pod-configmaps-9ad28c6a-3a68-43e7-9feb-2667b5ecfacb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006847182s
    Jan 19 04:41:11.513: INFO: Pod "pod-configmaps-9ad28c6a-3a68-43e7-9feb-2667b5ecfacb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007242768s
    STEP: Saw pod success 01/19/23 04:41:11.513
    Jan 19 04:41:11.514: INFO: Pod "pod-configmaps-9ad28c6a-3a68-43e7-9feb-2667b5ecfacb" satisfied condition "Succeeded or Failed"
    Jan 19 04:41:11.517: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-configmaps-9ad28c6a-3a68-43e7-9feb-2667b5ecfacb container agnhost-container: <nil>
    STEP: delete the pod 01/19/23 04:41:11.551
    Jan 19 04:41:11.564: INFO: Waiting for pod pod-configmaps-9ad28c6a-3a68-43e7-9feb-2667b5ecfacb to disappear
    Jan 19 04:41:11.567: INFO: Pod pod-configmaps-9ad28c6a-3a68-43e7-9feb-2667b5ecfacb no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 19 04:41:11.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6719" for this suite. 01/19/23 04:41:11.57
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:41:11.577
Jan 19 04:41:11.577: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename dns 01/19/23 04:41:11.578
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:41:11.595
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:41:11.598
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 01/19/23 04:41:11.6
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4356.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-4356.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4356.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4356.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4356.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-4356.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4356.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-4356.svc.cluster.local;sleep 1; done
 01/19/23 04:41:11.606
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4356.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-4356.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4356.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-4356.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4356.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-4356.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4356.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-4356.svc.cluster.local;sleep 1; done
 01/19/23 04:41:11.606
STEP: creating a pod to probe DNS 01/19/23 04:41:11.606
STEP: submitting the pod to kubernetes 01/19/23 04:41:11.606
Jan 19 04:41:11.616: INFO: Waiting up to 15m0s for pod "dns-test-9f7f5778-261a-4a96-a7a9-bbce77e4b42f" in namespace "dns-4356" to be "running"
Jan 19 04:41:11.619: INFO: Pod "dns-test-9f7f5778-261a-4a96-a7a9-bbce77e4b42f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.902733ms
Jan 19 04:41:13.626: INFO: Pod "dns-test-9f7f5778-261a-4a96-a7a9-bbce77e4b42f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009810514s
Jan 19 04:41:15.623: INFO: Pod "dns-test-9f7f5778-261a-4a96-a7a9-bbce77e4b42f": Phase="Running", Reason="", readiness=true. Elapsed: 4.006779698s
Jan 19 04:41:15.623: INFO: Pod "dns-test-9f7f5778-261a-4a96-a7a9-bbce77e4b42f" satisfied condition "running"
STEP: retrieving the pod 01/19/23 04:41:15.623
STEP: looking for the results for each expected name from probers 01/19/23 04:41:15.626
Jan 19 04:41:15.631: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4356.svc.cluster.local from pod dns-4356/dns-test-9f7f5778-261a-4a96-a7a9-bbce77e4b42f: the server could not find the requested resource (get pods dns-test-9f7f5778-261a-4a96-a7a9-bbce77e4b42f)
Jan 19 04:41:15.634: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4356.svc.cluster.local from pod dns-4356/dns-test-9f7f5778-261a-4a96-a7a9-bbce77e4b42f: the server could not find the requested resource (get pods dns-test-9f7f5778-261a-4a96-a7a9-bbce77e4b42f)
Jan 19 04:41:15.637: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4356.svc.cluster.local from pod dns-4356/dns-test-9f7f5778-261a-4a96-a7a9-bbce77e4b42f: the server could not find the requested resource (get pods dns-test-9f7f5778-261a-4a96-a7a9-bbce77e4b42f)
Jan 19 04:41:15.640: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4356.svc.cluster.local from pod dns-4356/dns-test-9f7f5778-261a-4a96-a7a9-bbce77e4b42f: the server could not find the requested resource (get pods dns-test-9f7f5778-261a-4a96-a7a9-bbce77e4b42f)
Jan 19 04:41:15.642: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4356.svc.cluster.local from pod dns-4356/dns-test-9f7f5778-261a-4a96-a7a9-bbce77e4b42f: the server could not find the requested resource (get pods dns-test-9f7f5778-261a-4a96-a7a9-bbce77e4b42f)
Jan 19 04:41:15.645: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4356.svc.cluster.local from pod dns-4356/dns-test-9f7f5778-261a-4a96-a7a9-bbce77e4b42f: the server could not find the requested resource (get pods dns-test-9f7f5778-261a-4a96-a7a9-bbce77e4b42f)
Jan 19 04:41:15.648: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4356.svc.cluster.local from pod dns-4356/dns-test-9f7f5778-261a-4a96-a7a9-bbce77e4b42f: the server could not find the requested resource (get pods dns-test-9f7f5778-261a-4a96-a7a9-bbce77e4b42f)
Jan 19 04:41:15.650: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4356.svc.cluster.local from pod dns-4356/dns-test-9f7f5778-261a-4a96-a7a9-bbce77e4b42f: the server could not find the requested resource (get pods dns-test-9f7f5778-261a-4a96-a7a9-bbce77e4b42f)
Jan 19 04:41:15.650: INFO: Lookups using dns-4356/dns-test-9f7f5778-261a-4a96-a7a9-bbce77e4b42f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4356.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4356.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4356.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4356.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4356.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4356.svc.cluster.local jessie_udp@dns-test-service-2.dns-4356.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4356.svc.cluster.local]

Jan 19 04:41:20.680: INFO: DNS probes using dns-4356/dns-test-9f7f5778-261a-4a96-a7a9-bbce77e4b42f succeeded

STEP: deleting the pod 01/19/23 04:41:20.68
STEP: deleting the test headless service 01/19/23 04:41:20.697
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 19 04:41:20.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4356" for this suite. 01/19/23 04:41:20.736
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","completed":76,"skipped":1509,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.167 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:41:11.577
    Jan 19 04:41:11.577: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename dns 01/19/23 04:41:11.578
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:41:11.595
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:41:11.598
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 01/19/23 04:41:11.6
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4356.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-4356.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4356.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4356.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4356.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-4356.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4356.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-4356.svc.cluster.local;sleep 1; done
     01/19/23 04:41:11.606
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4356.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-4356.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4356.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-4356.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4356.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-4356.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4356.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-4356.svc.cluster.local;sleep 1; done
     01/19/23 04:41:11.606
    STEP: creating a pod to probe DNS 01/19/23 04:41:11.606
    STEP: submitting the pod to kubernetes 01/19/23 04:41:11.606
    Jan 19 04:41:11.616: INFO: Waiting up to 15m0s for pod "dns-test-9f7f5778-261a-4a96-a7a9-bbce77e4b42f" in namespace "dns-4356" to be "running"
    Jan 19 04:41:11.619: INFO: Pod "dns-test-9f7f5778-261a-4a96-a7a9-bbce77e4b42f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.902733ms
    Jan 19 04:41:13.626: INFO: Pod "dns-test-9f7f5778-261a-4a96-a7a9-bbce77e4b42f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009810514s
    Jan 19 04:41:15.623: INFO: Pod "dns-test-9f7f5778-261a-4a96-a7a9-bbce77e4b42f": Phase="Running", Reason="", readiness=true. Elapsed: 4.006779698s
    Jan 19 04:41:15.623: INFO: Pod "dns-test-9f7f5778-261a-4a96-a7a9-bbce77e4b42f" satisfied condition "running"
    STEP: retrieving the pod 01/19/23 04:41:15.623
    STEP: looking for the results for each expected name from probers 01/19/23 04:41:15.626
    Jan 19 04:41:15.631: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4356.svc.cluster.local from pod dns-4356/dns-test-9f7f5778-261a-4a96-a7a9-bbce77e4b42f: the server could not find the requested resource (get pods dns-test-9f7f5778-261a-4a96-a7a9-bbce77e4b42f)
    Jan 19 04:41:15.634: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4356.svc.cluster.local from pod dns-4356/dns-test-9f7f5778-261a-4a96-a7a9-bbce77e4b42f: the server could not find the requested resource (get pods dns-test-9f7f5778-261a-4a96-a7a9-bbce77e4b42f)
    Jan 19 04:41:15.637: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4356.svc.cluster.local from pod dns-4356/dns-test-9f7f5778-261a-4a96-a7a9-bbce77e4b42f: the server could not find the requested resource (get pods dns-test-9f7f5778-261a-4a96-a7a9-bbce77e4b42f)
    Jan 19 04:41:15.640: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4356.svc.cluster.local from pod dns-4356/dns-test-9f7f5778-261a-4a96-a7a9-bbce77e4b42f: the server could not find the requested resource (get pods dns-test-9f7f5778-261a-4a96-a7a9-bbce77e4b42f)
    Jan 19 04:41:15.642: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4356.svc.cluster.local from pod dns-4356/dns-test-9f7f5778-261a-4a96-a7a9-bbce77e4b42f: the server could not find the requested resource (get pods dns-test-9f7f5778-261a-4a96-a7a9-bbce77e4b42f)
    Jan 19 04:41:15.645: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4356.svc.cluster.local from pod dns-4356/dns-test-9f7f5778-261a-4a96-a7a9-bbce77e4b42f: the server could not find the requested resource (get pods dns-test-9f7f5778-261a-4a96-a7a9-bbce77e4b42f)
    Jan 19 04:41:15.648: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4356.svc.cluster.local from pod dns-4356/dns-test-9f7f5778-261a-4a96-a7a9-bbce77e4b42f: the server could not find the requested resource (get pods dns-test-9f7f5778-261a-4a96-a7a9-bbce77e4b42f)
    Jan 19 04:41:15.650: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4356.svc.cluster.local from pod dns-4356/dns-test-9f7f5778-261a-4a96-a7a9-bbce77e4b42f: the server could not find the requested resource (get pods dns-test-9f7f5778-261a-4a96-a7a9-bbce77e4b42f)
    Jan 19 04:41:15.650: INFO: Lookups using dns-4356/dns-test-9f7f5778-261a-4a96-a7a9-bbce77e4b42f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4356.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4356.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4356.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4356.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4356.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4356.svc.cluster.local jessie_udp@dns-test-service-2.dns-4356.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4356.svc.cluster.local]

    Jan 19 04:41:20.680: INFO: DNS probes using dns-4356/dns-test-9f7f5778-261a-4a96-a7a9-bbce77e4b42f succeeded

    STEP: deleting the pod 01/19/23 04:41:20.68
    STEP: deleting the test headless service 01/19/23 04:41:20.697
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 19 04:41:20.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-4356" for this suite. 01/19/23 04:41:20.736
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:41:20.745
Jan 19 04:41:20.745: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename projected 01/19/23 04:41:20.746
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:41:20.759
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:41:20.761
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
STEP: Creating configMap with name projected-configmap-test-volume-149f17c8-adb5-40de-a44b-22ba9c76e5cd 01/19/23 04:41:20.763
STEP: Creating a pod to test consume configMaps 01/19/23 04:41:20.769
Jan 19 04:41:20.777: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-19a49270-6625-46d6-827e-3bf351bf0af0" in namespace "projected-4942" to be "Succeeded or Failed"
Jan 19 04:41:20.780: INFO: Pod "pod-projected-configmaps-19a49270-6625-46d6-827e-3bf351bf0af0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.865683ms
Jan 19 04:41:22.785: INFO: Pod "pod-projected-configmaps-19a49270-6625-46d6-827e-3bf351bf0af0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007234587s
Jan 19 04:41:24.784: INFO: Pod "pod-projected-configmaps-19a49270-6625-46d6-827e-3bf351bf0af0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007119s
Jan 19 04:41:26.785: INFO: Pod "pod-projected-configmaps-19a49270-6625-46d6-827e-3bf351bf0af0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007701391s
STEP: Saw pod success 01/19/23 04:41:26.785
Jan 19 04:41:26.785: INFO: Pod "pod-projected-configmaps-19a49270-6625-46d6-827e-3bf351bf0af0" satisfied condition "Succeeded or Failed"
Jan 19 04:41:26.788: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-projected-configmaps-19a49270-6625-46d6-827e-3bf351bf0af0 container agnhost-container: <nil>
STEP: delete the pod 01/19/23 04:41:26.794
Jan 19 04:41:26.804: INFO: Waiting for pod pod-projected-configmaps-19a49270-6625-46d6-827e-3bf351bf0af0 to disappear
Jan 19 04:41:26.807: INFO: Pod pod-projected-configmaps-19a49270-6625-46d6-827e-3bf351bf0af0 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 19 04:41:26.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4942" for this suite. 01/19/23 04:41:26.81
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":77,"skipped":1526,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.070 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:41:20.745
    Jan 19 04:41:20.745: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename projected 01/19/23 04:41:20.746
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:41:20.759
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:41:20.761
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:73
    STEP: Creating configMap with name projected-configmap-test-volume-149f17c8-adb5-40de-a44b-22ba9c76e5cd 01/19/23 04:41:20.763
    STEP: Creating a pod to test consume configMaps 01/19/23 04:41:20.769
    Jan 19 04:41:20.777: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-19a49270-6625-46d6-827e-3bf351bf0af0" in namespace "projected-4942" to be "Succeeded or Failed"
    Jan 19 04:41:20.780: INFO: Pod "pod-projected-configmaps-19a49270-6625-46d6-827e-3bf351bf0af0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.865683ms
    Jan 19 04:41:22.785: INFO: Pod "pod-projected-configmaps-19a49270-6625-46d6-827e-3bf351bf0af0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007234587s
    Jan 19 04:41:24.784: INFO: Pod "pod-projected-configmaps-19a49270-6625-46d6-827e-3bf351bf0af0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007119s
    Jan 19 04:41:26.785: INFO: Pod "pod-projected-configmaps-19a49270-6625-46d6-827e-3bf351bf0af0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007701391s
    STEP: Saw pod success 01/19/23 04:41:26.785
    Jan 19 04:41:26.785: INFO: Pod "pod-projected-configmaps-19a49270-6625-46d6-827e-3bf351bf0af0" satisfied condition "Succeeded or Failed"
    Jan 19 04:41:26.788: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-projected-configmaps-19a49270-6625-46d6-827e-3bf351bf0af0 container agnhost-container: <nil>
    STEP: delete the pod 01/19/23 04:41:26.794
    Jan 19 04:41:26.804: INFO: Waiting for pod pod-projected-configmaps-19a49270-6625-46d6-827e-3bf351bf0af0 to disappear
    Jan 19 04:41:26.807: INFO: Pod pod-projected-configmaps-19a49270-6625-46d6-827e-3bf351bf0af0 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 19 04:41:26.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4942" for this suite. 01/19/23 04:41:26.81
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:41:26.816
Jan 19 04:41:26.816: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename resourcequota 01/19/23 04:41:26.817
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:41:26.832
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:41:26.833
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
STEP: Creating a ResourceQuota with terminating scope 01/19/23 04:41:26.835
STEP: Ensuring ResourceQuota status is calculated 01/19/23 04:41:26.84
STEP: Creating a ResourceQuota with not terminating scope 01/19/23 04:41:28.859
STEP: Ensuring ResourceQuota status is calculated 01/19/23 04:41:28.864
STEP: Creating a long running pod 01/19/23 04:41:30.868
STEP: Ensuring resource quota with not terminating scope captures the pod usage 01/19/23 04:41:30.883
STEP: Ensuring resource quota with terminating scope ignored the pod usage 01/19/23 04:41:32.887
STEP: Deleting the pod 01/19/23 04:41:34.891
STEP: Ensuring resource quota status released the pod usage 01/19/23 04:41:34.905
STEP: Creating a terminating pod 01/19/23 04:41:36.911
STEP: Ensuring resource quota with terminating scope captures the pod usage 01/19/23 04:41:36.924
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 01/19/23 04:41:38.928
STEP: Deleting the pod 01/19/23 04:41:40.933
STEP: Ensuring resource quota status released the pod usage 01/19/23 04:41:40.947
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 19 04:41:42.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2121" for this suite. 01/19/23 04:41:42.955
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","completed":78,"skipped":1534,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.146 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:41:26.816
    Jan 19 04:41:26.816: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename resourcequota 01/19/23 04:41:26.817
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:41:26.832
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:41:26.833
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:680
    STEP: Creating a ResourceQuota with terminating scope 01/19/23 04:41:26.835
    STEP: Ensuring ResourceQuota status is calculated 01/19/23 04:41:26.84
    STEP: Creating a ResourceQuota with not terminating scope 01/19/23 04:41:28.859
    STEP: Ensuring ResourceQuota status is calculated 01/19/23 04:41:28.864
    STEP: Creating a long running pod 01/19/23 04:41:30.868
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 01/19/23 04:41:30.883
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 01/19/23 04:41:32.887
    STEP: Deleting the pod 01/19/23 04:41:34.891
    STEP: Ensuring resource quota status released the pod usage 01/19/23 04:41:34.905
    STEP: Creating a terminating pod 01/19/23 04:41:36.911
    STEP: Ensuring resource quota with terminating scope captures the pod usage 01/19/23 04:41:36.924
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 01/19/23 04:41:38.928
    STEP: Deleting the pod 01/19/23 04:41:40.933
    STEP: Ensuring resource quota status released the pod usage 01/19/23 04:41:40.947
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 19 04:41:42.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-2121" for this suite. 01/19/23 04:41:42.955
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:41:42.962
Jan 19 04:41:42.962: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename webhook 01/19/23 04:41:42.963
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:41:42.977
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:41:42.979
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/19/23 04:41:42.994
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 04:41:43.362
STEP: Deploying the webhook pod 01/19/23 04:41:43.372
STEP: Wait for the deployment to be ready 01/19/23 04:41:43.384
Jan 19 04:41:43.391: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 19 04:41:45.403: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 4, 41, 43, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 41, 43, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 4, 41, 43, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 41, 43, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/19/23 04:41:47.408
STEP: Verifying the service has paired with the endpoint 01/19/23 04:41:47.418
Jan 19 04:41:48.419: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
STEP: Registering the webhook via the AdmissionRegistration API 01/19/23 04:41:48.423
STEP: create a pod 01/19/23 04:41:48.437
Jan 19 04:41:48.448: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-5059" to be "running"
Jan 19 04:41:48.457: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.104186ms
Jan 19 04:41:50.461: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012831196s
Jan 19 04:41:52.461: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.013352222s
Jan 19 04:41:52.461: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 01/19/23 04:41:52.461
Jan 19 04:41:52.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=webhook-5059 attach --namespace=webhook-5059 to-be-attached-pod -i -c=container1'
Jan 19 04:41:52.571: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 04:41:52.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5059" for this suite. 01/19/23 04:41:52.582
STEP: Destroying namespace "webhook-5059-markers" for this suite. 01/19/23 04:41:52.588
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","completed":79,"skipped":1547,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.664 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:41:42.962
    Jan 19 04:41:42.962: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename webhook 01/19/23 04:41:42.963
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:41:42.977
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:41:42.979
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/19/23 04:41:42.994
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 04:41:43.362
    STEP: Deploying the webhook pod 01/19/23 04:41:43.372
    STEP: Wait for the deployment to be ready 01/19/23 04:41:43.384
    Jan 19 04:41:43.391: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 19 04:41:45.403: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 4, 41, 43, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 41, 43, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 4, 41, 43, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 41, 43, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/19/23 04:41:47.408
    STEP: Verifying the service has paired with the endpoint 01/19/23 04:41:47.418
    Jan 19 04:41:48.419: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:208
    STEP: Registering the webhook via the AdmissionRegistration API 01/19/23 04:41:48.423
    STEP: create a pod 01/19/23 04:41:48.437
    Jan 19 04:41:48.448: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-5059" to be "running"
    Jan 19 04:41:48.457: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.104186ms
    Jan 19 04:41:50.461: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012831196s
    Jan 19 04:41:52.461: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.013352222s
    Jan 19 04:41:52.461: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 01/19/23 04:41:52.461
    Jan 19 04:41:52.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=webhook-5059 attach --namespace=webhook-5059 to-be-attached-pod -i -c=container1'
    Jan 19 04:41:52.571: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 04:41:52.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5059" for this suite. 01/19/23 04:41:52.582
    STEP: Destroying namespace "webhook-5059-markers" for this suite. 01/19/23 04:41:52.588
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:41:52.627
Jan 19 04:41:52.627: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename crd-publish-openapi 01/19/23 04:41:52.628
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:41:52.654
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:41:52.656
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 01/19/23 04:41:52.66
Jan 19 04:41:52.683: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
Jan 19 04:41:54.778: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 04:42:05.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1068" for this suite. 01/19/23 04:42:05.063
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","completed":80,"skipped":1588,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.442 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:41:52.627
    Jan 19 04:41:52.627: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename crd-publish-openapi 01/19/23 04:41:52.628
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:41:52.654
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:41:52.656
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:356
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 01/19/23 04:41:52.66
    Jan 19 04:41:52.683: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    Jan 19 04:41:54.778: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 04:42:05.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-1068" for this suite. 01/19/23 04:42:05.063
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:42:05.069
Jan 19 04:42:05.070: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename statefulset 01/19/23 04:42:05.07
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:42:05.081
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:42:05.084
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-5277 01/19/23 04:42:05.087
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
STEP: Looking for a node to schedule stateful set and pod 01/19/23 04:42:05.096
STEP: Creating pod with conflicting port in namespace statefulset-5277 01/19/23 04:42:05.102
STEP: Waiting until pod test-pod will start running in namespace statefulset-5277 01/19/23 04:42:05.111
Jan 19 04:42:05.111: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-5277" to be "running"
Jan 19 04:42:05.117: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.207757ms
Jan 19 04:42:07.121: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009725703s
Jan 19 04:42:09.122: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.010662687s
Jan 19 04:42:09.122: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-5277 01/19/23 04:42:09.122
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5277 01/19/23 04:42:09.128
Jan 19 04:42:09.139: INFO: Observed stateful pod in namespace: statefulset-5277, name: ss-0, uid: 9eac9860-5824-4bbd-97cf-d682155bdd85, status phase: Pending. Waiting for statefulset controller to delete.
Jan 19 04:42:09.205: INFO: Observed stateful pod in namespace: statefulset-5277, name: ss-0, uid: 9eac9860-5824-4bbd-97cf-d682155bdd85, status phase: Failed. Waiting for statefulset controller to delete.
Jan 19 04:42:09.210: INFO: Observed stateful pod in namespace: statefulset-5277, name: ss-0, uid: 9eac9860-5824-4bbd-97cf-d682155bdd85, status phase: Failed. Waiting for statefulset controller to delete.
Jan 19 04:42:09.216: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5277
STEP: Removing pod with conflicting port in namespace statefulset-5277 01/19/23 04:42:09.216
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5277 and will be in running state 01/19/23 04:42:09.244
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 19 04:42:13.261: INFO: Deleting all statefulset in ns statefulset-5277
Jan 19 04:42:13.264: INFO: Scaling statefulset ss to 0
Jan 19 04:42:23.278: INFO: Waiting for statefulset status.replicas updated to 0
Jan 19 04:42:23.281: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 19 04:42:23.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5277" for this suite. 01/19/23 04:42:23.305
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","completed":81,"skipped":1604,"failed":0}
------------------------------
â€¢ [SLOW TEST] [18.244 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:737

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:42:05.069
    Jan 19 04:42:05.070: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename statefulset 01/19/23 04:42:05.07
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:42:05.081
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:42:05.084
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-5277 01/19/23 04:42:05.087
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:737
    STEP: Looking for a node to schedule stateful set and pod 01/19/23 04:42:05.096
    STEP: Creating pod with conflicting port in namespace statefulset-5277 01/19/23 04:42:05.102
    STEP: Waiting until pod test-pod will start running in namespace statefulset-5277 01/19/23 04:42:05.111
    Jan 19 04:42:05.111: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-5277" to be "running"
    Jan 19 04:42:05.117: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.207757ms
    Jan 19 04:42:07.121: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009725703s
    Jan 19 04:42:09.122: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.010662687s
    Jan 19 04:42:09.122: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-5277 01/19/23 04:42:09.122
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5277 01/19/23 04:42:09.128
    Jan 19 04:42:09.139: INFO: Observed stateful pod in namespace: statefulset-5277, name: ss-0, uid: 9eac9860-5824-4bbd-97cf-d682155bdd85, status phase: Pending. Waiting for statefulset controller to delete.
    Jan 19 04:42:09.205: INFO: Observed stateful pod in namespace: statefulset-5277, name: ss-0, uid: 9eac9860-5824-4bbd-97cf-d682155bdd85, status phase: Failed. Waiting for statefulset controller to delete.
    Jan 19 04:42:09.210: INFO: Observed stateful pod in namespace: statefulset-5277, name: ss-0, uid: 9eac9860-5824-4bbd-97cf-d682155bdd85, status phase: Failed. Waiting for statefulset controller to delete.
    Jan 19 04:42:09.216: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5277
    STEP: Removing pod with conflicting port in namespace statefulset-5277 01/19/23 04:42:09.216
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5277 and will be in running state 01/19/23 04:42:09.244
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 19 04:42:13.261: INFO: Deleting all statefulset in ns statefulset-5277
    Jan 19 04:42:13.264: INFO: Scaling statefulset ss to 0
    Jan 19 04:42:23.278: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 19 04:42:23.281: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 19 04:42:23.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-5277" for this suite. 01/19/23 04:42:23.305
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:42:23.313
Jan 19 04:42:23.314: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename emptydir 01/19/23 04:42:23.314
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:42:23.332
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:42:23.334
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
STEP: Creating a pod to test emptydir 0644 on node default medium 01/19/23 04:42:23.336
Jan 19 04:42:23.345: INFO: Waiting up to 5m0s for pod "pod-fb594125-29ea-438b-999e-d2a00b666ff2" in namespace "emptydir-5385" to be "Succeeded or Failed"
Jan 19 04:42:23.351: INFO: Pod "pod-fb594125-29ea-438b-999e-d2a00b666ff2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.734251ms
Jan 19 04:42:25.359: INFO: Pod "pod-fb594125-29ea-438b-999e-d2a00b666ff2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013414544s
Jan 19 04:42:27.355: INFO: Pod "pod-fb594125-29ea-438b-999e-d2a00b666ff2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009961568s
STEP: Saw pod success 01/19/23 04:42:27.355
Jan 19 04:42:27.355: INFO: Pod "pod-fb594125-29ea-438b-999e-d2a00b666ff2" satisfied condition "Succeeded or Failed"
Jan 19 04:42:27.359: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-fb594125-29ea-438b-999e-d2a00b666ff2 container test-container: <nil>
STEP: delete the pod 01/19/23 04:42:27.395
Jan 19 04:42:27.407: INFO: Waiting for pod pod-fb594125-29ea-438b-999e-d2a00b666ff2 to disappear
Jan 19 04:42:27.410: INFO: Pod pod-fb594125-29ea-438b-999e-d2a00b666ff2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 19 04:42:27.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5385" for this suite. 01/19/23 04:42:27.412
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":82,"skipped":1605,"failed":0}
------------------------------
â€¢ [4.105 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:42:23.313
    Jan 19 04:42:23.314: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename emptydir 01/19/23 04:42:23.314
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:42:23.332
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:42:23.334
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:166
    STEP: Creating a pod to test emptydir 0644 on node default medium 01/19/23 04:42:23.336
    Jan 19 04:42:23.345: INFO: Waiting up to 5m0s for pod "pod-fb594125-29ea-438b-999e-d2a00b666ff2" in namespace "emptydir-5385" to be "Succeeded or Failed"
    Jan 19 04:42:23.351: INFO: Pod "pod-fb594125-29ea-438b-999e-d2a00b666ff2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.734251ms
    Jan 19 04:42:25.359: INFO: Pod "pod-fb594125-29ea-438b-999e-d2a00b666ff2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013414544s
    Jan 19 04:42:27.355: INFO: Pod "pod-fb594125-29ea-438b-999e-d2a00b666ff2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009961568s
    STEP: Saw pod success 01/19/23 04:42:27.355
    Jan 19 04:42:27.355: INFO: Pod "pod-fb594125-29ea-438b-999e-d2a00b666ff2" satisfied condition "Succeeded or Failed"
    Jan 19 04:42:27.359: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-fb594125-29ea-438b-999e-d2a00b666ff2 container test-container: <nil>
    STEP: delete the pod 01/19/23 04:42:27.395
    Jan 19 04:42:27.407: INFO: Waiting for pod pod-fb594125-29ea-438b-999e-d2a00b666ff2 to disappear
    Jan 19 04:42:27.410: INFO: Pod pod-fb594125-29ea-438b-999e-d2a00b666ff2 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 19 04:42:27.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5385" for this suite. 01/19/23 04:42:27.412
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:42:27.419
Jan 19 04:42:27.419: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename crd-publish-openapi 01/19/23 04:42:27.419
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:42:27.449
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:42:27.451
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
STEP: set up a multi version CRD 01/19/23 04:42:27.454
Jan 19 04:42:27.455: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: rename a version 01/19/23 04:42:34.335
STEP: check the new version name is served 01/19/23 04:42:34.36
STEP: check the old version name is removed 01/19/23 04:42:37.379
STEP: check the other version is not changed 01/19/23 04:42:38.814
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 04:42:44.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8128" for this suite. 01/19/23 04:42:44.093
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","completed":83,"skipped":1621,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.681 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:42:27.419
    Jan 19 04:42:27.419: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename crd-publish-openapi 01/19/23 04:42:27.419
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:42:27.449
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:42:27.451
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:390
    STEP: set up a multi version CRD 01/19/23 04:42:27.454
    Jan 19 04:42:27.455: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: rename a version 01/19/23 04:42:34.335
    STEP: check the new version name is served 01/19/23 04:42:34.36
    STEP: check the old version name is removed 01/19/23 04:42:37.379
    STEP: check the other version is not changed 01/19/23 04:42:38.814
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 04:42:44.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-8128" for this suite. 01/19/23 04:42:44.093
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:42:44.1
Jan 19 04:42:44.100: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename emptydir 01/19/23 04:42:44.101
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:42:44.118
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:42:44.12
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
STEP: Creating a pod to test emptydir 0644 on node default medium 01/19/23 04:42:44.123
Jan 19 04:42:44.131: INFO: Waiting up to 5m0s for pod "pod-fd8fe9b1-5889-4363-80ed-b580a4a4f9d1" in namespace "emptydir-7717" to be "Succeeded or Failed"
Jan 19 04:42:44.134: INFO: Pod "pod-fd8fe9b1-5889-4363-80ed-b580a4a4f9d1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.255352ms
Jan 19 04:42:46.139: INFO: Pod "pod-fd8fe9b1-5889-4363-80ed-b580a4a4f9d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007655601s
Jan 19 04:42:48.140: INFO: Pod "pod-fd8fe9b1-5889-4363-80ed-b580a4a4f9d1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008678245s
Jan 19 04:42:50.139: INFO: Pod "pod-fd8fe9b1-5889-4363-80ed-b580a4a4f9d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008167029s
STEP: Saw pod success 01/19/23 04:42:50.139
Jan 19 04:42:50.139: INFO: Pod "pod-fd8fe9b1-5889-4363-80ed-b580a4a4f9d1" satisfied condition "Succeeded or Failed"
Jan 19 04:42:50.142: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-fd8fe9b1-5889-4363-80ed-b580a4a4f9d1 container test-container: <nil>
STEP: delete the pod 01/19/23 04:42:50.148
Jan 19 04:42:50.160: INFO: Waiting for pod pod-fd8fe9b1-5889-4363-80ed-b580a4a4f9d1 to disappear
Jan 19 04:42:50.162: INFO: Pod pod-fd8fe9b1-5889-4363-80ed-b580a4a4f9d1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 19 04:42:50.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7717" for this suite. 01/19/23 04:42:50.165
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":84,"skipped":1624,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.071 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:42:44.1
    Jan 19 04:42:44.100: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename emptydir 01/19/23 04:42:44.101
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:42:44.118
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:42:44.12
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:196
    STEP: Creating a pod to test emptydir 0644 on node default medium 01/19/23 04:42:44.123
    Jan 19 04:42:44.131: INFO: Waiting up to 5m0s for pod "pod-fd8fe9b1-5889-4363-80ed-b580a4a4f9d1" in namespace "emptydir-7717" to be "Succeeded or Failed"
    Jan 19 04:42:44.134: INFO: Pod "pod-fd8fe9b1-5889-4363-80ed-b580a4a4f9d1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.255352ms
    Jan 19 04:42:46.139: INFO: Pod "pod-fd8fe9b1-5889-4363-80ed-b580a4a4f9d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007655601s
    Jan 19 04:42:48.140: INFO: Pod "pod-fd8fe9b1-5889-4363-80ed-b580a4a4f9d1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008678245s
    Jan 19 04:42:50.139: INFO: Pod "pod-fd8fe9b1-5889-4363-80ed-b580a4a4f9d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008167029s
    STEP: Saw pod success 01/19/23 04:42:50.139
    Jan 19 04:42:50.139: INFO: Pod "pod-fd8fe9b1-5889-4363-80ed-b580a4a4f9d1" satisfied condition "Succeeded or Failed"
    Jan 19 04:42:50.142: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-fd8fe9b1-5889-4363-80ed-b580a4a4f9d1 container test-container: <nil>
    STEP: delete the pod 01/19/23 04:42:50.148
    Jan 19 04:42:50.160: INFO: Waiting for pod pod-fd8fe9b1-5889-4363-80ed-b580a4a4f9d1 to disappear
    Jan 19 04:42:50.162: INFO: Pod pod-fd8fe9b1-5889-4363-80ed-b580a4a4f9d1 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 19 04:42:50.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7717" for this suite. 01/19/23 04:42:50.165
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:42:50.172
Jan 19 04:42:50.172: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename pods 01/19/23 04:42:50.173
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:42:50.188
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:42:50.19
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
Jan 19 04:42:50.201: INFO: Waiting up to 5m0s for pod "server-envvars-1502d756-594f-454f-8a7b-92e386960062" in namespace "pods-8400" to be "running and ready"
Jan 19 04:42:50.208: INFO: Pod "server-envvars-1502d756-594f-454f-8a7b-92e386960062": Phase="Pending", Reason="", readiness=false. Elapsed: 6.327109ms
Jan 19 04:42:50.208: INFO: The phase of Pod server-envvars-1502d756-594f-454f-8a7b-92e386960062 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 04:42:52.213: INFO: Pod "server-envvars-1502d756-594f-454f-8a7b-92e386960062": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011404502s
Jan 19 04:42:52.213: INFO: The phase of Pod server-envvars-1502d756-594f-454f-8a7b-92e386960062 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 04:42:54.212: INFO: Pod "server-envvars-1502d756-594f-454f-8a7b-92e386960062": Phase="Running", Reason="", readiness=true. Elapsed: 4.010856234s
Jan 19 04:42:54.212: INFO: The phase of Pod server-envvars-1502d756-594f-454f-8a7b-92e386960062 is Running (Ready = true)
Jan 19 04:42:54.212: INFO: Pod "server-envvars-1502d756-594f-454f-8a7b-92e386960062" satisfied condition "running and ready"
Jan 19 04:42:54.231: INFO: Waiting up to 5m0s for pod "client-envvars-4236bf9b-f007-47c9-ba90-4b600c96e857" in namespace "pods-8400" to be "Succeeded or Failed"
Jan 19 04:42:54.234: INFO: Pod "client-envvars-4236bf9b-f007-47c9-ba90-4b600c96e857": Phase="Pending", Reason="", readiness=false. Elapsed: 2.811403ms
Jan 19 04:42:56.238: INFO: Pod "client-envvars-4236bf9b-f007-47c9-ba90-4b600c96e857": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00743511s
Jan 19 04:42:58.239: INFO: Pod "client-envvars-4236bf9b-f007-47c9-ba90-4b600c96e857": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008269511s
STEP: Saw pod success 01/19/23 04:42:58.239
Jan 19 04:42:58.239: INFO: Pod "client-envvars-4236bf9b-f007-47c9-ba90-4b600c96e857" satisfied condition "Succeeded or Failed"
Jan 19 04:42:58.242: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod client-envvars-4236bf9b-f007-47c9-ba90-4b600c96e857 container env3cont: <nil>
STEP: delete the pod 01/19/23 04:42:58.248
Jan 19 04:42:58.258: INFO: Waiting for pod client-envvars-4236bf9b-f007-47c9-ba90-4b600c96e857 to disappear
Jan 19 04:42:58.261: INFO: Pod client-envvars-4236bf9b-f007-47c9-ba90-4b600c96e857 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 19 04:42:58.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8400" for this suite. 01/19/23 04:42:58.263
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","completed":85,"skipped":1639,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.097 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:42:50.172
    Jan 19 04:42:50.172: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename pods 01/19/23 04:42:50.173
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:42:50.188
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:42:50.19
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:443
    Jan 19 04:42:50.201: INFO: Waiting up to 5m0s for pod "server-envvars-1502d756-594f-454f-8a7b-92e386960062" in namespace "pods-8400" to be "running and ready"
    Jan 19 04:42:50.208: INFO: Pod "server-envvars-1502d756-594f-454f-8a7b-92e386960062": Phase="Pending", Reason="", readiness=false. Elapsed: 6.327109ms
    Jan 19 04:42:50.208: INFO: The phase of Pod server-envvars-1502d756-594f-454f-8a7b-92e386960062 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 04:42:52.213: INFO: Pod "server-envvars-1502d756-594f-454f-8a7b-92e386960062": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011404502s
    Jan 19 04:42:52.213: INFO: The phase of Pod server-envvars-1502d756-594f-454f-8a7b-92e386960062 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 04:42:54.212: INFO: Pod "server-envvars-1502d756-594f-454f-8a7b-92e386960062": Phase="Running", Reason="", readiness=true. Elapsed: 4.010856234s
    Jan 19 04:42:54.212: INFO: The phase of Pod server-envvars-1502d756-594f-454f-8a7b-92e386960062 is Running (Ready = true)
    Jan 19 04:42:54.212: INFO: Pod "server-envvars-1502d756-594f-454f-8a7b-92e386960062" satisfied condition "running and ready"
    Jan 19 04:42:54.231: INFO: Waiting up to 5m0s for pod "client-envvars-4236bf9b-f007-47c9-ba90-4b600c96e857" in namespace "pods-8400" to be "Succeeded or Failed"
    Jan 19 04:42:54.234: INFO: Pod "client-envvars-4236bf9b-f007-47c9-ba90-4b600c96e857": Phase="Pending", Reason="", readiness=false. Elapsed: 2.811403ms
    Jan 19 04:42:56.238: INFO: Pod "client-envvars-4236bf9b-f007-47c9-ba90-4b600c96e857": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00743511s
    Jan 19 04:42:58.239: INFO: Pod "client-envvars-4236bf9b-f007-47c9-ba90-4b600c96e857": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008269511s
    STEP: Saw pod success 01/19/23 04:42:58.239
    Jan 19 04:42:58.239: INFO: Pod "client-envvars-4236bf9b-f007-47c9-ba90-4b600c96e857" satisfied condition "Succeeded or Failed"
    Jan 19 04:42:58.242: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod client-envvars-4236bf9b-f007-47c9-ba90-4b600c96e857 container env3cont: <nil>
    STEP: delete the pod 01/19/23 04:42:58.248
    Jan 19 04:42:58.258: INFO: Waiting for pod client-envvars-4236bf9b-f007-47c9-ba90-4b600c96e857 to disappear
    Jan 19 04:42:58.261: INFO: Pod client-envvars-4236bf9b-f007-47c9-ba90-4b600c96e857 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 19 04:42:58.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8400" for this suite. 01/19/23 04:42:58.263
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:42:58.269
Jan 19 04:42:58.269: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename emptydir 01/19/23 04:42:58.27
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:42:58.285
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:42:58.288
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
STEP: Creating a pod to test emptydir 0666 on node default medium 01/19/23 04:42:58.29
Jan 19 04:42:58.299: INFO: Waiting up to 5m0s for pod "pod-d4c02792-ac56-4c65-ae2c-0d6501d768c6" in namespace "emptydir-7046" to be "Succeeded or Failed"
Jan 19 04:42:58.305: INFO: Pod "pod-d4c02792-ac56-4c65-ae2c-0d6501d768c6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.896098ms
Jan 19 04:43:00.310: INFO: Pod "pod-d4c02792-ac56-4c65-ae2c-0d6501d768c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010153472s
Jan 19 04:43:02.310: INFO: Pod "pod-d4c02792-ac56-4c65-ae2c-0d6501d768c6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010169064s
Jan 19 04:43:04.310: INFO: Pod "pod-d4c02792-ac56-4c65-ae2c-0d6501d768c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011001836s
STEP: Saw pod success 01/19/23 04:43:04.31
Jan 19 04:43:04.311: INFO: Pod "pod-d4c02792-ac56-4c65-ae2c-0d6501d768c6" satisfied condition "Succeeded or Failed"
Jan 19 04:43:04.314: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-d4c02792-ac56-4c65-ae2c-0d6501d768c6 container test-container: <nil>
STEP: delete the pod 01/19/23 04:43:04.32
Jan 19 04:43:04.329: INFO: Waiting for pod pod-d4c02792-ac56-4c65-ae2c-0d6501d768c6 to disappear
Jan 19 04:43:04.331: INFO: Pod pod-d4c02792-ac56-4c65-ae2c-0d6501d768c6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 19 04:43:04.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7046" for this suite. 01/19/23 04:43:04.334
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":86,"skipped":1647,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.069 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:42:58.269
    Jan 19 04:42:58.269: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename emptydir 01/19/23 04:42:58.27
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:42:58.285
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:42:58.288
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:206
    STEP: Creating a pod to test emptydir 0666 on node default medium 01/19/23 04:42:58.29
    Jan 19 04:42:58.299: INFO: Waiting up to 5m0s for pod "pod-d4c02792-ac56-4c65-ae2c-0d6501d768c6" in namespace "emptydir-7046" to be "Succeeded or Failed"
    Jan 19 04:42:58.305: INFO: Pod "pod-d4c02792-ac56-4c65-ae2c-0d6501d768c6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.896098ms
    Jan 19 04:43:00.310: INFO: Pod "pod-d4c02792-ac56-4c65-ae2c-0d6501d768c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010153472s
    Jan 19 04:43:02.310: INFO: Pod "pod-d4c02792-ac56-4c65-ae2c-0d6501d768c6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010169064s
    Jan 19 04:43:04.310: INFO: Pod "pod-d4c02792-ac56-4c65-ae2c-0d6501d768c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011001836s
    STEP: Saw pod success 01/19/23 04:43:04.31
    Jan 19 04:43:04.311: INFO: Pod "pod-d4c02792-ac56-4c65-ae2c-0d6501d768c6" satisfied condition "Succeeded or Failed"
    Jan 19 04:43:04.314: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-d4c02792-ac56-4c65-ae2c-0d6501d768c6 container test-container: <nil>
    STEP: delete the pod 01/19/23 04:43:04.32
    Jan 19 04:43:04.329: INFO: Waiting for pod pod-d4c02792-ac56-4c65-ae2c-0d6501d768c6 to disappear
    Jan 19 04:43:04.331: INFO: Pod pod-d4c02792-ac56-4c65-ae2c-0d6501d768c6 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 19 04:43:04.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7046" for this suite. 01/19/23 04:43:04.334
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:43:04.339
Jan 19 04:43:04.339: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename kubelet-test 01/19/23 04:43:04.34
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:43:04.356
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:43:04.358
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 01/19/23 04:43:04.367
Jan 19 04:43:04.367: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases61fb1a88-d2cd-4228-976d-0eba3151557e" in namespace "kubelet-test-7242" to be "completed"
Jan 19 04:43:04.370: INFO: Pod "agnhost-host-aliases61fb1a88-d2cd-4228-976d-0eba3151557e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.277577ms
Jan 19 04:43:06.374: INFO: Pod "agnhost-host-aliases61fb1a88-d2cd-4228-976d-0eba3151557e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00626293s
Jan 19 04:43:08.374: INFO: Pod "agnhost-host-aliases61fb1a88-d2cd-4228-976d-0eba3151557e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006408388s
Jan 19 04:43:08.374: INFO: Pod "agnhost-host-aliases61fb1a88-d2cd-4228-976d-0eba3151557e" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan 19 04:43:08.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7242" for this suite. 01/19/23 04:43:08.381
{"msg":"PASSED [sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]","completed":87,"skipped":1662,"failed":0}
------------------------------
â€¢ [4.047 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:43:04.339
    Jan 19 04:43:04.339: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename kubelet-test 01/19/23 04:43:04.34
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:43:04.356
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:43:04.358
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 01/19/23 04:43:04.367
    Jan 19 04:43:04.367: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases61fb1a88-d2cd-4228-976d-0eba3151557e" in namespace "kubelet-test-7242" to be "completed"
    Jan 19 04:43:04.370: INFO: Pod "agnhost-host-aliases61fb1a88-d2cd-4228-976d-0eba3151557e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.277577ms
    Jan 19 04:43:06.374: INFO: Pod "agnhost-host-aliases61fb1a88-d2cd-4228-976d-0eba3151557e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00626293s
    Jan 19 04:43:08.374: INFO: Pod "agnhost-host-aliases61fb1a88-d2cd-4228-976d-0eba3151557e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006408388s
    Jan 19 04:43:08.374: INFO: Pod "agnhost-host-aliases61fb1a88-d2cd-4228-976d-0eba3151557e" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan 19 04:43:08.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-7242" for this suite. 01/19/23 04:43:08.381
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:43:08.388
Jan 19 04:43:08.388: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename var-expansion 01/19/23 04:43:08.389
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:43:08.402
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:43:08.404
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
STEP: creating the pod with failed condition 01/19/23 04:43:08.407
Jan 19 04:43:08.416: INFO: Waiting up to 2m0s for pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f" in namespace "var-expansion-2170" to be "running"
Jan 19 04:43:08.419: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.190368ms
Jan 19 04:43:10.422: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006836874s
Jan 19 04:43:12.422: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006508366s
Jan 19 04:43:14.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007154941s
Jan 19 04:43:16.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007581734s
Jan 19 04:43:18.422: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.006408967s
Jan 19 04:43:20.422: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.006359026s
Jan 19 04:43:22.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.007793764s
Jan 19 04:43:24.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.00701953s
Jan 19 04:43:26.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 18.00699864s
Jan 19 04:43:28.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 20.007495932s
Jan 19 04:43:30.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 22.007872441s
Jan 19 04:43:32.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 24.007206457s
Jan 19 04:43:34.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 26.007637471s
Jan 19 04:43:36.425: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 28.009080133s
Jan 19 04:43:38.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 30.006993655s
Jan 19 04:43:40.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 32.00730468s
Jan 19 04:43:42.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 34.00737224s
Jan 19 04:43:44.425: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 36.009111426s
Jan 19 04:43:46.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 38.007571272s
Jan 19 04:43:48.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 40.00779068s
Jan 19 04:43:50.422: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 42.006917714s
Jan 19 04:43:52.425: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 44.009330288s
Jan 19 04:43:54.424: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 46.008369812s
Jan 19 04:43:56.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 48.007462142s
Jan 19 04:43:58.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 50.007886063s
Jan 19 04:44:00.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 52.007754161s
Jan 19 04:44:02.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 54.007811567s
Jan 19 04:44:04.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 56.007208993s
Jan 19 04:44:06.424: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 58.008021449s
Jan 19 04:44:08.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.007075393s
Jan 19 04:44:10.424: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.008254337s
Jan 19 04:44:12.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.007520172s
Jan 19 04:44:14.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.007538942s
Jan 19 04:44:16.422: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.006500892s
Jan 19 04:44:18.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.007410317s
Jan 19 04:44:20.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.007443197s
Jan 19 04:44:22.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.007725564s
Jan 19 04:44:24.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.007053608s
Jan 19 04:44:26.422: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.00683062s
Jan 19 04:44:28.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.007534969s
Jan 19 04:44:30.424: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.008246461s
Jan 19 04:44:32.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.007861358s
Jan 19 04:44:34.425: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.009452925s
Jan 19 04:44:36.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.00756885s
Jan 19 04:44:38.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.007494775s
Jan 19 04:44:40.424: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.008074985s
Jan 19 04:44:42.425: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.009198s
Jan 19 04:44:44.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.007823956s
Jan 19 04:44:46.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.007525158s
Jan 19 04:44:48.422: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.006540287s
Jan 19 04:44:50.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.007075922s
Jan 19 04:44:52.422: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.006198808s
Jan 19 04:44:54.422: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.00681035s
Jan 19 04:44:56.422: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.00637279s
Jan 19 04:44:58.422: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.006770676s
Jan 19 04:45:00.422: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.006810541s
Jan 19 04:45:02.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.007425597s
Jan 19 04:45:04.424: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.008746776s
Jan 19 04:45:06.422: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.006699272s
Jan 19 04:45:08.422: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.006878719s
Jan 19 04:45:08.426: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.010278235s
STEP: updating the pod 01/19/23 04:45:08.426
Jan 19 04:45:08.941: INFO: Successfully updated pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f"
STEP: waiting for pod running 01/19/23 04:45:08.941
Jan 19 04:45:08.942: INFO: Waiting up to 2m0s for pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f" in namespace "var-expansion-2170" to be "running"
Jan 19 04:45:08.947: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.203464ms
Jan 19 04:45:10.950: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Running", Reason="", readiness=true. Elapsed: 2.008868493s
Jan 19 04:45:10.950: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f" satisfied condition "running"
STEP: deleting the pod gracefully 01/19/23 04:45:10.95
Jan 19 04:45:10.951: INFO: Deleting pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f" in namespace "var-expansion-2170"
Jan 19 04:45:10.960: INFO: Wait up to 5m0s for pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 19 04:45:42.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2170" for this suite. 01/19/23 04:45:42.973
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","completed":88,"skipped":1711,"failed":0}
------------------------------
â€¢ [SLOW TEST] [154.591 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:43:08.388
    Jan 19 04:43:08.388: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename var-expansion 01/19/23 04:43:08.389
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:43:08.402
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:43:08.404
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:224
    STEP: creating the pod with failed condition 01/19/23 04:43:08.407
    Jan 19 04:43:08.416: INFO: Waiting up to 2m0s for pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f" in namespace "var-expansion-2170" to be "running"
    Jan 19 04:43:08.419: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.190368ms
    Jan 19 04:43:10.422: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006836874s
    Jan 19 04:43:12.422: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006508366s
    Jan 19 04:43:14.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007154941s
    Jan 19 04:43:16.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007581734s
    Jan 19 04:43:18.422: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.006408967s
    Jan 19 04:43:20.422: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.006359026s
    Jan 19 04:43:22.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.007793764s
    Jan 19 04:43:24.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.00701953s
    Jan 19 04:43:26.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 18.00699864s
    Jan 19 04:43:28.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 20.007495932s
    Jan 19 04:43:30.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 22.007872441s
    Jan 19 04:43:32.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 24.007206457s
    Jan 19 04:43:34.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 26.007637471s
    Jan 19 04:43:36.425: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 28.009080133s
    Jan 19 04:43:38.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 30.006993655s
    Jan 19 04:43:40.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 32.00730468s
    Jan 19 04:43:42.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 34.00737224s
    Jan 19 04:43:44.425: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 36.009111426s
    Jan 19 04:43:46.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 38.007571272s
    Jan 19 04:43:48.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 40.00779068s
    Jan 19 04:43:50.422: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 42.006917714s
    Jan 19 04:43:52.425: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 44.009330288s
    Jan 19 04:43:54.424: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 46.008369812s
    Jan 19 04:43:56.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 48.007462142s
    Jan 19 04:43:58.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 50.007886063s
    Jan 19 04:44:00.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 52.007754161s
    Jan 19 04:44:02.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 54.007811567s
    Jan 19 04:44:04.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 56.007208993s
    Jan 19 04:44:06.424: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 58.008021449s
    Jan 19 04:44:08.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.007075393s
    Jan 19 04:44:10.424: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.008254337s
    Jan 19 04:44:12.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.007520172s
    Jan 19 04:44:14.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.007538942s
    Jan 19 04:44:16.422: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.006500892s
    Jan 19 04:44:18.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.007410317s
    Jan 19 04:44:20.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.007443197s
    Jan 19 04:44:22.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.007725564s
    Jan 19 04:44:24.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.007053608s
    Jan 19 04:44:26.422: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.00683062s
    Jan 19 04:44:28.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.007534969s
    Jan 19 04:44:30.424: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.008246461s
    Jan 19 04:44:32.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.007861358s
    Jan 19 04:44:34.425: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.009452925s
    Jan 19 04:44:36.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.00756885s
    Jan 19 04:44:38.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.007494775s
    Jan 19 04:44:40.424: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.008074985s
    Jan 19 04:44:42.425: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.009198s
    Jan 19 04:44:44.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.007823956s
    Jan 19 04:44:46.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.007525158s
    Jan 19 04:44:48.422: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.006540287s
    Jan 19 04:44:50.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.007075922s
    Jan 19 04:44:52.422: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.006198808s
    Jan 19 04:44:54.422: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.00681035s
    Jan 19 04:44:56.422: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.00637279s
    Jan 19 04:44:58.422: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.006770676s
    Jan 19 04:45:00.422: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.006810541s
    Jan 19 04:45:02.423: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.007425597s
    Jan 19 04:45:04.424: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.008746776s
    Jan 19 04:45:06.422: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.006699272s
    Jan 19 04:45:08.422: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.006878719s
    Jan 19 04:45:08.426: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.010278235s
    STEP: updating the pod 01/19/23 04:45:08.426
    Jan 19 04:45:08.941: INFO: Successfully updated pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f"
    STEP: waiting for pod running 01/19/23 04:45:08.941
    Jan 19 04:45:08.942: INFO: Waiting up to 2m0s for pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f" in namespace "var-expansion-2170" to be "running"
    Jan 19 04:45:08.947: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.203464ms
    Jan 19 04:45:10.950: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f": Phase="Running", Reason="", readiness=true. Elapsed: 2.008868493s
    Jan 19 04:45:10.950: INFO: Pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f" satisfied condition "running"
    STEP: deleting the pod gracefully 01/19/23 04:45:10.95
    Jan 19 04:45:10.951: INFO: Deleting pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f" in namespace "var-expansion-2170"
    Jan 19 04:45:10.960: INFO: Wait up to 5m0s for pod "var-expansion-89d0c5d2-2c71-4415-8cdd-ed7ece1f500f" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 19 04:45:42.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-2170" for this suite. 01/19/23 04:45:42.973
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:45:42.979
Jan 19 04:45:42.979: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename deployment 01/19/23 04:45:42.98
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:45:43.004
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:45:43.007
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 01/19/23 04:45:43.012
STEP: waiting for Deployment to be created 01/19/23 04:45:43.018
STEP: waiting for all Replicas to be Ready 01/19/23 04:45:43.019
Jan 19 04:45:43.020: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 19 04:45:43.020: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 19 04:45:43.030: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 19 04:45:43.030: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 19 04:45:43.046: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 19 04:45:43.046: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 19 04:45:43.080: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 19 04:45:43.080: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 19 04:45:45.447: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan 19 04:45:45.447: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan 19 04:45:45.461: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 01/19/23 04:45:45.461
W0119 04:45:45.472016      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 19 04:45:45.473: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 01/19/23 04:45:45.473
Jan 19 04:45:45.474: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 0
Jan 19 04:45:45.474: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 0
Jan 19 04:45:45.474: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 0
Jan 19 04:45:45.474: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 0
Jan 19 04:45:45.474: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 0
Jan 19 04:45:45.474: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 0
Jan 19 04:45:45.474: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 0
Jan 19 04:45:45.474: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 0
Jan 19 04:45:45.475: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 1
Jan 19 04:45:45.475: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 1
Jan 19 04:45:45.475: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 2
Jan 19 04:45:45.475: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 2
Jan 19 04:45:45.475: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 2
Jan 19 04:45:45.475: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 2
Jan 19 04:45:45.492: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 2
Jan 19 04:45:45.492: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 2
Jan 19 04:45:45.517: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 2
Jan 19 04:45:45.517: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 2
Jan 19 04:45:45.537: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 1
Jan 19 04:45:45.537: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 1
Jan 19 04:45:45.544: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 1
Jan 19 04:45:45.544: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 1
Jan 19 04:45:46.461: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 2
Jan 19 04:45:46.461: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 2
Jan 19 04:45:46.483: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 1
STEP: listing Deployments 01/19/23 04:45:46.483
Jan 19 04:45:46.487: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 01/19/23 04:45:46.487
Jan 19 04:45:46.548: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 01/19/23 04:45:46.548
Jan 19 04:45:46.555: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 19 04:45:46.562: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 19 04:45:46.591: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 19 04:45:46.635: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 19 04:45:49.475: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 19 04:45:49.491: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Jan 19 04:45:49.550: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 19 04:45:49.569: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 19 04:45:52.514: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 01/19/23 04:45:52.548
STEP: fetching the DeploymentStatus 01/19/23 04:45:52.554
Jan 19 04:45:52.560: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 1
Jan 19 04:45:52.560: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 1
Jan 19 04:45:52.560: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 1
Jan 19 04:45:52.560: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 1
Jan 19 04:45:52.560: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 2
Jan 19 04:45:52.560: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 3
Jan 19 04:45:52.560: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 2
Jan 19 04:45:52.560: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 2
Jan 19 04:45:52.560: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 3
STEP: deleting the Deployment 01/19/23 04:45:52.56
Jan 19 04:45:52.568: INFO: observed event type MODIFIED
Jan 19 04:45:52.568: INFO: observed event type MODIFIED
Jan 19 04:45:52.568: INFO: observed event type MODIFIED
Jan 19 04:45:52.569: INFO: observed event type MODIFIED
Jan 19 04:45:52.569: INFO: observed event type MODIFIED
Jan 19 04:45:52.569: INFO: observed event type MODIFIED
Jan 19 04:45:52.569: INFO: observed event type MODIFIED
Jan 19 04:45:52.569: INFO: observed event type MODIFIED
Jan 19 04:45:52.569: INFO: observed event type MODIFIED
Jan 19 04:45:52.569: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 19 04:45:52.571: INFO: Log out all the ReplicaSets if there is no deployment created
Jan 19 04:45:52.574: INFO: ReplicaSet "test-deployment-54cc775c4b":
&ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-7684  a1772939-64ee-47fb-9b35-160258416e1e 14760 4 2023-01-19 04:45:45 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 7821f711-2cef-4b3c-9806-34c31f0d916c 0xc002d43ba7 0xc002d43ba8}] [] [{kube-controller-manager Update apps/v1 2023-01-19 04:45:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7821f711-2cef-4b3c-9806-34c31f0d916c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 04:45:52 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002d43c30 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jan 19 04:45:52.577: INFO: pod: "test-deployment-54cc775c4b-8s5d4":
&Pod{ObjectMeta:{test-deployment-54cc775c4b-8s5d4 test-deployment-54cc775c4b- deployment-7684  a7530709-4b15-44dd-89ee-6f01460f58c1 14756 0 2023-01-19 04:45:45 +0000 UTC 2023-01-19 04:45:53 +0000 UTC 0xc001c95620 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-54cc775c4b a1772939-64ee-47fb-9b35-160258416e1e 0xc001c95657 0xc001c95658}] [] [{kube-controller-manager Update v1 2023-01-19 04:45:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a1772939-64ee-47fb-9b35-160258416e1e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 04:45:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.70.185\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pmnlp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pmnlp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:45:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:45:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:45:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:45:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.78,PodIP:10.100.70.185,StartTime:2023-01-19 04:45:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 04:45:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://ddf8204345bdde22b6050eb1a37f03d46dfac28d53bb20ab1d9b104d55a6d60b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.70.185,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 19 04:45:52.577: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
&ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-7684  a6218a00-0b86-4628-a5db-ce078d1ff8e3 14752 2 2023-01-19 04:45:46 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 7821f711-2cef-4b3c-9806-34c31f0d916c 0xc002d43c97 0xc002d43c98}] [] [{kube-controller-manager Update apps/v1 2023-01-19 04:45:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7821f711-2cef-4b3c-9806-34c31f0d916c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 04:45:52 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002d43d20 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Jan 19 04:45:52.580: INFO: pod: "test-deployment-7c7d8d58c8-477kj":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-477kj test-deployment-7c7d8d58c8- deployment-7684  3b608e32-47e7-4f1c-bd9f-ae278789bf86 14717 0 2023-01-19 04:45:46 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 a6218a00-0b86-4628-a5db-ce078d1ff8e3 0xc0019b8107 0xc0019b8108}] [] [{kube-controller-manager Update v1 2023-01-19 04:45:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a6218a00-0b86-4628-a5db-ce078d1ff8e3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 04:45:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.70.187\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hd95c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hd95c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:45:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:45:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:45:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:45:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.78,PodIP:10.100.70.187,StartTime:2023-01-19 04:45:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 04:45:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://16593f36c76f4475e45481b78f827a588176608ecb99769d57c69e589a972a75,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.70.187,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 19 04:45:52.580: INFO: pod: "test-deployment-7c7d8d58c8-dxtgd":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-dxtgd test-deployment-7c7d8d58c8- deployment-7684  b35d5623-4e91-471b-97ca-2e9c4578f0ce 14751 0 2023-01-19 04:45:49 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 a6218a00-0b86-4628-a5db-ce078d1ff8e3 0xc0019b82e7 0xc0019b82e8}] [] [{kube-controller-manager Update v1 2023-01-19 04:45:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a6218a00-0b86-4628-a5db-ce078d1ff8e3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 04:45:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.70.188\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n796d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n796d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:45:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:45:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:45:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:45:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.78,PodIP:10.100.70.188,StartTime:2023-01-19 04:45:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 04:45:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://fd7e9fc3c3e67337ce30100e4c81c448c0a440c46eeed481ae95b2a207ad5c03,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.70.188,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 19 04:45:52.580: INFO: ReplicaSet "test-deployment-8594bb6fdd":
&ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-7684  6f9a33a1-02bb-47b5-abc8-d0a8639a4537 14662 3 2023-01-19 04:45:43 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 7821f711-2cef-4b3c-9806-34c31f0d916c 0xc002d43d87 0xc002d43d88}] [] [{kube-controller-manager Update apps/v1 2023-01-19 04:45:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7821f711-2cef-4b3c-9806-34c31f0d916c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 04:45:46 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002d43e10 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 19 04:45:52.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7684" for this suite. 01/19/23 04:45:52.587
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","completed":89,"skipped":1714,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.616 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:45:42.979
    Jan 19 04:45:42.979: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename deployment 01/19/23 04:45:42.98
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:45:43.004
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:45:43.007
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 01/19/23 04:45:43.012
    STEP: waiting for Deployment to be created 01/19/23 04:45:43.018
    STEP: waiting for all Replicas to be Ready 01/19/23 04:45:43.019
    Jan 19 04:45:43.020: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 19 04:45:43.020: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 19 04:45:43.030: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 19 04:45:43.030: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 19 04:45:43.046: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 19 04:45:43.046: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 19 04:45:43.080: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 19 04:45:43.080: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 19 04:45:45.447: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jan 19 04:45:45.447: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jan 19 04:45:45.461: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 01/19/23 04:45:45.461
    W0119 04:45:45.472016      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 19 04:45:45.473: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 01/19/23 04:45:45.473
    Jan 19 04:45:45.474: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 0
    Jan 19 04:45:45.474: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 0
    Jan 19 04:45:45.474: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 0
    Jan 19 04:45:45.474: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 0
    Jan 19 04:45:45.474: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 0
    Jan 19 04:45:45.474: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 0
    Jan 19 04:45:45.474: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 0
    Jan 19 04:45:45.474: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 0
    Jan 19 04:45:45.475: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 1
    Jan 19 04:45:45.475: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 1
    Jan 19 04:45:45.475: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 2
    Jan 19 04:45:45.475: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 2
    Jan 19 04:45:45.475: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 2
    Jan 19 04:45:45.475: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 2
    Jan 19 04:45:45.492: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 2
    Jan 19 04:45:45.492: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 2
    Jan 19 04:45:45.517: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 2
    Jan 19 04:45:45.517: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 2
    Jan 19 04:45:45.537: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 1
    Jan 19 04:45:45.537: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 1
    Jan 19 04:45:45.544: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 1
    Jan 19 04:45:45.544: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 1
    Jan 19 04:45:46.461: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 2
    Jan 19 04:45:46.461: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 2
    Jan 19 04:45:46.483: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 1
    STEP: listing Deployments 01/19/23 04:45:46.483
    Jan 19 04:45:46.487: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 01/19/23 04:45:46.487
    Jan 19 04:45:46.548: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 01/19/23 04:45:46.548
    Jan 19 04:45:46.555: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 19 04:45:46.562: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 19 04:45:46.591: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 19 04:45:46.635: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 19 04:45:49.475: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 19 04:45:49.491: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 19 04:45:49.550: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 19 04:45:49.569: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 19 04:45:52.514: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 01/19/23 04:45:52.548
    STEP: fetching the DeploymentStatus 01/19/23 04:45:52.554
    Jan 19 04:45:52.560: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 1
    Jan 19 04:45:52.560: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 1
    Jan 19 04:45:52.560: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 1
    Jan 19 04:45:52.560: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 1
    Jan 19 04:45:52.560: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 2
    Jan 19 04:45:52.560: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 3
    Jan 19 04:45:52.560: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 2
    Jan 19 04:45:52.560: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 2
    Jan 19 04:45:52.560: INFO: observed Deployment test-deployment in namespace deployment-7684 with ReadyReplicas 3
    STEP: deleting the Deployment 01/19/23 04:45:52.56
    Jan 19 04:45:52.568: INFO: observed event type MODIFIED
    Jan 19 04:45:52.568: INFO: observed event type MODIFIED
    Jan 19 04:45:52.568: INFO: observed event type MODIFIED
    Jan 19 04:45:52.569: INFO: observed event type MODIFIED
    Jan 19 04:45:52.569: INFO: observed event type MODIFIED
    Jan 19 04:45:52.569: INFO: observed event type MODIFIED
    Jan 19 04:45:52.569: INFO: observed event type MODIFIED
    Jan 19 04:45:52.569: INFO: observed event type MODIFIED
    Jan 19 04:45:52.569: INFO: observed event type MODIFIED
    Jan 19 04:45:52.569: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 19 04:45:52.571: INFO: Log out all the ReplicaSets if there is no deployment created
    Jan 19 04:45:52.574: INFO: ReplicaSet "test-deployment-54cc775c4b":
    &ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-7684  a1772939-64ee-47fb-9b35-160258416e1e 14760 4 2023-01-19 04:45:45 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 7821f711-2cef-4b3c-9806-34c31f0d916c 0xc002d43ba7 0xc002d43ba8}] [] [{kube-controller-manager Update apps/v1 2023-01-19 04:45:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7821f711-2cef-4b3c-9806-34c31f0d916c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 04:45:52 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002d43c30 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Jan 19 04:45:52.577: INFO: pod: "test-deployment-54cc775c4b-8s5d4":
    &Pod{ObjectMeta:{test-deployment-54cc775c4b-8s5d4 test-deployment-54cc775c4b- deployment-7684  a7530709-4b15-44dd-89ee-6f01460f58c1 14756 0 2023-01-19 04:45:45 +0000 UTC 2023-01-19 04:45:53 +0000 UTC 0xc001c95620 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-54cc775c4b a1772939-64ee-47fb-9b35-160258416e1e 0xc001c95657 0xc001c95658}] [] [{kube-controller-manager Update v1 2023-01-19 04:45:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a1772939-64ee-47fb-9b35-160258416e1e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 04:45:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.70.185\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pmnlp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pmnlp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:45:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:45:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:45:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:45:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.78,PodIP:10.100.70.185,StartTime:2023-01-19 04:45:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 04:45:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://ddf8204345bdde22b6050eb1a37f03d46dfac28d53bb20ab1d9b104d55a6d60b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.70.185,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan 19 04:45:52.577: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
    &ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-7684  a6218a00-0b86-4628-a5db-ce078d1ff8e3 14752 2 2023-01-19 04:45:46 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 7821f711-2cef-4b3c-9806-34c31f0d916c 0xc002d43c97 0xc002d43c98}] [] [{kube-controller-manager Update apps/v1 2023-01-19 04:45:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7821f711-2cef-4b3c-9806-34c31f0d916c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 04:45:52 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002d43d20 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Jan 19 04:45:52.580: INFO: pod: "test-deployment-7c7d8d58c8-477kj":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-477kj test-deployment-7c7d8d58c8- deployment-7684  3b608e32-47e7-4f1c-bd9f-ae278789bf86 14717 0 2023-01-19 04:45:46 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 a6218a00-0b86-4628-a5db-ce078d1ff8e3 0xc0019b8107 0xc0019b8108}] [] [{kube-controller-manager Update v1 2023-01-19 04:45:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a6218a00-0b86-4628-a5db-ce078d1ff8e3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 04:45:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.70.187\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hd95c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hd95c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:45:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:45:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:45:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:45:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.78,PodIP:10.100.70.187,StartTime:2023-01-19 04:45:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 04:45:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://16593f36c76f4475e45481b78f827a588176608ecb99769d57c69e589a972a75,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.70.187,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan 19 04:45:52.580: INFO: pod: "test-deployment-7c7d8d58c8-dxtgd":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-dxtgd test-deployment-7c7d8d58c8- deployment-7684  b35d5623-4e91-471b-97ca-2e9c4578f0ce 14751 0 2023-01-19 04:45:49 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 a6218a00-0b86-4628-a5db-ce078d1ff8e3 0xc0019b82e7 0xc0019b82e8}] [] [{kube-controller-manager Update v1 2023-01-19 04:45:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a6218a00-0b86-4628-a5db-ce078d1ff8e3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 04:45:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.70.188\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n796d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n796d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:45:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:45:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:45:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:45:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.78,PodIP:10.100.70.188,StartTime:2023-01-19 04:45:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 04:45:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://fd7e9fc3c3e67337ce30100e4c81c448c0a440c46eeed481ae95b2a207ad5c03,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.70.188,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan 19 04:45:52.580: INFO: ReplicaSet "test-deployment-8594bb6fdd":
    &ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-7684  6f9a33a1-02bb-47b5-abc8-d0a8639a4537 14662 3 2023-01-19 04:45:43 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 7821f711-2cef-4b3c-9806-34c31f0d916c 0xc002d43d87 0xc002d43d88}] [] [{kube-controller-manager Update apps/v1 2023-01-19 04:45:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7821f711-2cef-4b3c-9806-34c31f0d916c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 04:45:46 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002d43e10 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 19 04:45:52.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-7684" for this suite. 01/19/23 04:45:52.587
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:45:52.595
Jan 19 04:45:52.595: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename projected 01/19/23 04:45:52.596
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:45:52.615
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:45:52.618
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
STEP: Creating a pod to test downward API volume plugin 01/19/23 04:45:52.621
Jan 19 04:45:52.631: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5072f335-bbcf-4234-a167-cbd5d29dff8b" in namespace "projected-3777" to be "Succeeded or Failed"
Jan 19 04:45:52.634: INFO: Pod "downwardapi-volume-5072f335-bbcf-4234-a167-cbd5d29dff8b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.004209ms
Jan 19 04:45:54.640: INFO: Pod "downwardapi-volume-5072f335-bbcf-4234-a167-cbd5d29dff8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009162494s
Jan 19 04:45:56.638: INFO: Pod "downwardapi-volume-5072f335-bbcf-4234-a167-cbd5d29dff8b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007737078s
Jan 19 04:45:58.638: INFO: Pod "downwardapi-volume-5072f335-bbcf-4234-a167-cbd5d29dff8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007333897s
STEP: Saw pod success 01/19/23 04:45:58.638
Jan 19 04:45:58.638: INFO: Pod "downwardapi-volume-5072f335-bbcf-4234-a167-cbd5d29dff8b" satisfied condition "Succeeded or Failed"
Jan 19 04:45:58.642: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod downwardapi-volume-5072f335-bbcf-4234-a167-cbd5d29dff8b container client-container: <nil>
STEP: delete the pod 01/19/23 04:45:58.673
Jan 19 04:45:58.685: INFO: Waiting for pod downwardapi-volume-5072f335-bbcf-4234-a167-cbd5d29dff8b to disappear
Jan 19 04:45:58.687: INFO: Pod downwardapi-volume-5072f335-bbcf-4234-a167-cbd5d29dff8b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 19 04:45:58.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3777" for this suite. 01/19/23 04:45:58.69
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","completed":90,"skipped":1718,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.100 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:45:52.595
    Jan 19 04:45:52.595: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename projected 01/19/23 04:45:52.596
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:45:52.615
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:45:52.618
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:220
    STEP: Creating a pod to test downward API volume plugin 01/19/23 04:45:52.621
    Jan 19 04:45:52.631: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5072f335-bbcf-4234-a167-cbd5d29dff8b" in namespace "projected-3777" to be "Succeeded or Failed"
    Jan 19 04:45:52.634: INFO: Pod "downwardapi-volume-5072f335-bbcf-4234-a167-cbd5d29dff8b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.004209ms
    Jan 19 04:45:54.640: INFO: Pod "downwardapi-volume-5072f335-bbcf-4234-a167-cbd5d29dff8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009162494s
    Jan 19 04:45:56.638: INFO: Pod "downwardapi-volume-5072f335-bbcf-4234-a167-cbd5d29dff8b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007737078s
    Jan 19 04:45:58.638: INFO: Pod "downwardapi-volume-5072f335-bbcf-4234-a167-cbd5d29dff8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007333897s
    STEP: Saw pod success 01/19/23 04:45:58.638
    Jan 19 04:45:58.638: INFO: Pod "downwardapi-volume-5072f335-bbcf-4234-a167-cbd5d29dff8b" satisfied condition "Succeeded or Failed"
    Jan 19 04:45:58.642: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod downwardapi-volume-5072f335-bbcf-4234-a167-cbd5d29dff8b container client-container: <nil>
    STEP: delete the pod 01/19/23 04:45:58.673
    Jan 19 04:45:58.685: INFO: Waiting for pod downwardapi-volume-5072f335-bbcf-4234-a167-cbd5d29dff8b to disappear
    Jan 19 04:45:58.687: INFO: Pod downwardapi-volume-5072f335-bbcf-4234-a167-cbd5d29dff8b no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 19 04:45:58.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3777" for this suite. 01/19/23 04:45:58.69
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:45:58.697
Jan 19 04:45:58.697: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename resourcequota 01/19/23 04:45:58.697
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:45:58.71
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:45:58.712
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
STEP: Counting existing ResourceQuota 01/19/23 04:45:58.714
STEP: Creating a ResourceQuota 01/19/23 04:46:03.718
STEP: Ensuring resource quota status is calculated 01/19/23 04:46:03.725
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 19 04:46:05.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5943" for this suite. 01/19/23 04:46:05.733
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","completed":91,"skipped":1725,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.043 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:45:58.697
    Jan 19 04:45:58.697: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename resourcequota 01/19/23 04:45:58.697
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:45:58.71
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:45:58.712
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:65
    STEP: Counting existing ResourceQuota 01/19/23 04:45:58.714
    STEP: Creating a ResourceQuota 01/19/23 04:46:03.718
    STEP: Ensuring resource quota status is calculated 01/19/23 04:46:03.725
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 19 04:46:05.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-5943" for this suite. 01/19/23 04:46:05.733
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:46:05.741
Jan 19 04:46:05.741: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename emptydir 01/19/23 04:46:05.742
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:46:05.758
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:46:05.759
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
STEP: Creating a pod to test emptydir 0666 on tmpfs 01/19/23 04:46:05.761
Jan 19 04:46:05.769: INFO: Waiting up to 5m0s for pod "pod-355dfc0f-1c4c-4361-8f6a-612452003f20" in namespace "emptydir-1258" to be "Succeeded or Failed"
Jan 19 04:46:05.775: INFO: Pod "pod-355dfc0f-1c4c-4361-8f6a-612452003f20": Phase="Pending", Reason="", readiness=false. Elapsed: 5.526248ms
Jan 19 04:46:07.778: INFO: Pod "pod-355dfc0f-1c4c-4361-8f6a-612452003f20": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009036319s
Jan 19 04:46:09.779: INFO: Pod "pod-355dfc0f-1c4c-4361-8f6a-612452003f20": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010324895s
Jan 19 04:46:11.780: INFO: Pod "pod-355dfc0f-1c4c-4361-8f6a-612452003f20": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010398224s
STEP: Saw pod success 01/19/23 04:46:11.78
Jan 19 04:46:11.780: INFO: Pod "pod-355dfc0f-1c4c-4361-8f6a-612452003f20" satisfied condition "Succeeded or Failed"
Jan 19 04:46:11.783: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-355dfc0f-1c4c-4361-8f6a-612452003f20 container test-container: <nil>
STEP: delete the pod 01/19/23 04:46:11.789
Jan 19 04:46:11.804: INFO: Waiting for pod pod-355dfc0f-1c4c-4361-8f6a-612452003f20 to disappear
Jan 19 04:46:11.807: INFO: Pod pod-355dfc0f-1c4c-4361-8f6a-612452003f20 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 19 04:46:11.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1258" for this suite. 01/19/23 04:46:11.812
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":92,"skipped":1755,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.079 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:46:05.741
    Jan 19 04:46:05.741: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename emptydir 01/19/23 04:46:05.742
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:46:05.758
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:46:05.759
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:136
    STEP: Creating a pod to test emptydir 0666 on tmpfs 01/19/23 04:46:05.761
    Jan 19 04:46:05.769: INFO: Waiting up to 5m0s for pod "pod-355dfc0f-1c4c-4361-8f6a-612452003f20" in namespace "emptydir-1258" to be "Succeeded or Failed"
    Jan 19 04:46:05.775: INFO: Pod "pod-355dfc0f-1c4c-4361-8f6a-612452003f20": Phase="Pending", Reason="", readiness=false. Elapsed: 5.526248ms
    Jan 19 04:46:07.778: INFO: Pod "pod-355dfc0f-1c4c-4361-8f6a-612452003f20": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009036319s
    Jan 19 04:46:09.779: INFO: Pod "pod-355dfc0f-1c4c-4361-8f6a-612452003f20": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010324895s
    Jan 19 04:46:11.780: INFO: Pod "pod-355dfc0f-1c4c-4361-8f6a-612452003f20": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010398224s
    STEP: Saw pod success 01/19/23 04:46:11.78
    Jan 19 04:46:11.780: INFO: Pod "pod-355dfc0f-1c4c-4361-8f6a-612452003f20" satisfied condition "Succeeded or Failed"
    Jan 19 04:46:11.783: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-355dfc0f-1c4c-4361-8f6a-612452003f20 container test-container: <nil>
    STEP: delete the pod 01/19/23 04:46:11.789
    Jan 19 04:46:11.804: INFO: Waiting for pod pod-355dfc0f-1c4c-4361-8f6a-612452003f20 to disappear
    Jan 19 04:46:11.807: INFO: Pod pod-355dfc0f-1c4c-4361-8f6a-612452003f20 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 19 04:46:11.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1258" for this suite. 01/19/23 04:46:11.812
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:46:11.821
Jan 19 04:46:11.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename emptydir 01/19/23 04:46:11.822
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:46:11.838
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:46:11.842
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
STEP: Creating Pod 01/19/23 04:46:11.845
Jan 19 04:46:11.856: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-415cf79b-c5ae-42a8-86d4-ad86a3d730db" in namespace "emptydir-3097" to be "running"
Jan 19 04:46:11.865: INFO: Pod "pod-sharedvolume-415cf79b-c5ae-42a8-86d4-ad86a3d730db": Phase="Pending", Reason="", readiness=false. Elapsed: 8.038571ms
Jan 19 04:46:13.869: INFO: Pod "pod-sharedvolume-415cf79b-c5ae-42a8-86d4-ad86a3d730db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012548936s
Jan 19 04:46:15.868: INFO: Pod "pod-sharedvolume-415cf79b-c5ae-42a8-86d4-ad86a3d730db": Phase="Running", Reason="", readiness=false. Elapsed: 4.011884766s
Jan 19 04:46:15.868: INFO: Pod "pod-sharedvolume-415cf79b-c5ae-42a8-86d4-ad86a3d730db" satisfied condition "running"
STEP: Reading file content from the nginx-container 01/19/23 04:46:15.868
Jan 19 04:46:15.869: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-3097 PodName:pod-sharedvolume-415cf79b-c5ae-42a8-86d4-ad86a3d730db ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 04:46:15.869: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
Jan 19 04:46:15.869: INFO: ExecWithOptions: Clientset creation
Jan 19 04:46:15.869: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/emptydir-3097/pods/pod-sharedvolume-415cf79b-c5ae-42a8-86d4-ad86a3d730db/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Jan 19 04:46:15.955: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 19 04:46:15.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3097" for this suite. 01/19/23 04:46:15.958
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","completed":93,"skipped":1762,"failed":0}
------------------------------
â€¢ [4.144 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:46:11.821
    Jan 19 04:46:11.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename emptydir 01/19/23 04:46:11.822
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:46:11.838
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:46:11.842
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:226
    STEP: Creating Pod 01/19/23 04:46:11.845
    Jan 19 04:46:11.856: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-415cf79b-c5ae-42a8-86d4-ad86a3d730db" in namespace "emptydir-3097" to be "running"
    Jan 19 04:46:11.865: INFO: Pod "pod-sharedvolume-415cf79b-c5ae-42a8-86d4-ad86a3d730db": Phase="Pending", Reason="", readiness=false. Elapsed: 8.038571ms
    Jan 19 04:46:13.869: INFO: Pod "pod-sharedvolume-415cf79b-c5ae-42a8-86d4-ad86a3d730db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012548936s
    Jan 19 04:46:15.868: INFO: Pod "pod-sharedvolume-415cf79b-c5ae-42a8-86d4-ad86a3d730db": Phase="Running", Reason="", readiness=false. Elapsed: 4.011884766s
    Jan 19 04:46:15.868: INFO: Pod "pod-sharedvolume-415cf79b-c5ae-42a8-86d4-ad86a3d730db" satisfied condition "running"
    STEP: Reading file content from the nginx-container 01/19/23 04:46:15.868
    Jan 19 04:46:15.869: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-3097 PodName:pod-sharedvolume-415cf79b-c5ae-42a8-86d4-ad86a3d730db ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 04:46:15.869: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    Jan 19 04:46:15.869: INFO: ExecWithOptions: Clientset creation
    Jan 19 04:46:15.869: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/emptydir-3097/pods/pod-sharedvolume-415cf79b-c5ae-42a8-86d4-ad86a3d730db/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Jan 19 04:46:15.955: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 19 04:46:15.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3097" for this suite. 01/19/23 04:46:15.958
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:46:15.965
Jan 19 04:46:15.965: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename statefulset 01/19/23 04:46:15.965
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:46:15.978
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:46:15.98
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-4101 01/19/23 04:46:15.982
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
STEP: Creating stateful set ss in namespace statefulset-4101 01/19/23 04:46:15.988
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4101 01/19/23 04:46:15.996
Jan 19 04:46:16.000: INFO: Found 0 stateful pods, waiting for 1
Jan 19 04:46:26.004: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 01/19/23 04:46:26.004
Jan 19 04:46:26.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=statefulset-4101 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 19 04:46:26.172: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 19 04:46:26.172: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 19 04:46:26.172: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 19 04:46:26.175: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 19 04:46:36.180: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 19 04:46:36.180: INFO: Waiting for statefulset status.replicas updated to 0
Jan 19 04:46:36.195: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Jan 19 04:46:36.195: INFO: ss-0  ckcp-nks-default-worker-node-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 04:46:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 04:46:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 04:46:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 04:46:15 +0000 UTC  }]
Jan 19 04:46:36.195: INFO: 
Jan 19 04:46:36.195: INFO: StatefulSet ss has not reached scale 3, at 1
Jan 19 04:46:37.200: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996842642s
Jan 19 04:46:38.207: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.990798324s
Jan 19 04:46:39.212: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.984798273s
Jan 19 04:46:40.217: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.979804923s
Jan 19 04:46:41.222: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.974959378s
Jan 19 04:46:42.226: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.969949325s
Jan 19 04:46:43.231: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.965652111s
Jan 19 04:46:44.234: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.960758832s
Jan 19 04:46:45.238: INFO: Verifying statefulset ss doesn't scale past 3 for another 957.096846ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4101 01/19/23 04:46:46.238
Jan 19 04:46:46.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=statefulset-4101 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 19 04:46:46.401: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 19 04:46:46.401: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 19 04:46:46.401: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 19 04:46:46.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=statefulset-4101 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 19 04:46:46.559: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 19 04:46:46.559: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 19 04:46:46.559: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 19 04:46:46.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=statefulset-4101 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 19 04:46:46.798: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 19 04:46:46.798: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 19 04:46:46.798: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 19 04:46:46.803: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Jan 19 04:46:56.809: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 04:46:56.809: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 04:46:56.810: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 01/19/23 04:46:56.81
Jan 19 04:46:56.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=statefulset-4101 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 19 04:46:56.973: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 19 04:46:56.973: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 19 04:46:56.973: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 19 04:46:56.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=statefulset-4101 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 19 04:46:57.126: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 19 04:46:57.126: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 19 04:46:57.126: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 19 04:46:57.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=statefulset-4101 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 19 04:46:57.280: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 19 04:46:57.280: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 19 04:46:57.280: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 19 04:46:57.280: INFO: Waiting for statefulset status.replicas updated to 0
Jan 19 04:46:57.286: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jan 19 04:47:07.296: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 19 04:47:07.296: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 19 04:47:07.296: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 19 04:47:07.325: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Jan 19 04:47:07.325: INFO: ss-0  ckcp-nks-default-worker-node-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 04:46:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 04:46:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 04:46:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 04:46:15 +0000 UTC  }]
Jan 19 04:47:07.325: INFO: ss-1  ckcp-nks-default-worker-node-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 04:46:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 04:46:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 04:46:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 04:46:36 +0000 UTC  }]
Jan 19 04:47:07.325: INFO: ss-2  ckcp-nks-default-worker-node-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 04:46:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 04:46:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 04:46:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 04:46:36 +0000 UTC  }]
Jan 19 04:47:07.325: INFO: 
Jan 19 04:47:07.325: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 19 04:47:08.329: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Jan 19 04:47:08.329: INFO: ss-0  ckcp-nks-default-worker-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 04:46:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 04:46:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 04:46:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 04:46:15 +0000 UTC  }]
Jan 19 04:47:08.329: INFO: ss-2  ckcp-nks-default-worker-node-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 04:46:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 04:46:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 04:46:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 04:46:36 +0000 UTC  }]
Jan 19 04:47:08.329: INFO: 
Jan 19 04:47:08.329: INFO: StatefulSet ss has not reached scale 0, at 2
Jan 19 04:47:09.332: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.992263707s
Jan 19 04:47:10.336: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.988287328s
Jan 19 04:47:11.340: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.984893051s
Jan 19 04:47:12.346: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.980865141s
Jan 19 04:47:13.360: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.973664193s
Jan 19 04:47:14.364: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.960598554s
Jan 19 04:47:15.368: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.957169391s
Jan 19 04:47:16.373: INFO: Verifying statefulset ss doesn't scale past 0 for another 953.119414ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4101 01/19/23 04:47:17.373
Jan 19 04:47:17.377: INFO: Scaling statefulset ss to 0
Jan 19 04:47:17.386: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 19 04:47:17.389: INFO: Deleting all statefulset in ns statefulset-4101
Jan 19 04:47:17.392: INFO: Scaling statefulset ss to 0
Jan 19 04:47:17.401: INFO: Waiting for statefulset status.replicas updated to 0
Jan 19 04:47:17.403: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 19 04:47:17.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4101" for this suite. 01/19/23 04:47:17.416
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","completed":94,"skipped":1769,"failed":0}
------------------------------
â€¢ [SLOW TEST] [61.457 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:695

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:46:15.965
    Jan 19 04:46:15.965: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename statefulset 01/19/23 04:46:15.965
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:46:15.978
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:46:15.98
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-4101 01/19/23 04:46:15.982
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:695
    STEP: Creating stateful set ss in namespace statefulset-4101 01/19/23 04:46:15.988
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4101 01/19/23 04:46:15.996
    Jan 19 04:46:16.000: INFO: Found 0 stateful pods, waiting for 1
    Jan 19 04:46:26.004: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 01/19/23 04:46:26.004
    Jan 19 04:46:26.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=statefulset-4101 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 19 04:46:26.172: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 19 04:46:26.172: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 19 04:46:26.172: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 19 04:46:26.175: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jan 19 04:46:36.180: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 19 04:46:36.180: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 19 04:46:36.195: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
    Jan 19 04:46:36.195: INFO: ss-0  ckcp-nks-default-worker-node-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 04:46:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 04:46:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 04:46:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 04:46:15 +0000 UTC  }]
    Jan 19 04:46:36.195: INFO: 
    Jan 19 04:46:36.195: INFO: StatefulSet ss has not reached scale 3, at 1
    Jan 19 04:46:37.200: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996842642s
    Jan 19 04:46:38.207: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.990798324s
    Jan 19 04:46:39.212: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.984798273s
    Jan 19 04:46:40.217: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.979804923s
    Jan 19 04:46:41.222: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.974959378s
    Jan 19 04:46:42.226: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.969949325s
    Jan 19 04:46:43.231: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.965652111s
    Jan 19 04:46:44.234: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.960758832s
    Jan 19 04:46:45.238: INFO: Verifying statefulset ss doesn't scale past 3 for another 957.096846ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4101 01/19/23 04:46:46.238
    Jan 19 04:46:46.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=statefulset-4101 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 19 04:46:46.401: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 19 04:46:46.401: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 19 04:46:46.401: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 19 04:46:46.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=statefulset-4101 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 19 04:46:46.559: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jan 19 04:46:46.559: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 19 04:46:46.559: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 19 04:46:46.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=statefulset-4101 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 19 04:46:46.798: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jan 19 04:46:46.798: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 19 04:46:46.798: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 19 04:46:46.803: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Jan 19 04:46:56.809: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 19 04:46:56.809: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 19 04:46:56.810: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 01/19/23 04:46:56.81
    Jan 19 04:46:56.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=statefulset-4101 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 19 04:46:56.973: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 19 04:46:56.973: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 19 04:46:56.973: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 19 04:46:56.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=statefulset-4101 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 19 04:46:57.126: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 19 04:46:57.126: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 19 04:46:57.126: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 19 04:46:57.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=statefulset-4101 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 19 04:46:57.280: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 19 04:46:57.280: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 19 04:46:57.280: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 19 04:46:57.280: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 19 04:46:57.286: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Jan 19 04:47:07.296: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 19 04:47:07.296: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jan 19 04:47:07.296: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jan 19 04:47:07.325: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
    Jan 19 04:47:07.325: INFO: ss-0  ckcp-nks-default-worker-node-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 04:46:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 04:46:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 04:46:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 04:46:15 +0000 UTC  }]
    Jan 19 04:47:07.325: INFO: ss-1  ckcp-nks-default-worker-node-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 04:46:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 04:46:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 04:46:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 04:46:36 +0000 UTC  }]
    Jan 19 04:47:07.325: INFO: ss-2  ckcp-nks-default-worker-node-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 04:46:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 04:46:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 04:46:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 04:46:36 +0000 UTC  }]
    Jan 19 04:47:07.325: INFO: 
    Jan 19 04:47:07.325: INFO: StatefulSet ss has not reached scale 0, at 3
    Jan 19 04:47:08.329: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
    Jan 19 04:47:08.329: INFO: ss-0  ckcp-nks-default-worker-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 04:46:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 04:46:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 04:46:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 04:46:15 +0000 UTC  }]
    Jan 19 04:47:08.329: INFO: ss-2  ckcp-nks-default-worker-node-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 04:46:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 04:46:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 04:46:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 04:46:36 +0000 UTC  }]
    Jan 19 04:47:08.329: INFO: 
    Jan 19 04:47:08.329: INFO: StatefulSet ss has not reached scale 0, at 2
    Jan 19 04:47:09.332: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.992263707s
    Jan 19 04:47:10.336: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.988287328s
    Jan 19 04:47:11.340: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.984893051s
    Jan 19 04:47:12.346: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.980865141s
    Jan 19 04:47:13.360: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.973664193s
    Jan 19 04:47:14.364: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.960598554s
    Jan 19 04:47:15.368: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.957169391s
    Jan 19 04:47:16.373: INFO: Verifying statefulset ss doesn't scale past 0 for another 953.119414ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4101 01/19/23 04:47:17.373
    Jan 19 04:47:17.377: INFO: Scaling statefulset ss to 0
    Jan 19 04:47:17.386: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 19 04:47:17.389: INFO: Deleting all statefulset in ns statefulset-4101
    Jan 19 04:47:17.392: INFO: Scaling statefulset ss to 0
    Jan 19 04:47:17.401: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 19 04:47:17.403: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 19 04:47:17.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-4101" for this suite. 01/19/23 04:47:17.416
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:47:17.422
Jan 19 04:47:17.422: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename events 01/19/23 04:47:17.423
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:47:17.44
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:47:17.442
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 01/19/23 04:47:17.444
Jan 19 04:47:17.448: INFO: created test-event-1
Jan 19 04:47:17.452: INFO: created test-event-2
Jan 19 04:47:17.455: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 01/19/23 04:47:17.455
STEP: delete collection of events 01/19/23 04:47:17.458
Jan 19 04:47:17.458: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 01/19/23 04:47:17.473
Jan 19 04:47:17.473: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Jan 19 04:47:17.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3733" for this suite. 01/19/23 04:47:17.478
{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","completed":95,"skipped":1781,"failed":0}
------------------------------
â€¢ [0.060 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:47:17.422
    Jan 19 04:47:17.422: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename events 01/19/23 04:47:17.423
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:47:17.44
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:47:17.442
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 01/19/23 04:47:17.444
    Jan 19 04:47:17.448: INFO: created test-event-1
    Jan 19 04:47:17.452: INFO: created test-event-2
    Jan 19 04:47:17.455: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 01/19/23 04:47:17.455
    STEP: delete collection of events 01/19/23 04:47:17.458
    Jan 19 04:47:17.458: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 01/19/23 04:47:17.473
    Jan 19 04:47:17.473: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Jan 19 04:47:17.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-3733" for this suite. 01/19/23 04:47:17.478
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:47:17.483
Jan 19 04:47:17.483: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename ingressclass 01/19/23 04:47:17.483
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:47:17.498
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:47:17.5
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 01/19/23 04:47:17.502
STEP: getting /apis/networking.k8s.io 01/19/23 04:47:17.504
STEP: getting /apis/networking.k8s.iov1 01/19/23 04:47:17.505
STEP: creating 01/19/23 04:47:17.505
STEP: getting 01/19/23 04:47:17.536
STEP: listing 01/19/23 04:47:17.539
STEP: watching 01/19/23 04:47:17.542
Jan 19 04:47:17.542: INFO: starting watch
STEP: patching 01/19/23 04:47:17.542
STEP: updating 01/19/23 04:47:17.547
Jan 19 04:47:17.553: INFO: waiting for watch events with expected annotations
Jan 19 04:47:17.553: INFO: saw patched and updated annotations
STEP: deleting 01/19/23 04:47:17.553
STEP: deleting a collection 01/19/23 04:47:17.563
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
Jan 19 04:47:17.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-1387" for this suite. 01/19/23 04:47:17.582
{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","completed":96,"skipped":1781,"failed":0}
------------------------------
â€¢ [0.105 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:47:17.483
    Jan 19 04:47:17.483: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename ingressclass 01/19/23 04:47:17.483
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:47:17.498
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:47:17.5
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 01/19/23 04:47:17.502
    STEP: getting /apis/networking.k8s.io 01/19/23 04:47:17.504
    STEP: getting /apis/networking.k8s.iov1 01/19/23 04:47:17.505
    STEP: creating 01/19/23 04:47:17.505
    STEP: getting 01/19/23 04:47:17.536
    STEP: listing 01/19/23 04:47:17.539
    STEP: watching 01/19/23 04:47:17.542
    Jan 19 04:47:17.542: INFO: starting watch
    STEP: patching 01/19/23 04:47:17.542
    STEP: updating 01/19/23 04:47:17.547
    Jan 19 04:47:17.553: INFO: waiting for watch events with expected annotations
    Jan 19 04:47:17.553: INFO: saw patched and updated annotations
    STEP: deleting 01/19/23 04:47:17.553
    STEP: deleting a collection 01/19/23 04:47:17.563
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:187
    Jan 19 04:47:17.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingressclass-1387" for this suite. 01/19/23 04:47:17.582
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:47:17.588
Jan 19 04:47:17.588: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename projected 01/19/23 04:47:17.589
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:47:17.605
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:47:17.608
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
STEP: Creating a pod to test downward API volume plugin 01/19/23 04:47:17.61
Jan 19 04:47:17.618: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d11581d4-bfb3-4899-93bd-032fbecc8b0f" in namespace "projected-3668" to be "Succeeded or Failed"
Jan 19 04:47:17.621: INFO: Pod "downwardapi-volume-d11581d4-bfb3-4899-93bd-032fbecc8b0f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.108767ms
Jan 19 04:47:19.626: INFO: Pod "downwardapi-volume-d11581d4-bfb3-4899-93bd-032fbecc8b0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00774467s
Jan 19 04:47:21.626: INFO: Pod "downwardapi-volume-d11581d4-bfb3-4899-93bd-032fbecc8b0f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007898352s
Jan 19 04:47:23.626: INFO: Pod "downwardapi-volume-d11581d4-bfb3-4899-93bd-032fbecc8b0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007538362s
STEP: Saw pod success 01/19/23 04:47:23.626
Jan 19 04:47:23.626: INFO: Pod "downwardapi-volume-d11581d4-bfb3-4899-93bd-032fbecc8b0f" satisfied condition "Succeeded or Failed"
Jan 19 04:47:23.629: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod downwardapi-volume-d11581d4-bfb3-4899-93bd-032fbecc8b0f container client-container: <nil>
STEP: delete the pod 01/19/23 04:47:23.636
Jan 19 04:47:23.648: INFO: Waiting for pod downwardapi-volume-d11581d4-bfb3-4899-93bd-032fbecc8b0f to disappear
Jan 19 04:47:23.652: INFO: Pod downwardapi-volume-d11581d4-bfb3-4899-93bd-032fbecc8b0f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 19 04:47:23.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3668" for this suite. 01/19/23 04:47:23.655
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","completed":97,"skipped":1825,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.073 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:47:17.588
    Jan 19 04:47:17.588: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename projected 01/19/23 04:47:17.589
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:47:17.605
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:47:17.608
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:52
    STEP: Creating a pod to test downward API volume plugin 01/19/23 04:47:17.61
    Jan 19 04:47:17.618: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d11581d4-bfb3-4899-93bd-032fbecc8b0f" in namespace "projected-3668" to be "Succeeded or Failed"
    Jan 19 04:47:17.621: INFO: Pod "downwardapi-volume-d11581d4-bfb3-4899-93bd-032fbecc8b0f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.108767ms
    Jan 19 04:47:19.626: INFO: Pod "downwardapi-volume-d11581d4-bfb3-4899-93bd-032fbecc8b0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00774467s
    Jan 19 04:47:21.626: INFO: Pod "downwardapi-volume-d11581d4-bfb3-4899-93bd-032fbecc8b0f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007898352s
    Jan 19 04:47:23.626: INFO: Pod "downwardapi-volume-d11581d4-bfb3-4899-93bd-032fbecc8b0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007538362s
    STEP: Saw pod success 01/19/23 04:47:23.626
    Jan 19 04:47:23.626: INFO: Pod "downwardapi-volume-d11581d4-bfb3-4899-93bd-032fbecc8b0f" satisfied condition "Succeeded or Failed"
    Jan 19 04:47:23.629: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod downwardapi-volume-d11581d4-bfb3-4899-93bd-032fbecc8b0f container client-container: <nil>
    STEP: delete the pod 01/19/23 04:47:23.636
    Jan 19 04:47:23.648: INFO: Waiting for pod downwardapi-volume-d11581d4-bfb3-4899-93bd-032fbecc8b0f to disappear
    Jan 19 04:47:23.652: INFO: Pod downwardapi-volume-d11581d4-bfb3-4899-93bd-032fbecc8b0f no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 19 04:47:23.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3668" for this suite. 01/19/23 04:47:23.655
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:47:23.662
Jan 19 04:47:23.662: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename services 01/19/23 04:47:23.663
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:47:23.678
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:47:23.68
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
STEP: creating an Endpoint 01/19/23 04:47:23.684
STEP: waiting for available Endpoint 01/19/23 04:47:23.688
STEP: listing all Endpoints 01/19/23 04:47:23.689
STEP: updating the Endpoint 01/19/23 04:47:23.692
STEP: fetching the Endpoint 01/19/23 04:47:23.706
STEP: patching the Endpoint 01/19/23 04:47:23.711
STEP: fetching the Endpoint 01/19/23 04:47:23.718
STEP: deleting the Endpoint by Collection 01/19/23 04:47:23.721
STEP: waiting for Endpoint deletion 01/19/23 04:47:23.729
STEP: fetching the Endpoint 01/19/23 04:47:23.73
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 19 04:47:23.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9603" for this suite. 01/19/23 04:47:23.735
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","completed":98,"skipped":1841,"failed":0}
------------------------------
â€¢ [0.079 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:47:23.662
    Jan 19 04:47:23.662: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename services 01/19/23 04:47:23.663
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:47:23.678
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:47:23.68
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3231
    STEP: creating an Endpoint 01/19/23 04:47:23.684
    STEP: waiting for available Endpoint 01/19/23 04:47:23.688
    STEP: listing all Endpoints 01/19/23 04:47:23.689
    STEP: updating the Endpoint 01/19/23 04:47:23.692
    STEP: fetching the Endpoint 01/19/23 04:47:23.706
    STEP: patching the Endpoint 01/19/23 04:47:23.711
    STEP: fetching the Endpoint 01/19/23 04:47:23.718
    STEP: deleting the Endpoint by Collection 01/19/23 04:47:23.721
    STEP: waiting for Endpoint deletion 01/19/23 04:47:23.729
    STEP: fetching the Endpoint 01/19/23 04:47:23.73
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 19 04:47:23.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9603" for this suite. 01/19/23 04:47:23.735
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:47:23.741
Jan 19 04:47:23.741: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename replicaset 01/19/23 04:47:23.741
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:47:23.758
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:47:23.759
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 01/19/23 04:47:23.761
Jan 19 04:47:23.770: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-1208" to be "running and ready"
Jan 19 04:47:23.775: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 4.959107ms
Jan 19 04:47:23.775: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan 19 04:47:25.780: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009503251s
Jan 19 04:47:25.780: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan 19 04:47:27.780: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 4.01001214s
Jan 19 04:47:27.780: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Jan 19 04:47:27.780: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 01/19/23 04:47:27.783
STEP: Then the orphan pod is adopted 01/19/23 04:47:27.789
STEP: When the matched label of one of its pods change 01/19/23 04:47:28.8
Jan 19 04:47:28.803: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 01/19/23 04:47:28.814
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 19 04:47:29.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1208" for this suite. 01/19/23 04:47:29.825
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","completed":99,"skipped":1842,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.091 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:47:23.741
    Jan 19 04:47:23.741: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename replicaset 01/19/23 04:47:23.741
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:47:23.758
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:47:23.759
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 01/19/23 04:47:23.761
    Jan 19 04:47:23.770: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-1208" to be "running and ready"
    Jan 19 04:47:23.775: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 4.959107ms
    Jan 19 04:47:23.775: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 04:47:25.780: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009503251s
    Jan 19 04:47:25.780: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 04:47:27.780: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 4.01001214s
    Jan 19 04:47:27.780: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Jan 19 04:47:27.780: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 01/19/23 04:47:27.783
    STEP: Then the orphan pod is adopted 01/19/23 04:47:27.789
    STEP: When the matched label of one of its pods change 01/19/23 04:47:28.8
    Jan 19 04:47:28.803: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 01/19/23 04:47:28.814
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 19 04:47:29.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-1208" for this suite. 01/19/23 04:47:29.825
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:47:29.833
Jan 19 04:47:29.833: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename svcaccounts 01/19/23 04:47:29.833
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:47:29.847
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:47:29.85
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
Jan 19 04:47:29.870: INFO: created pod pod-service-account-defaultsa
Jan 19 04:47:29.870: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jan 19 04:47:29.877: INFO: created pod pod-service-account-mountsa
Jan 19 04:47:29.878: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jan 19 04:47:29.884: INFO: created pod pod-service-account-nomountsa
Jan 19 04:47:29.884: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jan 19 04:47:29.891: INFO: created pod pod-service-account-defaultsa-mountspec
Jan 19 04:47:29.891: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jan 19 04:47:29.897: INFO: created pod pod-service-account-mountsa-mountspec
Jan 19 04:47:29.897: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jan 19 04:47:29.906: INFO: created pod pod-service-account-nomountsa-mountspec
Jan 19 04:47:29.906: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jan 19 04:47:29.913: INFO: created pod pod-service-account-defaultsa-nomountspec
Jan 19 04:47:29.913: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jan 19 04:47:29.922: INFO: created pod pod-service-account-mountsa-nomountspec
Jan 19 04:47:29.922: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jan 19 04:47:29.929: INFO: created pod pod-service-account-nomountsa-nomountspec
Jan 19 04:47:29.929: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 19 04:47:29.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4915" for this suite. 01/19/23 04:47:29.935
{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","completed":100,"skipped":1872,"failed":0}
------------------------------
â€¢ [0.110 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:47:29.833
    Jan 19 04:47:29.833: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename svcaccounts 01/19/23 04:47:29.833
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:47:29.847
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:47:29.85
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:158
    Jan 19 04:47:29.870: INFO: created pod pod-service-account-defaultsa
    Jan 19 04:47:29.870: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Jan 19 04:47:29.877: INFO: created pod pod-service-account-mountsa
    Jan 19 04:47:29.878: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Jan 19 04:47:29.884: INFO: created pod pod-service-account-nomountsa
    Jan 19 04:47:29.884: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Jan 19 04:47:29.891: INFO: created pod pod-service-account-defaultsa-mountspec
    Jan 19 04:47:29.891: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Jan 19 04:47:29.897: INFO: created pod pod-service-account-mountsa-mountspec
    Jan 19 04:47:29.897: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Jan 19 04:47:29.906: INFO: created pod pod-service-account-nomountsa-mountspec
    Jan 19 04:47:29.906: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Jan 19 04:47:29.913: INFO: created pod pod-service-account-defaultsa-nomountspec
    Jan 19 04:47:29.913: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Jan 19 04:47:29.922: INFO: created pod pod-service-account-mountsa-nomountspec
    Jan 19 04:47:29.922: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Jan 19 04:47:29.929: INFO: created pod pod-service-account-nomountsa-nomountspec
    Jan 19 04:47:29.929: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 19 04:47:29.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-4915" for this suite. 01/19/23 04:47:29.935
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:47:29.944
Jan 19 04:47:29.944: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename svcaccounts 01/19/23 04:47:29.944
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:47:29.962
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:47:29.964
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
Jan 19 04:47:29.968: INFO: Got root ca configmap in namespace "svcaccounts-2502"
Jan 19 04:47:29.974: INFO: Deleted root ca configmap in namespace "svcaccounts-2502"
STEP: waiting for a new root ca configmap created 01/19/23 04:47:30.475
Jan 19 04:47:30.479: INFO: Recreated root ca configmap in namespace "svcaccounts-2502"
Jan 19 04:47:30.485: INFO: Updated root ca configmap in namespace "svcaccounts-2502"
STEP: waiting for the root ca configmap reconciled 01/19/23 04:47:30.986
Jan 19 04:47:30.991: INFO: Reconciled root ca configmap in namespace "svcaccounts-2502"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 19 04:47:30.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2502" for this suite. 01/19/23 04:47:30.996
{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","completed":101,"skipped":1920,"failed":0}
------------------------------
â€¢ [1.061 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:47:29.944
    Jan 19 04:47:29.944: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename svcaccounts 01/19/23 04:47:29.944
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:47:29.962
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:47:29.964
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:739
    Jan 19 04:47:29.968: INFO: Got root ca configmap in namespace "svcaccounts-2502"
    Jan 19 04:47:29.974: INFO: Deleted root ca configmap in namespace "svcaccounts-2502"
    STEP: waiting for a new root ca configmap created 01/19/23 04:47:30.475
    Jan 19 04:47:30.479: INFO: Recreated root ca configmap in namespace "svcaccounts-2502"
    Jan 19 04:47:30.485: INFO: Updated root ca configmap in namespace "svcaccounts-2502"
    STEP: waiting for the root ca configmap reconciled 01/19/23 04:47:30.986
    Jan 19 04:47:30.991: INFO: Reconciled root ca configmap in namespace "svcaccounts-2502"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 19 04:47:30.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-2502" for this suite. 01/19/23 04:47:30.996
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:47:31.005
Jan 19 04:47:31.005: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename namespaces 01/19/23 04:47:31.006
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:47:31.021
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:47:31.028
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
STEP: Creating a test namespace 01/19/23 04:47:31.032
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:47:31.047
STEP: Creating a service in the namespace 01/19/23 04:47:31.05
STEP: Deleting the namespace 01/19/23 04:47:31.063
STEP: Waiting for the namespace to be removed. 01/19/23 04:47:31.071
STEP: Recreating the namespace 01/19/23 04:47:37.076
STEP: Verifying there is no service in the namespace 01/19/23 04:47:37.091
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jan 19 04:47:37.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-857" for this suite. 01/19/23 04:47:37.097
STEP: Destroying namespace "nsdeletetest-5" for this suite. 01/19/23 04:47:37.103
Jan 19 04:47:37.106: INFO: Namespace nsdeletetest-5 was already deleted
STEP: Destroying namespace "nsdeletetest-5518" for this suite. 01/19/23 04:47:37.106
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","completed":102,"skipped":1942,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.107 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:47:31.005
    Jan 19 04:47:31.005: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename namespaces 01/19/23 04:47:31.006
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:47:31.021
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:47:31.028
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:250
    STEP: Creating a test namespace 01/19/23 04:47:31.032
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:47:31.047
    STEP: Creating a service in the namespace 01/19/23 04:47:31.05
    STEP: Deleting the namespace 01/19/23 04:47:31.063
    STEP: Waiting for the namespace to be removed. 01/19/23 04:47:31.071
    STEP: Recreating the namespace 01/19/23 04:47:37.076
    STEP: Verifying there is no service in the namespace 01/19/23 04:47:37.091
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jan 19 04:47:37.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-857" for this suite. 01/19/23 04:47:37.097
    STEP: Destroying namespace "nsdeletetest-5" for this suite. 01/19/23 04:47:37.103
    Jan 19 04:47:37.106: INFO: Namespace nsdeletetest-5 was already deleted
    STEP: Destroying namespace "nsdeletetest-5518" for this suite. 01/19/23 04:47:37.106
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:47:37.112
Jan 19 04:47:37.112: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename services 01/19/23 04:47:37.113
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:47:37.129
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:47:37.131
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
STEP: creating service in namespace services-1473 01/19/23 04:47:37.134
Jan 19 04:47:37.142: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-1473" to be "running and ready"
Jan 19 04:47:37.145: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 2.785126ms
Jan 19 04:47:37.145: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jan 19 04:47:39.149: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.007175547s
Jan 19 04:47:39.149: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Jan 19 04:47:39.149: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Jan 19 04:47:39.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-1473 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jan 19 04:47:39.385: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jan 19 04:47:39.385: INFO: stdout: "iptables"
Jan 19 04:47:39.385: INFO: proxyMode: iptables
Jan 19 04:47:39.394: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan 19 04:47:39.396: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-1473 01/19/23 04:47:39.396
STEP: creating replication controller affinity-nodeport-timeout in namespace services-1473 01/19/23 04:47:39.41
I0119 04:47:39.417440      18 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-1473, replica count: 3
I0119 04:47:42.468939      18 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0119 04:47:45.469107      18 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 19 04:47:45.478: INFO: Creating new exec pod
Jan 19 04:47:45.483: INFO: Waiting up to 5m0s for pod "execpod-affinityps6p8" in namespace "services-1473" to be "running"
Jan 19 04:47:45.486: INFO: Pod "execpod-affinityps6p8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.580406ms
Jan 19 04:47:47.491: INFO: Pod "execpod-affinityps6p8": Phase="Running", Reason="", readiness=true. Elapsed: 2.007716846s
Jan 19 04:47:47.491: INFO: Pod "execpod-affinityps6p8" satisfied condition "running"
Jan 19 04:47:48.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-1473 exec execpod-affinityps6p8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Jan 19 04:47:48.662: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Jan 19 04:47:48.662: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 04:47:48.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-1473 exec execpod-affinityps6p8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.223.115 80'
Jan 19 04:47:48.829: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.223.115 80\nConnection to 10.254.223.115 80 port [tcp/http] succeeded!\n"
Jan 19 04:47:48.829: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 04:47:48.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-1473 exec execpod-affinityps6p8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.0.99 30197'
Jan 19 04:47:48.989: INFO: stderr: "+ nc -v -t -w 2 192.168.0.99 30197\n+ echo hostName\nConnection to 192.168.0.99 30197 port [tcp/*] succeeded!\n"
Jan 19 04:47:48.989: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 04:47:48.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-1473 exec execpod-affinityps6p8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.0.78 30197'
Jan 19 04:47:49.149: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.0.78 30197\nConnection to 192.168.0.78 30197 port [tcp/*] succeeded!\n"
Jan 19 04:47:49.149: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 04:47:49.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-1473 exec execpod-affinityps6p8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.0.99:30197/ ; done'
Jan 19 04:47:49.372: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:30197/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:30197/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:30197/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:30197/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:30197/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:30197/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:30197/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:30197/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:30197/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:30197/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:30197/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:30197/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:30197/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:30197/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:30197/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:30197/\n"
Jan 19 04:47:49.372: INFO: stdout: "\naffinity-nodeport-timeout-r7bpg\naffinity-nodeport-timeout-r7bpg\naffinity-nodeport-timeout-r7bpg\naffinity-nodeport-timeout-r7bpg\naffinity-nodeport-timeout-r7bpg\naffinity-nodeport-timeout-r7bpg\naffinity-nodeport-timeout-r7bpg\naffinity-nodeport-timeout-r7bpg\naffinity-nodeport-timeout-r7bpg\naffinity-nodeport-timeout-r7bpg\naffinity-nodeport-timeout-r7bpg\naffinity-nodeport-timeout-r7bpg\naffinity-nodeport-timeout-r7bpg\naffinity-nodeport-timeout-r7bpg\naffinity-nodeport-timeout-r7bpg\naffinity-nodeport-timeout-r7bpg"
Jan 19 04:47:49.372: INFO: Received response from host: affinity-nodeport-timeout-r7bpg
Jan 19 04:47:49.372: INFO: Received response from host: affinity-nodeport-timeout-r7bpg
Jan 19 04:47:49.372: INFO: Received response from host: affinity-nodeport-timeout-r7bpg
Jan 19 04:47:49.372: INFO: Received response from host: affinity-nodeport-timeout-r7bpg
Jan 19 04:47:49.372: INFO: Received response from host: affinity-nodeport-timeout-r7bpg
Jan 19 04:47:49.372: INFO: Received response from host: affinity-nodeport-timeout-r7bpg
Jan 19 04:47:49.372: INFO: Received response from host: affinity-nodeport-timeout-r7bpg
Jan 19 04:47:49.372: INFO: Received response from host: affinity-nodeport-timeout-r7bpg
Jan 19 04:47:49.372: INFO: Received response from host: affinity-nodeport-timeout-r7bpg
Jan 19 04:47:49.372: INFO: Received response from host: affinity-nodeport-timeout-r7bpg
Jan 19 04:47:49.372: INFO: Received response from host: affinity-nodeport-timeout-r7bpg
Jan 19 04:47:49.372: INFO: Received response from host: affinity-nodeport-timeout-r7bpg
Jan 19 04:47:49.372: INFO: Received response from host: affinity-nodeport-timeout-r7bpg
Jan 19 04:47:49.372: INFO: Received response from host: affinity-nodeport-timeout-r7bpg
Jan 19 04:47:49.372: INFO: Received response from host: affinity-nodeport-timeout-r7bpg
Jan 19 04:47:49.372: INFO: Received response from host: affinity-nodeport-timeout-r7bpg
Jan 19 04:47:49.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-1473 exec execpod-affinityps6p8 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.0.99:30197/'
Jan 19 04:47:49.538: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.0.99:30197/\n"
Jan 19 04:47:49.538: INFO: stdout: "affinity-nodeport-timeout-r7bpg"
Jan 19 04:48:09.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-1473 exec execpod-affinityps6p8 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.0.99:30197/'
Jan 19 04:48:09.703: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.0.99:30197/\n"
Jan 19 04:48:09.703: INFO: stdout: "affinity-nodeport-timeout-r7bpg"
Jan 19 04:48:29.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-1473 exec execpod-affinityps6p8 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.0.99:30197/'
Jan 19 04:48:29.874: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.0.99:30197/\n"
Jan 19 04:48:29.874: INFO: stdout: "affinity-nodeport-timeout-t65gx"
Jan 19 04:48:29.874: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-1473, will wait for the garbage collector to delete the pods 01/19/23 04:48:29.887
Jan 19 04:48:29.951: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 7.862423ms
Jan 19 04:48:30.052: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.805133ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 19 04:48:31.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1473" for this suite. 01/19/23 04:48:31.979
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","completed":103,"skipped":1946,"failed":0}
------------------------------
â€¢ [SLOW TEST] [54.872 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:47:37.112
    Jan 19 04:47:37.112: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename services 01/19/23 04:47:37.113
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:47:37.129
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:47:37.131
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2221
    STEP: creating service in namespace services-1473 01/19/23 04:47:37.134
    Jan 19 04:47:37.142: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-1473" to be "running and ready"
    Jan 19 04:47:37.145: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 2.785126ms
    Jan 19 04:47:37.145: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 04:47:39.149: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.007175547s
    Jan 19 04:47:39.149: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Jan 19 04:47:39.149: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Jan 19 04:47:39.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-1473 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Jan 19 04:47:39.385: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Jan 19 04:47:39.385: INFO: stdout: "iptables"
    Jan 19 04:47:39.385: INFO: proxyMode: iptables
    Jan 19 04:47:39.394: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Jan 19 04:47:39.396: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-nodeport-timeout in namespace services-1473 01/19/23 04:47:39.396
    STEP: creating replication controller affinity-nodeport-timeout in namespace services-1473 01/19/23 04:47:39.41
    I0119 04:47:39.417440      18 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-1473, replica count: 3
    I0119 04:47:42.468939      18 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0119 04:47:45.469107      18 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 19 04:47:45.478: INFO: Creating new exec pod
    Jan 19 04:47:45.483: INFO: Waiting up to 5m0s for pod "execpod-affinityps6p8" in namespace "services-1473" to be "running"
    Jan 19 04:47:45.486: INFO: Pod "execpod-affinityps6p8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.580406ms
    Jan 19 04:47:47.491: INFO: Pod "execpod-affinityps6p8": Phase="Running", Reason="", readiness=true. Elapsed: 2.007716846s
    Jan 19 04:47:47.491: INFO: Pod "execpod-affinityps6p8" satisfied condition "running"
    Jan 19 04:47:48.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-1473 exec execpod-affinityps6p8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
    Jan 19 04:47:48.662: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
    Jan 19 04:47:48.662: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 19 04:47:48.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-1473 exec execpod-affinityps6p8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.223.115 80'
    Jan 19 04:47:48.829: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.223.115 80\nConnection to 10.254.223.115 80 port [tcp/http] succeeded!\n"
    Jan 19 04:47:48.829: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 19 04:47:48.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-1473 exec execpod-affinityps6p8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.0.99 30197'
    Jan 19 04:47:48.989: INFO: stderr: "+ nc -v -t -w 2 192.168.0.99 30197\n+ echo hostName\nConnection to 192.168.0.99 30197 port [tcp/*] succeeded!\n"
    Jan 19 04:47:48.989: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 19 04:47:48.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-1473 exec execpod-affinityps6p8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.0.78 30197'
    Jan 19 04:47:49.149: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.0.78 30197\nConnection to 192.168.0.78 30197 port [tcp/*] succeeded!\n"
    Jan 19 04:47:49.149: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 19 04:47:49.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-1473 exec execpod-affinityps6p8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.0.99:30197/ ; done'
    Jan 19 04:47:49.372: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:30197/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:30197/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:30197/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:30197/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:30197/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:30197/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:30197/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:30197/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:30197/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:30197/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:30197/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:30197/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:30197/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:30197/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:30197/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:30197/\n"
    Jan 19 04:47:49.372: INFO: stdout: "\naffinity-nodeport-timeout-r7bpg\naffinity-nodeport-timeout-r7bpg\naffinity-nodeport-timeout-r7bpg\naffinity-nodeport-timeout-r7bpg\naffinity-nodeport-timeout-r7bpg\naffinity-nodeport-timeout-r7bpg\naffinity-nodeport-timeout-r7bpg\naffinity-nodeport-timeout-r7bpg\naffinity-nodeport-timeout-r7bpg\naffinity-nodeport-timeout-r7bpg\naffinity-nodeport-timeout-r7bpg\naffinity-nodeport-timeout-r7bpg\naffinity-nodeport-timeout-r7bpg\naffinity-nodeport-timeout-r7bpg\naffinity-nodeport-timeout-r7bpg\naffinity-nodeport-timeout-r7bpg"
    Jan 19 04:47:49.372: INFO: Received response from host: affinity-nodeport-timeout-r7bpg
    Jan 19 04:47:49.372: INFO: Received response from host: affinity-nodeport-timeout-r7bpg
    Jan 19 04:47:49.372: INFO: Received response from host: affinity-nodeport-timeout-r7bpg
    Jan 19 04:47:49.372: INFO: Received response from host: affinity-nodeport-timeout-r7bpg
    Jan 19 04:47:49.372: INFO: Received response from host: affinity-nodeport-timeout-r7bpg
    Jan 19 04:47:49.372: INFO: Received response from host: affinity-nodeport-timeout-r7bpg
    Jan 19 04:47:49.372: INFO: Received response from host: affinity-nodeport-timeout-r7bpg
    Jan 19 04:47:49.372: INFO: Received response from host: affinity-nodeport-timeout-r7bpg
    Jan 19 04:47:49.372: INFO: Received response from host: affinity-nodeport-timeout-r7bpg
    Jan 19 04:47:49.372: INFO: Received response from host: affinity-nodeport-timeout-r7bpg
    Jan 19 04:47:49.372: INFO: Received response from host: affinity-nodeport-timeout-r7bpg
    Jan 19 04:47:49.372: INFO: Received response from host: affinity-nodeport-timeout-r7bpg
    Jan 19 04:47:49.372: INFO: Received response from host: affinity-nodeport-timeout-r7bpg
    Jan 19 04:47:49.372: INFO: Received response from host: affinity-nodeport-timeout-r7bpg
    Jan 19 04:47:49.372: INFO: Received response from host: affinity-nodeport-timeout-r7bpg
    Jan 19 04:47:49.372: INFO: Received response from host: affinity-nodeport-timeout-r7bpg
    Jan 19 04:47:49.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-1473 exec execpod-affinityps6p8 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.0.99:30197/'
    Jan 19 04:47:49.538: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.0.99:30197/\n"
    Jan 19 04:47:49.538: INFO: stdout: "affinity-nodeport-timeout-r7bpg"
    Jan 19 04:48:09.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-1473 exec execpod-affinityps6p8 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.0.99:30197/'
    Jan 19 04:48:09.703: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.0.99:30197/\n"
    Jan 19 04:48:09.703: INFO: stdout: "affinity-nodeport-timeout-r7bpg"
    Jan 19 04:48:29.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-1473 exec execpod-affinityps6p8 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.0.99:30197/'
    Jan 19 04:48:29.874: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.0.99:30197/\n"
    Jan 19 04:48:29.874: INFO: stdout: "affinity-nodeport-timeout-t65gx"
    Jan 19 04:48:29.874: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-1473, will wait for the garbage collector to delete the pods 01/19/23 04:48:29.887
    Jan 19 04:48:29.951: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 7.862423ms
    Jan 19 04:48:30.052: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.805133ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 19 04:48:31.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1473" for this suite. 01/19/23 04:48:31.979
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:48:31.985
Jan 19 04:48:31.986: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename container-probe 01/19/23 04:48:31.986
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:48:32.003
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:48:32.005
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 19 04:49:32.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6636" for this suite. 01/19/23 04:49:32.047
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","completed":104,"skipped":1968,"failed":0}
------------------------------
â€¢ [SLOW TEST] [60.069 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:48:31.985
    Jan 19 04:48:31.986: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename container-probe 01/19/23 04:48:31.986
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:48:32.003
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:48:32.005
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:104
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 19 04:49:32.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-6636" for this suite. 01/19/23 04:49:32.047
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:49:32.054
Jan 19 04:49:32.054: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename security-context 01/19/23 04:49:32.055
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:49:32.069
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:49:32.07
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/19/23 04:49:32.072
Jan 19 04:49:32.080: INFO: Waiting up to 5m0s for pod "security-context-944b6f81-f99b-4b1e-a017-de43d1982534" in namespace "security-context-5737" to be "Succeeded or Failed"
Jan 19 04:49:32.083: INFO: Pod "security-context-944b6f81-f99b-4b1e-a017-de43d1982534": Phase="Pending", Reason="", readiness=false. Elapsed: 2.963644ms
Jan 19 04:49:34.089: INFO: Pod "security-context-944b6f81-f99b-4b1e-a017-de43d1982534": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009377479s
Jan 19 04:49:36.087: INFO: Pod "security-context-944b6f81-f99b-4b1e-a017-de43d1982534": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007469191s
Jan 19 04:49:38.088: INFO: Pod "security-context-944b6f81-f99b-4b1e-a017-de43d1982534": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008555048s
STEP: Saw pod success 01/19/23 04:49:38.088
Jan 19 04:49:38.088: INFO: Pod "security-context-944b6f81-f99b-4b1e-a017-de43d1982534" satisfied condition "Succeeded or Failed"
Jan 19 04:49:38.091: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod security-context-944b6f81-f99b-4b1e-a017-de43d1982534 container test-container: <nil>
STEP: delete the pod 01/19/23 04:49:38.124
Jan 19 04:49:38.137: INFO: Waiting for pod security-context-944b6f81-f99b-4b1e-a017-de43d1982534 to disappear
Jan 19 04:49:38.140: INFO: Pod security-context-944b6f81-f99b-4b1e-a017-de43d1982534 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 19 04:49:38.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-5737" for this suite. 01/19/23 04:49:38.143
{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":105,"skipped":1970,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.095 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:49:32.054
    Jan 19 04:49:32.054: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename security-context 01/19/23 04:49:32.055
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:49:32.069
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:49:32.07
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:132
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/19/23 04:49:32.072
    Jan 19 04:49:32.080: INFO: Waiting up to 5m0s for pod "security-context-944b6f81-f99b-4b1e-a017-de43d1982534" in namespace "security-context-5737" to be "Succeeded or Failed"
    Jan 19 04:49:32.083: INFO: Pod "security-context-944b6f81-f99b-4b1e-a017-de43d1982534": Phase="Pending", Reason="", readiness=false. Elapsed: 2.963644ms
    Jan 19 04:49:34.089: INFO: Pod "security-context-944b6f81-f99b-4b1e-a017-de43d1982534": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009377479s
    Jan 19 04:49:36.087: INFO: Pod "security-context-944b6f81-f99b-4b1e-a017-de43d1982534": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007469191s
    Jan 19 04:49:38.088: INFO: Pod "security-context-944b6f81-f99b-4b1e-a017-de43d1982534": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008555048s
    STEP: Saw pod success 01/19/23 04:49:38.088
    Jan 19 04:49:38.088: INFO: Pod "security-context-944b6f81-f99b-4b1e-a017-de43d1982534" satisfied condition "Succeeded or Failed"
    Jan 19 04:49:38.091: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod security-context-944b6f81-f99b-4b1e-a017-de43d1982534 container test-container: <nil>
    STEP: delete the pod 01/19/23 04:49:38.124
    Jan 19 04:49:38.137: INFO: Waiting for pod security-context-944b6f81-f99b-4b1e-a017-de43d1982534 to disappear
    Jan 19 04:49:38.140: INFO: Pod security-context-944b6f81-f99b-4b1e-a017-de43d1982534 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 19 04:49:38.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-5737" for this suite. 01/19/23 04:49:38.143
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:49:38.15
Jan 19 04:49:38.150: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename var-expansion 01/19/23 04:49:38.151
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:49:38.164
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:49:38.167
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
STEP: creating the pod 01/19/23 04:49:38.169
STEP: waiting for pod running 01/19/23 04:49:38.178
Jan 19 04:49:38.179: INFO: Waiting up to 2m0s for pod "var-expansion-de37e5ce-7a85-4287-a515-b598881ed166" in namespace "var-expansion-8937" to be "running"
Jan 19 04:49:38.184: INFO: Pod "var-expansion-de37e5ce-7a85-4287-a515-b598881ed166": Phase="Pending", Reason="", readiness=false. Elapsed: 4.940265ms
Jan 19 04:49:40.188: INFO: Pod "var-expansion-de37e5ce-7a85-4287-a515-b598881ed166": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009262582s
Jan 19 04:49:42.189: INFO: Pod "var-expansion-de37e5ce-7a85-4287-a515-b598881ed166": Phase="Running", Reason="", readiness=true. Elapsed: 4.010837693s
Jan 19 04:49:42.189: INFO: Pod "var-expansion-de37e5ce-7a85-4287-a515-b598881ed166" satisfied condition "running"
STEP: creating a file in subpath 01/19/23 04:49:42.189
Jan 19 04:49:42.193: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-8937 PodName:var-expansion-de37e5ce-7a85-4287-a515-b598881ed166 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 04:49:42.193: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
Jan 19 04:49:42.193: INFO: ExecWithOptions: Clientset creation
Jan 19 04:49:42.193: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/var-expansion-8937/pods/var-expansion-de37e5ce-7a85-4287-a515-b598881ed166/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 01/19/23 04:49:42.285
Jan 19 04:49:42.289: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-8937 PodName:var-expansion-de37e5ce-7a85-4287-a515-b598881ed166 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 04:49:42.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
Jan 19 04:49:42.289: INFO: ExecWithOptions: Clientset creation
Jan 19 04:49:42.289: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/var-expansion-8937/pods/var-expansion-de37e5ce-7a85-4287-a515-b598881ed166/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 01/19/23 04:49:42.368
Jan 19 04:49:42.881: INFO: Successfully updated pod "var-expansion-de37e5ce-7a85-4287-a515-b598881ed166"
STEP: waiting for annotated pod running 01/19/23 04:49:42.881
Jan 19 04:49:42.881: INFO: Waiting up to 2m0s for pod "var-expansion-de37e5ce-7a85-4287-a515-b598881ed166" in namespace "var-expansion-8937" to be "running"
Jan 19 04:49:42.884: INFO: Pod "var-expansion-de37e5ce-7a85-4287-a515-b598881ed166": Phase="Running", Reason="", readiness=true. Elapsed: 2.739943ms
Jan 19 04:49:42.884: INFO: Pod "var-expansion-de37e5ce-7a85-4287-a515-b598881ed166" satisfied condition "running"
STEP: deleting the pod gracefully 01/19/23 04:49:42.884
Jan 19 04:49:42.884: INFO: Deleting pod "var-expansion-de37e5ce-7a85-4287-a515-b598881ed166" in namespace "var-expansion-8937"
Jan 19 04:49:42.890: INFO: Wait up to 5m0s for pod "var-expansion-de37e5ce-7a85-4287-a515-b598881ed166" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 19 04:50:16.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8937" for this suite. 01/19/23 04:50:16.899
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","completed":106,"skipped":2001,"failed":0}
------------------------------
â€¢ [SLOW TEST] [38.755 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:49:38.15
    Jan 19 04:49:38.150: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename var-expansion 01/19/23 04:49:38.151
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:49:38.164
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:49:38.167
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:296
    STEP: creating the pod 01/19/23 04:49:38.169
    STEP: waiting for pod running 01/19/23 04:49:38.178
    Jan 19 04:49:38.179: INFO: Waiting up to 2m0s for pod "var-expansion-de37e5ce-7a85-4287-a515-b598881ed166" in namespace "var-expansion-8937" to be "running"
    Jan 19 04:49:38.184: INFO: Pod "var-expansion-de37e5ce-7a85-4287-a515-b598881ed166": Phase="Pending", Reason="", readiness=false. Elapsed: 4.940265ms
    Jan 19 04:49:40.188: INFO: Pod "var-expansion-de37e5ce-7a85-4287-a515-b598881ed166": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009262582s
    Jan 19 04:49:42.189: INFO: Pod "var-expansion-de37e5ce-7a85-4287-a515-b598881ed166": Phase="Running", Reason="", readiness=true. Elapsed: 4.010837693s
    Jan 19 04:49:42.189: INFO: Pod "var-expansion-de37e5ce-7a85-4287-a515-b598881ed166" satisfied condition "running"
    STEP: creating a file in subpath 01/19/23 04:49:42.189
    Jan 19 04:49:42.193: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-8937 PodName:var-expansion-de37e5ce-7a85-4287-a515-b598881ed166 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 04:49:42.193: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    Jan 19 04:49:42.193: INFO: ExecWithOptions: Clientset creation
    Jan 19 04:49:42.193: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/var-expansion-8937/pods/var-expansion-de37e5ce-7a85-4287-a515-b598881ed166/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 01/19/23 04:49:42.285
    Jan 19 04:49:42.289: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-8937 PodName:var-expansion-de37e5ce-7a85-4287-a515-b598881ed166 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 04:49:42.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    Jan 19 04:49:42.289: INFO: ExecWithOptions: Clientset creation
    Jan 19 04:49:42.289: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/var-expansion-8937/pods/var-expansion-de37e5ce-7a85-4287-a515-b598881ed166/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 01/19/23 04:49:42.368
    Jan 19 04:49:42.881: INFO: Successfully updated pod "var-expansion-de37e5ce-7a85-4287-a515-b598881ed166"
    STEP: waiting for annotated pod running 01/19/23 04:49:42.881
    Jan 19 04:49:42.881: INFO: Waiting up to 2m0s for pod "var-expansion-de37e5ce-7a85-4287-a515-b598881ed166" in namespace "var-expansion-8937" to be "running"
    Jan 19 04:49:42.884: INFO: Pod "var-expansion-de37e5ce-7a85-4287-a515-b598881ed166": Phase="Running", Reason="", readiness=true. Elapsed: 2.739943ms
    Jan 19 04:49:42.884: INFO: Pod "var-expansion-de37e5ce-7a85-4287-a515-b598881ed166" satisfied condition "running"
    STEP: deleting the pod gracefully 01/19/23 04:49:42.884
    Jan 19 04:49:42.884: INFO: Deleting pod "var-expansion-de37e5ce-7a85-4287-a515-b598881ed166" in namespace "var-expansion-8937"
    Jan 19 04:49:42.890: INFO: Wait up to 5m0s for pod "var-expansion-de37e5ce-7a85-4287-a515-b598881ed166" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 19 04:50:16.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-8937" for this suite. 01/19/23 04:50:16.899
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:50:16.906
Jan 19 04:50:16.906: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename replicaset 01/19/23 04:50:16.906
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:50:16.921
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:50:16.922
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 01/19/23 04:50:16.924
Jan 19 04:50:16.932: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 19 04:50:21.937: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/19/23 04:50:21.937
STEP: getting scale subresource 01/19/23 04:50:21.937
STEP: updating a scale subresource 01/19/23 04:50:21.94
STEP: verifying the replicaset Spec.Replicas was modified 01/19/23 04:50:21.947
STEP: Patch a scale subresource 01/19/23 04:50:21.95
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 19 04:50:21.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8219" for this suite. 01/19/23 04:50:21.969
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","completed":107,"skipped":2026,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.072 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:50:16.906
    Jan 19 04:50:16.906: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename replicaset 01/19/23 04:50:16.906
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:50:16.921
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:50:16.922
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 01/19/23 04:50:16.924
    Jan 19 04:50:16.932: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 19 04:50:21.937: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/19/23 04:50:21.937
    STEP: getting scale subresource 01/19/23 04:50:21.937
    STEP: updating a scale subresource 01/19/23 04:50:21.94
    STEP: verifying the replicaset Spec.Replicas was modified 01/19/23 04:50:21.947
    STEP: Patch a scale subresource 01/19/23 04:50:21.95
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 19 04:50:21.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-8219" for this suite. 01/19/23 04:50:21.969
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:50:21.978
Jan 19 04:50:21.978: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename sched-pred 01/19/23 04:50:21.979
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:50:21.999
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:50:22.001
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 19 04:50:22.003: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 19 04:50:22.008: INFO: Waiting for terminating namespaces to be deleted...
Jan 19 04:50:22.011: INFO: 
Logging pods the apiserver thinks is on node ckcp-nks-default-worker-node-0 before test
Jan 19 04:50:22.016: INFO: kube-flannel-ds-amd64-djm45 from kube-flannel started at 2023-01-19 04:04:25 +0000 UTC (1 container statuses recorded)
Jan 19 04:50:22.016: INFO: 	Container kube-flannel ready: true, restart count 0
Jan 19 04:50:22.016: INFO: coredns-557b76dc76-cg7b6 from kube-system started at 2023-01-19 04:04:25 +0000 UTC (1 container statuses recorded)
Jan 19 04:50:22.016: INFO: 	Container coredns ready: true, restart count 0
Jan 19 04:50:22.016: INFO: coredns-557b76dc76-ncmw9 from kube-system started at 2023-01-19 04:04:25 +0000 UTC (1 container statuses recorded)
Jan 19 04:50:22.016: INFO: 	Container coredns ready: true, restart count 0
Jan 19 04:50:22.016: INFO: csi-cinder-controllerplugin-0 from kube-system started at 2023-01-19 04:04:35 +0000 UTC (5 container statuses recorded)
Jan 19 04:50:22.016: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 19 04:50:22.016: INFO: 	Container csi-attacher ready: true, restart count 0
Jan 19 04:50:22.016: INFO: 	Container csi-provisioner ready: true, restart count 0
Jan 19 04:50:22.016: INFO: 	Container csi-resizer ready: true, restart count 0
Jan 19 04:50:22.016: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jan 19 04:50:22.016: INFO: csi-cinder-nodeplugin-r2d6v from kube-system started at 2023-01-19 04:04:35 +0000 UTC (2 container statuses recorded)
Jan 19 04:50:22.016: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 19 04:50:22.016: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 19 04:50:22.016: INFO: dashboard-metrics-scraper-7cc7856cfb-p9hnn from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
Jan 19 04:50:22.016: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jan 19 04:50:22.016: INFO: kube-dns-autoscaler-7986c68b66-q7lq5 from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
Jan 19 04:50:22.016: INFO: 	Container autoscaler ready: true, restart count 0
Jan 19 04:50:22.016: INFO: kubernetes-dashboard-d7b78f849-zfz2z from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
Jan 19 04:50:22.016: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jan 19 04:50:22.016: INFO: metrics-server-c494c94fc-lkqhw from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
Jan 19 04:50:22.016: INFO: 	Container metrics-server ready: true, restart count 0
Jan 19 04:50:22.016: INFO: npd-428hd from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
Jan 19 04:50:22.016: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 19 04:50:22.016: INFO: snapshot-controller-6bd8bc5484-94b94 from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
Jan 19 04:50:22.016: INFO: 	Container snapshot-controller ready: true, restart count 0
Jan 19 04:50:22.016: INFO: snapshot-controller-6bd8bc5484-nh4pv from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
Jan 19 04:50:22.016: INFO: 	Container snapshot-controller ready: true, restart count 0
Jan 19 04:50:22.016: INFO: sonobuoy-systemd-logs-daemon-set-dbd8990eaea742d4-9s2t2 from sonobuoy started at 2023-01-19 04:19:31 +0000 UTC (2 container statuses recorded)
Jan 19 04:50:22.016: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 04:50:22.016: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 19 04:50:22.016: INFO: 
Logging pods the apiserver thinks is on node ckcp-nks-default-worker-node-1 before test
Jan 19 04:50:22.020: INFO: kube-flannel-ds-amd64-jqq8g from kube-flannel started at 2023-01-19 04:04:25 +0000 UTC (1 container statuses recorded)
Jan 19 04:50:22.020: INFO: 	Container kube-flannel ready: true, restart count 0
Jan 19 04:50:22.020: INFO: csi-cinder-nodeplugin-nrrr6 from kube-system started at 2023-01-19 04:04:35 +0000 UTC (2 container statuses recorded)
Jan 19 04:50:22.020: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 19 04:50:22.020: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 19 04:50:22.020: INFO: npd-wz6bt from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
Jan 19 04:50:22.020: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 19 04:50:22.020: INFO: test-rs-dx2gv from replicaset-8219 started at 2023-01-19 04:50:16 +0000 UTC (1 container statuses recorded)
Jan 19 04:50:22.020: INFO: 	Container httpd ready: true, restart count 0
Jan 19 04:50:22.020: INFO: test-rs-k7kw5 from replicaset-8219 started at 2023-01-19 04:50:21 +0000 UTC (1 container statuses recorded)
Jan 19 04:50:22.020: INFO: 	Container httpd ready: false, restart count 0
Jan 19 04:50:22.020: INFO: sonobuoy from sonobuoy started at 2023-01-19 04:19:22 +0000 UTC (1 container statuses recorded)
Jan 19 04:50:22.020: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 19 04:50:22.020: INFO: sonobuoy-e2e-job-35712e4fe0b540a2 from sonobuoy started at 2023-01-19 04:19:31 +0000 UTC (2 container statuses recorded)
Jan 19 04:50:22.020: INFO: 	Container e2e ready: true, restart count 0
Jan 19 04:50:22.020: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 04:50:22.020: INFO: sonobuoy-systemd-logs-daemon-set-dbd8990eaea742d4-7w8p4 from sonobuoy started at 2023-01-19 04:19:31 +0000 UTC (2 container statuses recorded)
Jan 19 04:50:22.020: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 04:50:22.020: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
STEP: Trying to schedule Pod with nonempty NodeSelector. 01/19/23 04:50:22.02
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.173b9c8e6cc94eaf], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling.] 01/19/23 04:50:22.044
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jan 19 04:50:23.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-781" for this suite. 01/19/23 04:50:23.047
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","completed":108,"skipped":2034,"failed":0}
------------------------------
â€¢ [1.074 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:50:21.978
    Jan 19 04:50:21.978: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename sched-pred 01/19/23 04:50:21.979
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:50:21.999
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:50:22.001
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jan 19 04:50:22.003: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 19 04:50:22.008: INFO: Waiting for terminating namespaces to be deleted...
    Jan 19 04:50:22.011: INFO: 
    Logging pods the apiserver thinks is on node ckcp-nks-default-worker-node-0 before test
    Jan 19 04:50:22.016: INFO: kube-flannel-ds-amd64-djm45 from kube-flannel started at 2023-01-19 04:04:25 +0000 UTC (1 container statuses recorded)
    Jan 19 04:50:22.016: INFO: 	Container kube-flannel ready: true, restart count 0
    Jan 19 04:50:22.016: INFO: coredns-557b76dc76-cg7b6 from kube-system started at 2023-01-19 04:04:25 +0000 UTC (1 container statuses recorded)
    Jan 19 04:50:22.016: INFO: 	Container coredns ready: true, restart count 0
    Jan 19 04:50:22.016: INFO: coredns-557b76dc76-ncmw9 from kube-system started at 2023-01-19 04:04:25 +0000 UTC (1 container statuses recorded)
    Jan 19 04:50:22.016: INFO: 	Container coredns ready: true, restart count 0
    Jan 19 04:50:22.016: INFO: csi-cinder-controllerplugin-0 from kube-system started at 2023-01-19 04:04:35 +0000 UTC (5 container statuses recorded)
    Jan 19 04:50:22.016: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Jan 19 04:50:22.016: INFO: 	Container csi-attacher ready: true, restart count 0
    Jan 19 04:50:22.016: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jan 19 04:50:22.016: INFO: 	Container csi-resizer ready: true, restart count 0
    Jan 19 04:50:22.016: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jan 19 04:50:22.016: INFO: csi-cinder-nodeplugin-r2d6v from kube-system started at 2023-01-19 04:04:35 +0000 UTC (2 container statuses recorded)
    Jan 19 04:50:22.016: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Jan 19 04:50:22.016: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jan 19 04:50:22.016: INFO: dashboard-metrics-scraper-7cc7856cfb-p9hnn from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
    Jan 19 04:50:22.016: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    Jan 19 04:50:22.016: INFO: kube-dns-autoscaler-7986c68b66-q7lq5 from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
    Jan 19 04:50:22.016: INFO: 	Container autoscaler ready: true, restart count 0
    Jan 19 04:50:22.016: INFO: kubernetes-dashboard-d7b78f849-zfz2z from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
    Jan 19 04:50:22.016: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    Jan 19 04:50:22.016: INFO: metrics-server-c494c94fc-lkqhw from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
    Jan 19 04:50:22.016: INFO: 	Container metrics-server ready: true, restart count 0
    Jan 19 04:50:22.016: INFO: npd-428hd from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
    Jan 19 04:50:22.016: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan 19 04:50:22.016: INFO: snapshot-controller-6bd8bc5484-94b94 from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
    Jan 19 04:50:22.016: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jan 19 04:50:22.016: INFO: snapshot-controller-6bd8bc5484-nh4pv from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
    Jan 19 04:50:22.016: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jan 19 04:50:22.016: INFO: sonobuoy-systemd-logs-daemon-set-dbd8990eaea742d4-9s2t2 from sonobuoy started at 2023-01-19 04:19:31 +0000 UTC (2 container statuses recorded)
    Jan 19 04:50:22.016: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 19 04:50:22.016: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 19 04:50:22.016: INFO: 
    Logging pods the apiserver thinks is on node ckcp-nks-default-worker-node-1 before test
    Jan 19 04:50:22.020: INFO: kube-flannel-ds-amd64-jqq8g from kube-flannel started at 2023-01-19 04:04:25 +0000 UTC (1 container statuses recorded)
    Jan 19 04:50:22.020: INFO: 	Container kube-flannel ready: true, restart count 0
    Jan 19 04:50:22.020: INFO: csi-cinder-nodeplugin-nrrr6 from kube-system started at 2023-01-19 04:04:35 +0000 UTC (2 container statuses recorded)
    Jan 19 04:50:22.020: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Jan 19 04:50:22.020: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jan 19 04:50:22.020: INFO: npd-wz6bt from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
    Jan 19 04:50:22.020: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan 19 04:50:22.020: INFO: test-rs-dx2gv from replicaset-8219 started at 2023-01-19 04:50:16 +0000 UTC (1 container statuses recorded)
    Jan 19 04:50:22.020: INFO: 	Container httpd ready: true, restart count 0
    Jan 19 04:50:22.020: INFO: test-rs-k7kw5 from replicaset-8219 started at 2023-01-19 04:50:21 +0000 UTC (1 container statuses recorded)
    Jan 19 04:50:22.020: INFO: 	Container httpd ready: false, restart count 0
    Jan 19 04:50:22.020: INFO: sonobuoy from sonobuoy started at 2023-01-19 04:19:22 +0000 UTC (1 container statuses recorded)
    Jan 19 04:50:22.020: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 19 04:50:22.020: INFO: sonobuoy-e2e-job-35712e4fe0b540a2 from sonobuoy started at 2023-01-19 04:19:31 +0000 UTC (2 container statuses recorded)
    Jan 19 04:50:22.020: INFO: 	Container e2e ready: true, restart count 0
    Jan 19 04:50:22.020: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 19 04:50:22.020: INFO: sonobuoy-systemd-logs-daemon-set-dbd8990eaea742d4-7w8p4 from sonobuoy started at 2023-01-19 04:19:31 +0000 UTC (2 container statuses recorded)
    Jan 19 04:50:22.020: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 19 04:50:22.020: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:438
    STEP: Trying to schedule Pod with nonempty NodeSelector. 01/19/23 04:50:22.02
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.173b9c8e6cc94eaf], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling.] 01/19/23 04:50:22.044
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jan 19 04:50:23.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-781" for this suite. 01/19/23 04:50:23.047
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:50:23.052
Jan 19 04:50:23.052: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename services 01/19/23 04:50:23.053
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:50:23.068
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:50:23.07
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
STEP: creating a collection of services 01/19/23 04:50:23.072
Jan 19 04:50:23.072: INFO: Creating e2e-svc-a-b6xnb
Jan 19 04:50:23.081: INFO: Creating e2e-svc-b-6mxjf
Jan 19 04:50:23.091: INFO: Creating e2e-svc-c-kgnwj
STEP: deleting service collection 01/19/23 04:50:23.104
Jan 19 04:50:23.132: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 19 04:50:23.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8064" for this suite. 01/19/23 04:50:23.136
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","completed":109,"skipped":2035,"failed":0}
------------------------------
â€¢ [0.089 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:50:23.052
    Jan 19 04:50:23.052: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename services 01/19/23 04:50:23.053
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:50:23.068
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:50:23.07
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3641
    STEP: creating a collection of services 01/19/23 04:50:23.072
    Jan 19 04:50:23.072: INFO: Creating e2e-svc-a-b6xnb
    Jan 19 04:50:23.081: INFO: Creating e2e-svc-b-6mxjf
    Jan 19 04:50:23.091: INFO: Creating e2e-svc-c-kgnwj
    STEP: deleting service collection 01/19/23 04:50:23.104
    Jan 19 04:50:23.132: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 19 04:50:23.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8064" for this suite. 01/19/23 04:50:23.136
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:50:23.142
Jan 19 04:50:23.142: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename emptydir 01/19/23 04:50:23.143
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:50:23.156
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:50:23.158
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
STEP: Creating a pod to test emptydir 0777 on node default medium 01/19/23 04:50:23.16
Jan 19 04:50:23.167: INFO: Waiting up to 5m0s for pod "pod-ccb27019-28c7-4fab-86e9-2af9092093f2" in namespace "emptydir-8807" to be "Succeeded or Failed"
Jan 19 04:50:23.170: INFO: Pod "pod-ccb27019-28c7-4fab-86e9-2af9092093f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.95755ms
Jan 19 04:50:25.177: INFO: Pod "pod-ccb27019-28c7-4fab-86e9-2af9092093f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00976911s
Jan 19 04:50:27.176: INFO: Pod "pod-ccb27019-28c7-4fab-86e9-2af9092093f2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008373159s
Jan 19 04:50:29.174: INFO: Pod "pod-ccb27019-28c7-4fab-86e9-2af9092093f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006437823s
STEP: Saw pod success 01/19/23 04:50:29.174
Jan 19 04:50:29.174: INFO: Pod "pod-ccb27019-28c7-4fab-86e9-2af9092093f2" satisfied condition "Succeeded or Failed"
Jan 19 04:50:29.177: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-ccb27019-28c7-4fab-86e9-2af9092093f2 container test-container: <nil>
STEP: delete the pod 01/19/23 04:50:29.181
Jan 19 04:50:29.193: INFO: Waiting for pod pod-ccb27019-28c7-4fab-86e9-2af9092093f2 to disappear
Jan 19 04:50:29.195: INFO: Pod pod-ccb27019-28c7-4fab-86e9-2af9092093f2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 19 04:50:29.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8807" for this suite. 01/19/23 04:50:29.198
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":110,"skipped":2077,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.063 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:50:23.142
    Jan 19 04:50:23.142: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename emptydir 01/19/23 04:50:23.143
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:50:23.156
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:50:23.158
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:186
    STEP: Creating a pod to test emptydir 0777 on node default medium 01/19/23 04:50:23.16
    Jan 19 04:50:23.167: INFO: Waiting up to 5m0s for pod "pod-ccb27019-28c7-4fab-86e9-2af9092093f2" in namespace "emptydir-8807" to be "Succeeded or Failed"
    Jan 19 04:50:23.170: INFO: Pod "pod-ccb27019-28c7-4fab-86e9-2af9092093f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.95755ms
    Jan 19 04:50:25.177: INFO: Pod "pod-ccb27019-28c7-4fab-86e9-2af9092093f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00976911s
    Jan 19 04:50:27.176: INFO: Pod "pod-ccb27019-28c7-4fab-86e9-2af9092093f2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008373159s
    Jan 19 04:50:29.174: INFO: Pod "pod-ccb27019-28c7-4fab-86e9-2af9092093f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006437823s
    STEP: Saw pod success 01/19/23 04:50:29.174
    Jan 19 04:50:29.174: INFO: Pod "pod-ccb27019-28c7-4fab-86e9-2af9092093f2" satisfied condition "Succeeded or Failed"
    Jan 19 04:50:29.177: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-ccb27019-28c7-4fab-86e9-2af9092093f2 container test-container: <nil>
    STEP: delete the pod 01/19/23 04:50:29.181
    Jan 19 04:50:29.193: INFO: Waiting for pod pod-ccb27019-28c7-4fab-86e9-2af9092093f2 to disappear
    Jan 19 04:50:29.195: INFO: Pod pod-ccb27019-28c7-4fab-86e9-2af9092093f2 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 19 04:50:29.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8807" for this suite. 01/19/23 04:50:29.198
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:50:29.205
Jan 19 04:50:29.205: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename configmap 01/19/23 04:50:29.206
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:50:29.219
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:50:29.221
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
STEP: Creating configMap with name configmap-test-volume-56df7787-9b0c-4c0b-8b68-2626c5f47f14 01/19/23 04:50:29.224
STEP: Creating a pod to test consume configMaps 01/19/23 04:50:29.229
Jan 19 04:50:29.238: INFO: Waiting up to 5m0s for pod "pod-configmaps-647f6489-5cb2-44e7-bfc5-98447fda308f" in namespace "configmap-5909" to be "Succeeded or Failed"
Jan 19 04:50:29.241: INFO: Pod "pod-configmaps-647f6489-5cb2-44e7-bfc5-98447fda308f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.525244ms
Jan 19 04:50:31.245: INFO: Pod "pod-configmaps-647f6489-5cb2-44e7-bfc5-98447fda308f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007365398s
Jan 19 04:50:33.246: INFO: Pod "pod-configmaps-647f6489-5cb2-44e7-bfc5-98447fda308f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008514859s
Jan 19 04:50:35.246: INFO: Pod "pod-configmaps-647f6489-5cb2-44e7-bfc5-98447fda308f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008726283s
STEP: Saw pod success 01/19/23 04:50:35.246
Jan 19 04:50:35.247: INFO: Pod "pod-configmaps-647f6489-5cb2-44e7-bfc5-98447fda308f" satisfied condition "Succeeded or Failed"
Jan 19 04:50:35.250: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-configmaps-647f6489-5cb2-44e7-bfc5-98447fda308f container agnhost-container: <nil>
STEP: delete the pod 01/19/23 04:50:35.256
Jan 19 04:50:35.267: INFO: Waiting for pod pod-configmaps-647f6489-5cb2-44e7-bfc5-98447fda308f to disappear
Jan 19 04:50:35.269: INFO: Pod pod-configmaps-647f6489-5cb2-44e7-bfc5-98447fda308f no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 19 04:50:35.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5909" for this suite. 01/19/23 04:50:35.272
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":111,"skipped":2102,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.071 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:50:29.205
    Jan 19 04:50:29.205: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename configmap 01/19/23 04:50:29.206
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:50:29.219
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:50:29.221
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:46
    STEP: Creating configMap with name configmap-test-volume-56df7787-9b0c-4c0b-8b68-2626c5f47f14 01/19/23 04:50:29.224
    STEP: Creating a pod to test consume configMaps 01/19/23 04:50:29.229
    Jan 19 04:50:29.238: INFO: Waiting up to 5m0s for pod "pod-configmaps-647f6489-5cb2-44e7-bfc5-98447fda308f" in namespace "configmap-5909" to be "Succeeded or Failed"
    Jan 19 04:50:29.241: INFO: Pod "pod-configmaps-647f6489-5cb2-44e7-bfc5-98447fda308f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.525244ms
    Jan 19 04:50:31.245: INFO: Pod "pod-configmaps-647f6489-5cb2-44e7-bfc5-98447fda308f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007365398s
    Jan 19 04:50:33.246: INFO: Pod "pod-configmaps-647f6489-5cb2-44e7-bfc5-98447fda308f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008514859s
    Jan 19 04:50:35.246: INFO: Pod "pod-configmaps-647f6489-5cb2-44e7-bfc5-98447fda308f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008726283s
    STEP: Saw pod success 01/19/23 04:50:35.246
    Jan 19 04:50:35.247: INFO: Pod "pod-configmaps-647f6489-5cb2-44e7-bfc5-98447fda308f" satisfied condition "Succeeded or Failed"
    Jan 19 04:50:35.250: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-configmaps-647f6489-5cb2-44e7-bfc5-98447fda308f container agnhost-container: <nil>
    STEP: delete the pod 01/19/23 04:50:35.256
    Jan 19 04:50:35.267: INFO: Waiting for pod pod-configmaps-647f6489-5cb2-44e7-bfc5-98447fda308f to disappear
    Jan 19 04:50:35.269: INFO: Pod pod-configmaps-647f6489-5cb2-44e7-bfc5-98447fda308f no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 19 04:50:35.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-5909" for this suite. 01/19/23 04:50:35.272
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:50:35.277
Jan 19 04:50:35.277: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename emptydir 01/19/23 04:50:35.278
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:50:35.292
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:50:35.294
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
STEP: Creating a pod to test emptydir 0777 on node default medium 01/19/23 04:50:35.295
Jan 19 04:50:35.303: INFO: Waiting up to 5m0s for pod "pod-ea508efe-f1de-4ead-ac3f-1e5023b2d37e" in namespace "emptydir-45" to be "Succeeded or Failed"
Jan 19 04:50:35.307: INFO: Pod "pod-ea508efe-f1de-4ead-ac3f-1e5023b2d37e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.259747ms
Jan 19 04:50:37.310: INFO: Pod "pod-ea508efe-f1de-4ead-ac3f-1e5023b2d37e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007401008s
Jan 19 04:50:39.311: INFO: Pod "pod-ea508efe-f1de-4ead-ac3f-1e5023b2d37e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008579011s
Jan 19 04:50:41.312: INFO: Pod "pod-ea508efe-f1de-4ead-ac3f-1e5023b2d37e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009352819s
STEP: Saw pod success 01/19/23 04:50:41.312
Jan 19 04:50:41.312: INFO: Pod "pod-ea508efe-f1de-4ead-ac3f-1e5023b2d37e" satisfied condition "Succeeded or Failed"
Jan 19 04:50:41.315: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-ea508efe-f1de-4ead-ac3f-1e5023b2d37e container test-container: <nil>
STEP: delete the pod 01/19/23 04:50:41.319
Jan 19 04:50:41.331: INFO: Waiting for pod pod-ea508efe-f1de-4ead-ac3f-1e5023b2d37e to disappear
Jan 19 04:50:41.333: INFO: Pod pod-ea508efe-f1de-4ead-ac3f-1e5023b2d37e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 19 04:50:41.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-45" for this suite. 01/19/23 04:50:41.348
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":112,"skipped":2107,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.077 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:50:35.277
    Jan 19 04:50:35.277: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename emptydir 01/19/23 04:50:35.278
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:50:35.292
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:50:35.294
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:216
    STEP: Creating a pod to test emptydir 0777 on node default medium 01/19/23 04:50:35.295
    Jan 19 04:50:35.303: INFO: Waiting up to 5m0s for pod "pod-ea508efe-f1de-4ead-ac3f-1e5023b2d37e" in namespace "emptydir-45" to be "Succeeded or Failed"
    Jan 19 04:50:35.307: INFO: Pod "pod-ea508efe-f1de-4ead-ac3f-1e5023b2d37e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.259747ms
    Jan 19 04:50:37.310: INFO: Pod "pod-ea508efe-f1de-4ead-ac3f-1e5023b2d37e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007401008s
    Jan 19 04:50:39.311: INFO: Pod "pod-ea508efe-f1de-4ead-ac3f-1e5023b2d37e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008579011s
    Jan 19 04:50:41.312: INFO: Pod "pod-ea508efe-f1de-4ead-ac3f-1e5023b2d37e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009352819s
    STEP: Saw pod success 01/19/23 04:50:41.312
    Jan 19 04:50:41.312: INFO: Pod "pod-ea508efe-f1de-4ead-ac3f-1e5023b2d37e" satisfied condition "Succeeded or Failed"
    Jan 19 04:50:41.315: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-ea508efe-f1de-4ead-ac3f-1e5023b2d37e container test-container: <nil>
    STEP: delete the pod 01/19/23 04:50:41.319
    Jan 19 04:50:41.331: INFO: Waiting for pod pod-ea508efe-f1de-4ead-ac3f-1e5023b2d37e to disappear
    Jan 19 04:50:41.333: INFO: Pod pod-ea508efe-f1de-4ead-ac3f-1e5023b2d37e no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 19 04:50:41.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-45" for this suite. 01/19/23 04:50:41.348
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:50:41.355
Jan 19 04:50:41.355: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename downward-api 01/19/23 04:50:41.355
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:50:41.37
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:50:41.372
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
STEP: Creating a pod to test downward API volume plugin 01/19/23 04:50:41.374
Jan 19 04:50:41.382: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9bf55edc-a453-4490-aede-5388153ed114" in namespace "downward-api-7608" to be "Succeeded or Failed"
Jan 19 04:50:41.385: INFO: Pod "downwardapi-volume-9bf55edc-a453-4490-aede-5388153ed114": Phase="Pending", Reason="", readiness=false. Elapsed: 3.130612ms
Jan 19 04:50:43.389: INFO: Pod "downwardapi-volume-9bf55edc-a453-4490-aede-5388153ed114": Phase="Running", Reason="", readiness=true. Elapsed: 2.006786167s
Jan 19 04:50:45.391: INFO: Pod "downwardapi-volume-9bf55edc-a453-4490-aede-5388153ed114": Phase="Running", Reason="", readiness=false. Elapsed: 4.008621018s
Jan 19 04:50:47.389: INFO: Pod "downwardapi-volume-9bf55edc-a453-4490-aede-5388153ed114": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006666837s
STEP: Saw pod success 01/19/23 04:50:47.389
Jan 19 04:50:47.389: INFO: Pod "downwardapi-volume-9bf55edc-a453-4490-aede-5388153ed114" satisfied condition "Succeeded or Failed"
Jan 19 04:50:47.391: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod downwardapi-volume-9bf55edc-a453-4490-aede-5388153ed114 container client-container: <nil>
STEP: delete the pod 01/19/23 04:50:47.397
Jan 19 04:50:47.406: INFO: Waiting for pod downwardapi-volume-9bf55edc-a453-4490-aede-5388153ed114 to disappear
Jan 19 04:50:47.408: INFO: Pod downwardapi-volume-9bf55edc-a453-4490-aede-5388153ed114 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 19 04:50:47.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7608" for this suite. 01/19/23 04:50:47.411
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":113,"skipped":2113,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.061 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:50:41.355
    Jan 19 04:50:41.355: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename downward-api 01/19/23 04:50:41.355
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:50:41.37
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:50:41.372
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:83
    STEP: Creating a pod to test downward API volume plugin 01/19/23 04:50:41.374
    Jan 19 04:50:41.382: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9bf55edc-a453-4490-aede-5388153ed114" in namespace "downward-api-7608" to be "Succeeded or Failed"
    Jan 19 04:50:41.385: INFO: Pod "downwardapi-volume-9bf55edc-a453-4490-aede-5388153ed114": Phase="Pending", Reason="", readiness=false. Elapsed: 3.130612ms
    Jan 19 04:50:43.389: INFO: Pod "downwardapi-volume-9bf55edc-a453-4490-aede-5388153ed114": Phase="Running", Reason="", readiness=true. Elapsed: 2.006786167s
    Jan 19 04:50:45.391: INFO: Pod "downwardapi-volume-9bf55edc-a453-4490-aede-5388153ed114": Phase="Running", Reason="", readiness=false. Elapsed: 4.008621018s
    Jan 19 04:50:47.389: INFO: Pod "downwardapi-volume-9bf55edc-a453-4490-aede-5388153ed114": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006666837s
    STEP: Saw pod success 01/19/23 04:50:47.389
    Jan 19 04:50:47.389: INFO: Pod "downwardapi-volume-9bf55edc-a453-4490-aede-5388153ed114" satisfied condition "Succeeded or Failed"
    Jan 19 04:50:47.391: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod downwardapi-volume-9bf55edc-a453-4490-aede-5388153ed114 container client-container: <nil>
    STEP: delete the pod 01/19/23 04:50:47.397
    Jan 19 04:50:47.406: INFO: Waiting for pod downwardapi-volume-9bf55edc-a453-4490-aede-5388153ed114 to disappear
    Jan 19 04:50:47.408: INFO: Pod downwardapi-volume-9bf55edc-a453-4490-aede-5388153ed114 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 19 04:50:47.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7608" for this suite. 01/19/23 04:50:47.411
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:50:47.417
Jan 19 04:50:47.417: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename configmap 01/19/23 04:50:47.417
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:50:47.431
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:50:47.433
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
STEP: Creating configMap with name configmap-test-volume-map-c747f1e2-ff06-4a25-8a68-f3f85388ef7d 01/19/23 04:50:47.434
STEP: Creating a pod to test consume configMaps 01/19/23 04:50:47.442
Jan 19 04:50:47.453: INFO: Waiting up to 5m0s for pod "pod-configmaps-0153fa89-4d74-44bb-89c8-c21470304744" in namespace "configmap-2764" to be "Succeeded or Failed"
Jan 19 04:50:47.462: INFO: Pod "pod-configmaps-0153fa89-4d74-44bb-89c8-c21470304744": Phase="Pending", Reason="", readiness=false. Elapsed: 8.893125ms
Jan 19 04:50:49.466: INFO: Pod "pod-configmaps-0153fa89-4d74-44bb-89c8-c21470304744": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012989089s
Jan 19 04:50:51.466: INFO: Pod "pod-configmaps-0153fa89-4d74-44bb-89c8-c21470304744": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012623225s
Jan 19 04:50:53.466: INFO: Pod "pod-configmaps-0153fa89-4d74-44bb-89c8-c21470304744": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013038012s
STEP: Saw pod success 01/19/23 04:50:53.466
Jan 19 04:50:53.466: INFO: Pod "pod-configmaps-0153fa89-4d74-44bb-89c8-c21470304744" satisfied condition "Succeeded or Failed"
Jan 19 04:50:53.470: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-configmaps-0153fa89-4d74-44bb-89c8-c21470304744 container agnhost-container: <nil>
STEP: delete the pod 01/19/23 04:50:53.475
Jan 19 04:50:53.491: INFO: Waiting for pod pod-configmaps-0153fa89-4d74-44bb-89c8-c21470304744 to disappear
Jan 19 04:50:53.494: INFO: Pod pod-configmaps-0153fa89-4d74-44bb-89c8-c21470304744 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 19 04:50:53.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2764" for this suite. 01/19/23 04:50:53.497
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":114,"skipped":2137,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.087 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:50:47.417
    Jan 19 04:50:47.417: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename configmap 01/19/23 04:50:47.417
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:50:47.431
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:50:47.433
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:98
    STEP: Creating configMap with name configmap-test-volume-map-c747f1e2-ff06-4a25-8a68-f3f85388ef7d 01/19/23 04:50:47.434
    STEP: Creating a pod to test consume configMaps 01/19/23 04:50:47.442
    Jan 19 04:50:47.453: INFO: Waiting up to 5m0s for pod "pod-configmaps-0153fa89-4d74-44bb-89c8-c21470304744" in namespace "configmap-2764" to be "Succeeded or Failed"
    Jan 19 04:50:47.462: INFO: Pod "pod-configmaps-0153fa89-4d74-44bb-89c8-c21470304744": Phase="Pending", Reason="", readiness=false. Elapsed: 8.893125ms
    Jan 19 04:50:49.466: INFO: Pod "pod-configmaps-0153fa89-4d74-44bb-89c8-c21470304744": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012989089s
    Jan 19 04:50:51.466: INFO: Pod "pod-configmaps-0153fa89-4d74-44bb-89c8-c21470304744": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012623225s
    Jan 19 04:50:53.466: INFO: Pod "pod-configmaps-0153fa89-4d74-44bb-89c8-c21470304744": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013038012s
    STEP: Saw pod success 01/19/23 04:50:53.466
    Jan 19 04:50:53.466: INFO: Pod "pod-configmaps-0153fa89-4d74-44bb-89c8-c21470304744" satisfied condition "Succeeded or Failed"
    Jan 19 04:50:53.470: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-configmaps-0153fa89-4d74-44bb-89c8-c21470304744 container agnhost-container: <nil>
    STEP: delete the pod 01/19/23 04:50:53.475
    Jan 19 04:50:53.491: INFO: Waiting for pod pod-configmaps-0153fa89-4d74-44bb-89c8-c21470304744 to disappear
    Jan 19 04:50:53.494: INFO: Pod pod-configmaps-0153fa89-4d74-44bb-89c8-c21470304744 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 19 04:50:53.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2764" for this suite. 01/19/23 04:50:53.497
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:50:53.504
Jan 19 04:50:53.504: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename emptydir 01/19/23 04:50:53.505
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:50:53.521
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:50:53.525
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
STEP: Creating a pod to test emptydir 0666 on node default medium 01/19/23 04:50:53.527
Jan 19 04:50:53.546: INFO: Waiting up to 5m0s for pod "pod-f99ae5c7-71c5-4562-a18e-2c180794ff70" in namespace "emptydir-381" to be "Succeeded or Failed"
Jan 19 04:50:53.551: INFO: Pod "pod-f99ae5c7-71c5-4562-a18e-2c180794ff70": Phase="Pending", Reason="", readiness=false. Elapsed: 5.149828ms
Jan 19 04:50:55.556: INFO: Pod "pod-f99ae5c7-71c5-4562-a18e-2c180794ff70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010182643s
Jan 19 04:50:57.555: INFO: Pod "pod-f99ae5c7-71c5-4562-a18e-2c180794ff70": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008911189s
Jan 19 04:50:59.556: INFO: Pod "pod-f99ae5c7-71c5-4562-a18e-2c180794ff70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009269382s
STEP: Saw pod success 01/19/23 04:50:59.556
Jan 19 04:50:59.556: INFO: Pod "pod-f99ae5c7-71c5-4562-a18e-2c180794ff70" satisfied condition "Succeeded or Failed"
Jan 19 04:50:59.558: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-f99ae5c7-71c5-4562-a18e-2c180794ff70 container test-container: <nil>
STEP: delete the pod 01/19/23 04:50:59.565
Jan 19 04:50:59.576: INFO: Waiting for pod pod-f99ae5c7-71c5-4562-a18e-2c180794ff70 to disappear
Jan 19 04:50:59.578: INFO: Pod pod-f99ae5c7-71c5-4562-a18e-2c180794ff70 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 19 04:50:59.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-381" for this suite. 01/19/23 04:50:59.581
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":115,"skipped":2142,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.083 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:50:53.504
    Jan 19 04:50:53.504: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename emptydir 01/19/23 04:50:53.505
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:50:53.521
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:50:53.525
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:176
    STEP: Creating a pod to test emptydir 0666 on node default medium 01/19/23 04:50:53.527
    Jan 19 04:50:53.546: INFO: Waiting up to 5m0s for pod "pod-f99ae5c7-71c5-4562-a18e-2c180794ff70" in namespace "emptydir-381" to be "Succeeded or Failed"
    Jan 19 04:50:53.551: INFO: Pod "pod-f99ae5c7-71c5-4562-a18e-2c180794ff70": Phase="Pending", Reason="", readiness=false. Elapsed: 5.149828ms
    Jan 19 04:50:55.556: INFO: Pod "pod-f99ae5c7-71c5-4562-a18e-2c180794ff70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010182643s
    Jan 19 04:50:57.555: INFO: Pod "pod-f99ae5c7-71c5-4562-a18e-2c180794ff70": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008911189s
    Jan 19 04:50:59.556: INFO: Pod "pod-f99ae5c7-71c5-4562-a18e-2c180794ff70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009269382s
    STEP: Saw pod success 01/19/23 04:50:59.556
    Jan 19 04:50:59.556: INFO: Pod "pod-f99ae5c7-71c5-4562-a18e-2c180794ff70" satisfied condition "Succeeded or Failed"
    Jan 19 04:50:59.558: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-f99ae5c7-71c5-4562-a18e-2c180794ff70 container test-container: <nil>
    STEP: delete the pod 01/19/23 04:50:59.565
    Jan 19 04:50:59.576: INFO: Waiting for pod pod-f99ae5c7-71c5-4562-a18e-2c180794ff70 to disappear
    Jan 19 04:50:59.578: INFO: Pod pod-f99ae5c7-71c5-4562-a18e-2c180794ff70 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 19 04:50:59.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-381" for this suite. 01/19/23 04:50:59.581
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:50:59.587
Jan 19 04:50:59.587: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename security-context-test 01/19/23 04:50:59.588
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:50:59.603
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:50:59.604
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
Jan 19 04:50:59.619: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-4a434dc1-0d56-4177-895d-50fdc0f7e937" in namespace "security-context-test-1719" to be "Succeeded or Failed"
Jan 19 04:50:59.627: INFO: Pod "busybox-readonly-false-4a434dc1-0d56-4177-895d-50fdc0f7e937": Phase="Pending", Reason="", readiness=false. Elapsed: 8.388302ms
Jan 19 04:51:01.632: INFO: Pod "busybox-readonly-false-4a434dc1-0d56-4177-895d-50fdc0f7e937": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013071856s
Jan 19 04:51:03.631: INFO: Pod "busybox-readonly-false-4a434dc1-0d56-4177-895d-50fdc0f7e937": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011843749s
Jan 19 04:51:05.633: INFO: Pod "busybox-readonly-false-4a434dc1-0d56-4177-895d-50fdc0f7e937": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013752777s
Jan 19 04:51:05.633: INFO: Pod "busybox-readonly-false-4a434dc1-0d56-4177-895d-50fdc0f7e937" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 19 04:51:05.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1719" for this suite. 01/19/23 04:51:05.639
{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","completed":116,"skipped":2152,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.059 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:429
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:485

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:50:59.587
    Jan 19 04:50:59.587: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename security-context-test 01/19/23 04:50:59.588
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:50:59.603
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:50:59.604
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:485
    Jan 19 04:50:59.619: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-4a434dc1-0d56-4177-895d-50fdc0f7e937" in namespace "security-context-test-1719" to be "Succeeded or Failed"
    Jan 19 04:50:59.627: INFO: Pod "busybox-readonly-false-4a434dc1-0d56-4177-895d-50fdc0f7e937": Phase="Pending", Reason="", readiness=false. Elapsed: 8.388302ms
    Jan 19 04:51:01.632: INFO: Pod "busybox-readonly-false-4a434dc1-0d56-4177-895d-50fdc0f7e937": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013071856s
    Jan 19 04:51:03.631: INFO: Pod "busybox-readonly-false-4a434dc1-0d56-4177-895d-50fdc0f7e937": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011843749s
    Jan 19 04:51:05.633: INFO: Pod "busybox-readonly-false-4a434dc1-0d56-4177-895d-50fdc0f7e937": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013752777s
    Jan 19 04:51:05.633: INFO: Pod "busybox-readonly-false-4a434dc1-0d56-4177-895d-50fdc0f7e937" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 19 04:51:05.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-1719" for this suite. 01/19/23 04:51:05.639
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:51:05.646
Jan 19 04:51:05.647: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename disruption 01/19/23 04:51:05.647
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:51:05.662
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:51:05.665
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
STEP: Waiting for the pdb to be processed 01/19/23 04:51:05.672
STEP: Updating PodDisruptionBudget status 01/19/23 04:51:07.681
STEP: Waiting for all pods to be running 01/19/23 04:51:07.689
Jan 19 04:51:07.693: INFO: running pods: 0 < 1
STEP: locating a running pod 01/19/23 04:51:09.698
STEP: Waiting for the pdb to be processed 01/19/23 04:51:09.709
STEP: Patching PodDisruptionBudget status 01/19/23 04:51:09.716
STEP: Waiting for the pdb to be processed 01/19/23 04:51:09.724
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan 19 04:51:09.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1837" for this suite. 01/19/23 04:51:09.73
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","completed":117,"skipped":2157,"failed":0}
------------------------------
â€¢ [4.088 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:51:05.646
    Jan 19 04:51:05.647: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename disruption 01/19/23 04:51:05.647
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:51:05.662
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:51:05.665
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:163
    STEP: Waiting for the pdb to be processed 01/19/23 04:51:05.672
    STEP: Updating PodDisruptionBudget status 01/19/23 04:51:07.681
    STEP: Waiting for all pods to be running 01/19/23 04:51:07.689
    Jan 19 04:51:07.693: INFO: running pods: 0 < 1
    STEP: locating a running pod 01/19/23 04:51:09.698
    STEP: Waiting for the pdb to be processed 01/19/23 04:51:09.709
    STEP: Patching PodDisruptionBudget status 01/19/23 04:51:09.716
    STEP: Waiting for the pdb to be processed 01/19/23 04:51:09.724
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan 19 04:51:09.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-1837" for this suite. 01/19/23 04:51:09.73
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:51:09.735
Jan 19 04:51:09.735: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename projected 01/19/23 04:51:09.736
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:51:09.749
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:51:09.751
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
STEP: Creating configMap with name projected-configmap-test-volume-ade8cdd8-a89b-43b1-9080-055c50cd1396 01/19/23 04:51:09.754
STEP: Creating a pod to test consume configMaps 01/19/23 04:51:09.758
Jan 19 04:51:09.767: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1bff05d9-18ba-4210-95bc-e89e1dda209b" in namespace "projected-1658" to be "Succeeded or Failed"
Jan 19 04:51:09.771: INFO: Pod "pod-projected-configmaps-1bff05d9-18ba-4210-95bc-e89e1dda209b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.673864ms
Jan 19 04:51:11.775: INFO: Pod "pod-projected-configmaps-1bff05d9-18ba-4210-95bc-e89e1dda209b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008344521s
Jan 19 04:51:13.776: INFO: Pod "pod-projected-configmaps-1bff05d9-18ba-4210-95bc-e89e1dda209b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009545247s
Jan 19 04:51:15.776: INFO: Pod "pod-projected-configmaps-1bff05d9-18ba-4210-95bc-e89e1dda209b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009582903s
STEP: Saw pod success 01/19/23 04:51:15.776
Jan 19 04:51:15.776: INFO: Pod "pod-projected-configmaps-1bff05d9-18ba-4210-95bc-e89e1dda209b" satisfied condition "Succeeded or Failed"
Jan 19 04:51:15.779: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-projected-configmaps-1bff05d9-18ba-4210-95bc-e89e1dda209b container projected-configmap-volume-test: <nil>
STEP: delete the pod 01/19/23 04:51:15.784
Jan 19 04:51:15.796: INFO: Waiting for pod pod-projected-configmaps-1bff05d9-18ba-4210-95bc-e89e1dda209b to disappear
Jan 19 04:51:15.802: INFO: Pod pod-projected-configmaps-1bff05d9-18ba-4210-95bc-e89e1dda209b no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 19 04:51:15.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1658" for this suite. 01/19/23 04:51:15.805
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":118,"skipped":2177,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.075 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:51:09.735
    Jan 19 04:51:09.735: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename projected 01/19/23 04:51:09.736
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:51:09.749
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:51:09.751
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:374
    STEP: Creating configMap with name projected-configmap-test-volume-ade8cdd8-a89b-43b1-9080-055c50cd1396 01/19/23 04:51:09.754
    STEP: Creating a pod to test consume configMaps 01/19/23 04:51:09.758
    Jan 19 04:51:09.767: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1bff05d9-18ba-4210-95bc-e89e1dda209b" in namespace "projected-1658" to be "Succeeded or Failed"
    Jan 19 04:51:09.771: INFO: Pod "pod-projected-configmaps-1bff05d9-18ba-4210-95bc-e89e1dda209b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.673864ms
    Jan 19 04:51:11.775: INFO: Pod "pod-projected-configmaps-1bff05d9-18ba-4210-95bc-e89e1dda209b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008344521s
    Jan 19 04:51:13.776: INFO: Pod "pod-projected-configmaps-1bff05d9-18ba-4210-95bc-e89e1dda209b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009545247s
    Jan 19 04:51:15.776: INFO: Pod "pod-projected-configmaps-1bff05d9-18ba-4210-95bc-e89e1dda209b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009582903s
    STEP: Saw pod success 01/19/23 04:51:15.776
    Jan 19 04:51:15.776: INFO: Pod "pod-projected-configmaps-1bff05d9-18ba-4210-95bc-e89e1dda209b" satisfied condition "Succeeded or Failed"
    Jan 19 04:51:15.779: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-projected-configmaps-1bff05d9-18ba-4210-95bc-e89e1dda209b container projected-configmap-volume-test: <nil>
    STEP: delete the pod 01/19/23 04:51:15.784
    Jan 19 04:51:15.796: INFO: Waiting for pod pod-projected-configmaps-1bff05d9-18ba-4210-95bc-e89e1dda209b to disappear
    Jan 19 04:51:15.802: INFO: Pod pod-projected-configmaps-1bff05d9-18ba-4210-95bc-e89e1dda209b no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 19 04:51:15.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1658" for this suite. 01/19/23 04:51:15.805
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:51:15.81
Jan 19 04:51:15.810: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename pods 01/19/23 04:51:15.811
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:51:15.826
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:51:15.828
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
Jan 19 04:51:15.830: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: creating the pod 01/19/23 04:51:15.831
STEP: submitting the pod to kubernetes 01/19/23 04:51:15.831
Jan 19 04:51:15.839: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-7a494f1a-2308-44b9-8f57-e901cf8bf862" in namespace "pods-5032" to be "running and ready"
Jan 19 04:51:15.844: INFO: Pod "pod-exec-websocket-7a494f1a-2308-44b9-8f57-e901cf8bf862": Phase="Pending", Reason="", readiness=false. Elapsed: 4.345922ms
Jan 19 04:51:15.844: INFO: The phase of Pod pod-exec-websocket-7a494f1a-2308-44b9-8f57-e901cf8bf862 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 04:51:17.848: INFO: Pod "pod-exec-websocket-7a494f1a-2308-44b9-8f57-e901cf8bf862": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008840574s
Jan 19 04:51:17.848: INFO: The phase of Pod pod-exec-websocket-7a494f1a-2308-44b9-8f57-e901cf8bf862 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 04:51:19.847: INFO: Pod "pod-exec-websocket-7a494f1a-2308-44b9-8f57-e901cf8bf862": Phase="Running", Reason="", readiness=true. Elapsed: 4.007898616s
Jan 19 04:51:19.847: INFO: The phase of Pod pod-exec-websocket-7a494f1a-2308-44b9-8f57-e901cf8bf862 is Running (Ready = true)
Jan 19 04:51:19.847: INFO: Pod "pod-exec-websocket-7a494f1a-2308-44b9-8f57-e901cf8bf862" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 19 04:51:19.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5032" for this suite. 01/19/23 04:51:19.961
{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","completed":119,"skipped":2180,"failed":0}
------------------------------
â€¢ [4.158 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:51:15.81
    Jan 19 04:51:15.810: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename pods 01/19/23 04:51:15.811
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:51:15.826
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:51:15.828
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:535
    Jan 19 04:51:15.830: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: creating the pod 01/19/23 04:51:15.831
    STEP: submitting the pod to kubernetes 01/19/23 04:51:15.831
    Jan 19 04:51:15.839: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-7a494f1a-2308-44b9-8f57-e901cf8bf862" in namespace "pods-5032" to be "running and ready"
    Jan 19 04:51:15.844: INFO: Pod "pod-exec-websocket-7a494f1a-2308-44b9-8f57-e901cf8bf862": Phase="Pending", Reason="", readiness=false. Elapsed: 4.345922ms
    Jan 19 04:51:15.844: INFO: The phase of Pod pod-exec-websocket-7a494f1a-2308-44b9-8f57-e901cf8bf862 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 04:51:17.848: INFO: Pod "pod-exec-websocket-7a494f1a-2308-44b9-8f57-e901cf8bf862": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008840574s
    Jan 19 04:51:17.848: INFO: The phase of Pod pod-exec-websocket-7a494f1a-2308-44b9-8f57-e901cf8bf862 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 04:51:19.847: INFO: Pod "pod-exec-websocket-7a494f1a-2308-44b9-8f57-e901cf8bf862": Phase="Running", Reason="", readiness=true. Elapsed: 4.007898616s
    Jan 19 04:51:19.847: INFO: The phase of Pod pod-exec-websocket-7a494f1a-2308-44b9-8f57-e901cf8bf862 is Running (Ready = true)
    Jan 19 04:51:19.847: INFO: Pod "pod-exec-websocket-7a494f1a-2308-44b9-8f57-e901cf8bf862" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 19 04:51:19.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-5032" for this suite. 01/19/23 04:51:19.961
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:51:19.969
Jan 19 04:51:19.969: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename var-expansion 01/19/23 04:51:19.97
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:51:19.985
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:51:19.988
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
Jan 19 04:51:19.997: INFO: Waiting up to 2m0s for pod "var-expansion-9d8d9301-254e-4828-95b5-364861e4b395" in namespace "var-expansion-8808" to be "container 0 failed with reason CreateContainerConfigError"
Jan 19 04:51:20.000: INFO: Pod "var-expansion-9d8d9301-254e-4828-95b5-364861e4b395": Phase="Pending", Reason="", readiness=false. Elapsed: 2.695829ms
Jan 19 04:51:22.004: INFO: Pod "var-expansion-9d8d9301-254e-4828-95b5-364861e4b395": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006873715s
Jan 19 04:51:24.004: INFO: Pod "var-expansion-9d8d9301-254e-4828-95b5-364861e4b395": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007151072s
Jan 19 04:51:24.004: INFO: Pod "var-expansion-9d8d9301-254e-4828-95b5-364861e4b395" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jan 19 04:51:24.004: INFO: Deleting pod "var-expansion-9d8d9301-254e-4828-95b5-364861e4b395" in namespace "var-expansion-8808"
Jan 19 04:51:24.012: INFO: Wait up to 5m0s for pod "var-expansion-9d8d9301-254e-4828-95b5-364861e4b395" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 19 04:51:26.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8808" for this suite. 01/19/23 04:51:26.024
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","completed":120,"skipped":2232,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.062 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:51:19.969
    Jan 19 04:51:19.969: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename var-expansion 01/19/23 04:51:19.97
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:51:19.985
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:51:19.988
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:185
    Jan 19 04:51:19.997: INFO: Waiting up to 2m0s for pod "var-expansion-9d8d9301-254e-4828-95b5-364861e4b395" in namespace "var-expansion-8808" to be "container 0 failed with reason CreateContainerConfigError"
    Jan 19 04:51:20.000: INFO: Pod "var-expansion-9d8d9301-254e-4828-95b5-364861e4b395": Phase="Pending", Reason="", readiness=false. Elapsed: 2.695829ms
    Jan 19 04:51:22.004: INFO: Pod "var-expansion-9d8d9301-254e-4828-95b5-364861e4b395": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006873715s
    Jan 19 04:51:24.004: INFO: Pod "var-expansion-9d8d9301-254e-4828-95b5-364861e4b395": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007151072s
    Jan 19 04:51:24.004: INFO: Pod "var-expansion-9d8d9301-254e-4828-95b5-364861e4b395" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jan 19 04:51:24.004: INFO: Deleting pod "var-expansion-9d8d9301-254e-4828-95b5-364861e4b395" in namespace "var-expansion-8808"
    Jan 19 04:51:24.012: INFO: Wait up to 5m0s for pod "var-expansion-9d8d9301-254e-4828-95b5-364861e4b395" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 19 04:51:26.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-8808" for this suite. 01/19/23 04:51:26.024
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:51:26.032
Jan 19 04:51:26.032: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename downward-api 01/19/23 04:51:26.033
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:51:26.047
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:51:26.05
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
STEP: Creating a pod to test downward API volume plugin 01/19/23 04:51:26.053
Jan 19 04:51:26.061: INFO: Waiting up to 5m0s for pod "downwardapi-volume-34e2f750-2876-4678-bca2-ddf6b80f5234" in namespace "downward-api-3915" to be "Succeeded or Failed"
Jan 19 04:51:26.063: INFO: Pod "downwardapi-volume-34e2f750-2876-4678-bca2-ddf6b80f5234": Phase="Pending", Reason="", readiness=false. Elapsed: 2.115462ms
Jan 19 04:51:28.066: INFO: Pod "downwardapi-volume-34e2f750-2876-4678-bca2-ddf6b80f5234": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005708373s
Jan 19 04:51:30.068: INFO: Pod "downwardapi-volume-34e2f750-2876-4678-bca2-ddf6b80f5234": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006976165s
Jan 19 04:51:32.069: INFO: Pod "downwardapi-volume-34e2f750-2876-4678-bca2-ddf6b80f5234": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008543219s
STEP: Saw pod success 01/19/23 04:51:32.069
Jan 19 04:51:32.069: INFO: Pod "downwardapi-volume-34e2f750-2876-4678-bca2-ddf6b80f5234" satisfied condition "Succeeded or Failed"
Jan 19 04:51:32.074: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod downwardapi-volume-34e2f750-2876-4678-bca2-ddf6b80f5234 container client-container: <nil>
STEP: delete the pod 01/19/23 04:51:32.08
Jan 19 04:51:32.096: INFO: Waiting for pod downwardapi-volume-34e2f750-2876-4678-bca2-ddf6b80f5234 to disappear
Jan 19 04:51:32.099: INFO: Pod downwardapi-volume-34e2f750-2876-4678-bca2-ddf6b80f5234 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 19 04:51:32.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3915" for this suite. 01/19/23 04:51:32.101
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":121,"skipped":2271,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.076 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:51:26.032
    Jan 19 04:51:26.032: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename downward-api 01/19/23 04:51:26.033
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:51:26.047
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:51:26.05
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:248
    STEP: Creating a pod to test downward API volume plugin 01/19/23 04:51:26.053
    Jan 19 04:51:26.061: INFO: Waiting up to 5m0s for pod "downwardapi-volume-34e2f750-2876-4678-bca2-ddf6b80f5234" in namespace "downward-api-3915" to be "Succeeded or Failed"
    Jan 19 04:51:26.063: INFO: Pod "downwardapi-volume-34e2f750-2876-4678-bca2-ddf6b80f5234": Phase="Pending", Reason="", readiness=false. Elapsed: 2.115462ms
    Jan 19 04:51:28.066: INFO: Pod "downwardapi-volume-34e2f750-2876-4678-bca2-ddf6b80f5234": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005708373s
    Jan 19 04:51:30.068: INFO: Pod "downwardapi-volume-34e2f750-2876-4678-bca2-ddf6b80f5234": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006976165s
    Jan 19 04:51:32.069: INFO: Pod "downwardapi-volume-34e2f750-2876-4678-bca2-ddf6b80f5234": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008543219s
    STEP: Saw pod success 01/19/23 04:51:32.069
    Jan 19 04:51:32.069: INFO: Pod "downwardapi-volume-34e2f750-2876-4678-bca2-ddf6b80f5234" satisfied condition "Succeeded or Failed"
    Jan 19 04:51:32.074: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod downwardapi-volume-34e2f750-2876-4678-bca2-ddf6b80f5234 container client-container: <nil>
    STEP: delete the pod 01/19/23 04:51:32.08
    Jan 19 04:51:32.096: INFO: Waiting for pod downwardapi-volume-34e2f750-2876-4678-bca2-ddf6b80f5234 to disappear
    Jan 19 04:51:32.099: INFO: Pod downwardapi-volume-34e2f750-2876-4678-bca2-ddf6b80f5234 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 19 04:51:32.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-3915" for this suite. 01/19/23 04:51:32.101
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:51:32.109
Jan 19 04:51:32.109: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename proxy 01/19/23 04:51:32.109
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:51:32.125
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:51:32.128
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 01/19/23 04:51:32.141
STEP: creating replication controller proxy-service-bchnr in namespace proxy-3316 01/19/23 04:51:32.141
I0119 04:51:32.148743      18 runners.go:193] Created replication controller with name: proxy-service-bchnr, namespace: proxy-3316, replica count: 1
I0119 04:51:33.199810      18 runners.go:193] proxy-service-bchnr Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0119 04:51:34.199917      18 runners.go:193] proxy-service-bchnr Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0119 04:51:35.200149      18 runners.go:193] proxy-service-bchnr Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0119 04:51:36.200882      18 runners.go:193] proxy-service-bchnr Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 19 04:51:36.205: INFO: setup took 4.075287404s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 01/19/23 04:51:36.205
Jan 19 04:51:36.213: INFO: (0) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 7.85339ms)
Jan 19 04:51:36.216: INFO: (0) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 10.96662ms)
Jan 19 04:51:36.222: INFO: (0) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname1/proxy/: foo (200; 16.112339ms)
Jan 19 04:51:36.222: INFO: (0) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">test<... (200; 16.453401ms)
Jan 19 04:51:36.222: INFO: (0) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname2/proxy/: bar (200; 16.579766ms)
Jan 19 04:51:36.222: INFO: (0) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">... (200; 16.329539ms)
Jan 19 04:51:36.222: INFO: (0) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/rewriteme">test</a> (200; 16.084135ms)
Jan 19 04:51:36.222: INFO: (0) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname2/proxy/: bar (200; 16.407161ms)
Jan 19 04:51:36.225: INFO: (0) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 19.189324ms)
Jan 19 04:51:36.225: INFO: (0) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 19.557828ms)
Jan 19 04:51:36.225: INFO: (0) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname1/proxy/: foo (200; 19.046756ms)
Jan 19 04:51:36.225: INFO: (0) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname2/proxy/: tls qux (200; 19.648491ms)
Jan 19 04:51:36.225: INFO: (0) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname1/proxy/: tls baz (200; 19.503564ms)
Jan 19 04:51:36.225: INFO: (0) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:460/proxy/: tls baz (200; 19.156675ms)
Jan 19 04:51:36.225: INFO: (0) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:462/proxy/: tls qux (200; 19.069083ms)
Jan 19 04:51:36.226: INFO: (0) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/tlsrewritem... (200; 20.600419ms)
Jan 19 04:51:36.233: INFO: (1) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:462/proxy/: tls qux (200; 6.677983ms)
Jan 19 04:51:36.233: INFO: (1) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 7.033534ms)
Jan 19 04:51:36.241: INFO: (1) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname2/proxy/: bar (200; 14.518543ms)
Jan 19 04:51:36.241: INFO: (1) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/rewriteme">test</a> (200; 13.972089ms)
Jan 19 04:51:36.241: INFO: (1) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 13.942306ms)
Jan 19 04:51:36.241: INFO: (1) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/tlsrewritem... (200; 14.036676ms)
Jan 19 04:51:36.241: INFO: (1) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 14.08471ms)
Jan 19 04:51:36.241: INFO: (1) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">... (200; 14.130515ms)
Jan 19 04:51:36.241: INFO: (1) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:460/proxy/: tls baz (200; 14.301098ms)
Jan 19 04:51:36.241: INFO: (1) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 14.366864ms)
Jan 19 04:51:36.241: INFO: (1) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">test<... (200; 14.41981ms)
Jan 19 04:51:36.242: INFO: (1) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname2/proxy/: bar (200; 15.317569ms)
Jan 19 04:51:36.242: INFO: (1) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname2/proxy/: tls qux (200; 15.874013ms)
Jan 19 04:51:36.242: INFO: (1) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname1/proxy/: foo (200; 15.807181ms)
Jan 19 04:51:36.243: INFO: (1) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname1/proxy/: tls baz (200; 16.412221ms)
Jan 19 04:51:36.243: INFO: (1) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname1/proxy/: foo (200; 16.360551ms)
Jan 19 04:51:36.259: INFO: (2) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">... (200; 15.808385ms)
Jan 19 04:51:36.260: INFO: (2) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 16.995405ms)
Jan 19 04:51:36.260: INFO: (2) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:462/proxy/: tls qux (200; 16.728712ms)
Jan 19 04:51:36.260: INFO: (2) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:460/proxy/: tls baz (200; 17.212121ms)
Jan 19 04:51:36.260: INFO: (2) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/tlsrewritem... (200; 17.292779ms)
Jan 19 04:51:36.260: INFO: (2) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 16.84935ms)
Jan 19 04:51:36.260: INFO: (2) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/rewriteme">test</a> (200; 17.007416ms)
Jan 19 04:51:36.260: INFO: (2) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 17.057621ms)
Jan 19 04:51:36.260: INFO: (2) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">test<... (200; 17.432868ms)
Jan 19 04:51:36.262: INFO: (2) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname1/proxy/: foo (200; 18.60378ms)
Jan 19 04:51:36.263: INFO: (2) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 19.799746ms)
Jan 19 04:51:36.263: INFO: (2) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname2/proxy/: bar (200; 19.441938ms)
Jan 19 04:51:36.263: INFO: (2) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname1/proxy/: foo (200; 19.719031ms)
Jan 19 04:51:36.263: INFO: (2) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname1/proxy/: tls baz (200; 19.284574ms)
Jan 19 04:51:36.264: INFO: (2) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname2/proxy/: tls qux (200; 19.841835ms)
Jan 19 04:51:36.264: INFO: (2) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname2/proxy/: bar (200; 19.947524ms)
Jan 19 04:51:36.279: INFO: (3) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 14.644274ms)
Jan 19 04:51:36.279: INFO: (3) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/rewriteme">test</a> (200; 14.786188ms)
Jan 19 04:51:36.279: INFO: (3) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:460/proxy/: tls baz (200; 14.958953ms)
Jan 19 04:51:36.279: INFO: (3) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/tlsrewritem... (200; 15.013499ms)
Jan 19 04:51:36.279: INFO: (3) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">... (200; 15.128357ms)
Jan 19 04:51:36.279: INFO: (3) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 15.184399ms)
Jan 19 04:51:36.279: INFO: (3) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">test<... (200; 15.24698ms)
Jan 19 04:51:36.279: INFO: (3) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 14.816069ms)
Jan 19 04:51:36.279: INFO: (3) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 15.483269ms)
Jan 19 04:51:36.281: INFO: (3) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname1/proxy/: foo (200; 16.477584ms)
Jan 19 04:51:36.281: INFO: (3) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:462/proxy/: tls qux (200; 16.348869ms)
Jan 19 04:51:36.281: INFO: (3) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname2/proxy/: bar (200; 16.273911ms)
Jan 19 04:51:36.281: INFO: (3) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname1/proxy/: foo (200; 16.591116ms)
Jan 19 04:51:36.281: INFO: (3) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname1/proxy/: tls baz (200; 16.872799ms)
Jan 19 04:51:36.281: INFO: (3) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname2/proxy/: bar (200; 17.130789ms)
Jan 19 04:51:36.281: INFO: (3) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname2/proxy/: tls qux (200; 17.799335ms)
Jan 19 04:51:36.286: INFO: (4) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/rewriteme">test</a> (200; 4.321449ms)
Jan 19 04:51:36.294: INFO: (4) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 12.835034ms)
Jan 19 04:51:36.294: INFO: (4) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">test<... (200; 12.801136ms)
Jan 19 04:51:36.294: INFO: (4) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">... (200; 12.769741ms)
Jan 19 04:51:36.295: INFO: (4) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:460/proxy/: tls baz (200; 13.281251ms)
Jan 19 04:51:36.295: INFO: (4) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 13.335216ms)
Jan 19 04:51:36.295: INFO: (4) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 13.513208ms)
Jan 19 04:51:36.295: INFO: (4) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname1/proxy/: foo (200; 13.569084ms)
Jan 19 04:51:36.296: INFO: (4) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 14.159825ms)
Jan 19 04:51:36.299: INFO: (4) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/tlsrewritem... (200; 17.50458ms)
Jan 19 04:51:36.299: INFO: (4) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:462/proxy/: tls qux (200; 17.474595ms)
Jan 19 04:51:36.299: INFO: (4) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname1/proxy/: foo (200; 17.503441ms)
Jan 19 04:51:36.299: INFO: (4) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname2/proxy/: bar (200; 17.59927ms)
Jan 19 04:51:36.299: INFO: (4) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname1/proxy/: tls baz (200; 17.664608ms)
Jan 19 04:51:36.299: INFO: (4) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname2/proxy/: tls qux (200; 17.740246ms)
Jan 19 04:51:36.299: INFO: (4) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname2/proxy/: bar (200; 17.703933ms)
Jan 19 04:51:36.312: INFO: (5) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 12.428464ms)
Jan 19 04:51:36.313: INFO: (5) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:462/proxy/: tls qux (200; 13.394525ms)
Jan 19 04:51:36.313: INFO: (5) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:460/proxy/: tls baz (200; 12.82867ms)
Jan 19 04:51:36.317: INFO: (5) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">test<... (200; 16.988735ms)
Jan 19 04:51:36.317: INFO: (5) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 16.6162ms)
Jan 19 04:51:36.317: INFO: (5) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/rewriteme">test</a> (200; 16.66845ms)
Jan 19 04:51:36.317: INFO: (5) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/tlsrewritem... (200; 16.752317ms)
Jan 19 04:51:36.317: INFO: (5) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 16.810451ms)
Jan 19 04:51:36.317: INFO: (5) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">... (200; 16.861276ms)
Jan 19 04:51:36.317: INFO: (5) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 17.028111ms)
Jan 19 04:51:36.317: INFO: (5) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname2/proxy/: tls qux (200; 17.175001ms)
Jan 19 04:51:36.317: INFO: (5) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname2/proxy/: bar (200; 16.922508ms)
Jan 19 04:51:36.317: INFO: (5) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname1/proxy/: foo (200; 17.609711ms)
Jan 19 04:51:36.317: INFO: (5) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname1/proxy/: tls baz (200; 17.995489ms)
Jan 19 04:51:36.317: INFO: (5) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname1/proxy/: foo (200; 17.703344ms)
Jan 19 04:51:36.318: INFO: (5) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname2/proxy/: bar (200; 18.087353ms)
Jan 19 04:51:36.322: INFO: (6) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:460/proxy/: tls baz (200; 4.802554ms)
Jan 19 04:51:36.326: INFO: (6) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">test<... (200; 7.643459ms)
Jan 19 04:51:36.327: INFO: (6) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/tlsrewritem... (200; 8.329848ms)
Jan 19 04:51:36.328: INFO: (6) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 10.231193ms)
Jan 19 04:51:36.329: INFO: (6) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 10.541969ms)
Jan 19 04:51:36.329: INFO: (6) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">... (200; 10.716105ms)
Jan 19 04:51:36.329: INFO: (6) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 10.97018ms)
Jan 19 04:51:36.329: INFO: (6) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/rewriteme">test</a> (200; 11.263914ms)
Jan 19 04:51:36.329: INFO: (6) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 11.221661ms)
Jan 19 04:51:36.329: INFO: (6) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:462/proxy/: tls qux (200; 11.183756ms)
Jan 19 04:51:36.329: INFO: (6) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname1/proxy/: foo (200; 11.374412ms)
Jan 19 04:51:36.331: INFO: (6) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname1/proxy/: tls baz (200; 13.620443ms)
Jan 19 04:51:36.332: INFO: (6) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname1/proxy/: foo (200; 14.06016ms)
Jan 19 04:51:36.332: INFO: (6) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname2/proxy/: tls qux (200; 14.275569ms)
Jan 19 04:51:36.333: INFO: (6) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname2/proxy/: bar (200; 15.010789ms)
Jan 19 04:51:36.333: INFO: (6) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname2/proxy/: bar (200; 14.997517ms)
Jan 19 04:51:36.339: INFO: (7) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/rewriteme">test</a> (200; 5.73981ms)
Jan 19 04:51:36.340: INFO: (7) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:462/proxy/: tls qux (200; 6.354279ms)
Jan 19 04:51:36.342: INFO: (7) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">test<... (200; 8.374778ms)
Jan 19 04:51:36.342: INFO: (7) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 8.669769ms)
Jan 19 04:51:36.345: INFO: (7) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 11.425367ms)
Jan 19 04:51:36.345: INFO: (7) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/tlsrewritem... (200; 11.794997ms)
Jan 19 04:51:36.346: INFO: (7) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 12.002435ms)
Jan 19 04:51:36.348: INFO: (7) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">... (200; 14.317011ms)
Jan 19 04:51:36.348: INFO: (7) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:460/proxy/: tls baz (200; 14.583594ms)
Jan 19 04:51:36.349: INFO: (7) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname2/proxy/: bar (200; 16.026246ms)
Jan 19 04:51:36.349: INFO: (7) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 15.571621ms)
Jan 19 04:51:36.350: INFO: (7) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname1/proxy/: foo (200; 16.033979ms)
Jan 19 04:51:36.350: INFO: (7) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname1/proxy/: foo (200; 16.09051ms)
Jan 19 04:51:36.350: INFO: (7) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname1/proxy/: tls baz (200; 16.377957ms)
Jan 19 04:51:36.350: INFO: (7) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname2/proxy/: bar (200; 16.273849ms)
Jan 19 04:51:36.350: INFO: (7) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname2/proxy/: tls qux (200; 16.878079ms)
Jan 19 04:51:36.355: INFO: (8) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 4.093326ms)
Jan 19 04:51:36.356: INFO: (8) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">test<... (200; 5.649907ms)
Jan 19 04:51:36.361: INFO: (8) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 9.895768ms)
Jan 19 04:51:36.361: INFO: (8) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">... (200; 10.335557ms)
Jan 19 04:51:36.361: INFO: (8) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname2/proxy/: bar (200; 10.670304ms)
Jan 19 04:51:36.361: INFO: (8) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/rewriteme">test</a> (200; 10.116244ms)
Jan 19 04:51:36.361: INFO: (8) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 10.63443ms)
Jan 19 04:51:36.362: INFO: (8) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/tlsrewritem... (200; 10.904654ms)
Jan 19 04:51:36.362: INFO: (8) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:460/proxy/: tls baz (200; 10.900179ms)
Jan 19 04:51:36.362: INFO: (8) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname2/proxy/: bar (200; 11.135765ms)
Jan 19 04:51:36.362: INFO: (8) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:462/proxy/: tls qux (200; 10.865387ms)
Jan 19 04:51:36.362: INFO: (8) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname1/proxy/: foo (200; 11.580588ms)
Jan 19 04:51:36.362: INFO: (8) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname1/proxy/: tls baz (200; 11.179918ms)
Jan 19 04:51:36.362: INFO: (8) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname2/proxy/: tls qux (200; 11.147815ms)
Jan 19 04:51:36.362: INFO: (8) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname1/proxy/: foo (200; 11.355182ms)
Jan 19 04:51:36.363: INFO: (8) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 12.066557ms)
Jan 19 04:51:36.369: INFO: (9) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/rewriteme">test</a> (200; 5.87602ms)
Jan 19 04:51:36.369: INFO: (9) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">... (200; 5.968293ms)
Jan 19 04:51:36.369: INFO: (9) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 6.21371ms)
Jan 19 04:51:36.370: INFO: (9) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:462/proxy/: tls qux (200; 7.095816ms)
Jan 19 04:51:36.371: INFO: (9) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">test<... (200; 7.972311ms)
Jan 19 04:51:36.372: INFO: (9) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 8.519589ms)
Jan 19 04:51:36.372: INFO: (9) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 8.419111ms)
Jan 19 04:51:36.372: INFO: (9) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:460/proxy/: tls baz (200; 8.363186ms)
Jan 19 04:51:36.373: INFO: (9) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 9.823891ms)
Jan 19 04:51:36.374: INFO: (9) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/tlsrewritem... (200; 10.249628ms)
Jan 19 04:51:36.377: INFO: (9) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname2/proxy/: tls qux (200; 13.551019ms)
Jan 19 04:51:36.377: INFO: (9) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname1/proxy/: tls baz (200; 13.978489ms)
Jan 19 04:51:36.377: INFO: (9) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname2/proxy/: bar (200; 14.10737ms)
Jan 19 04:51:36.378: INFO: (9) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname2/proxy/: bar (200; 14.437091ms)
Jan 19 04:51:36.378: INFO: (9) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname1/proxy/: foo (200; 14.538324ms)
Jan 19 04:51:36.378: INFO: (9) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname1/proxy/: foo (200; 14.776499ms)
Jan 19 04:51:36.385: INFO: (10) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 6.59891ms)
Jan 19 04:51:36.386: INFO: (10) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/rewriteme">test</a> (200; 7.219161ms)
Jan 19 04:51:36.388: INFO: (10) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 9.058339ms)
Jan 19 04:51:36.388: INFO: (10) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 9.323854ms)
Jan 19 04:51:36.388: INFO: (10) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 9.283029ms)
Jan 19 04:51:36.388: INFO: (10) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/tlsrewritem... (200; 9.801451ms)
Jan 19 04:51:36.388: INFO: (10) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:460/proxy/: tls baz (200; 9.758518ms)
Jan 19 04:51:36.389: INFO: (10) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">... (200; 10.503932ms)
Jan 19 04:51:36.390: INFO: (10) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">test<... (200; 11.645521ms)
Jan 19 04:51:36.390: INFO: (10) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname2/proxy/: bar (200; 11.349071ms)
Jan 19 04:51:36.390: INFO: (10) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:462/proxy/: tls qux (200; 11.764154ms)
Jan 19 04:51:36.390: INFO: (10) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname1/proxy/: tls baz (200; 11.751518ms)
Jan 19 04:51:36.390: INFO: (10) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname1/proxy/: foo (200; 11.696412ms)
Jan 19 04:51:36.390: INFO: (10) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname2/proxy/: tls qux (200; 11.747368ms)
Jan 19 04:51:36.391: INFO: (10) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname2/proxy/: bar (200; 11.937264ms)
Jan 19 04:51:36.391: INFO: (10) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname1/proxy/: foo (200; 11.992782ms)
Jan 19 04:51:36.399: INFO: (11) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 8.525848ms)
Jan 19 04:51:36.399: INFO: (11) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 8.24183ms)
Jan 19 04:51:36.399: INFO: (11) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">... (200; 8.069086ms)
Jan 19 04:51:36.400: INFO: (11) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:460/proxy/: tls baz (200; 8.516596ms)
Jan 19 04:51:36.400: INFO: (11) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:462/proxy/: tls qux (200; 8.869384ms)
Jan 19 04:51:36.400: INFO: (11) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/tlsrewritem... (200; 9.052034ms)
Jan 19 04:51:36.400: INFO: (11) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">test<... (200; 8.721741ms)
Jan 19 04:51:36.400: INFO: (11) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/rewriteme">test</a> (200; 9.062193ms)
Jan 19 04:51:36.400: INFO: (11) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 8.456812ms)
Jan 19 04:51:36.400: INFO: (11) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 8.801185ms)
Jan 19 04:51:36.402: INFO: (11) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname2/proxy/: bar (200; 10.785047ms)
Jan 19 04:51:36.403: INFO: (11) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname1/proxy/: foo (200; 11.581531ms)
Jan 19 04:51:36.405: INFO: (11) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname1/proxy/: tls baz (200; 13.785627ms)
Jan 19 04:51:36.405: INFO: (11) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname2/proxy/: tls qux (200; 13.934858ms)
Jan 19 04:51:36.405: INFO: (11) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname2/proxy/: bar (200; 13.654385ms)
Jan 19 04:51:36.405: INFO: (11) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname1/proxy/: foo (200; 14.133288ms)
Jan 19 04:51:36.414: INFO: (12) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">... (200; 8.641621ms)
Jan 19 04:51:36.414: INFO: (12) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 8.354807ms)
Jan 19 04:51:36.415: INFO: (12) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:460/proxy/: tls baz (200; 8.844348ms)
Jan 19 04:51:36.415: INFO: (12) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname2/proxy/: bar (200; 9.198639ms)
Jan 19 04:51:36.415: INFO: (12) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 8.960764ms)
Jan 19 04:51:36.415: INFO: (12) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:462/proxy/: tls qux (200; 9.354096ms)
Jan 19 04:51:36.415: INFO: (12) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">test<... (200; 9.269931ms)
Jan 19 04:51:36.415: INFO: (12) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 9.248283ms)
Jan 19 04:51:36.415: INFO: (12) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/tlsrewritem... (200; 9.153824ms)
Jan 19 04:51:36.415: INFO: (12) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 9.631346ms)
Jan 19 04:51:36.415: INFO: (12) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/rewriteme">test</a> (200; 9.095318ms)
Jan 19 04:51:36.417: INFO: (12) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname1/proxy/: foo (200; 11.540244ms)
Jan 19 04:51:36.418: INFO: (12) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname1/proxy/: foo (200; 11.740849ms)
Jan 19 04:51:36.418: INFO: (12) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname2/proxy/: tls qux (200; 12.079861ms)
Jan 19 04:51:36.418: INFO: (12) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname2/proxy/: bar (200; 11.733774ms)
Jan 19 04:51:36.418: INFO: (12) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname1/proxy/: tls baz (200; 12.15928ms)
Jan 19 04:51:36.426: INFO: (13) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/rewriteme">test</a> (200; 7.949303ms)
Jan 19 04:51:36.426: INFO: (13) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">test<... (200; 8.410245ms)
Jan 19 04:51:36.426: INFO: (13) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 8.520254ms)
Jan 19 04:51:36.426: INFO: (13) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 8.658901ms)
Jan 19 04:51:36.426: INFO: (13) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 8.212207ms)
Jan 19 04:51:36.426: INFO: (13) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">... (200; 8.273701ms)
Jan 19 04:51:36.426: INFO: (13) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/tlsrewritem... (200; 8.529109ms)
Jan 19 04:51:36.426: INFO: (13) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 8.162017ms)
Jan 19 04:51:36.426: INFO: (13) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:460/proxy/: tls baz (200; 8.505924ms)
Jan 19 04:51:36.426: INFO: (13) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:462/proxy/: tls qux (200; 8.140742ms)
Jan 19 04:51:36.429: INFO: (13) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname1/proxy/: foo (200; 10.830361ms)
Jan 19 04:51:36.431: INFO: (13) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname2/proxy/: bar (200; 12.737664ms)
Jan 19 04:51:36.437: INFO: (13) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname1/proxy/: tls baz (200; 18.8561ms)
Jan 19 04:51:36.437: INFO: (13) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname2/proxy/: tls qux (200; 18.838398ms)
Jan 19 04:51:36.437: INFO: (13) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname2/proxy/: bar (200; 18.957474ms)
Jan 19 04:51:36.437: INFO: (13) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname1/proxy/: foo (200; 19.339115ms)
Jan 19 04:51:36.443: INFO: (14) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/rewriteme">test</a> (200; 5.989852ms)
Jan 19 04:51:36.454: INFO: (14) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname1/proxy/: tls baz (200; 16.405919ms)
Jan 19 04:51:36.454: INFO: (14) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 15.823968ms)
Jan 19 04:51:36.454: INFO: (14) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:462/proxy/: tls qux (200; 16.01775ms)
Jan 19 04:51:36.455: INFO: (14) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">test<... (200; 17.531508ms)
Jan 19 04:51:36.456: INFO: (14) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 18.066657ms)
Jan 19 04:51:36.456: INFO: (14) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">... (200; 17.931786ms)
Jan 19 04:51:36.456: INFO: (14) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:460/proxy/: tls baz (200; 17.887243ms)
Jan 19 04:51:36.456: INFO: (14) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/tlsrewritem... (200; 17.950578ms)
Jan 19 04:51:36.456: INFO: (14) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 17.997811ms)
Jan 19 04:51:36.456: INFO: (14) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 18.371323ms)
Jan 19 04:51:36.458: INFO: (14) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname1/proxy/: foo (200; 19.996074ms)
Jan 19 04:51:36.458: INFO: (14) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname2/proxy/: bar (200; 20.391867ms)
Jan 19 04:51:36.458: INFO: (14) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname2/proxy/: bar (200; 20.248923ms)
Jan 19 04:51:36.458: INFO: (14) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname2/proxy/: tls qux (200; 20.588234ms)
Jan 19 04:51:36.459: INFO: (14) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname1/proxy/: foo (200; 20.940851ms)
Jan 19 04:51:36.469: INFO: (15) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">... (200; 9.494194ms)
Jan 19 04:51:36.469: INFO: (15) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 9.796492ms)
Jan 19 04:51:36.470: INFO: (15) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">test<... (200; 10.62504ms)
Jan 19 04:51:36.470: INFO: (15) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 10.865548ms)
Jan 19 04:51:36.471: INFO: (15) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:462/proxy/: tls qux (200; 11.824922ms)
Jan 19 04:51:36.471: INFO: (15) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 11.990276ms)
Jan 19 04:51:36.473: INFO: (15) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/rewriteme">test</a> (200; 14.291403ms)
Jan 19 04:51:36.473: INFO: (15) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:460/proxy/: tls baz (200; 14.492073ms)
Jan 19 04:51:36.473: INFO: (15) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/tlsrewritem... (200; 13.933397ms)
Jan 19 04:51:36.475: INFO: (15) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 15.383966ms)
Jan 19 04:51:36.476: INFO: (15) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname2/proxy/: bar (200; 17.336594ms)
Jan 19 04:51:36.479: INFO: (15) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname1/proxy/: foo (200; 19.670696ms)
Jan 19 04:51:36.479: INFO: (15) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname1/proxy/: foo (200; 19.941955ms)
Jan 19 04:51:36.479: INFO: (15) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname2/proxy/: bar (200; 19.879348ms)
Jan 19 04:51:36.479: INFO: (15) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname2/proxy/: tls qux (200; 20.341256ms)
Jan 19 04:51:36.479: INFO: (15) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname1/proxy/: tls baz (200; 20.39779ms)
Jan 19 04:51:36.483: INFO: (16) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">... (200; 3.960162ms)
Jan 19 04:51:36.485: INFO: (16) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/rewriteme">test</a> (200; 4.963766ms)
Jan 19 04:51:36.488: INFO: (16) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 7.795536ms)
Jan 19 04:51:36.488: INFO: (16) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 7.858358ms)
Jan 19 04:51:36.488: INFO: (16) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">test<... (200; 7.912802ms)
Jan 19 04:51:36.488: INFO: (16) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 8.382796ms)
Jan 19 04:51:36.488: INFO: (16) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 8.180183ms)
Jan 19 04:51:36.488: INFO: (16) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:460/proxy/: tls baz (200; 8.01995ms)
Jan 19 04:51:36.488: INFO: (16) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:462/proxy/: tls qux (200; 8.423664ms)
Jan 19 04:51:36.488: INFO: (16) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/tlsrewritem... (200; 8.261826ms)
Jan 19 04:51:36.490: INFO: (16) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname1/proxy/: tls baz (200; 10.616971ms)
Jan 19 04:51:36.493: INFO: (16) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname2/proxy/: bar (200; 12.971247ms)
Jan 19 04:51:36.493: INFO: (16) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname2/proxy/: bar (200; 12.528004ms)
Jan 19 04:51:36.493: INFO: (16) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname1/proxy/: foo (200; 12.604761ms)
Jan 19 04:51:36.493: INFO: (16) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname1/proxy/: foo (200; 12.653885ms)
Jan 19 04:51:36.493: INFO: (16) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname2/proxy/: tls qux (200; 12.981063ms)
Jan 19 04:51:36.498: INFO: (17) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">test<... (200; 4.895097ms)
Jan 19 04:51:36.501: INFO: (17) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/tlsrewritem... (200; 7.558157ms)
Jan 19 04:51:36.501: INFO: (17) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 7.112655ms)
Jan 19 04:51:36.501: INFO: (17) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 7.278815ms)
Jan 19 04:51:36.501: INFO: (17) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 7.550056ms)
Jan 19 04:51:36.501: INFO: (17) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/rewriteme">test</a> (200; 7.730669ms)
Jan 19 04:51:36.501: INFO: (17) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 8.000834ms)
Jan 19 04:51:36.501: INFO: (17) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">... (200; 8.064927ms)
Jan 19 04:51:36.501: INFO: (17) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:460/proxy/: tls baz (200; 7.937031ms)
Jan 19 04:51:36.501: INFO: (17) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname1/proxy/: foo (200; 8.181943ms)
Jan 19 04:51:36.501: INFO: (17) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:462/proxy/: tls qux (200; 7.937314ms)
Jan 19 04:51:36.503: INFO: (17) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname2/proxy/: bar (200; 9.702887ms)
Jan 19 04:51:36.504: INFO: (17) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname2/proxy/: bar (200; 10.324778ms)
Jan 19 04:51:36.504: INFO: (17) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname1/proxy/: foo (200; 10.990458ms)
Jan 19 04:51:36.504: INFO: (17) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname1/proxy/: tls baz (200; 10.824685ms)
Jan 19 04:51:36.504: INFO: (17) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname2/proxy/: tls qux (200; 10.793067ms)
Jan 19 04:51:36.512: INFO: (18) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">... (200; 7.344216ms)
Jan 19 04:51:36.512: INFO: (18) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 7.672776ms)
Jan 19 04:51:36.513: INFO: (18) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">test<... (200; 7.847178ms)
Jan 19 04:51:36.514: INFO: (18) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 8.872475ms)
Jan 19 04:51:36.514: INFO: (18) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 9.049198ms)
Jan 19 04:51:36.514: INFO: (18) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:460/proxy/: tls baz (200; 9.3318ms)
Jan 19 04:51:36.514: INFO: (18) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/tlsrewritem... (200; 9.022962ms)
Jan 19 04:51:36.514: INFO: (18) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname1/proxy/: foo (200; 9.432013ms)
Jan 19 04:51:36.514: INFO: (18) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname1/proxy/: tls baz (200; 8.974576ms)
Jan 19 04:51:36.514: INFO: (18) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/rewriteme">test</a> (200; 9.159924ms)
Jan 19 04:51:36.514: INFO: (18) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 8.920556ms)
Jan 19 04:51:36.514: INFO: (18) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:462/proxy/: tls qux (200; 9.135419ms)
Jan 19 04:51:36.517: INFO: (18) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname1/proxy/: foo (200; 12.131018ms)
Jan 19 04:51:36.517: INFO: (18) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname2/proxy/: bar (200; 12.223745ms)
Jan 19 04:51:36.517: INFO: (18) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname2/proxy/: tls qux (200; 11.877774ms)
Jan 19 04:51:36.519: INFO: (18) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname2/proxy/: bar (200; 13.695107ms)
Jan 19 04:51:36.524: INFO: (19) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 5.188599ms)
Jan 19 04:51:36.524: INFO: (19) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/rewriteme">test</a> (200; 5.66568ms)
Jan 19 04:51:36.525: INFO: (19) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:462/proxy/: tls qux (200; 5.865465ms)
Jan 19 04:51:36.529: INFO: (19) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 9.383812ms)
Jan 19 04:51:36.529: INFO: (19) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname1/proxy/: foo (200; 10.186836ms)
Jan 19 04:51:36.529: INFO: (19) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:460/proxy/: tls baz (200; 9.499326ms)
Jan 19 04:51:36.531: INFO: (19) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 11.81789ms)
Jan 19 04:51:36.531: INFO: (19) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 12.100652ms)
Jan 19 04:51:36.531: INFO: (19) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/tlsrewritem... (200; 11.891169ms)
Jan 19 04:51:36.531: INFO: (19) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname2/proxy/: tls qux (200; 12.336989ms)
Jan 19 04:51:36.531: INFO: (19) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname1/proxy/: tls baz (200; 12.400126ms)
Jan 19 04:51:36.531: INFO: (19) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">... (200; 12.225454ms)
Jan 19 04:51:36.531: INFO: (19) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">test<... (200; 12.386604ms)
Jan 19 04:51:36.534: INFO: (19) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname2/proxy/: bar (200; 14.75325ms)
Jan 19 04:51:36.534: INFO: (19) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname2/proxy/: bar (200; 15.034218ms)
Jan 19 04:51:36.534: INFO: (19) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname1/proxy/: foo (200; 15.01946ms)
STEP: deleting ReplicationController proxy-service-bchnr in namespace proxy-3316, will wait for the garbage collector to delete the pods 01/19/23 04:51:36.534
Jan 19 04:51:36.595: INFO: Deleting ReplicationController proxy-service-bchnr took: 8.23952ms
Jan 19 04:51:36.697: INFO: Terminating ReplicationController proxy-service-bchnr pods took: 101.191687ms
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Jan 19 04:51:39.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3316" for this suite. 01/19/23 04:51:39.302
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","completed":122,"skipped":2287,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.200 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:51:32.109
    Jan 19 04:51:32.109: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename proxy 01/19/23 04:51:32.109
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:51:32.125
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:51:32.128
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 01/19/23 04:51:32.141
    STEP: creating replication controller proxy-service-bchnr in namespace proxy-3316 01/19/23 04:51:32.141
    I0119 04:51:32.148743      18 runners.go:193] Created replication controller with name: proxy-service-bchnr, namespace: proxy-3316, replica count: 1
    I0119 04:51:33.199810      18 runners.go:193] proxy-service-bchnr Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0119 04:51:34.199917      18 runners.go:193] proxy-service-bchnr Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0119 04:51:35.200149      18 runners.go:193] proxy-service-bchnr Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0119 04:51:36.200882      18 runners.go:193] proxy-service-bchnr Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 19 04:51:36.205: INFO: setup took 4.075287404s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 01/19/23 04:51:36.205
    Jan 19 04:51:36.213: INFO: (0) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 7.85339ms)
    Jan 19 04:51:36.216: INFO: (0) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 10.96662ms)
    Jan 19 04:51:36.222: INFO: (0) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname1/proxy/: foo (200; 16.112339ms)
    Jan 19 04:51:36.222: INFO: (0) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">test<... (200; 16.453401ms)
    Jan 19 04:51:36.222: INFO: (0) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname2/proxy/: bar (200; 16.579766ms)
    Jan 19 04:51:36.222: INFO: (0) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">... (200; 16.329539ms)
    Jan 19 04:51:36.222: INFO: (0) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/rewriteme">test</a> (200; 16.084135ms)
    Jan 19 04:51:36.222: INFO: (0) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname2/proxy/: bar (200; 16.407161ms)
    Jan 19 04:51:36.225: INFO: (0) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 19.189324ms)
    Jan 19 04:51:36.225: INFO: (0) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 19.557828ms)
    Jan 19 04:51:36.225: INFO: (0) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname1/proxy/: foo (200; 19.046756ms)
    Jan 19 04:51:36.225: INFO: (0) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname2/proxy/: tls qux (200; 19.648491ms)
    Jan 19 04:51:36.225: INFO: (0) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname1/proxy/: tls baz (200; 19.503564ms)
    Jan 19 04:51:36.225: INFO: (0) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:460/proxy/: tls baz (200; 19.156675ms)
    Jan 19 04:51:36.225: INFO: (0) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:462/proxy/: tls qux (200; 19.069083ms)
    Jan 19 04:51:36.226: INFO: (0) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/tlsrewritem... (200; 20.600419ms)
    Jan 19 04:51:36.233: INFO: (1) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:462/proxy/: tls qux (200; 6.677983ms)
    Jan 19 04:51:36.233: INFO: (1) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 7.033534ms)
    Jan 19 04:51:36.241: INFO: (1) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname2/proxy/: bar (200; 14.518543ms)
    Jan 19 04:51:36.241: INFO: (1) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/rewriteme">test</a> (200; 13.972089ms)
    Jan 19 04:51:36.241: INFO: (1) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 13.942306ms)
    Jan 19 04:51:36.241: INFO: (1) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/tlsrewritem... (200; 14.036676ms)
    Jan 19 04:51:36.241: INFO: (1) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 14.08471ms)
    Jan 19 04:51:36.241: INFO: (1) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">... (200; 14.130515ms)
    Jan 19 04:51:36.241: INFO: (1) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:460/proxy/: tls baz (200; 14.301098ms)
    Jan 19 04:51:36.241: INFO: (1) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 14.366864ms)
    Jan 19 04:51:36.241: INFO: (1) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">test<... (200; 14.41981ms)
    Jan 19 04:51:36.242: INFO: (1) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname2/proxy/: bar (200; 15.317569ms)
    Jan 19 04:51:36.242: INFO: (1) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname2/proxy/: tls qux (200; 15.874013ms)
    Jan 19 04:51:36.242: INFO: (1) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname1/proxy/: foo (200; 15.807181ms)
    Jan 19 04:51:36.243: INFO: (1) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname1/proxy/: tls baz (200; 16.412221ms)
    Jan 19 04:51:36.243: INFO: (1) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname1/proxy/: foo (200; 16.360551ms)
    Jan 19 04:51:36.259: INFO: (2) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">... (200; 15.808385ms)
    Jan 19 04:51:36.260: INFO: (2) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 16.995405ms)
    Jan 19 04:51:36.260: INFO: (2) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:462/proxy/: tls qux (200; 16.728712ms)
    Jan 19 04:51:36.260: INFO: (2) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:460/proxy/: tls baz (200; 17.212121ms)
    Jan 19 04:51:36.260: INFO: (2) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/tlsrewritem... (200; 17.292779ms)
    Jan 19 04:51:36.260: INFO: (2) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 16.84935ms)
    Jan 19 04:51:36.260: INFO: (2) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/rewriteme">test</a> (200; 17.007416ms)
    Jan 19 04:51:36.260: INFO: (2) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 17.057621ms)
    Jan 19 04:51:36.260: INFO: (2) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">test<... (200; 17.432868ms)
    Jan 19 04:51:36.262: INFO: (2) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname1/proxy/: foo (200; 18.60378ms)
    Jan 19 04:51:36.263: INFO: (2) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 19.799746ms)
    Jan 19 04:51:36.263: INFO: (2) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname2/proxy/: bar (200; 19.441938ms)
    Jan 19 04:51:36.263: INFO: (2) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname1/proxy/: foo (200; 19.719031ms)
    Jan 19 04:51:36.263: INFO: (2) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname1/proxy/: tls baz (200; 19.284574ms)
    Jan 19 04:51:36.264: INFO: (2) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname2/proxy/: tls qux (200; 19.841835ms)
    Jan 19 04:51:36.264: INFO: (2) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname2/proxy/: bar (200; 19.947524ms)
    Jan 19 04:51:36.279: INFO: (3) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 14.644274ms)
    Jan 19 04:51:36.279: INFO: (3) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/rewriteme">test</a> (200; 14.786188ms)
    Jan 19 04:51:36.279: INFO: (3) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:460/proxy/: tls baz (200; 14.958953ms)
    Jan 19 04:51:36.279: INFO: (3) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/tlsrewritem... (200; 15.013499ms)
    Jan 19 04:51:36.279: INFO: (3) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">... (200; 15.128357ms)
    Jan 19 04:51:36.279: INFO: (3) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 15.184399ms)
    Jan 19 04:51:36.279: INFO: (3) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">test<... (200; 15.24698ms)
    Jan 19 04:51:36.279: INFO: (3) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 14.816069ms)
    Jan 19 04:51:36.279: INFO: (3) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 15.483269ms)
    Jan 19 04:51:36.281: INFO: (3) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname1/proxy/: foo (200; 16.477584ms)
    Jan 19 04:51:36.281: INFO: (3) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:462/proxy/: tls qux (200; 16.348869ms)
    Jan 19 04:51:36.281: INFO: (3) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname2/proxy/: bar (200; 16.273911ms)
    Jan 19 04:51:36.281: INFO: (3) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname1/proxy/: foo (200; 16.591116ms)
    Jan 19 04:51:36.281: INFO: (3) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname1/proxy/: tls baz (200; 16.872799ms)
    Jan 19 04:51:36.281: INFO: (3) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname2/proxy/: bar (200; 17.130789ms)
    Jan 19 04:51:36.281: INFO: (3) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname2/proxy/: tls qux (200; 17.799335ms)
    Jan 19 04:51:36.286: INFO: (4) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/rewriteme">test</a> (200; 4.321449ms)
    Jan 19 04:51:36.294: INFO: (4) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 12.835034ms)
    Jan 19 04:51:36.294: INFO: (4) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">test<... (200; 12.801136ms)
    Jan 19 04:51:36.294: INFO: (4) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">... (200; 12.769741ms)
    Jan 19 04:51:36.295: INFO: (4) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:460/proxy/: tls baz (200; 13.281251ms)
    Jan 19 04:51:36.295: INFO: (4) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 13.335216ms)
    Jan 19 04:51:36.295: INFO: (4) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 13.513208ms)
    Jan 19 04:51:36.295: INFO: (4) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname1/proxy/: foo (200; 13.569084ms)
    Jan 19 04:51:36.296: INFO: (4) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 14.159825ms)
    Jan 19 04:51:36.299: INFO: (4) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/tlsrewritem... (200; 17.50458ms)
    Jan 19 04:51:36.299: INFO: (4) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:462/proxy/: tls qux (200; 17.474595ms)
    Jan 19 04:51:36.299: INFO: (4) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname1/proxy/: foo (200; 17.503441ms)
    Jan 19 04:51:36.299: INFO: (4) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname2/proxy/: bar (200; 17.59927ms)
    Jan 19 04:51:36.299: INFO: (4) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname1/proxy/: tls baz (200; 17.664608ms)
    Jan 19 04:51:36.299: INFO: (4) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname2/proxy/: tls qux (200; 17.740246ms)
    Jan 19 04:51:36.299: INFO: (4) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname2/proxy/: bar (200; 17.703933ms)
    Jan 19 04:51:36.312: INFO: (5) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 12.428464ms)
    Jan 19 04:51:36.313: INFO: (5) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:462/proxy/: tls qux (200; 13.394525ms)
    Jan 19 04:51:36.313: INFO: (5) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:460/proxy/: tls baz (200; 12.82867ms)
    Jan 19 04:51:36.317: INFO: (5) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">test<... (200; 16.988735ms)
    Jan 19 04:51:36.317: INFO: (5) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 16.6162ms)
    Jan 19 04:51:36.317: INFO: (5) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/rewriteme">test</a> (200; 16.66845ms)
    Jan 19 04:51:36.317: INFO: (5) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/tlsrewritem... (200; 16.752317ms)
    Jan 19 04:51:36.317: INFO: (5) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 16.810451ms)
    Jan 19 04:51:36.317: INFO: (5) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">... (200; 16.861276ms)
    Jan 19 04:51:36.317: INFO: (5) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 17.028111ms)
    Jan 19 04:51:36.317: INFO: (5) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname2/proxy/: tls qux (200; 17.175001ms)
    Jan 19 04:51:36.317: INFO: (5) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname2/proxy/: bar (200; 16.922508ms)
    Jan 19 04:51:36.317: INFO: (5) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname1/proxy/: foo (200; 17.609711ms)
    Jan 19 04:51:36.317: INFO: (5) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname1/proxy/: tls baz (200; 17.995489ms)
    Jan 19 04:51:36.317: INFO: (5) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname1/proxy/: foo (200; 17.703344ms)
    Jan 19 04:51:36.318: INFO: (5) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname2/proxy/: bar (200; 18.087353ms)
    Jan 19 04:51:36.322: INFO: (6) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:460/proxy/: tls baz (200; 4.802554ms)
    Jan 19 04:51:36.326: INFO: (6) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">test<... (200; 7.643459ms)
    Jan 19 04:51:36.327: INFO: (6) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/tlsrewritem... (200; 8.329848ms)
    Jan 19 04:51:36.328: INFO: (6) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 10.231193ms)
    Jan 19 04:51:36.329: INFO: (6) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 10.541969ms)
    Jan 19 04:51:36.329: INFO: (6) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">... (200; 10.716105ms)
    Jan 19 04:51:36.329: INFO: (6) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 10.97018ms)
    Jan 19 04:51:36.329: INFO: (6) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/rewriteme">test</a> (200; 11.263914ms)
    Jan 19 04:51:36.329: INFO: (6) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 11.221661ms)
    Jan 19 04:51:36.329: INFO: (6) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:462/proxy/: tls qux (200; 11.183756ms)
    Jan 19 04:51:36.329: INFO: (6) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname1/proxy/: foo (200; 11.374412ms)
    Jan 19 04:51:36.331: INFO: (6) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname1/proxy/: tls baz (200; 13.620443ms)
    Jan 19 04:51:36.332: INFO: (6) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname1/proxy/: foo (200; 14.06016ms)
    Jan 19 04:51:36.332: INFO: (6) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname2/proxy/: tls qux (200; 14.275569ms)
    Jan 19 04:51:36.333: INFO: (6) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname2/proxy/: bar (200; 15.010789ms)
    Jan 19 04:51:36.333: INFO: (6) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname2/proxy/: bar (200; 14.997517ms)
    Jan 19 04:51:36.339: INFO: (7) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/rewriteme">test</a> (200; 5.73981ms)
    Jan 19 04:51:36.340: INFO: (7) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:462/proxy/: tls qux (200; 6.354279ms)
    Jan 19 04:51:36.342: INFO: (7) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">test<... (200; 8.374778ms)
    Jan 19 04:51:36.342: INFO: (7) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 8.669769ms)
    Jan 19 04:51:36.345: INFO: (7) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 11.425367ms)
    Jan 19 04:51:36.345: INFO: (7) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/tlsrewritem... (200; 11.794997ms)
    Jan 19 04:51:36.346: INFO: (7) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 12.002435ms)
    Jan 19 04:51:36.348: INFO: (7) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">... (200; 14.317011ms)
    Jan 19 04:51:36.348: INFO: (7) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:460/proxy/: tls baz (200; 14.583594ms)
    Jan 19 04:51:36.349: INFO: (7) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname2/proxy/: bar (200; 16.026246ms)
    Jan 19 04:51:36.349: INFO: (7) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 15.571621ms)
    Jan 19 04:51:36.350: INFO: (7) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname1/proxy/: foo (200; 16.033979ms)
    Jan 19 04:51:36.350: INFO: (7) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname1/proxy/: foo (200; 16.09051ms)
    Jan 19 04:51:36.350: INFO: (7) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname1/proxy/: tls baz (200; 16.377957ms)
    Jan 19 04:51:36.350: INFO: (7) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname2/proxy/: bar (200; 16.273849ms)
    Jan 19 04:51:36.350: INFO: (7) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname2/proxy/: tls qux (200; 16.878079ms)
    Jan 19 04:51:36.355: INFO: (8) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 4.093326ms)
    Jan 19 04:51:36.356: INFO: (8) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">test<... (200; 5.649907ms)
    Jan 19 04:51:36.361: INFO: (8) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 9.895768ms)
    Jan 19 04:51:36.361: INFO: (8) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">... (200; 10.335557ms)
    Jan 19 04:51:36.361: INFO: (8) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname2/proxy/: bar (200; 10.670304ms)
    Jan 19 04:51:36.361: INFO: (8) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/rewriteme">test</a> (200; 10.116244ms)
    Jan 19 04:51:36.361: INFO: (8) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 10.63443ms)
    Jan 19 04:51:36.362: INFO: (8) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/tlsrewritem... (200; 10.904654ms)
    Jan 19 04:51:36.362: INFO: (8) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:460/proxy/: tls baz (200; 10.900179ms)
    Jan 19 04:51:36.362: INFO: (8) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname2/proxy/: bar (200; 11.135765ms)
    Jan 19 04:51:36.362: INFO: (8) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:462/proxy/: tls qux (200; 10.865387ms)
    Jan 19 04:51:36.362: INFO: (8) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname1/proxy/: foo (200; 11.580588ms)
    Jan 19 04:51:36.362: INFO: (8) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname1/proxy/: tls baz (200; 11.179918ms)
    Jan 19 04:51:36.362: INFO: (8) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname2/proxy/: tls qux (200; 11.147815ms)
    Jan 19 04:51:36.362: INFO: (8) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname1/proxy/: foo (200; 11.355182ms)
    Jan 19 04:51:36.363: INFO: (8) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 12.066557ms)
    Jan 19 04:51:36.369: INFO: (9) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/rewriteme">test</a> (200; 5.87602ms)
    Jan 19 04:51:36.369: INFO: (9) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">... (200; 5.968293ms)
    Jan 19 04:51:36.369: INFO: (9) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 6.21371ms)
    Jan 19 04:51:36.370: INFO: (9) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:462/proxy/: tls qux (200; 7.095816ms)
    Jan 19 04:51:36.371: INFO: (9) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">test<... (200; 7.972311ms)
    Jan 19 04:51:36.372: INFO: (9) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 8.519589ms)
    Jan 19 04:51:36.372: INFO: (9) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 8.419111ms)
    Jan 19 04:51:36.372: INFO: (9) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:460/proxy/: tls baz (200; 8.363186ms)
    Jan 19 04:51:36.373: INFO: (9) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 9.823891ms)
    Jan 19 04:51:36.374: INFO: (9) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/tlsrewritem... (200; 10.249628ms)
    Jan 19 04:51:36.377: INFO: (9) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname2/proxy/: tls qux (200; 13.551019ms)
    Jan 19 04:51:36.377: INFO: (9) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname1/proxy/: tls baz (200; 13.978489ms)
    Jan 19 04:51:36.377: INFO: (9) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname2/proxy/: bar (200; 14.10737ms)
    Jan 19 04:51:36.378: INFO: (9) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname2/proxy/: bar (200; 14.437091ms)
    Jan 19 04:51:36.378: INFO: (9) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname1/proxy/: foo (200; 14.538324ms)
    Jan 19 04:51:36.378: INFO: (9) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname1/proxy/: foo (200; 14.776499ms)
    Jan 19 04:51:36.385: INFO: (10) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 6.59891ms)
    Jan 19 04:51:36.386: INFO: (10) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/rewriteme">test</a> (200; 7.219161ms)
    Jan 19 04:51:36.388: INFO: (10) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 9.058339ms)
    Jan 19 04:51:36.388: INFO: (10) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 9.323854ms)
    Jan 19 04:51:36.388: INFO: (10) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 9.283029ms)
    Jan 19 04:51:36.388: INFO: (10) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/tlsrewritem... (200; 9.801451ms)
    Jan 19 04:51:36.388: INFO: (10) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:460/proxy/: tls baz (200; 9.758518ms)
    Jan 19 04:51:36.389: INFO: (10) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">... (200; 10.503932ms)
    Jan 19 04:51:36.390: INFO: (10) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">test<... (200; 11.645521ms)
    Jan 19 04:51:36.390: INFO: (10) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname2/proxy/: bar (200; 11.349071ms)
    Jan 19 04:51:36.390: INFO: (10) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:462/proxy/: tls qux (200; 11.764154ms)
    Jan 19 04:51:36.390: INFO: (10) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname1/proxy/: tls baz (200; 11.751518ms)
    Jan 19 04:51:36.390: INFO: (10) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname1/proxy/: foo (200; 11.696412ms)
    Jan 19 04:51:36.390: INFO: (10) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname2/proxy/: tls qux (200; 11.747368ms)
    Jan 19 04:51:36.391: INFO: (10) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname2/proxy/: bar (200; 11.937264ms)
    Jan 19 04:51:36.391: INFO: (10) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname1/proxy/: foo (200; 11.992782ms)
    Jan 19 04:51:36.399: INFO: (11) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 8.525848ms)
    Jan 19 04:51:36.399: INFO: (11) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 8.24183ms)
    Jan 19 04:51:36.399: INFO: (11) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">... (200; 8.069086ms)
    Jan 19 04:51:36.400: INFO: (11) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:460/proxy/: tls baz (200; 8.516596ms)
    Jan 19 04:51:36.400: INFO: (11) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:462/proxy/: tls qux (200; 8.869384ms)
    Jan 19 04:51:36.400: INFO: (11) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/tlsrewritem... (200; 9.052034ms)
    Jan 19 04:51:36.400: INFO: (11) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">test<... (200; 8.721741ms)
    Jan 19 04:51:36.400: INFO: (11) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/rewriteme">test</a> (200; 9.062193ms)
    Jan 19 04:51:36.400: INFO: (11) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 8.456812ms)
    Jan 19 04:51:36.400: INFO: (11) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 8.801185ms)
    Jan 19 04:51:36.402: INFO: (11) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname2/proxy/: bar (200; 10.785047ms)
    Jan 19 04:51:36.403: INFO: (11) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname1/proxy/: foo (200; 11.581531ms)
    Jan 19 04:51:36.405: INFO: (11) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname1/proxy/: tls baz (200; 13.785627ms)
    Jan 19 04:51:36.405: INFO: (11) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname2/proxy/: tls qux (200; 13.934858ms)
    Jan 19 04:51:36.405: INFO: (11) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname2/proxy/: bar (200; 13.654385ms)
    Jan 19 04:51:36.405: INFO: (11) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname1/proxy/: foo (200; 14.133288ms)
    Jan 19 04:51:36.414: INFO: (12) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">... (200; 8.641621ms)
    Jan 19 04:51:36.414: INFO: (12) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 8.354807ms)
    Jan 19 04:51:36.415: INFO: (12) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:460/proxy/: tls baz (200; 8.844348ms)
    Jan 19 04:51:36.415: INFO: (12) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname2/proxy/: bar (200; 9.198639ms)
    Jan 19 04:51:36.415: INFO: (12) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 8.960764ms)
    Jan 19 04:51:36.415: INFO: (12) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:462/proxy/: tls qux (200; 9.354096ms)
    Jan 19 04:51:36.415: INFO: (12) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">test<... (200; 9.269931ms)
    Jan 19 04:51:36.415: INFO: (12) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 9.248283ms)
    Jan 19 04:51:36.415: INFO: (12) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/tlsrewritem... (200; 9.153824ms)
    Jan 19 04:51:36.415: INFO: (12) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 9.631346ms)
    Jan 19 04:51:36.415: INFO: (12) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/rewriteme">test</a> (200; 9.095318ms)
    Jan 19 04:51:36.417: INFO: (12) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname1/proxy/: foo (200; 11.540244ms)
    Jan 19 04:51:36.418: INFO: (12) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname1/proxy/: foo (200; 11.740849ms)
    Jan 19 04:51:36.418: INFO: (12) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname2/proxy/: tls qux (200; 12.079861ms)
    Jan 19 04:51:36.418: INFO: (12) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname2/proxy/: bar (200; 11.733774ms)
    Jan 19 04:51:36.418: INFO: (12) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname1/proxy/: tls baz (200; 12.15928ms)
    Jan 19 04:51:36.426: INFO: (13) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/rewriteme">test</a> (200; 7.949303ms)
    Jan 19 04:51:36.426: INFO: (13) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">test<... (200; 8.410245ms)
    Jan 19 04:51:36.426: INFO: (13) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 8.520254ms)
    Jan 19 04:51:36.426: INFO: (13) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 8.658901ms)
    Jan 19 04:51:36.426: INFO: (13) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 8.212207ms)
    Jan 19 04:51:36.426: INFO: (13) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">... (200; 8.273701ms)
    Jan 19 04:51:36.426: INFO: (13) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/tlsrewritem... (200; 8.529109ms)
    Jan 19 04:51:36.426: INFO: (13) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 8.162017ms)
    Jan 19 04:51:36.426: INFO: (13) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:460/proxy/: tls baz (200; 8.505924ms)
    Jan 19 04:51:36.426: INFO: (13) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:462/proxy/: tls qux (200; 8.140742ms)
    Jan 19 04:51:36.429: INFO: (13) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname1/proxy/: foo (200; 10.830361ms)
    Jan 19 04:51:36.431: INFO: (13) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname2/proxy/: bar (200; 12.737664ms)
    Jan 19 04:51:36.437: INFO: (13) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname1/proxy/: tls baz (200; 18.8561ms)
    Jan 19 04:51:36.437: INFO: (13) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname2/proxy/: tls qux (200; 18.838398ms)
    Jan 19 04:51:36.437: INFO: (13) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname2/proxy/: bar (200; 18.957474ms)
    Jan 19 04:51:36.437: INFO: (13) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname1/proxy/: foo (200; 19.339115ms)
    Jan 19 04:51:36.443: INFO: (14) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/rewriteme">test</a> (200; 5.989852ms)
    Jan 19 04:51:36.454: INFO: (14) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname1/proxy/: tls baz (200; 16.405919ms)
    Jan 19 04:51:36.454: INFO: (14) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 15.823968ms)
    Jan 19 04:51:36.454: INFO: (14) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:462/proxy/: tls qux (200; 16.01775ms)
    Jan 19 04:51:36.455: INFO: (14) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">test<... (200; 17.531508ms)
    Jan 19 04:51:36.456: INFO: (14) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 18.066657ms)
    Jan 19 04:51:36.456: INFO: (14) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">... (200; 17.931786ms)
    Jan 19 04:51:36.456: INFO: (14) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:460/proxy/: tls baz (200; 17.887243ms)
    Jan 19 04:51:36.456: INFO: (14) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/tlsrewritem... (200; 17.950578ms)
    Jan 19 04:51:36.456: INFO: (14) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 17.997811ms)
    Jan 19 04:51:36.456: INFO: (14) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 18.371323ms)
    Jan 19 04:51:36.458: INFO: (14) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname1/proxy/: foo (200; 19.996074ms)
    Jan 19 04:51:36.458: INFO: (14) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname2/proxy/: bar (200; 20.391867ms)
    Jan 19 04:51:36.458: INFO: (14) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname2/proxy/: bar (200; 20.248923ms)
    Jan 19 04:51:36.458: INFO: (14) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname2/proxy/: tls qux (200; 20.588234ms)
    Jan 19 04:51:36.459: INFO: (14) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname1/proxy/: foo (200; 20.940851ms)
    Jan 19 04:51:36.469: INFO: (15) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">... (200; 9.494194ms)
    Jan 19 04:51:36.469: INFO: (15) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 9.796492ms)
    Jan 19 04:51:36.470: INFO: (15) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">test<... (200; 10.62504ms)
    Jan 19 04:51:36.470: INFO: (15) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 10.865548ms)
    Jan 19 04:51:36.471: INFO: (15) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:462/proxy/: tls qux (200; 11.824922ms)
    Jan 19 04:51:36.471: INFO: (15) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 11.990276ms)
    Jan 19 04:51:36.473: INFO: (15) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/rewriteme">test</a> (200; 14.291403ms)
    Jan 19 04:51:36.473: INFO: (15) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:460/proxy/: tls baz (200; 14.492073ms)
    Jan 19 04:51:36.473: INFO: (15) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/tlsrewritem... (200; 13.933397ms)
    Jan 19 04:51:36.475: INFO: (15) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 15.383966ms)
    Jan 19 04:51:36.476: INFO: (15) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname2/proxy/: bar (200; 17.336594ms)
    Jan 19 04:51:36.479: INFO: (15) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname1/proxy/: foo (200; 19.670696ms)
    Jan 19 04:51:36.479: INFO: (15) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname1/proxy/: foo (200; 19.941955ms)
    Jan 19 04:51:36.479: INFO: (15) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname2/proxy/: bar (200; 19.879348ms)
    Jan 19 04:51:36.479: INFO: (15) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname2/proxy/: tls qux (200; 20.341256ms)
    Jan 19 04:51:36.479: INFO: (15) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname1/proxy/: tls baz (200; 20.39779ms)
    Jan 19 04:51:36.483: INFO: (16) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">... (200; 3.960162ms)
    Jan 19 04:51:36.485: INFO: (16) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/rewriteme">test</a> (200; 4.963766ms)
    Jan 19 04:51:36.488: INFO: (16) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 7.795536ms)
    Jan 19 04:51:36.488: INFO: (16) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 7.858358ms)
    Jan 19 04:51:36.488: INFO: (16) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">test<... (200; 7.912802ms)
    Jan 19 04:51:36.488: INFO: (16) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 8.382796ms)
    Jan 19 04:51:36.488: INFO: (16) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 8.180183ms)
    Jan 19 04:51:36.488: INFO: (16) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:460/proxy/: tls baz (200; 8.01995ms)
    Jan 19 04:51:36.488: INFO: (16) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:462/proxy/: tls qux (200; 8.423664ms)
    Jan 19 04:51:36.488: INFO: (16) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/tlsrewritem... (200; 8.261826ms)
    Jan 19 04:51:36.490: INFO: (16) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname1/proxy/: tls baz (200; 10.616971ms)
    Jan 19 04:51:36.493: INFO: (16) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname2/proxy/: bar (200; 12.971247ms)
    Jan 19 04:51:36.493: INFO: (16) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname2/proxy/: bar (200; 12.528004ms)
    Jan 19 04:51:36.493: INFO: (16) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname1/proxy/: foo (200; 12.604761ms)
    Jan 19 04:51:36.493: INFO: (16) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname1/proxy/: foo (200; 12.653885ms)
    Jan 19 04:51:36.493: INFO: (16) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname2/proxy/: tls qux (200; 12.981063ms)
    Jan 19 04:51:36.498: INFO: (17) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">test<... (200; 4.895097ms)
    Jan 19 04:51:36.501: INFO: (17) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/tlsrewritem... (200; 7.558157ms)
    Jan 19 04:51:36.501: INFO: (17) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 7.112655ms)
    Jan 19 04:51:36.501: INFO: (17) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 7.278815ms)
    Jan 19 04:51:36.501: INFO: (17) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 7.550056ms)
    Jan 19 04:51:36.501: INFO: (17) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/rewriteme">test</a> (200; 7.730669ms)
    Jan 19 04:51:36.501: INFO: (17) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 8.000834ms)
    Jan 19 04:51:36.501: INFO: (17) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">... (200; 8.064927ms)
    Jan 19 04:51:36.501: INFO: (17) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:460/proxy/: tls baz (200; 7.937031ms)
    Jan 19 04:51:36.501: INFO: (17) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname1/proxy/: foo (200; 8.181943ms)
    Jan 19 04:51:36.501: INFO: (17) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:462/proxy/: tls qux (200; 7.937314ms)
    Jan 19 04:51:36.503: INFO: (17) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname2/proxy/: bar (200; 9.702887ms)
    Jan 19 04:51:36.504: INFO: (17) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname2/proxy/: bar (200; 10.324778ms)
    Jan 19 04:51:36.504: INFO: (17) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname1/proxy/: foo (200; 10.990458ms)
    Jan 19 04:51:36.504: INFO: (17) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname1/proxy/: tls baz (200; 10.824685ms)
    Jan 19 04:51:36.504: INFO: (17) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname2/proxy/: tls qux (200; 10.793067ms)
    Jan 19 04:51:36.512: INFO: (18) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">... (200; 7.344216ms)
    Jan 19 04:51:36.512: INFO: (18) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 7.672776ms)
    Jan 19 04:51:36.513: INFO: (18) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">test<... (200; 7.847178ms)
    Jan 19 04:51:36.514: INFO: (18) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 8.872475ms)
    Jan 19 04:51:36.514: INFO: (18) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 9.049198ms)
    Jan 19 04:51:36.514: INFO: (18) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:460/proxy/: tls baz (200; 9.3318ms)
    Jan 19 04:51:36.514: INFO: (18) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/tlsrewritem... (200; 9.022962ms)
    Jan 19 04:51:36.514: INFO: (18) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname1/proxy/: foo (200; 9.432013ms)
    Jan 19 04:51:36.514: INFO: (18) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname1/proxy/: tls baz (200; 8.974576ms)
    Jan 19 04:51:36.514: INFO: (18) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/rewriteme">test</a> (200; 9.159924ms)
    Jan 19 04:51:36.514: INFO: (18) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 8.920556ms)
    Jan 19 04:51:36.514: INFO: (18) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:462/proxy/: tls qux (200; 9.135419ms)
    Jan 19 04:51:36.517: INFO: (18) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname1/proxy/: foo (200; 12.131018ms)
    Jan 19 04:51:36.517: INFO: (18) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname2/proxy/: bar (200; 12.223745ms)
    Jan 19 04:51:36.517: INFO: (18) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname2/proxy/: tls qux (200; 11.877774ms)
    Jan 19 04:51:36.519: INFO: (18) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname2/proxy/: bar (200; 13.695107ms)
    Jan 19 04:51:36.524: INFO: (19) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 5.188599ms)
    Jan 19 04:51:36.524: INFO: (19) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g/proxy/rewriteme">test</a> (200; 5.66568ms)
    Jan 19 04:51:36.525: INFO: (19) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:462/proxy/: tls qux (200; 5.865465ms)
    Jan 19 04:51:36.529: INFO: (19) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 9.383812ms)
    Jan 19 04:51:36.529: INFO: (19) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname1/proxy/: foo (200; 10.186836ms)
    Jan 19 04:51:36.529: INFO: (19) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:460/proxy/: tls baz (200; 9.499326ms)
    Jan 19 04:51:36.531: INFO: (19) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:160/proxy/: foo (200; 11.81789ms)
    Jan 19 04:51:36.531: INFO: (19) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:162/proxy/: bar (200; 12.100652ms)
    Jan 19 04:51:36.531: INFO: (19) /api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/https:proxy-service-bchnr-l5h6g:443/proxy/tlsrewritem... (200; 11.891169ms)
    Jan 19 04:51:36.531: INFO: (19) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname2/proxy/: tls qux (200; 12.336989ms)
    Jan 19 04:51:36.531: INFO: (19) /api/v1/namespaces/proxy-3316/services/https:proxy-service-bchnr:tlsportname1/proxy/: tls baz (200; 12.400126ms)
    Jan 19 04:51:36.531: INFO: (19) /api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/http:proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">... (200; 12.225454ms)
    Jan 19 04:51:36.531: INFO: (19) /api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-3316/pods/proxy-service-bchnr-l5h6g:1080/proxy/rewriteme">test<... (200; 12.386604ms)
    Jan 19 04:51:36.534: INFO: (19) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname2/proxy/: bar (200; 14.75325ms)
    Jan 19 04:51:36.534: INFO: (19) /api/v1/namespaces/proxy-3316/services/proxy-service-bchnr:portname2/proxy/: bar (200; 15.034218ms)
    Jan 19 04:51:36.534: INFO: (19) /api/v1/namespaces/proxy-3316/services/http:proxy-service-bchnr:portname1/proxy/: foo (200; 15.01946ms)
    STEP: deleting ReplicationController proxy-service-bchnr in namespace proxy-3316, will wait for the garbage collector to delete the pods 01/19/23 04:51:36.534
    Jan 19 04:51:36.595: INFO: Deleting ReplicationController proxy-service-bchnr took: 8.23952ms
    Jan 19 04:51:36.697: INFO: Terminating ReplicationController proxy-service-bchnr pods took: 101.191687ms
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Jan 19 04:51:39.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-3316" for this suite. 01/19/23 04:51:39.302
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:51:39.31
Jan 19 04:51:39.310: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename projected 01/19/23 04:51:39.311
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:51:39.326
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:51:39.328
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
STEP: Creating a pod to test downward API volume plugin 01/19/23 04:51:39.33
Jan 19 04:51:39.339: INFO: Waiting up to 5m0s for pod "downwardapi-volume-970d41be-f0bc-43e1-9e27-1949d64aeff3" in namespace "projected-37" to be "Succeeded or Failed"
Jan 19 04:51:39.342: INFO: Pod "downwardapi-volume-970d41be-f0bc-43e1-9e27-1949d64aeff3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.120322ms
Jan 19 04:51:41.346: INFO: Pod "downwardapi-volume-970d41be-f0bc-43e1-9e27-1949d64aeff3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00779867s
Jan 19 04:51:43.349: INFO: Pod "downwardapi-volume-970d41be-f0bc-43e1-9e27-1949d64aeff3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010200317s
Jan 19 04:51:45.352: INFO: Pod "downwardapi-volume-970d41be-f0bc-43e1-9e27-1949d64aeff3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013794881s
STEP: Saw pod success 01/19/23 04:51:45.352
Jan 19 04:51:45.353: INFO: Pod "downwardapi-volume-970d41be-f0bc-43e1-9e27-1949d64aeff3" satisfied condition "Succeeded or Failed"
Jan 19 04:51:45.356: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod downwardapi-volume-970d41be-f0bc-43e1-9e27-1949d64aeff3 container client-container: <nil>
STEP: delete the pod 01/19/23 04:51:45.362
Jan 19 04:51:45.375: INFO: Waiting for pod downwardapi-volume-970d41be-f0bc-43e1-9e27-1949d64aeff3 to disappear
Jan 19 04:51:45.378: INFO: Pod downwardapi-volume-970d41be-f0bc-43e1-9e27-1949d64aeff3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 19 04:51:45.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-37" for this suite. 01/19/23 04:51:45.382
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","completed":123,"skipped":2300,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.078 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:51:39.31
    Jan 19 04:51:39.310: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename projected 01/19/23 04:51:39.311
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:51:39.326
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:51:39.328
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:192
    STEP: Creating a pod to test downward API volume plugin 01/19/23 04:51:39.33
    Jan 19 04:51:39.339: INFO: Waiting up to 5m0s for pod "downwardapi-volume-970d41be-f0bc-43e1-9e27-1949d64aeff3" in namespace "projected-37" to be "Succeeded or Failed"
    Jan 19 04:51:39.342: INFO: Pod "downwardapi-volume-970d41be-f0bc-43e1-9e27-1949d64aeff3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.120322ms
    Jan 19 04:51:41.346: INFO: Pod "downwardapi-volume-970d41be-f0bc-43e1-9e27-1949d64aeff3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00779867s
    Jan 19 04:51:43.349: INFO: Pod "downwardapi-volume-970d41be-f0bc-43e1-9e27-1949d64aeff3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010200317s
    Jan 19 04:51:45.352: INFO: Pod "downwardapi-volume-970d41be-f0bc-43e1-9e27-1949d64aeff3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013794881s
    STEP: Saw pod success 01/19/23 04:51:45.352
    Jan 19 04:51:45.353: INFO: Pod "downwardapi-volume-970d41be-f0bc-43e1-9e27-1949d64aeff3" satisfied condition "Succeeded or Failed"
    Jan 19 04:51:45.356: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod downwardapi-volume-970d41be-f0bc-43e1-9e27-1949d64aeff3 container client-container: <nil>
    STEP: delete the pod 01/19/23 04:51:45.362
    Jan 19 04:51:45.375: INFO: Waiting for pod downwardapi-volume-970d41be-f0bc-43e1-9e27-1949d64aeff3 to disappear
    Jan 19 04:51:45.378: INFO: Pod downwardapi-volume-970d41be-f0bc-43e1-9e27-1949d64aeff3 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 19 04:51:45.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-37" for this suite. 01/19/23 04:51:45.382
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:51:45.389
Jan 19 04:51:45.389: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename sched-pred 01/19/23 04:51:45.389
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:51:45.404
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:51:45.406
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 19 04:51:45.408: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 19 04:51:45.415: INFO: Waiting for terminating namespaces to be deleted...
Jan 19 04:51:45.417: INFO: 
Logging pods the apiserver thinks is on node ckcp-nks-default-worker-node-0 before test
Jan 19 04:51:45.423: INFO: kube-flannel-ds-amd64-djm45 from kube-flannel started at 2023-01-19 04:04:25 +0000 UTC (1 container statuses recorded)
Jan 19 04:51:45.423: INFO: 	Container kube-flannel ready: true, restart count 0
Jan 19 04:51:45.423: INFO: coredns-557b76dc76-cg7b6 from kube-system started at 2023-01-19 04:04:25 +0000 UTC (1 container statuses recorded)
Jan 19 04:51:45.423: INFO: 	Container coredns ready: true, restart count 0
Jan 19 04:51:45.423: INFO: coredns-557b76dc76-ncmw9 from kube-system started at 2023-01-19 04:04:25 +0000 UTC (1 container statuses recorded)
Jan 19 04:51:45.423: INFO: 	Container coredns ready: true, restart count 0
Jan 19 04:51:45.423: INFO: csi-cinder-controllerplugin-0 from kube-system started at 2023-01-19 04:04:35 +0000 UTC (5 container statuses recorded)
Jan 19 04:51:45.423: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 19 04:51:45.423: INFO: 	Container csi-attacher ready: true, restart count 0
Jan 19 04:51:45.423: INFO: 	Container csi-provisioner ready: true, restart count 0
Jan 19 04:51:45.423: INFO: 	Container csi-resizer ready: true, restart count 0
Jan 19 04:51:45.423: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jan 19 04:51:45.423: INFO: csi-cinder-nodeplugin-r2d6v from kube-system started at 2023-01-19 04:04:35 +0000 UTC (2 container statuses recorded)
Jan 19 04:51:45.423: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 19 04:51:45.423: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 19 04:51:45.423: INFO: dashboard-metrics-scraper-7cc7856cfb-p9hnn from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
Jan 19 04:51:45.423: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jan 19 04:51:45.423: INFO: kube-dns-autoscaler-7986c68b66-q7lq5 from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
Jan 19 04:51:45.423: INFO: 	Container autoscaler ready: true, restart count 0
Jan 19 04:51:45.423: INFO: kubernetes-dashboard-d7b78f849-zfz2z from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
Jan 19 04:51:45.423: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jan 19 04:51:45.423: INFO: metrics-server-c494c94fc-lkqhw from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
Jan 19 04:51:45.423: INFO: 	Container metrics-server ready: true, restart count 0
Jan 19 04:51:45.423: INFO: npd-428hd from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
Jan 19 04:51:45.423: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 19 04:51:45.423: INFO: snapshot-controller-6bd8bc5484-94b94 from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
Jan 19 04:51:45.423: INFO: 	Container snapshot-controller ready: true, restart count 0
Jan 19 04:51:45.423: INFO: snapshot-controller-6bd8bc5484-nh4pv from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
Jan 19 04:51:45.423: INFO: 	Container snapshot-controller ready: true, restart count 0
Jan 19 04:51:45.423: INFO: sonobuoy-systemd-logs-daemon-set-dbd8990eaea742d4-9s2t2 from sonobuoy started at 2023-01-19 04:19:31 +0000 UTC (2 container statuses recorded)
Jan 19 04:51:45.423: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 04:51:45.423: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 19 04:51:45.423: INFO: 
Logging pods the apiserver thinks is on node ckcp-nks-default-worker-node-1 before test
Jan 19 04:51:45.428: INFO: kube-flannel-ds-amd64-jqq8g from kube-flannel started at 2023-01-19 04:04:25 +0000 UTC (1 container statuses recorded)
Jan 19 04:51:45.428: INFO: 	Container kube-flannel ready: true, restart count 0
Jan 19 04:51:45.428: INFO: csi-cinder-nodeplugin-nrrr6 from kube-system started at 2023-01-19 04:04:35 +0000 UTC (2 container statuses recorded)
Jan 19 04:51:45.428: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 19 04:51:45.428: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 19 04:51:45.428: INFO: npd-wz6bt from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
Jan 19 04:51:45.428: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 19 04:51:45.428: INFO: pod-exec-websocket-7a494f1a-2308-44b9-8f57-e901cf8bf862 from pods-5032 started at 2023-01-19 04:51:15 +0000 UTC (1 container statuses recorded)
Jan 19 04:51:45.428: INFO: 	Container main ready: true, restart count 0
Jan 19 04:51:45.428: INFO: sonobuoy from sonobuoy started at 2023-01-19 04:19:22 +0000 UTC (1 container statuses recorded)
Jan 19 04:51:45.428: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 19 04:51:45.428: INFO: sonobuoy-e2e-job-35712e4fe0b540a2 from sonobuoy started at 2023-01-19 04:19:31 +0000 UTC (2 container statuses recorded)
Jan 19 04:51:45.428: INFO: 	Container e2e ready: true, restart count 0
Jan 19 04:51:45.428: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 04:51:45.428: INFO: sonobuoy-systemd-logs-daemon-set-dbd8990eaea742d4-7w8p4 from sonobuoy started at 2023-01-19 04:19:31 +0000 UTC (2 container statuses recorded)
Jan 19 04:51:45.428: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 04:51:45.428: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
STEP: verifying the node has the label node ckcp-nks-default-worker-node-0 01/19/23 04:51:45.446
STEP: verifying the node has the label node ckcp-nks-default-worker-node-1 01/19/23 04:51:45.462
Jan 19 04:51:45.474: INFO: Pod kube-flannel-ds-amd64-djm45 requesting resource cpu=100m on Node ckcp-nks-default-worker-node-0
Jan 19 04:51:45.474: INFO: Pod kube-flannel-ds-amd64-jqq8g requesting resource cpu=100m on Node ckcp-nks-default-worker-node-1
Jan 19 04:51:45.474: INFO: Pod coredns-557b76dc76-cg7b6 requesting resource cpu=100m on Node ckcp-nks-default-worker-node-0
Jan 19 04:51:45.474: INFO: Pod coredns-557b76dc76-ncmw9 requesting resource cpu=100m on Node ckcp-nks-default-worker-node-0
Jan 19 04:51:45.474: INFO: Pod csi-cinder-controllerplugin-0 requesting resource cpu=0m on Node ckcp-nks-default-worker-node-0
Jan 19 04:51:45.474: INFO: Pod csi-cinder-nodeplugin-nrrr6 requesting resource cpu=0m on Node ckcp-nks-default-worker-node-1
Jan 19 04:51:45.474: INFO: Pod csi-cinder-nodeplugin-r2d6v requesting resource cpu=0m on Node ckcp-nks-default-worker-node-0
Jan 19 04:51:45.474: INFO: Pod dashboard-metrics-scraper-7cc7856cfb-p9hnn requesting resource cpu=0m on Node ckcp-nks-default-worker-node-0
Jan 19 04:51:45.474: INFO: Pod kube-dns-autoscaler-7986c68b66-q7lq5 requesting resource cpu=20m on Node ckcp-nks-default-worker-node-0
Jan 19 04:51:45.474: INFO: Pod kubernetes-dashboard-d7b78f849-zfz2z requesting resource cpu=0m on Node ckcp-nks-default-worker-node-0
Jan 19 04:51:45.474: INFO: Pod metrics-server-c494c94fc-lkqhw requesting resource cpu=0m on Node ckcp-nks-default-worker-node-0
Jan 19 04:51:45.474: INFO: Pod npd-428hd requesting resource cpu=20m on Node ckcp-nks-default-worker-node-0
Jan 19 04:51:45.474: INFO: Pod npd-wz6bt requesting resource cpu=20m on Node ckcp-nks-default-worker-node-1
Jan 19 04:51:45.474: INFO: Pod snapshot-controller-6bd8bc5484-94b94 requesting resource cpu=0m on Node ckcp-nks-default-worker-node-0
Jan 19 04:51:45.474: INFO: Pod snapshot-controller-6bd8bc5484-nh4pv requesting resource cpu=0m on Node ckcp-nks-default-worker-node-0
Jan 19 04:51:45.474: INFO: Pod pod-exec-websocket-7a494f1a-2308-44b9-8f57-e901cf8bf862 requesting resource cpu=0m on Node ckcp-nks-default-worker-node-1
Jan 19 04:51:45.474: INFO: Pod sonobuoy requesting resource cpu=0m on Node ckcp-nks-default-worker-node-1
Jan 19 04:51:45.474: INFO: Pod sonobuoy-e2e-job-35712e4fe0b540a2 requesting resource cpu=0m on Node ckcp-nks-default-worker-node-1
Jan 19 04:51:45.474: INFO: Pod sonobuoy-systemd-logs-daemon-set-dbd8990eaea742d4-7w8p4 requesting resource cpu=0m on Node ckcp-nks-default-worker-node-1
Jan 19 04:51:45.474: INFO: Pod sonobuoy-systemd-logs-daemon-set-dbd8990eaea742d4-9s2t2 requesting resource cpu=0m on Node ckcp-nks-default-worker-node-0
STEP: Starting Pods to consume most of the cluster CPU. 01/19/23 04:51:45.474
Jan 19 04:51:45.474: INFO: Creating a pod which consumes cpu=1162m on Node ckcp-nks-default-worker-node-0
Jan 19 04:51:45.484: INFO: Creating a pod which consumes cpu=1316m on Node ckcp-nks-default-worker-node-1
Jan 19 04:51:45.496: INFO: Waiting up to 5m0s for pod "filler-pod-5d2784ff-fb1e-47be-a37c-de9b13b921e4" in namespace "sched-pred-2751" to be "running"
Jan 19 04:51:45.503: INFO: Pod "filler-pod-5d2784ff-fb1e-47be-a37c-de9b13b921e4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.004357ms
Jan 19 04:51:47.507: INFO: Pod "filler-pod-5d2784ff-fb1e-47be-a37c-de9b13b921e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011247661s
Jan 19 04:51:49.508: INFO: Pod "filler-pod-5d2784ff-fb1e-47be-a37c-de9b13b921e4": Phase="Running", Reason="", readiness=true. Elapsed: 4.012061274s
Jan 19 04:51:49.508: INFO: Pod "filler-pod-5d2784ff-fb1e-47be-a37c-de9b13b921e4" satisfied condition "running"
Jan 19 04:51:49.508: INFO: Waiting up to 5m0s for pod "filler-pod-280a108f-51e7-4d6c-87a3-764fa06b0ca6" in namespace "sched-pred-2751" to be "running"
Jan 19 04:51:49.511: INFO: Pod "filler-pod-280a108f-51e7-4d6c-87a3-764fa06b0ca6": Phase="Running", Reason="", readiness=true. Elapsed: 2.844235ms
Jan 19 04:51:49.511: INFO: Pod "filler-pod-280a108f-51e7-4d6c-87a3-764fa06b0ca6" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 01/19/23 04:51:49.511
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-280a108f-51e7-4d6c-87a3-764fa06b0ca6.173b9ca1dbf4dbbe], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2751/filler-pod-280a108f-51e7-4d6c-87a3-764fa06b0ca6 to ckcp-nks-default-worker-node-1] 01/19/23 04:51:49.514
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-280a108f-51e7-4d6c-87a3-764fa06b0ca6.173b9ca244c531c8], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/19/23 04:51:49.514
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-280a108f-51e7-4d6c-87a3-764fa06b0ca6.173b9ca24630f30f], Reason = [Created], Message = [Created container filler-pod-280a108f-51e7-4d6c-87a3-764fa06b0ca6] 01/19/23 04:51:49.514
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-280a108f-51e7-4d6c-87a3-764fa06b0ca6.173b9ca24bda9598], Reason = [Started], Message = [Started container filler-pod-280a108f-51e7-4d6c-87a3-764fa06b0ca6] 01/19/23 04:51:49.514
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5d2784ff-fb1e-47be-a37c-de9b13b921e4.173b9ca1dac01c2e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2751/filler-pod-5d2784ff-fb1e-47be-a37c-de9b13b921e4 to ckcp-nks-default-worker-node-0] 01/19/23 04:51:49.514
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5d2784ff-fb1e-47be-a37c-de9b13b921e4.173b9ca269bc227e], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/19/23 04:51:49.514
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5d2784ff-fb1e-47be-a37c-de9b13b921e4.173b9ca26b488135], Reason = [Created], Message = [Created container filler-pod-5d2784ff-fb1e-47be-a37c-de9b13b921e4] 01/19/23 04:51:49.514
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5d2784ff-fb1e-47be-a37c-de9b13b921e4.173b9ca270c69041], Reason = [Started], Message = [Started container filler-pod-5d2784ff-fb1e-47be-a37c-de9b13b921e4] 01/19/23 04:51:49.514
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.173b9ca2cafe0095], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod.] 01/19/23 04:51:49.529
STEP: removing the label node off the node ckcp-nks-default-worker-node-0 01/19/23 04:51:50.526
STEP: verifying the node doesn't have the label node 01/19/23 04:51:50.537
STEP: removing the label node off the node ckcp-nks-default-worker-node-1 01/19/23 04:51:50.541
STEP: verifying the node doesn't have the label node 01/19/23 04:51:50.552
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jan 19 04:51:50.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2751" for this suite. 01/19/23 04:51:50.565
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","completed":124,"skipped":2303,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.188 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:51:45.389
    Jan 19 04:51:45.389: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename sched-pred 01/19/23 04:51:45.389
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:51:45.404
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:51:45.406
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jan 19 04:51:45.408: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 19 04:51:45.415: INFO: Waiting for terminating namespaces to be deleted...
    Jan 19 04:51:45.417: INFO: 
    Logging pods the apiserver thinks is on node ckcp-nks-default-worker-node-0 before test
    Jan 19 04:51:45.423: INFO: kube-flannel-ds-amd64-djm45 from kube-flannel started at 2023-01-19 04:04:25 +0000 UTC (1 container statuses recorded)
    Jan 19 04:51:45.423: INFO: 	Container kube-flannel ready: true, restart count 0
    Jan 19 04:51:45.423: INFO: coredns-557b76dc76-cg7b6 from kube-system started at 2023-01-19 04:04:25 +0000 UTC (1 container statuses recorded)
    Jan 19 04:51:45.423: INFO: 	Container coredns ready: true, restart count 0
    Jan 19 04:51:45.423: INFO: coredns-557b76dc76-ncmw9 from kube-system started at 2023-01-19 04:04:25 +0000 UTC (1 container statuses recorded)
    Jan 19 04:51:45.423: INFO: 	Container coredns ready: true, restart count 0
    Jan 19 04:51:45.423: INFO: csi-cinder-controllerplugin-0 from kube-system started at 2023-01-19 04:04:35 +0000 UTC (5 container statuses recorded)
    Jan 19 04:51:45.423: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Jan 19 04:51:45.423: INFO: 	Container csi-attacher ready: true, restart count 0
    Jan 19 04:51:45.423: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jan 19 04:51:45.423: INFO: 	Container csi-resizer ready: true, restart count 0
    Jan 19 04:51:45.423: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jan 19 04:51:45.423: INFO: csi-cinder-nodeplugin-r2d6v from kube-system started at 2023-01-19 04:04:35 +0000 UTC (2 container statuses recorded)
    Jan 19 04:51:45.423: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Jan 19 04:51:45.423: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jan 19 04:51:45.423: INFO: dashboard-metrics-scraper-7cc7856cfb-p9hnn from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
    Jan 19 04:51:45.423: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    Jan 19 04:51:45.423: INFO: kube-dns-autoscaler-7986c68b66-q7lq5 from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
    Jan 19 04:51:45.423: INFO: 	Container autoscaler ready: true, restart count 0
    Jan 19 04:51:45.423: INFO: kubernetes-dashboard-d7b78f849-zfz2z from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
    Jan 19 04:51:45.423: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    Jan 19 04:51:45.423: INFO: metrics-server-c494c94fc-lkqhw from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
    Jan 19 04:51:45.423: INFO: 	Container metrics-server ready: true, restart count 0
    Jan 19 04:51:45.423: INFO: npd-428hd from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
    Jan 19 04:51:45.423: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan 19 04:51:45.423: INFO: snapshot-controller-6bd8bc5484-94b94 from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
    Jan 19 04:51:45.423: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jan 19 04:51:45.423: INFO: snapshot-controller-6bd8bc5484-nh4pv from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
    Jan 19 04:51:45.423: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jan 19 04:51:45.423: INFO: sonobuoy-systemd-logs-daemon-set-dbd8990eaea742d4-9s2t2 from sonobuoy started at 2023-01-19 04:19:31 +0000 UTC (2 container statuses recorded)
    Jan 19 04:51:45.423: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 19 04:51:45.423: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 19 04:51:45.423: INFO: 
    Logging pods the apiserver thinks is on node ckcp-nks-default-worker-node-1 before test
    Jan 19 04:51:45.428: INFO: kube-flannel-ds-amd64-jqq8g from kube-flannel started at 2023-01-19 04:04:25 +0000 UTC (1 container statuses recorded)
    Jan 19 04:51:45.428: INFO: 	Container kube-flannel ready: true, restart count 0
    Jan 19 04:51:45.428: INFO: csi-cinder-nodeplugin-nrrr6 from kube-system started at 2023-01-19 04:04:35 +0000 UTC (2 container statuses recorded)
    Jan 19 04:51:45.428: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Jan 19 04:51:45.428: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jan 19 04:51:45.428: INFO: npd-wz6bt from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
    Jan 19 04:51:45.428: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan 19 04:51:45.428: INFO: pod-exec-websocket-7a494f1a-2308-44b9-8f57-e901cf8bf862 from pods-5032 started at 2023-01-19 04:51:15 +0000 UTC (1 container statuses recorded)
    Jan 19 04:51:45.428: INFO: 	Container main ready: true, restart count 0
    Jan 19 04:51:45.428: INFO: sonobuoy from sonobuoy started at 2023-01-19 04:19:22 +0000 UTC (1 container statuses recorded)
    Jan 19 04:51:45.428: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 19 04:51:45.428: INFO: sonobuoy-e2e-job-35712e4fe0b540a2 from sonobuoy started at 2023-01-19 04:19:31 +0000 UTC (2 container statuses recorded)
    Jan 19 04:51:45.428: INFO: 	Container e2e ready: true, restart count 0
    Jan 19 04:51:45.428: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 19 04:51:45.428: INFO: sonobuoy-systemd-logs-daemon-set-dbd8990eaea742d4-7w8p4 from sonobuoy started at 2023-01-19 04:19:31 +0000 UTC (2 container statuses recorded)
    Jan 19 04:51:45.428: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 19 04:51:45.428: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:326
    STEP: verifying the node has the label node ckcp-nks-default-worker-node-0 01/19/23 04:51:45.446
    STEP: verifying the node has the label node ckcp-nks-default-worker-node-1 01/19/23 04:51:45.462
    Jan 19 04:51:45.474: INFO: Pod kube-flannel-ds-amd64-djm45 requesting resource cpu=100m on Node ckcp-nks-default-worker-node-0
    Jan 19 04:51:45.474: INFO: Pod kube-flannel-ds-amd64-jqq8g requesting resource cpu=100m on Node ckcp-nks-default-worker-node-1
    Jan 19 04:51:45.474: INFO: Pod coredns-557b76dc76-cg7b6 requesting resource cpu=100m on Node ckcp-nks-default-worker-node-0
    Jan 19 04:51:45.474: INFO: Pod coredns-557b76dc76-ncmw9 requesting resource cpu=100m on Node ckcp-nks-default-worker-node-0
    Jan 19 04:51:45.474: INFO: Pod csi-cinder-controllerplugin-0 requesting resource cpu=0m on Node ckcp-nks-default-worker-node-0
    Jan 19 04:51:45.474: INFO: Pod csi-cinder-nodeplugin-nrrr6 requesting resource cpu=0m on Node ckcp-nks-default-worker-node-1
    Jan 19 04:51:45.474: INFO: Pod csi-cinder-nodeplugin-r2d6v requesting resource cpu=0m on Node ckcp-nks-default-worker-node-0
    Jan 19 04:51:45.474: INFO: Pod dashboard-metrics-scraper-7cc7856cfb-p9hnn requesting resource cpu=0m on Node ckcp-nks-default-worker-node-0
    Jan 19 04:51:45.474: INFO: Pod kube-dns-autoscaler-7986c68b66-q7lq5 requesting resource cpu=20m on Node ckcp-nks-default-worker-node-0
    Jan 19 04:51:45.474: INFO: Pod kubernetes-dashboard-d7b78f849-zfz2z requesting resource cpu=0m on Node ckcp-nks-default-worker-node-0
    Jan 19 04:51:45.474: INFO: Pod metrics-server-c494c94fc-lkqhw requesting resource cpu=0m on Node ckcp-nks-default-worker-node-0
    Jan 19 04:51:45.474: INFO: Pod npd-428hd requesting resource cpu=20m on Node ckcp-nks-default-worker-node-0
    Jan 19 04:51:45.474: INFO: Pod npd-wz6bt requesting resource cpu=20m on Node ckcp-nks-default-worker-node-1
    Jan 19 04:51:45.474: INFO: Pod snapshot-controller-6bd8bc5484-94b94 requesting resource cpu=0m on Node ckcp-nks-default-worker-node-0
    Jan 19 04:51:45.474: INFO: Pod snapshot-controller-6bd8bc5484-nh4pv requesting resource cpu=0m on Node ckcp-nks-default-worker-node-0
    Jan 19 04:51:45.474: INFO: Pod pod-exec-websocket-7a494f1a-2308-44b9-8f57-e901cf8bf862 requesting resource cpu=0m on Node ckcp-nks-default-worker-node-1
    Jan 19 04:51:45.474: INFO: Pod sonobuoy requesting resource cpu=0m on Node ckcp-nks-default-worker-node-1
    Jan 19 04:51:45.474: INFO: Pod sonobuoy-e2e-job-35712e4fe0b540a2 requesting resource cpu=0m on Node ckcp-nks-default-worker-node-1
    Jan 19 04:51:45.474: INFO: Pod sonobuoy-systemd-logs-daemon-set-dbd8990eaea742d4-7w8p4 requesting resource cpu=0m on Node ckcp-nks-default-worker-node-1
    Jan 19 04:51:45.474: INFO: Pod sonobuoy-systemd-logs-daemon-set-dbd8990eaea742d4-9s2t2 requesting resource cpu=0m on Node ckcp-nks-default-worker-node-0
    STEP: Starting Pods to consume most of the cluster CPU. 01/19/23 04:51:45.474
    Jan 19 04:51:45.474: INFO: Creating a pod which consumes cpu=1162m on Node ckcp-nks-default-worker-node-0
    Jan 19 04:51:45.484: INFO: Creating a pod which consumes cpu=1316m on Node ckcp-nks-default-worker-node-1
    Jan 19 04:51:45.496: INFO: Waiting up to 5m0s for pod "filler-pod-5d2784ff-fb1e-47be-a37c-de9b13b921e4" in namespace "sched-pred-2751" to be "running"
    Jan 19 04:51:45.503: INFO: Pod "filler-pod-5d2784ff-fb1e-47be-a37c-de9b13b921e4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.004357ms
    Jan 19 04:51:47.507: INFO: Pod "filler-pod-5d2784ff-fb1e-47be-a37c-de9b13b921e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011247661s
    Jan 19 04:51:49.508: INFO: Pod "filler-pod-5d2784ff-fb1e-47be-a37c-de9b13b921e4": Phase="Running", Reason="", readiness=true. Elapsed: 4.012061274s
    Jan 19 04:51:49.508: INFO: Pod "filler-pod-5d2784ff-fb1e-47be-a37c-de9b13b921e4" satisfied condition "running"
    Jan 19 04:51:49.508: INFO: Waiting up to 5m0s for pod "filler-pod-280a108f-51e7-4d6c-87a3-764fa06b0ca6" in namespace "sched-pred-2751" to be "running"
    Jan 19 04:51:49.511: INFO: Pod "filler-pod-280a108f-51e7-4d6c-87a3-764fa06b0ca6": Phase="Running", Reason="", readiness=true. Elapsed: 2.844235ms
    Jan 19 04:51:49.511: INFO: Pod "filler-pod-280a108f-51e7-4d6c-87a3-764fa06b0ca6" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 01/19/23 04:51:49.511
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-280a108f-51e7-4d6c-87a3-764fa06b0ca6.173b9ca1dbf4dbbe], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2751/filler-pod-280a108f-51e7-4d6c-87a3-764fa06b0ca6 to ckcp-nks-default-worker-node-1] 01/19/23 04:51:49.514
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-280a108f-51e7-4d6c-87a3-764fa06b0ca6.173b9ca244c531c8], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/19/23 04:51:49.514
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-280a108f-51e7-4d6c-87a3-764fa06b0ca6.173b9ca24630f30f], Reason = [Created], Message = [Created container filler-pod-280a108f-51e7-4d6c-87a3-764fa06b0ca6] 01/19/23 04:51:49.514
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-280a108f-51e7-4d6c-87a3-764fa06b0ca6.173b9ca24bda9598], Reason = [Started], Message = [Started container filler-pod-280a108f-51e7-4d6c-87a3-764fa06b0ca6] 01/19/23 04:51:49.514
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-5d2784ff-fb1e-47be-a37c-de9b13b921e4.173b9ca1dac01c2e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2751/filler-pod-5d2784ff-fb1e-47be-a37c-de9b13b921e4 to ckcp-nks-default-worker-node-0] 01/19/23 04:51:49.514
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-5d2784ff-fb1e-47be-a37c-de9b13b921e4.173b9ca269bc227e], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/19/23 04:51:49.514
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-5d2784ff-fb1e-47be-a37c-de9b13b921e4.173b9ca26b488135], Reason = [Created], Message = [Created container filler-pod-5d2784ff-fb1e-47be-a37c-de9b13b921e4] 01/19/23 04:51:49.514
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-5d2784ff-fb1e-47be-a37c-de9b13b921e4.173b9ca270c69041], Reason = [Started], Message = [Started container filler-pod-5d2784ff-fb1e-47be-a37c-de9b13b921e4] 01/19/23 04:51:49.514
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.173b9ca2cafe0095], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod.] 01/19/23 04:51:49.529
    STEP: removing the label node off the node ckcp-nks-default-worker-node-0 01/19/23 04:51:50.526
    STEP: verifying the node doesn't have the label node 01/19/23 04:51:50.537
    STEP: removing the label node off the node ckcp-nks-default-worker-node-1 01/19/23 04:51:50.541
    STEP: verifying the node doesn't have the label node 01/19/23 04:51:50.552
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jan 19 04:51:50.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-2751" for this suite. 01/19/23 04:51:50.565
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:51:50.577
Jan 19 04:51:50.578: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename webhook 01/19/23 04:51:50.578
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:51:50.595
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:51:50.598
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/19/23 04:51:50.611
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 04:51:50.893
STEP: Deploying the webhook pod 01/19/23 04:51:50.902
STEP: Wait for the deployment to be ready 01/19/23 04:51:50.911
Jan 19 04:51:50.917: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 19 04:51:52.927: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 4, 51, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 51, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 4, 51, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 51, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/19/23 04:51:54.933
STEP: Verifying the service has paired with the endpoint 01/19/23 04:51:54.947
Jan 19 04:51:55.948: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
Jan 19 04:51:55.951: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5460-crds.webhook.example.com via the AdmissionRegistration API 01/19/23 04:51:56.466
STEP: Creating a custom resource while v1 is storage version 01/19/23 04:51:56.481
STEP: Patching Custom Resource Definition to set v2 as storage 01/19/23 04:51:58.572
STEP: Patching the custom resource while v2 is storage version 01/19/23 04:51:58.589
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 04:51:59.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7181" for this suite. 01/19/23 04:51:59.211
STEP: Destroying namespace "webhook-7181-markers" for this suite. 01/19/23 04:51:59.216
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","completed":125,"skipped":2322,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.686 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:51:50.577
    Jan 19 04:51:50.578: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename webhook 01/19/23 04:51:50.578
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:51:50.595
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:51:50.598
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/19/23 04:51:50.611
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 04:51:50.893
    STEP: Deploying the webhook pod 01/19/23 04:51:50.902
    STEP: Wait for the deployment to be ready 01/19/23 04:51:50.911
    Jan 19 04:51:50.917: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 19 04:51:52.927: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 4, 51, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 51, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 4, 51, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 51, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/19/23 04:51:54.933
    STEP: Verifying the service has paired with the endpoint 01/19/23 04:51:54.947
    Jan 19 04:51:55.948: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:322
    Jan 19 04:51:55.951: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5460-crds.webhook.example.com via the AdmissionRegistration API 01/19/23 04:51:56.466
    STEP: Creating a custom resource while v1 is storage version 01/19/23 04:51:56.481
    STEP: Patching Custom Resource Definition to set v2 as storage 01/19/23 04:51:58.572
    STEP: Patching the custom resource while v2 is storage version 01/19/23 04:51:58.589
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 04:51:59.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7181" for this suite. 01/19/23 04:51:59.211
    STEP: Destroying namespace "webhook-7181-markers" for this suite. 01/19/23 04:51:59.216
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:51:59.268
Jan 19 04:51:59.269: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename daemonsets 01/19/23 04:51:59.27
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:51:59.303
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:51:59.309
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
Jan 19 04:51:59.347: INFO: Create a RollingUpdate DaemonSet
Jan 19 04:51:59.354: INFO: Check that daemon pods launch on every node of the cluster
Jan 19 04:51:59.367: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 04:51:59.367: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
Jan 19 04:52:00.374: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 04:52:00.374: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
Jan 19 04:52:01.377: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 19 04:52:01.377: INFO: Node ckcp-nks-default-worker-node-1 is running 0 daemon pod, expected 1
Jan 19 04:52:02.374: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 19 04:52:02.375: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
Jan 19 04:52:02.375: INFO: Update the DaemonSet to trigger a rollout
Jan 19 04:52:02.384: INFO: Updating DaemonSet daemon-set
Jan 19 04:52:04.398: INFO: Roll back the DaemonSet before rollout is complete
Jan 19 04:52:04.407: INFO: Updating DaemonSet daemon-set
Jan 19 04:52:04.407: INFO: Make sure DaemonSet rollback is complete
Jan 19 04:52:04.410: INFO: Wrong image for pod: daemon-set-jqbf6. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Jan 19 04:52:04.410: INFO: Pod daemon-set-jqbf6 is not available
Jan 19 04:52:11.419: INFO: Pod daemon-set-2x799 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/19/23 04:52:11.43
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7534, will wait for the garbage collector to delete the pods 01/19/23 04:52:11.43
Jan 19 04:52:11.491: INFO: Deleting DaemonSet.extensions daemon-set took: 7.748469ms
Jan 19 04:52:11.591: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.626467ms
Jan 19 04:52:14.295: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 04:52:14.295: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 19 04:52:14.298: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"17051"},"items":null}

Jan 19 04:52:14.301: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"17051"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 19 04:52:14.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7534" for this suite. 01/19/23 04:52:14.31
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","completed":126,"skipped":2350,"failed":0}
------------------------------
â€¢ [SLOW TEST] [15.047 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:51:59.268
    Jan 19 04:51:59.269: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename daemonsets 01/19/23 04:51:59.27
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:51:59.303
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:51:59.309
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:431
    Jan 19 04:51:59.347: INFO: Create a RollingUpdate DaemonSet
    Jan 19 04:51:59.354: INFO: Check that daemon pods launch on every node of the cluster
    Jan 19 04:51:59.367: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 04:51:59.367: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
    Jan 19 04:52:00.374: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 04:52:00.374: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
    Jan 19 04:52:01.377: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 19 04:52:01.377: INFO: Node ckcp-nks-default-worker-node-1 is running 0 daemon pod, expected 1
    Jan 19 04:52:02.374: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 19 04:52:02.375: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    Jan 19 04:52:02.375: INFO: Update the DaemonSet to trigger a rollout
    Jan 19 04:52:02.384: INFO: Updating DaemonSet daemon-set
    Jan 19 04:52:04.398: INFO: Roll back the DaemonSet before rollout is complete
    Jan 19 04:52:04.407: INFO: Updating DaemonSet daemon-set
    Jan 19 04:52:04.407: INFO: Make sure DaemonSet rollback is complete
    Jan 19 04:52:04.410: INFO: Wrong image for pod: daemon-set-jqbf6. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
    Jan 19 04:52:04.410: INFO: Pod daemon-set-jqbf6 is not available
    Jan 19 04:52:11.419: INFO: Pod daemon-set-2x799 is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/19/23 04:52:11.43
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7534, will wait for the garbage collector to delete the pods 01/19/23 04:52:11.43
    Jan 19 04:52:11.491: INFO: Deleting DaemonSet.extensions daemon-set took: 7.748469ms
    Jan 19 04:52:11.591: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.626467ms
    Jan 19 04:52:14.295: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 04:52:14.295: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 19 04:52:14.298: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"17051"},"items":null}

    Jan 19 04:52:14.301: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"17051"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 19 04:52:14.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-7534" for this suite. 01/19/23 04:52:14.31
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:52:14.316
Jan 19 04:52:14.316: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename disruption 01/19/23 04:52:14.317
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:52:14.331
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:52:14.333
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
STEP: Waiting for the pdb to be processed 01/19/23 04:52:14.34
STEP: Waiting for all pods to be running 01/19/23 04:52:16.37
Jan 19 04:52:16.379: INFO: running pods: 0 < 3
Jan 19 04:52:18.387: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan 19 04:52:20.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-7006" for this suite. 01/19/23 04:52:20.391
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","completed":127,"skipped":2366,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.081 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:52:14.316
    Jan 19 04:52:14.316: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename disruption 01/19/23 04:52:14.317
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:52:14.331
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:52:14.333
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:140
    STEP: Waiting for the pdb to be processed 01/19/23 04:52:14.34
    STEP: Waiting for all pods to be running 01/19/23 04:52:16.37
    Jan 19 04:52:16.379: INFO: running pods: 0 < 3
    Jan 19 04:52:18.387: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan 19 04:52:20.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-7006" for this suite. 01/19/23 04:52:20.391
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:52:20.397
Jan 19 04:52:20.397: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename secrets 01/19/23 04:52:20.398
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:52:20.413
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:52:20.415
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
STEP: Creating secret with name secret-test-b0a87331-9aab-4910-9364-a810cc6a8012 01/19/23 04:52:20.417
STEP: Creating a pod to test consume secrets 01/19/23 04:52:20.422
Jan 19 04:52:20.431: INFO: Waiting up to 5m0s for pod "pod-secrets-7b47e108-4d03-4d45-bd11-c3fbbadebbb7" in namespace "secrets-2246" to be "Succeeded or Failed"
Jan 19 04:52:20.433: INFO: Pod "pod-secrets-7b47e108-4d03-4d45-bd11-c3fbbadebbb7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.737674ms
Jan 19 04:52:22.437: INFO: Pod "pod-secrets-7b47e108-4d03-4d45-bd11-c3fbbadebbb7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006683382s
Jan 19 04:52:24.437: INFO: Pod "pod-secrets-7b47e108-4d03-4d45-bd11-c3fbbadebbb7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006810741s
STEP: Saw pod success 01/19/23 04:52:24.437
Jan 19 04:52:24.437: INFO: Pod "pod-secrets-7b47e108-4d03-4d45-bd11-c3fbbadebbb7" satisfied condition "Succeeded or Failed"
Jan 19 04:52:24.440: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-secrets-7b47e108-4d03-4d45-bd11-c3fbbadebbb7 container secret-volume-test: <nil>
STEP: delete the pod 01/19/23 04:52:24.444
Jan 19 04:52:24.454: INFO: Waiting for pod pod-secrets-7b47e108-4d03-4d45-bd11-c3fbbadebbb7 to disappear
Jan 19 04:52:24.456: INFO: Pod pod-secrets-7b47e108-4d03-4d45-bd11-c3fbbadebbb7 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 19 04:52:24.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2246" for this suite. 01/19/23 04:52:24.46
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","completed":128,"skipped":2375,"failed":0}
------------------------------
â€¢ [4.068 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:52:20.397
    Jan 19 04:52:20.397: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename secrets 01/19/23 04:52:20.398
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:52:20.413
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:52:20.415
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:46
    STEP: Creating secret with name secret-test-b0a87331-9aab-4910-9364-a810cc6a8012 01/19/23 04:52:20.417
    STEP: Creating a pod to test consume secrets 01/19/23 04:52:20.422
    Jan 19 04:52:20.431: INFO: Waiting up to 5m0s for pod "pod-secrets-7b47e108-4d03-4d45-bd11-c3fbbadebbb7" in namespace "secrets-2246" to be "Succeeded or Failed"
    Jan 19 04:52:20.433: INFO: Pod "pod-secrets-7b47e108-4d03-4d45-bd11-c3fbbadebbb7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.737674ms
    Jan 19 04:52:22.437: INFO: Pod "pod-secrets-7b47e108-4d03-4d45-bd11-c3fbbadebbb7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006683382s
    Jan 19 04:52:24.437: INFO: Pod "pod-secrets-7b47e108-4d03-4d45-bd11-c3fbbadebbb7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006810741s
    STEP: Saw pod success 01/19/23 04:52:24.437
    Jan 19 04:52:24.437: INFO: Pod "pod-secrets-7b47e108-4d03-4d45-bd11-c3fbbadebbb7" satisfied condition "Succeeded or Failed"
    Jan 19 04:52:24.440: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-secrets-7b47e108-4d03-4d45-bd11-c3fbbadebbb7 container secret-volume-test: <nil>
    STEP: delete the pod 01/19/23 04:52:24.444
    Jan 19 04:52:24.454: INFO: Waiting for pod pod-secrets-7b47e108-4d03-4d45-bd11-c3fbbadebbb7 to disappear
    Jan 19 04:52:24.456: INFO: Pod pod-secrets-7b47e108-4d03-4d45-bd11-c3fbbadebbb7 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 19 04:52:24.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-2246" for this suite. 01/19/23 04:52:24.46
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:52:24.465
Jan 19 04:52:24.466: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename job 01/19/23 04:52:24.466
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:52:24.477
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:52:24.479
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
STEP: Creating Indexed job 01/19/23 04:52:24.481
STEP: Ensuring job reaches completions 01/19/23 04:52:24.488
STEP: Ensuring pods with index for job exist 01/19/23 04:52:36.493
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 19 04:52:36.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9003" for this suite. 01/19/23 04:52:36.5
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","completed":129,"skipped":2385,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.042 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:52:24.465
    Jan 19 04:52:24.466: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename job 01/19/23 04:52:24.466
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:52:24.477
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:52:24.479
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:194
    STEP: Creating Indexed job 01/19/23 04:52:24.481
    STEP: Ensuring job reaches completions 01/19/23 04:52:24.488
    STEP: Ensuring pods with index for job exist 01/19/23 04:52:36.493
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 19 04:52:36.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-9003" for this suite. 01/19/23 04:52:36.5
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:52:36.507
Jan 19 04:52:36.507: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename kubectl 01/19/23 04:52:36.508
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:52:36.521
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:52:36.523
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
STEP: validating cluster-info 01/19/23 04:52:36.525
Jan 19 04:52:36.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-3595 cluster-info'
Jan 19 04:52:36.601: INFO: stderr: ""
Jan 19 04:52:36.601: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.254.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 19 04:52:36.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3595" for this suite. 01/19/23 04:52:36.605
{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","completed":130,"skipped":2390,"failed":0}
------------------------------
â€¢ [0.105 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1242
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:52:36.507
    Jan 19 04:52:36.507: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename kubectl 01/19/23 04:52:36.508
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:52:36.521
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:52:36.523
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1248
    STEP: validating cluster-info 01/19/23 04:52:36.525
    Jan 19 04:52:36.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-3595 cluster-info'
    Jan 19 04:52:36.601: INFO: stderr: ""
    Jan 19 04:52:36.601: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.254.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 19 04:52:36.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3595" for this suite. 01/19/23 04:52:36.605
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:52:36.614
Jan 19 04:52:36.614: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename kubectl 01/19/23 04:52:36.615
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:52:36.64
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:52:36.641
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/19/23 04:52:36.643
Jan 19 04:52:36.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-7259 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan 19 04:52:36.726: INFO: stderr: ""
Jan 19 04:52:36.726: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 01/19/23 04:52:36.726
Jan 19 04:52:36.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-7259 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Jan 19 04:52:37.560: INFO: stderr: ""
Jan 19 04:52:37.560: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/19/23 04:52:37.56
Jan 19 04:52:37.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-7259 delete pods e2e-test-httpd-pod'
Jan 19 04:52:40.426: INFO: stderr: ""
Jan 19 04:52:40.426: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 19 04:52:40.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7259" for this suite. 01/19/23 04:52:40.429
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","completed":131,"skipped":2457,"failed":0}
------------------------------
â€¢ [3.820 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:954
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:960

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:52:36.614
    Jan 19 04:52:36.614: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename kubectl 01/19/23 04:52:36.615
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:52:36.64
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:52:36.641
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:960
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/19/23 04:52:36.643
    Jan 19 04:52:36.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-7259 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jan 19 04:52:36.726: INFO: stderr: ""
    Jan 19 04:52:36.726: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 01/19/23 04:52:36.726
    Jan 19 04:52:36.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-7259 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
    Jan 19 04:52:37.560: INFO: stderr: ""
    Jan 19 04:52:37.560: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/19/23 04:52:37.56
    Jan 19 04:52:37.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-7259 delete pods e2e-test-httpd-pod'
    Jan 19 04:52:40.426: INFO: stderr: ""
    Jan 19 04:52:40.426: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 19 04:52:40.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7259" for this suite. 01/19/23 04:52:40.429
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:52:40.435
Jan 19 04:52:40.435: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename container-probe 01/19/23 04:52:40.436
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:52:40.449
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:52:40.451
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
STEP: Creating pod liveness-ebeb14e4-a72c-4553-adf2-13d1d6a196da in namespace container-probe-5994 01/19/23 04:52:40.453
Jan 19 04:52:40.461: INFO: Waiting up to 5m0s for pod "liveness-ebeb14e4-a72c-4553-adf2-13d1d6a196da" in namespace "container-probe-5994" to be "not pending"
Jan 19 04:52:40.465: INFO: Pod "liveness-ebeb14e4-a72c-4553-adf2-13d1d6a196da": Phase="Pending", Reason="", readiness=false. Elapsed: 4.413803ms
Jan 19 04:52:42.469: INFO: Pod "liveness-ebeb14e4-a72c-4553-adf2-13d1d6a196da": Phase="Running", Reason="", readiness=true. Elapsed: 2.008237611s
Jan 19 04:52:42.469: INFO: Pod "liveness-ebeb14e4-a72c-4553-adf2-13d1d6a196da" satisfied condition "not pending"
Jan 19 04:52:42.469: INFO: Started pod liveness-ebeb14e4-a72c-4553-adf2-13d1d6a196da in namespace container-probe-5994
STEP: checking the pod's current state and verifying that restartCount is present 01/19/23 04:52:42.469
Jan 19 04:52:42.472: INFO: Initial restart count of pod liveness-ebeb14e4-a72c-4553-adf2-13d1d6a196da is 0
Jan 19 04:53:02.517: INFO: Restart count of pod container-probe-5994/liveness-ebeb14e4-a72c-4553-adf2-13d1d6a196da is now 1 (20.044928836s elapsed)
Jan 19 04:53:22.559: INFO: Restart count of pod container-probe-5994/liveness-ebeb14e4-a72c-4553-adf2-13d1d6a196da is now 2 (40.087700173s elapsed)
Jan 19 04:53:42.602: INFO: Restart count of pod container-probe-5994/liveness-ebeb14e4-a72c-4553-adf2-13d1d6a196da is now 3 (1m0.130375764s elapsed)
Jan 19 04:54:02.650: INFO: Restart count of pod container-probe-5994/liveness-ebeb14e4-a72c-4553-adf2-13d1d6a196da is now 4 (1m20.178522252s elapsed)
Jan 19 04:55:08.803: INFO: Restart count of pod container-probe-5994/liveness-ebeb14e4-a72c-4553-adf2-13d1d6a196da is now 5 (2m26.330905506s elapsed)
STEP: deleting the pod 01/19/23 04:55:08.803
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 19 04:55:08.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5994" for this suite. 01/19/23 04:55:08.82
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","completed":132,"skipped":2499,"failed":0}
------------------------------
â€¢ [SLOW TEST] [148.390 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:52:40.435
    Jan 19 04:52:40.435: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename container-probe 01/19/23 04:52:40.436
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:52:40.449
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:52:40.451
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:195
    STEP: Creating pod liveness-ebeb14e4-a72c-4553-adf2-13d1d6a196da in namespace container-probe-5994 01/19/23 04:52:40.453
    Jan 19 04:52:40.461: INFO: Waiting up to 5m0s for pod "liveness-ebeb14e4-a72c-4553-adf2-13d1d6a196da" in namespace "container-probe-5994" to be "not pending"
    Jan 19 04:52:40.465: INFO: Pod "liveness-ebeb14e4-a72c-4553-adf2-13d1d6a196da": Phase="Pending", Reason="", readiness=false. Elapsed: 4.413803ms
    Jan 19 04:52:42.469: INFO: Pod "liveness-ebeb14e4-a72c-4553-adf2-13d1d6a196da": Phase="Running", Reason="", readiness=true. Elapsed: 2.008237611s
    Jan 19 04:52:42.469: INFO: Pod "liveness-ebeb14e4-a72c-4553-adf2-13d1d6a196da" satisfied condition "not pending"
    Jan 19 04:52:42.469: INFO: Started pod liveness-ebeb14e4-a72c-4553-adf2-13d1d6a196da in namespace container-probe-5994
    STEP: checking the pod's current state and verifying that restartCount is present 01/19/23 04:52:42.469
    Jan 19 04:52:42.472: INFO: Initial restart count of pod liveness-ebeb14e4-a72c-4553-adf2-13d1d6a196da is 0
    Jan 19 04:53:02.517: INFO: Restart count of pod container-probe-5994/liveness-ebeb14e4-a72c-4553-adf2-13d1d6a196da is now 1 (20.044928836s elapsed)
    Jan 19 04:53:22.559: INFO: Restart count of pod container-probe-5994/liveness-ebeb14e4-a72c-4553-adf2-13d1d6a196da is now 2 (40.087700173s elapsed)
    Jan 19 04:53:42.602: INFO: Restart count of pod container-probe-5994/liveness-ebeb14e4-a72c-4553-adf2-13d1d6a196da is now 3 (1m0.130375764s elapsed)
    Jan 19 04:54:02.650: INFO: Restart count of pod container-probe-5994/liveness-ebeb14e4-a72c-4553-adf2-13d1d6a196da is now 4 (1m20.178522252s elapsed)
    Jan 19 04:55:08.803: INFO: Restart count of pod container-probe-5994/liveness-ebeb14e4-a72c-4553-adf2-13d1d6a196da is now 5 (2m26.330905506s elapsed)
    STEP: deleting the pod 01/19/23 04:55:08.803
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 19 04:55:08.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-5994" for this suite. 01/19/23 04:55:08.82
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:55:08.826
Jan 19 04:55:08.826: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename secrets 01/19/23 04:55:08.826
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:55:08.843
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:55:08.845
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
STEP: creating a secret 01/19/23 04:55:08.847
STEP: listing secrets in all namespaces to ensure that there are more than zero 01/19/23 04:55:08.854
STEP: patching the secret 01/19/23 04:55:08.858
STEP: deleting the secret using a LabelSelector 01/19/23 04:55:08.866
STEP: listing secrets in all namespaces, searching for label name and value in patch 01/19/23 04:55:08.875
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jan 19 04:55:08.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6832" for this suite. 01/19/23 04:55:08.902
{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","completed":133,"skipped":2509,"failed":0}
------------------------------
â€¢ [0.082 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:55:08.826
    Jan 19 04:55:08.826: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename secrets 01/19/23 04:55:08.826
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:55:08.843
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:55:08.845
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:153
    STEP: creating a secret 01/19/23 04:55:08.847
    STEP: listing secrets in all namespaces to ensure that there are more than zero 01/19/23 04:55:08.854
    STEP: patching the secret 01/19/23 04:55:08.858
    STEP: deleting the secret using a LabelSelector 01/19/23 04:55:08.866
    STEP: listing secrets in all namespaces, searching for label name and value in patch 01/19/23 04:55:08.875
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jan 19 04:55:08.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-6832" for this suite. 01/19/23 04:55:08.902
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:55:08.909
Jan 19 04:55:08.909: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename tables 01/19/23 04:55:08.909
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:55:08.924
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:55:08.926
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
Jan 19 04:55:08.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-8917" for this suite. 01/19/23 04:55:08.94
{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","completed":134,"skipped":2540,"failed":0}
------------------------------
â€¢ [0.038 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:55:08.909
    Jan 19 04:55:08.909: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename tables 01/19/23 04:55:08.909
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:55:08.924
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:55:08.926
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:187
    Jan 19 04:55:08.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "tables-8917" for this suite. 01/19/23 04:55:08.94
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:55:08.947
Jan 19 04:55:08.947: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename emptydir 01/19/23 04:55:08.948
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:55:08.96
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:55:08.963
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
STEP: Creating a pod to test emptydir volume type on tmpfs 01/19/23 04:55:08.965
Jan 19 04:55:08.974: INFO: Waiting up to 5m0s for pod "pod-69529e5a-ad5a-4b10-80fa-c14801caff48" in namespace "emptydir-7461" to be "Succeeded or Failed"
Jan 19 04:55:08.978: INFO: Pod "pod-69529e5a-ad5a-4b10-80fa-c14801caff48": Phase="Pending", Reason="", readiness=false. Elapsed: 3.609573ms
Jan 19 04:55:10.982: INFO: Pod "pod-69529e5a-ad5a-4b10-80fa-c14801caff48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007712685s
Jan 19 04:55:12.982: INFO: Pod "pod-69529e5a-ad5a-4b10-80fa-c14801caff48": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007820835s
Jan 19 04:55:14.982: INFO: Pod "pod-69529e5a-ad5a-4b10-80fa-c14801caff48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007473131s
STEP: Saw pod success 01/19/23 04:55:14.982
Jan 19 04:55:14.982: INFO: Pod "pod-69529e5a-ad5a-4b10-80fa-c14801caff48" satisfied condition "Succeeded or Failed"
Jan 19 04:55:14.984: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-69529e5a-ad5a-4b10-80fa-c14801caff48 container test-container: <nil>
STEP: delete the pod 01/19/23 04:55:15.017
Jan 19 04:55:15.031: INFO: Waiting for pod pod-69529e5a-ad5a-4b10-80fa-c14801caff48 to disappear
Jan 19 04:55:15.034: INFO: Pod pod-69529e5a-ad5a-4b10-80fa-c14801caff48 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 19 04:55:15.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7461" for this suite. 01/19/23 04:55:15.037
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":135,"skipped":2548,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.096 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:55:08.947
    Jan 19 04:55:08.947: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename emptydir 01/19/23 04:55:08.948
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:55:08.96
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:55:08.963
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:86
    STEP: Creating a pod to test emptydir volume type on tmpfs 01/19/23 04:55:08.965
    Jan 19 04:55:08.974: INFO: Waiting up to 5m0s for pod "pod-69529e5a-ad5a-4b10-80fa-c14801caff48" in namespace "emptydir-7461" to be "Succeeded or Failed"
    Jan 19 04:55:08.978: INFO: Pod "pod-69529e5a-ad5a-4b10-80fa-c14801caff48": Phase="Pending", Reason="", readiness=false. Elapsed: 3.609573ms
    Jan 19 04:55:10.982: INFO: Pod "pod-69529e5a-ad5a-4b10-80fa-c14801caff48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007712685s
    Jan 19 04:55:12.982: INFO: Pod "pod-69529e5a-ad5a-4b10-80fa-c14801caff48": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007820835s
    Jan 19 04:55:14.982: INFO: Pod "pod-69529e5a-ad5a-4b10-80fa-c14801caff48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007473131s
    STEP: Saw pod success 01/19/23 04:55:14.982
    Jan 19 04:55:14.982: INFO: Pod "pod-69529e5a-ad5a-4b10-80fa-c14801caff48" satisfied condition "Succeeded or Failed"
    Jan 19 04:55:14.984: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-69529e5a-ad5a-4b10-80fa-c14801caff48 container test-container: <nil>
    STEP: delete the pod 01/19/23 04:55:15.017
    Jan 19 04:55:15.031: INFO: Waiting for pod pod-69529e5a-ad5a-4b10-80fa-c14801caff48 to disappear
    Jan 19 04:55:15.034: INFO: Pod pod-69529e5a-ad5a-4b10-80fa-c14801caff48 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 19 04:55:15.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7461" for this suite. 01/19/23 04:55:15.037
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:55:15.044
Jan 19 04:55:15.044: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename prestop 01/19/23 04:55:15.044
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:55:15.057
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:55:15.059
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-9125 01/19/23 04:55:15.061
STEP: Waiting for pods to come up. 01/19/23 04:55:15.068
Jan 19 04:55:15.068: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-9125" to be "running"
Jan 19 04:55:15.070: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.463156ms
Jan 19 04:55:17.075: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006795555s
Jan 19 04:55:19.076: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 4.008325604s
Jan 19 04:55:19.076: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-9125 01/19/23 04:55:19.08
Jan 19 04:55:19.087: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-9125" to be "running"
Jan 19 04:55:19.092: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 4.973795ms
Jan 19 04:55:21.097: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009425254s
Jan 19 04:55:23.097: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 4.00994264s
Jan 19 04:55:23.097: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 01/19/23 04:55:23.097
Jan 19 04:55:28.111: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 01/19/23 04:55:28.111
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
Jan 19 04:55:28.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-9125" for this suite. 01/19/23 04:55:28.147
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","completed":136,"skipped":2574,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.110 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:55:15.044
    Jan 19 04:55:15.044: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename prestop 01/19/23 04:55:15.044
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:55:15.057
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:55:15.059
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-9125 01/19/23 04:55:15.061
    STEP: Waiting for pods to come up. 01/19/23 04:55:15.068
    Jan 19 04:55:15.068: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-9125" to be "running"
    Jan 19 04:55:15.070: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.463156ms
    Jan 19 04:55:17.075: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006795555s
    Jan 19 04:55:19.076: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 4.008325604s
    Jan 19 04:55:19.076: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-9125 01/19/23 04:55:19.08
    Jan 19 04:55:19.087: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-9125" to be "running"
    Jan 19 04:55:19.092: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 4.973795ms
    Jan 19 04:55:21.097: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009425254s
    Jan 19 04:55:23.097: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 4.00994264s
    Jan 19 04:55:23.097: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 01/19/23 04:55:23.097
    Jan 19 04:55:28.111: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 01/19/23 04:55:28.111
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/framework.go:187
    Jan 19 04:55:28.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "prestop-9125" for this suite. 01/19/23 04:55:28.147
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:55:28.154
Jan 19 04:55:28.154: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename projected 01/19/23 04:55:28.155
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:55:28.169
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:55:28.171
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
STEP: Creating the pod 01/19/23 04:55:28.174
Jan 19 04:55:28.183: INFO: Waiting up to 5m0s for pod "annotationupdate16ea01da-7dc9-447b-8885-d05e1cb3a840" in namespace "projected-4467" to be "running and ready"
Jan 19 04:55:28.186: INFO: Pod "annotationupdate16ea01da-7dc9-447b-8885-d05e1cb3a840": Phase="Pending", Reason="", readiness=false. Elapsed: 3.276587ms
Jan 19 04:55:28.186: INFO: The phase of Pod annotationupdate16ea01da-7dc9-447b-8885-d05e1cb3a840 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 04:55:30.192: INFO: Pod "annotationupdate16ea01da-7dc9-447b-8885-d05e1cb3a840": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008879893s
Jan 19 04:55:30.192: INFO: The phase of Pod annotationupdate16ea01da-7dc9-447b-8885-d05e1cb3a840 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 04:55:32.190: INFO: Pod "annotationupdate16ea01da-7dc9-447b-8885-d05e1cb3a840": Phase="Running", Reason="", readiness=true. Elapsed: 4.007112038s
Jan 19 04:55:32.190: INFO: The phase of Pod annotationupdate16ea01da-7dc9-447b-8885-d05e1cb3a840 is Running (Ready = true)
Jan 19 04:55:32.190: INFO: Pod "annotationupdate16ea01da-7dc9-447b-8885-d05e1cb3a840" satisfied condition "running and ready"
Jan 19 04:55:32.710: INFO: Successfully updated pod "annotationupdate16ea01da-7dc9-447b-8885-d05e1cb3a840"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 19 04:55:34.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4467" for this suite. 01/19/23 04:55:34.727
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","completed":137,"skipped":2596,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.580 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:55:28.154
    Jan 19 04:55:28.154: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename projected 01/19/23 04:55:28.155
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:55:28.169
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:55:28.171
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:161
    STEP: Creating the pod 01/19/23 04:55:28.174
    Jan 19 04:55:28.183: INFO: Waiting up to 5m0s for pod "annotationupdate16ea01da-7dc9-447b-8885-d05e1cb3a840" in namespace "projected-4467" to be "running and ready"
    Jan 19 04:55:28.186: INFO: Pod "annotationupdate16ea01da-7dc9-447b-8885-d05e1cb3a840": Phase="Pending", Reason="", readiness=false. Elapsed: 3.276587ms
    Jan 19 04:55:28.186: INFO: The phase of Pod annotationupdate16ea01da-7dc9-447b-8885-d05e1cb3a840 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 04:55:30.192: INFO: Pod "annotationupdate16ea01da-7dc9-447b-8885-d05e1cb3a840": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008879893s
    Jan 19 04:55:30.192: INFO: The phase of Pod annotationupdate16ea01da-7dc9-447b-8885-d05e1cb3a840 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 04:55:32.190: INFO: Pod "annotationupdate16ea01da-7dc9-447b-8885-d05e1cb3a840": Phase="Running", Reason="", readiness=true. Elapsed: 4.007112038s
    Jan 19 04:55:32.190: INFO: The phase of Pod annotationupdate16ea01da-7dc9-447b-8885-d05e1cb3a840 is Running (Ready = true)
    Jan 19 04:55:32.190: INFO: Pod "annotationupdate16ea01da-7dc9-447b-8885-d05e1cb3a840" satisfied condition "running and ready"
    Jan 19 04:55:32.710: INFO: Successfully updated pod "annotationupdate16ea01da-7dc9-447b-8885-d05e1cb3a840"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 19 04:55:34.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4467" for this suite. 01/19/23 04:55:34.727
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:55:34.735
Jan 19 04:55:34.735: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename kubelet-test 01/19/23 04:55:34.735
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:55:34.752
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:55:34.753
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan 19 04:55:34.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5614" for this suite. 01/19/23 04:55:34.795
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","completed":138,"skipped":2602,"failed":0}
------------------------------
â€¢ [0.071 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:55:34.735
    Jan 19 04:55:34.735: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename kubelet-test 01/19/23 04:55:34.735
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:55:34.752
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:55:34.753
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan 19 04:55:34.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-5614" for this suite. 01/19/23 04:55:34.795
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:55:34.806
Jan 19 04:55:34.806: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename replication-controller 01/19/23 04:55:34.806
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:55:34.82
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:55:34.821
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
STEP: Creating replication controller my-hostname-basic-eee74745-d513-486b-9827-b50a807bc171 01/19/23 04:55:34.823
Jan 19 04:55:34.832: INFO: Pod name my-hostname-basic-eee74745-d513-486b-9827-b50a807bc171: Found 0 pods out of 1
Jan 19 04:55:39.837: INFO: Pod name my-hostname-basic-eee74745-d513-486b-9827-b50a807bc171: Found 1 pods out of 1
Jan 19 04:55:39.837: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-eee74745-d513-486b-9827-b50a807bc171" are running
Jan 19 04:55:39.837: INFO: Waiting up to 5m0s for pod "my-hostname-basic-eee74745-d513-486b-9827-b50a807bc171-jkrzv" in namespace "replication-controller-4012" to be "running"
Jan 19 04:55:39.840: INFO: Pod "my-hostname-basic-eee74745-d513-486b-9827-b50a807bc171-jkrzv": Phase="Running", Reason="", readiness=true. Elapsed: 3.737036ms
Jan 19 04:55:39.840: INFO: Pod "my-hostname-basic-eee74745-d513-486b-9827-b50a807bc171-jkrzv" satisfied condition "running"
Jan 19 04:55:39.840: INFO: Pod "my-hostname-basic-eee74745-d513-486b-9827-b50a807bc171-jkrzv" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-19 04:55:34 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-19 04:55:36 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-19 04:55:36 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-19 04:55:34 +0000 UTC Reason: Message:}])
Jan 19 04:55:39.840: INFO: Trying to dial the pod
Jan 19 04:55:44.852: INFO: Controller my-hostname-basic-eee74745-d513-486b-9827-b50a807bc171: Got expected result from replica 1 [my-hostname-basic-eee74745-d513-486b-9827-b50a807bc171-jkrzv]: "my-hostname-basic-eee74745-d513-486b-9827-b50a807bc171-jkrzv", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan 19 04:55:44.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4012" for this suite. 01/19/23 04:55:44.856
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","completed":139,"skipped":2602,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.058 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:55:34.806
    Jan 19 04:55:34.806: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename replication-controller 01/19/23 04:55:34.806
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:55:34.82
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:55:34.821
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:66
    STEP: Creating replication controller my-hostname-basic-eee74745-d513-486b-9827-b50a807bc171 01/19/23 04:55:34.823
    Jan 19 04:55:34.832: INFO: Pod name my-hostname-basic-eee74745-d513-486b-9827-b50a807bc171: Found 0 pods out of 1
    Jan 19 04:55:39.837: INFO: Pod name my-hostname-basic-eee74745-d513-486b-9827-b50a807bc171: Found 1 pods out of 1
    Jan 19 04:55:39.837: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-eee74745-d513-486b-9827-b50a807bc171" are running
    Jan 19 04:55:39.837: INFO: Waiting up to 5m0s for pod "my-hostname-basic-eee74745-d513-486b-9827-b50a807bc171-jkrzv" in namespace "replication-controller-4012" to be "running"
    Jan 19 04:55:39.840: INFO: Pod "my-hostname-basic-eee74745-d513-486b-9827-b50a807bc171-jkrzv": Phase="Running", Reason="", readiness=true. Elapsed: 3.737036ms
    Jan 19 04:55:39.840: INFO: Pod "my-hostname-basic-eee74745-d513-486b-9827-b50a807bc171-jkrzv" satisfied condition "running"
    Jan 19 04:55:39.840: INFO: Pod "my-hostname-basic-eee74745-d513-486b-9827-b50a807bc171-jkrzv" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-19 04:55:34 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-19 04:55:36 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-19 04:55:36 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-19 04:55:34 +0000 UTC Reason: Message:}])
    Jan 19 04:55:39.840: INFO: Trying to dial the pod
    Jan 19 04:55:44.852: INFO: Controller my-hostname-basic-eee74745-d513-486b-9827-b50a807bc171: Got expected result from replica 1 [my-hostname-basic-eee74745-d513-486b-9827-b50a807bc171-jkrzv]: "my-hostname-basic-eee74745-d513-486b-9827-b50a807bc171-jkrzv", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan 19 04:55:44.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-4012" for this suite. 01/19/23 04:55:44.856
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:55:44.864
Jan 19 04:55:44.864: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename replication-controller 01/19/23 04:55:44.864
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:55:44.88
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:55:44.882
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
STEP: Given a ReplicationController is created 01/19/23 04:55:44.884
STEP: When the matched label of one of its pods change 01/19/23 04:55:44.889
Jan 19 04:55:44.892: INFO: Pod name pod-release: Found 0 pods out of 1
Jan 19 04:55:49.896: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 01/19/23 04:55:49.905
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan 19 04:55:50.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3211" for this suite. 01/19/23 04:55:50.919
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","completed":140,"skipped":2604,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.061 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:55:44.864
    Jan 19 04:55:44.864: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename replication-controller 01/19/23 04:55:44.864
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:55:44.88
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:55:44.882
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:100
    STEP: Given a ReplicationController is created 01/19/23 04:55:44.884
    STEP: When the matched label of one of its pods change 01/19/23 04:55:44.889
    Jan 19 04:55:44.892: INFO: Pod name pod-release: Found 0 pods out of 1
    Jan 19 04:55:49.896: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 01/19/23 04:55:49.905
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan 19 04:55:50.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-3211" for this suite. 01/19/23 04:55:50.919
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:55:50.925
Jan 19 04:55:50.925: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename kubectl 01/19/23 04:55:50.926
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:55:50.941
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:55:50.944
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
STEP: creating all guestbook components 01/19/23 04:55:50.945
Jan 19 04:55:50.946: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jan 19 04:55:50.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-6377 create -f -'
Jan 19 04:55:51.756: INFO: stderr: ""
Jan 19 04:55:51.756: INFO: stdout: "service/agnhost-replica created\n"
Jan 19 04:55:51.756: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jan 19 04:55:51.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-6377 create -f -'
Jan 19 04:55:51.982: INFO: stderr: ""
Jan 19 04:55:51.982: INFO: stdout: "service/agnhost-primary created\n"
Jan 19 04:55:51.982: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jan 19 04:55:51.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-6377 create -f -'
Jan 19 04:55:52.192: INFO: stderr: ""
Jan 19 04:55:52.192: INFO: stdout: "service/frontend created\n"
Jan 19 04:55:52.192: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jan 19 04:55:52.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-6377 create -f -'
Jan 19 04:55:52.404: INFO: stderr: ""
Jan 19 04:55:52.404: INFO: stdout: "deployment.apps/frontend created\n"
Jan 19 04:55:52.404: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 19 04:55:52.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-6377 create -f -'
Jan 19 04:55:52.622: INFO: stderr: ""
Jan 19 04:55:52.622: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jan 19 04:55:52.622: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 19 04:55:52.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-6377 create -f -'
Jan 19 04:55:52.942: INFO: stderr: ""
Jan 19 04:55:52.942: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 01/19/23 04:55:52.942
Jan 19 04:55:52.942: INFO: Waiting for all frontend pods to be Running.
Jan 19 04:55:57.993: INFO: Waiting for frontend to serve content.
Jan 19 04:55:58.003: INFO: Trying to add a new entry to the guestbook.
Jan 19 04:55:58.024: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 01/19/23 04:55:58.038
Jan 19 04:55:58.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-6377 delete --grace-period=0 --force -f -'
Jan 19 04:55:58.152: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 19 04:55:58.152: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 01/19/23 04:55:58.152
Jan 19 04:55:58.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-6377 delete --grace-period=0 --force -f -'
Jan 19 04:55:58.251: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 19 04:55:58.251: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 01/19/23 04:55:58.251
Jan 19 04:55:58.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-6377 delete --grace-period=0 --force -f -'
Jan 19 04:55:58.356: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 19 04:55:58.356: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 01/19/23 04:55:58.356
Jan 19 04:55:58.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-6377 delete --grace-period=0 --force -f -'
Jan 19 04:55:58.435: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 19 04:55:58.435: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 01/19/23 04:55:58.435
Jan 19 04:55:58.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-6377 delete --grace-period=0 --force -f -'
Jan 19 04:55:58.572: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 19 04:55:58.572: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 01/19/23 04:55:58.572
Jan 19 04:55:58.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-6377 delete --grace-period=0 --force -f -'
Jan 19 04:55:58.703: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 19 04:55:58.703: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 19 04:55:58.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6377" for this suite. 01/19/23 04:55:58.709
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","completed":141,"skipped":2626,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.799 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:367
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:55:50.925
    Jan 19 04:55:50.925: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename kubectl 01/19/23 04:55:50.926
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:55:50.941
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:55:50.944
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:392
    STEP: creating all guestbook components 01/19/23 04:55:50.945
    Jan 19 04:55:50.946: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Jan 19 04:55:50.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-6377 create -f -'
    Jan 19 04:55:51.756: INFO: stderr: ""
    Jan 19 04:55:51.756: INFO: stdout: "service/agnhost-replica created\n"
    Jan 19 04:55:51.756: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Jan 19 04:55:51.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-6377 create -f -'
    Jan 19 04:55:51.982: INFO: stderr: ""
    Jan 19 04:55:51.982: INFO: stdout: "service/agnhost-primary created\n"
    Jan 19 04:55:51.982: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Jan 19 04:55:51.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-6377 create -f -'
    Jan 19 04:55:52.192: INFO: stderr: ""
    Jan 19 04:55:52.192: INFO: stdout: "service/frontend created\n"
    Jan 19 04:55:52.192: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Jan 19 04:55:52.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-6377 create -f -'
    Jan 19 04:55:52.404: INFO: stderr: ""
    Jan 19 04:55:52.404: INFO: stdout: "deployment.apps/frontend created\n"
    Jan 19 04:55:52.404: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jan 19 04:55:52.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-6377 create -f -'
    Jan 19 04:55:52.622: INFO: stderr: ""
    Jan 19 04:55:52.622: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Jan 19 04:55:52.622: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jan 19 04:55:52.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-6377 create -f -'
    Jan 19 04:55:52.942: INFO: stderr: ""
    Jan 19 04:55:52.942: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 01/19/23 04:55:52.942
    Jan 19 04:55:52.942: INFO: Waiting for all frontend pods to be Running.
    Jan 19 04:55:57.993: INFO: Waiting for frontend to serve content.
    Jan 19 04:55:58.003: INFO: Trying to add a new entry to the guestbook.
    Jan 19 04:55:58.024: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 01/19/23 04:55:58.038
    Jan 19 04:55:58.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-6377 delete --grace-period=0 --force -f -'
    Jan 19 04:55:58.152: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 19 04:55:58.152: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 01/19/23 04:55:58.152
    Jan 19 04:55:58.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-6377 delete --grace-period=0 --force -f -'
    Jan 19 04:55:58.251: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 19 04:55:58.251: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 01/19/23 04:55:58.251
    Jan 19 04:55:58.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-6377 delete --grace-period=0 --force -f -'
    Jan 19 04:55:58.356: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 19 04:55:58.356: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 01/19/23 04:55:58.356
    Jan 19 04:55:58.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-6377 delete --grace-period=0 --force -f -'
    Jan 19 04:55:58.435: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 19 04:55:58.435: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 01/19/23 04:55:58.435
    Jan 19 04:55:58.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-6377 delete --grace-period=0 --force -f -'
    Jan 19 04:55:58.572: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 19 04:55:58.572: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 01/19/23 04:55:58.572
    Jan 19 04:55:58.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-6377 delete --grace-period=0 --force -f -'
    Jan 19 04:55:58.703: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 19 04:55:58.703: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 19 04:55:58.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6377" for this suite. 01/19/23 04:55:58.709
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:55:58.724
Jan 19 04:55:58.724: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename kubectl 01/19/23 04:55:58.725
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:55:58.741
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:55:58.746
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
STEP: validating api versions 01/19/23 04:55:58.749
Jan 19 04:55:58.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1674 api-versions'
Jan 19 04:55:58.874: INFO: stderr: ""
Jan 19 04:55:58.874: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\ninternal.apiserver.k8s.io/v1alpha1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1alpha1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 19 04:55:58.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1674" for this suite. 01/19/23 04:55:58.879
{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","completed":142,"skipped":2630,"failed":0}
------------------------------
â€¢ [0.163 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:816
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:55:58.724
    Jan 19 04:55:58.724: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename kubectl 01/19/23 04:55:58.725
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:55:58.741
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:55:58.746
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:822
    STEP: validating api versions 01/19/23 04:55:58.749
    Jan 19 04:55:58.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1674 api-versions'
    Jan 19 04:55:58.874: INFO: stderr: ""
    Jan 19 04:55:58.874: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\ninternal.apiserver.k8s.io/v1alpha1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1alpha1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 19 04:55:58.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1674" for this suite. 01/19/23 04:55:58.879
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:55:58.887
Jan 19 04:55:58.887: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename job 01/19/23 04:55:58.888
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:55:58.905
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:55:58.91
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
STEP: Creating a job 01/19/23 04:55:58.913
STEP: Ensuring job reaches completions 01/19/23 04:55:58.918
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 19 04:56:12.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6110" for this suite. 01/19/23 04:56:12.925
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","completed":143,"skipped":2657,"failed":0}
------------------------------
â€¢ [SLOW TEST] [14.044 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:55:58.887
    Jan 19 04:55:58.887: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename job 01/19/23 04:55:58.888
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:55:58.905
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:55:58.91
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:254
    STEP: Creating a job 01/19/23 04:55:58.913
    STEP: Ensuring job reaches completions 01/19/23 04:55:58.918
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 19 04:56:12.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-6110" for this suite. 01/19/23 04:56:12.925
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:56:12.932
Jan 19 04:56:12.932: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename daemonsets 01/19/23 04:56:12.932
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:56:12.949
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:56:12.952
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
STEP: Creating simple DaemonSet "daemon-set" 01/19/23 04:56:12.968
STEP: Check that daemon pods launch on every node of the cluster. 01/19/23 04:56:12.974
Jan 19 04:56:12.980: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 04:56:12.980: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
Jan 19 04:56:13.990: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 04:56:13.990: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
Jan 19 04:56:14.989: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 04:56:14.989: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
Jan 19 04:56:15.986: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 19 04:56:15.986: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: listing all DeamonSets 01/19/23 04:56:15.989
STEP: DeleteCollection of the DaemonSets 01/19/23 04:56:15.993
STEP: Verify that ReplicaSets have been deleted 01/19/23 04:56:15.999
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Jan 19 04:56:16.011: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"18376"},"items":null}

Jan 19 04:56:16.014: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"18379"},"items":[{"metadata":{"name":"daemon-set-mx4gb","generateName":"daemon-set-","namespace":"daemonsets-9248","uid":"331de928-e9cf-4966-ab2f-aaa46fb11882","resourceVersion":"18378","creationTimestamp":"2023-01-19T04:56:12Z","deletionTimestamp":"2023-01-19T04:56:45Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"a8013195-7a0d-473c-aa0e-732747dd3d7b","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-19T04:56:12Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a8013195-7a0d-473c-aa0e-732747dd3d7b\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-19T04:56:15Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.24.126\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-5mnpt","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-5mnpt","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ckcp-nks-default-worker-node-0","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ckcp-nks-default-worker-node-0"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T04:56:12Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T04:56:15Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T04:56:15Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T04:56:12Z"}],"hostIP":"192.168.0.99","podIP":"10.100.24.126","podIPs":[{"ip":"10.100.24.126"}],"startTime":"2023-01-19T04:56:12Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-19T04:56:15Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://be1741cd0d4a3b61c58688d018d57cc31b9e3807807bd9e4a91eb7989481c1ff","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-tvs9n","generateName":"daemon-set-","namespace":"daemonsets-9248","uid":"ea3832a8-badb-4303-ab3e-f4422778d8cb","resourceVersion":"18379","creationTimestamp":"2023-01-19T04:56:12Z","deletionTimestamp":"2023-01-19T04:56:45Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"a8013195-7a0d-473c-aa0e-732747dd3d7b","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-19T04:56:12Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a8013195-7a0d-473c-aa0e-732747dd3d7b\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-19T04:56:15Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.70.249\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-crtfp","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-crtfp","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ckcp-nks-default-worker-node-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ckcp-nks-default-worker-node-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T04:56:13Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T04:56:15Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T04:56:15Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T04:56:12Z"}],"hostIP":"192.168.0.78","podIP":"10.100.70.249","podIPs":[{"ip":"10.100.70.249"}],"startTime":"2023-01-19T04:56:13Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-19T04:56:15Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://e8943218748fb8aed539bb3cd52757500de4d27fc1bdab743d3fe0a6eca033cd","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 19 04:56:16.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9248" for this suite. 01/19/23 04:56:16.024
{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","completed":144,"skipped":2679,"failed":0}
------------------------------
â€¢ [3.098 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:56:12.932
    Jan 19 04:56:12.932: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename daemonsets 01/19/23 04:56:12.932
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:56:12.949
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:56:12.952
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:822
    STEP: Creating simple DaemonSet "daemon-set" 01/19/23 04:56:12.968
    STEP: Check that daemon pods launch on every node of the cluster. 01/19/23 04:56:12.974
    Jan 19 04:56:12.980: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 04:56:12.980: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
    Jan 19 04:56:13.990: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 04:56:13.990: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
    Jan 19 04:56:14.989: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 04:56:14.989: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
    Jan 19 04:56:15.986: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 19 04:56:15.986: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: listing all DeamonSets 01/19/23 04:56:15.989
    STEP: DeleteCollection of the DaemonSets 01/19/23 04:56:15.993
    STEP: Verify that ReplicaSets have been deleted 01/19/23 04:56:15.999
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    Jan 19 04:56:16.011: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"18376"},"items":null}

    Jan 19 04:56:16.014: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"18379"},"items":[{"metadata":{"name":"daemon-set-mx4gb","generateName":"daemon-set-","namespace":"daemonsets-9248","uid":"331de928-e9cf-4966-ab2f-aaa46fb11882","resourceVersion":"18378","creationTimestamp":"2023-01-19T04:56:12Z","deletionTimestamp":"2023-01-19T04:56:45Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"a8013195-7a0d-473c-aa0e-732747dd3d7b","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-19T04:56:12Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a8013195-7a0d-473c-aa0e-732747dd3d7b\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-19T04:56:15Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.24.126\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-5mnpt","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-5mnpt","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ckcp-nks-default-worker-node-0","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ckcp-nks-default-worker-node-0"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T04:56:12Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T04:56:15Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T04:56:15Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T04:56:12Z"}],"hostIP":"192.168.0.99","podIP":"10.100.24.126","podIPs":[{"ip":"10.100.24.126"}],"startTime":"2023-01-19T04:56:12Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-19T04:56:15Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://be1741cd0d4a3b61c58688d018d57cc31b9e3807807bd9e4a91eb7989481c1ff","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-tvs9n","generateName":"daemon-set-","namespace":"daemonsets-9248","uid":"ea3832a8-badb-4303-ab3e-f4422778d8cb","resourceVersion":"18379","creationTimestamp":"2023-01-19T04:56:12Z","deletionTimestamp":"2023-01-19T04:56:45Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"a8013195-7a0d-473c-aa0e-732747dd3d7b","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-19T04:56:12Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a8013195-7a0d-473c-aa0e-732747dd3d7b\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-19T04:56:15Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.70.249\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-crtfp","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-crtfp","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ckcp-nks-default-worker-node-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ckcp-nks-default-worker-node-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T04:56:13Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T04:56:15Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T04:56:15Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T04:56:12Z"}],"hostIP":"192.168.0.78","podIP":"10.100.70.249","podIPs":[{"ip":"10.100.70.249"}],"startTime":"2023-01-19T04:56:13Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-19T04:56:15Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://e8943218748fb8aed539bb3cd52757500de4d27fc1bdab743d3fe0a6eca033cd","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 19 04:56:16.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-9248" for this suite. 01/19/23 04:56:16.024
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:56:16.03
Jan 19 04:56:16.030: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename crd-publish-openapi 01/19/23 04:56:16.031
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:56:16.046
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:56:16.048
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 01/19/23 04:56:16.051
Jan 19 04:56:16.052: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
Jan 19 04:56:18.659: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 04:56:30.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5503" for this suite. 01/19/23 04:56:30.14
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","completed":145,"skipped":2686,"failed":0}
------------------------------
â€¢ [SLOW TEST] [14.116 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:56:16.03
    Jan 19 04:56:16.030: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename crd-publish-openapi 01/19/23 04:56:16.031
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:56:16.046
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:56:16.048
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:275
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 01/19/23 04:56:16.051
    Jan 19 04:56:16.052: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    Jan 19 04:56:18.659: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 04:56:30.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-5503" for this suite. 01/19/23 04:56:30.14
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:56:30.146
Jan 19 04:56:30.146: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename events 01/19/23 04:56:30.147
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:56:30.16
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:56:30.164
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 01/19/23 04:56:30.167
STEP: listing all events in all namespaces 01/19/23 04:56:30.17
STEP: patching the test event 01/19/23 04:56:30.174
STEP: fetching the test event 01/19/23 04:56:30.179
STEP: updating the test event 01/19/23 04:56:30.182
STEP: getting the test event 01/19/23 04:56:30.189
STEP: deleting the test event 01/19/23 04:56:30.191
STEP: listing all events in all namespaces 01/19/23 04:56:30.196
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Jan 19 04:56:30.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6311" for this suite. 01/19/23 04:56:30.203
{"msg":"PASSED [sig-instrumentation] Events should manage the lifecycle of an event [Conformance]","completed":146,"skipped":2690,"failed":0}
------------------------------
â€¢ [0.063 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:56:30.146
    Jan 19 04:56:30.146: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename events 01/19/23 04:56:30.147
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:56:30.16
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:56:30.164
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 01/19/23 04:56:30.167
    STEP: listing all events in all namespaces 01/19/23 04:56:30.17
    STEP: patching the test event 01/19/23 04:56:30.174
    STEP: fetching the test event 01/19/23 04:56:30.179
    STEP: updating the test event 01/19/23 04:56:30.182
    STEP: getting the test event 01/19/23 04:56:30.189
    STEP: deleting the test event 01/19/23 04:56:30.191
    STEP: listing all events in all namespaces 01/19/23 04:56:30.196
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Jan 19 04:56:30.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-6311" for this suite. 01/19/23 04:56:30.203
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:56:30.209
Jan 19 04:56:30.209: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename webhook 01/19/23 04:56:30.21
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:56:30.22
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:56:30.223
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/19/23 04:56:30.238
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 04:56:30.558
STEP: Deploying the webhook pod 01/19/23 04:56:30.566
STEP: Wait for the deployment to be ready 01/19/23 04:56:30.578
Jan 19 04:56:30.592: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 19 04:56:32.604: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 4, 56, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 56, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 4, 56, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 56, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/19/23 04:56:34.607
STEP: Verifying the service has paired with the endpoint 01/19/23 04:56:34.618
Jan 19 04:56:35.618: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
Jan 19 04:56:35.623: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8359-crds.webhook.example.com via the AdmissionRegistration API 01/19/23 04:56:36.136
STEP: Creating a custom resource that should be mutated by the webhook 01/19/23 04:56:36.152
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 04:56:38.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4732" for this suite. 01/19/23 04:56:38.774
STEP: Destroying namespace "webhook-4732-markers" for this suite. 01/19/23 04:56:38.787
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","completed":147,"skipped":2693,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.700 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:56:30.209
    Jan 19 04:56:30.209: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename webhook 01/19/23 04:56:30.21
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:56:30.22
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:56:30.223
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/19/23 04:56:30.238
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 04:56:30.558
    STEP: Deploying the webhook pod 01/19/23 04:56:30.566
    STEP: Wait for the deployment to be ready 01/19/23 04:56:30.578
    Jan 19 04:56:30.592: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Jan 19 04:56:32.604: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 4, 56, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 56, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 4, 56, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 56, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/19/23 04:56:34.607
    STEP: Verifying the service has paired with the endpoint 01/19/23 04:56:34.618
    Jan 19 04:56:35.618: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:340
    Jan 19 04:56:35.623: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8359-crds.webhook.example.com via the AdmissionRegistration API 01/19/23 04:56:36.136
    STEP: Creating a custom resource that should be mutated by the webhook 01/19/23 04:56:36.152
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 04:56:38.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4732" for this suite. 01/19/23 04:56:38.774
    STEP: Destroying namespace "webhook-4732-markers" for this suite. 01/19/23 04:56:38.787
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:56:38.909
Jan 19 04:56:38.909: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename configmap 01/19/23 04:56:38.91
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:56:38.931
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:56:38.934
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
STEP: Creating configMap that has name configmap-test-emptyKey-9d98a1c3-37ab-4c73-b2a0-d16b8e688b75 01/19/23 04:56:38.936
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jan 19 04:56:38.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6861" for this suite. 01/19/23 04:56:38.942
{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","completed":148,"skipped":2708,"failed":0}
------------------------------
â€¢ [0.037 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:56:38.909
    Jan 19 04:56:38.909: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename configmap 01/19/23 04:56:38.91
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:56:38.931
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:56:38.934
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:137
    STEP: Creating configMap that has name configmap-test-emptyKey-9d98a1c3-37ab-4c73-b2a0-d16b8e688b75 01/19/23 04:56:38.936
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 19 04:56:38.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6861" for this suite. 01/19/23 04:56:38.942
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:56:38.947
Jan 19 04:56:38.947: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename services 01/19/23 04:56:38.948
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:56:38.96
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:56:38.962
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
STEP: creating a service externalname-service with the type=ExternalName in namespace services-965 01/19/23 04:56:38.966
STEP: changing the ExternalName service to type=ClusterIP 01/19/23 04:56:39.024
STEP: creating replication controller externalname-service in namespace services-965 01/19/23 04:56:39.043
I0119 04:56:39.061575      18 runners.go:193] Created replication controller with name: externalname-service, namespace: services-965, replica count: 2
I0119 04:56:42.112849      18 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 19 04:56:42.112: INFO: Creating new exec pod
Jan 19 04:56:42.120: INFO: Waiting up to 5m0s for pod "execpod78vmk" in namespace "services-965" to be "running"
Jan 19 04:56:42.127: INFO: Pod "execpod78vmk": Phase="Pending", Reason="", readiness=false. Elapsed: 7.60386ms
Jan 19 04:56:44.132: INFO: Pod "execpod78vmk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011646587s
Jan 19 04:56:46.132: INFO: Pod "execpod78vmk": Phase="Running", Reason="", readiness=true. Elapsed: 4.012133134s
Jan 19 04:56:46.132: INFO: Pod "execpod78vmk" satisfied condition "running"
Jan 19 04:56:47.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-965 exec execpod78vmk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 19 04:56:47.307: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 19 04:56:47.308: INFO: stdout: "externalname-service-kmlcf"
Jan 19 04:56:47.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-965 exec execpod78vmk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.238.200 80'
Jan 19 04:56:47.462: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.238.200 80\nConnection to 10.254.238.200 80 port [tcp/http] succeeded!\n"
Jan 19 04:56:47.462: INFO: stdout: ""
Jan 19 04:56:48.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-965 exec execpod78vmk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.238.200 80'
Jan 19 04:56:48.623: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.238.200 80\nConnection to 10.254.238.200 80 port [tcp/http] succeeded!\n"
Jan 19 04:56:48.623: INFO: stdout: "externalname-service-cndgv"
Jan 19 04:56:48.623: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 19 04:56:48.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-965" for this suite. 01/19/23 04:56:48.649
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","completed":149,"skipped":2732,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.709 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:56:38.947
    Jan 19 04:56:38.947: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename services 01/19/23 04:56:38.948
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:56:38.96
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:56:38.962
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1404
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-965 01/19/23 04:56:38.966
    STEP: changing the ExternalName service to type=ClusterIP 01/19/23 04:56:39.024
    STEP: creating replication controller externalname-service in namespace services-965 01/19/23 04:56:39.043
    I0119 04:56:39.061575      18 runners.go:193] Created replication controller with name: externalname-service, namespace: services-965, replica count: 2
    I0119 04:56:42.112849      18 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 19 04:56:42.112: INFO: Creating new exec pod
    Jan 19 04:56:42.120: INFO: Waiting up to 5m0s for pod "execpod78vmk" in namespace "services-965" to be "running"
    Jan 19 04:56:42.127: INFO: Pod "execpod78vmk": Phase="Pending", Reason="", readiness=false. Elapsed: 7.60386ms
    Jan 19 04:56:44.132: INFO: Pod "execpod78vmk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011646587s
    Jan 19 04:56:46.132: INFO: Pod "execpod78vmk": Phase="Running", Reason="", readiness=true. Elapsed: 4.012133134s
    Jan 19 04:56:46.132: INFO: Pod "execpod78vmk" satisfied condition "running"
    Jan 19 04:56:47.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-965 exec execpod78vmk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jan 19 04:56:47.307: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan 19 04:56:47.308: INFO: stdout: "externalname-service-kmlcf"
    Jan 19 04:56:47.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-965 exec execpod78vmk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.238.200 80'
    Jan 19 04:56:47.462: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.238.200 80\nConnection to 10.254.238.200 80 port [tcp/http] succeeded!\n"
    Jan 19 04:56:47.462: INFO: stdout: ""
    Jan 19 04:56:48.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-965 exec execpod78vmk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.238.200 80'
    Jan 19 04:56:48.623: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.238.200 80\nConnection to 10.254.238.200 80 port [tcp/http] succeeded!\n"
    Jan 19 04:56:48.623: INFO: stdout: "externalname-service-cndgv"
    Jan 19 04:56:48.623: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 19 04:56:48.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-965" for this suite. 01/19/23 04:56:48.649
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:56:48.66
Jan 19 04:56:48.660: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename container-lifecycle-hook 01/19/23 04:56:48.66
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:56:48.676
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:56:48.678
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 01/19/23 04:56:48.684
Jan 19 04:56:48.693: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-821" to be "running and ready"
Jan 19 04:56:48.700: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.845008ms
Jan 19 04:56:48.700: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 19 04:56:50.704: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011333791s
Jan 19 04:56:50.704: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 19 04:56:52.704: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.011135459s
Jan 19 04:56:52.704: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 19 04:56:52.704: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
STEP: create the pod with lifecycle hook 01/19/23 04:56:52.707
Jan 19 04:56:52.714: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-821" to be "running and ready"
Jan 19 04:56:52.719: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.30263ms
Jan 19 04:56:52.719: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 19 04:56:54.726: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01187356s
Jan 19 04:56:54.726: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 19 04:56:56.725: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.011252842s
Jan 19 04:56:56.725: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Jan 19 04:56:56.725: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 01/19/23 04:56:56.728
STEP: delete the pod with lifecycle hook 01/19/23 04:56:56.761
Jan 19 04:56:56.769: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 19 04:56:56.772: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 19 04:56:58.772: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 19 04:56:58.776: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jan 19 04:56:58.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-821" for this suite. 01/19/23 04:56:58.778
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","completed":150,"skipped":2843,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.124 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:56:48.66
    Jan 19 04:56:48.660: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/19/23 04:56:48.66
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:56:48.676
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:56:48.678
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 01/19/23 04:56:48.684
    Jan 19 04:56:48.693: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-821" to be "running and ready"
    Jan 19 04:56:48.700: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.845008ms
    Jan 19 04:56:48.700: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 04:56:50.704: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011333791s
    Jan 19 04:56:50.704: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 04:56:52.704: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.011135459s
    Jan 19 04:56:52.704: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 19 04:56:52.704: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:97
    STEP: create the pod with lifecycle hook 01/19/23 04:56:52.707
    Jan 19 04:56:52.714: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-821" to be "running and ready"
    Jan 19 04:56:52.719: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.30263ms
    Jan 19 04:56:52.719: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 04:56:54.726: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01187356s
    Jan 19 04:56:54.726: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 04:56:56.725: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.011252842s
    Jan 19 04:56:56.725: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Jan 19 04:56:56.725: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 01/19/23 04:56:56.728
    STEP: delete the pod with lifecycle hook 01/19/23 04:56:56.761
    Jan 19 04:56:56.769: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan 19 04:56:56.772: INFO: Pod pod-with-poststart-exec-hook still exists
    Jan 19 04:56:58.772: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan 19 04:56:58.776: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jan 19 04:56:58.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-821" for this suite. 01/19/23 04:56:58.778
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:56:58.784
Jan 19 04:56:58.784: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename services 01/19/23 04:56:58.784
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:56:58.794
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:56:58.797
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
STEP: fetching services 01/19/23 04:56:58.799
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 19 04:56:58.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-683" for this suite. 01/19/23 04:56:58.804
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","completed":151,"skipped":2846,"failed":0}
------------------------------
â€¢ [0.024 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:56:58.784
    Jan 19 04:56:58.784: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename services 01/19/23 04:56:58.784
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:56:58.794
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:56:58.797
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3206
    STEP: fetching services 01/19/23 04:56:58.799
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 19 04:56:58.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-683" for this suite. 01/19/23 04:56:58.804
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:56:58.808
Jan 19 04:56:58.808: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename deployment 01/19/23 04:56:58.809
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:56:58.819
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:56:58.825
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Jan 19 04:56:58.836: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jan 19 04:57:03.843: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/19/23 04:57:03.843
Jan 19 04:57:03.843: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 01/19/23 04:57:03.86
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 19 04:57:07.881: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-6501  478a90d9-a97d-43c6-b10e-1d26f7308554 18839 1 2023-01-19 04:57:03 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-01-19 04:57:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 04:57:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004386008 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-19 04:57:03 +0000 UTC,LastTransitionTime:2023-01-19 04:57:03 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-69cb9c5497" has successfully progressed.,LastUpdateTime:2023-01-19 04:57:07 +0000 UTC,LastTransitionTime:2023-01-19 04:57:03 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 19 04:57:07.884: INFO: New ReplicaSet "test-cleanup-deployment-69cb9c5497" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-69cb9c5497  deployment-6501  e2af50e2-94a7-4e80-bc88-2a7878b1032d 18829 1 2023-01-19 04:57:03 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 478a90d9-a97d-43c6-b10e-1d26f7308554 0xc0043504b7 0xc0043504b8}] [] [{kube-controller-manager Update apps/v1 2023-01-19 04:57:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"478a90d9-a97d-43c6-b10e-1d26f7308554\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 04:57:07 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 69cb9c5497,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004350568 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 19 04:57:07.886: INFO: Pod "test-cleanup-deployment-69cb9c5497-xj7nv" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-69cb9c5497-xj7nv test-cleanup-deployment-69cb9c5497- deployment-6501  4f43edf7-461b-4e9b-8a2a-302a93ac28a7 18828 0 2023-01-19 04:57:03 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-69cb9c5497 e2af50e2-94a7-4e80-bc88-2a7878b1032d 0xc004386517 0xc004386518}] [] [{kube-controller-manager Update v1 2023-01-19 04:57:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2af50e2-94a7-4e80-bc88-2a7878b1032d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 04:57:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.70.7\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-szbwx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-szbwx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:57:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:57:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:57:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:57:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.78,PodIP:10.100.70.7,StartTime:2023-01-19 04:57:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 04:57:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://f770091c4d7ce60f832c2217a64971591b217e4af3e79f4f73a51cfa6eefe463,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.70.7,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 19 04:57:07.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6501" for this suite. 01/19/23 04:57:07.888
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","completed":152,"skipped":2846,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.086 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:56:58.808
    Jan 19 04:56:58.808: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename deployment 01/19/23 04:56:58.809
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:56:58.819
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:56:58.825
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Jan 19 04:56:58.836: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Jan 19 04:57:03.843: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/19/23 04:57:03.843
    Jan 19 04:57:03.843: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 01/19/23 04:57:03.86
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 19 04:57:07.881: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-6501  478a90d9-a97d-43c6-b10e-1d26f7308554 18839 1 2023-01-19 04:57:03 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-01-19 04:57:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 04:57:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004386008 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-19 04:57:03 +0000 UTC,LastTransitionTime:2023-01-19 04:57:03 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-69cb9c5497" has successfully progressed.,LastUpdateTime:2023-01-19 04:57:07 +0000 UTC,LastTransitionTime:2023-01-19 04:57:03 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 19 04:57:07.884: INFO: New ReplicaSet "test-cleanup-deployment-69cb9c5497" of Deployment "test-cleanup-deployment":
    &ReplicaSet{ObjectMeta:{test-cleanup-deployment-69cb9c5497  deployment-6501  e2af50e2-94a7-4e80-bc88-2a7878b1032d 18829 1 2023-01-19 04:57:03 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 478a90d9-a97d-43c6-b10e-1d26f7308554 0xc0043504b7 0xc0043504b8}] [] [{kube-controller-manager Update apps/v1 2023-01-19 04:57:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"478a90d9-a97d-43c6-b10e-1d26f7308554\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 04:57:07 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 69cb9c5497,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004350568 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 19 04:57:07.886: INFO: Pod "test-cleanup-deployment-69cb9c5497-xj7nv" is available:
    &Pod{ObjectMeta:{test-cleanup-deployment-69cb9c5497-xj7nv test-cleanup-deployment-69cb9c5497- deployment-6501  4f43edf7-461b-4e9b-8a2a-302a93ac28a7 18828 0 2023-01-19 04:57:03 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-69cb9c5497 e2af50e2-94a7-4e80-bc88-2a7878b1032d 0xc004386517 0xc004386518}] [] [{kube-controller-manager Update v1 2023-01-19 04:57:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2af50e2-94a7-4e80-bc88-2a7878b1032d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 04:57:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.70.7\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-szbwx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-szbwx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:57:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:57:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:57:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 04:57:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.78,PodIP:10.100.70.7,StartTime:2023-01-19 04:57:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 04:57:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://f770091c4d7ce60f832c2217a64971591b217e4af3e79f4f73a51cfa6eefe463,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.70.7,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 19 04:57:07.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-6501" for this suite. 01/19/23 04:57:07.888
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:309
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:57:07.894
Jan 19 04:57:07.895: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename job 01/19/23 04:57:07.895
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:57:07.906
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:57:07.909
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:309
STEP: Creating a job 01/19/23 04:57:07.911
STEP: Ensuring active pods == parallelism 01/19/23 04:57:07.916
STEP: delete a job 01/19/23 04:57:11.921
STEP: deleting Job.batch foo in namespace job-5363, will wait for the garbage collector to delete the pods 01/19/23 04:57:11.921
Jan 19 04:57:11.987: INFO: Deleting Job.batch foo took: 12.598323ms
Jan 19 04:57:12.087: INFO: Terminating Job.batch foo pods took: 100.573929ms
STEP: Ensuring job was deleted 01/19/23 04:57:44.188
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 19 04:57:44.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5363" for this suite. 01/19/23 04:57:44.194
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","completed":153,"skipped":2865,"failed":0}
------------------------------
â€¢ [SLOW TEST] [36.304 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:57:07.894
    Jan 19 04:57:07.895: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename job 01/19/23 04:57:07.895
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:57:07.906
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:57:07.909
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:309
    STEP: Creating a job 01/19/23 04:57:07.911
    STEP: Ensuring active pods == parallelism 01/19/23 04:57:07.916
    STEP: delete a job 01/19/23 04:57:11.921
    STEP: deleting Job.batch foo in namespace job-5363, will wait for the garbage collector to delete the pods 01/19/23 04:57:11.921
    Jan 19 04:57:11.987: INFO: Deleting Job.batch foo took: 12.598323ms
    Jan 19 04:57:12.087: INFO: Terminating Job.batch foo pods took: 100.573929ms
    STEP: Ensuring job was deleted 01/19/23 04:57:44.188
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 19 04:57:44.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-5363" for this suite. 01/19/23 04:57:44.194
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:57:44.199
Jan 19 04:57:44.199: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename svcaccounts 01/19/23 04:57:44.22
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:57:44.233
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:57:44.235
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
Jan 19 04:57:44.248: INFO: Waiting up to 5m0s for pod "pod-service-account-71bc65e2-8044-4c2e-84ea-b099782111fb" in namespace "svcaccounts-8482" to be "running"
Jan 19 04:57:44.251: INFO: Pod "pod-service-account-71bc65e2-8044-4c2e-84ea-b099782111fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.963463ms
Jan 19 04:57:46.255: INFO: Pod "pod-service-account-71bc65e2-8044-4c2e-84ea-b099782111fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00647525s
Jan 19 04:57:48.255: INFO: Pod "pod-service-account-71bc65e2-8044-4c2e-84ea-b099782111fb": Phase="Running", Reason="", readiness=true. Elapsed: 4.006716109s
Jan 19 04:57:48.255: INFO: Pod "pod-service-account-71bc65e2-8044-4c2e-84ea-b099782111fb" satisfied condition "running"
STEP: reading a file in the container 01/19/23 04:57:48.255
Jan 19 04:57:48.255: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8482 pod-service-account-71bc65e2-8044-4c2e-84ea-b099782111fb -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 01/19/23 04:57:48.411
Jan 19 04:57:48.411: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8482 pod-service-account-71bc65e2-8044-4c2e-84ea-b099782111fb -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 01/19/23 04:57:48.579
Jan 19 04:57:48.579: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8482 pod-service-account-71bc65e2-8044-4c2e-84ea-b099782111fb -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Jan 19 04:57:48.763: INFO: Got root ca configmap in namespace "svcaccounts-8482"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 19 04:57:48.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8482" for this suite. 01/19/23 04:57:48.769
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","completed":154,"skipped":2872,"failed":0}
------------------------------
â€¢ [4.575 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:57:44.199
    Jan 19 04:57:44.199: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename svcaccounts 01/19/23 04:57:44.22
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:57:44.233
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:57:44.235
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:75
    Jan 19 04:57:44.248: INFO: Waiting up to 5m0s for pod "pod-service-account-71bc65e2-8044-4c2e-84ea-b099782111fb" in namespace "svcaccounts-8482" to be "running"
    Jan 19 04:57:44.251: INFO: Pod "pod-service-account-71bc65e2-8044-4c2e-84ea-b099782111fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.963463ms
    Jan 19 04:57:46.255: INFO: Pod "pod-service-account-71bc65e2-8044-4c2e-84ea-b099782111fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00647525s
    Jan 19 04:57:48.255: INFO: Pod "pod-service-account-71bc65e2-8044-4c2e-84ea-b099782111fb": Phase="Running", Reason="", readiness=true. Elapsed: 4.006716109s
    Jan 19 04:57:48.255: INFO: Pod "pod-service-account-71bc65e2-8044-4c2e-84ea-b099782111fb" satisfied condition "running"
    STEP: reading a file in the container 01/19/23 04:57:48.255
    Jan 19 04:57:48.255: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8482 pod-service-account-71bc65e2-8044-4c2e-84ea-b099782111fb -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 01/19/23 04:57:48.411
    Jan 19 04:57:48.411: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8482 pod-service-account-71bc65e2-8044-4c2e-84ea-b099782111fb -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 01/19/23 04:57:48.579
    Jan 19 04:57:48.579: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8482 pod-service-account-71bc65e2-8044-4c2e-84ea-b099782111fb -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Jan 19 04:57:48.763: INFO: Got root ca configmap in namespace "svcaccounts-8482"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 19 04:57:48.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-8482" for this suite. 01/19/23 04:57:48.769
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:57:48.776
Jan 19 04:57:48.776: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename services 01/19/23 04:57:48.777
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:57:48.788
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:57:48.79
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
STEP: creating a service externalname-service with the type=ExternalName in namespace services-3342 01/19/23 04:57:48.792
STEP: changing the ExternalName service to type=NodePort 01/19/23 04:57:48.796
STEP: creating replication controller externalname-service in namespace services-3342 01/19/23 04:57:48.813
I0119 04:57:48.820073      18 runners.go:193] Created replication controller with name: externalname-service, namespace: services-3342, replica count: 2
I0119 04:57:51.871125      18 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 19 04:57:51.871: INFO: Creating new exec pod
Jan 19 04:57:51.880: INFO: Waiting up to 5m0s for pod "execpodpf6lq" in namespace "services-3342" to be "running"
Jan 19 04:57:51.884: INFO: Pod "execpodpf6lq": Phase="Pending", Reason="", readiness=false. Elapsed: 3.383311ms
Jan 19 04:57:53.891: INFO: Pod "execpodpf6lq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01052707s
Jan 19 04:57:55.887: INFO: Pod "execpodpf6lq": Phase="Running", Reason="", readiness=true. Elapsed: 4.007087448s
Jan 19 04:57:55.887: INFO: Pod "execpodpf6lq" satisfied condition "running"
Jan 19 04:57:56.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3342 exec execpodpf6lq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 19 04:57:57.060: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 19 04:57:57.060: INFO: stdout: "externalname-service-tp25n"
Jan 19 04:57:57.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3342 exec execpodpf6lq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.24.17 80'
Jan 19 04:57:57.212: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.24.17 80\nConnection to 10.254.24.17 80 port [tcp/http] succeeded!\n"
Jan 19 04:57:57.212: INFO: stdout: ""
Jan 19 04:57:58.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3342 exec execpodpf6lq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.24.17 80'
Jan 19 04:57:58.378: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.24.17 80\nConnection to 10.254.24.17 80 port [tcp/http] succeeded!\n"
Jan 19 04:57:58.378: INFO: stdout: ""
Jan 19 04:57:59.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3342 exec execpodpf6lq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.24.17 80'
Jan 19 04:57:59.380: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.24.17 80\nConnection to 10.254.24.17 80 port [tcp/http] succeeded!\n"
Jan 19 04:57:59.380: INFO: stdout: "externalname-service-stkkv"
Jan 19 04:57:59.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3342 exec execpodpf6lq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.0.99 31530'
Jan 19 04:57:59.543: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.0.99 31530\nConnection to 192.168.0.99 31530 port [tcp/*] succeeded!\n"
Jan 19 04:57:59.543: INFO: stdout: "externalname-service-stkkv"
Jan 19 04:57:59.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3342 exec execpodpf6lq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.0.78 31530'
Jan 19 04:57:59.714: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.0.78 31530\nConnection to 192.168.0.78 31530 port [tcp/*] succeeded!\n"
Jan 19 04:57:59.714: INFO: stdout: ""
Jan 19 04:58:00.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3342 exec execpodpf6lq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.0.78 31530'
Jan 19 04:58:00.891: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.0.78 31530\nConnection to 192.168.0.78 31530 port [tcp/*] succeeded!\n"
Jan 19 04:58:00.891: INFO: stdout: "externalname-service-tp25n"
Jan 19 04:58:00.891: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 19 04:58:00.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3342" for this suite. 01/19/23 04:58:00.935
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","completed":155,"skipped":2922,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.165 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:57:48.776
    Jan 19 04:57:48.776: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename services 01/19/23 04:57:48.777
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:57:48.788
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:57:48.79
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1443
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-3342 01/19/23 04:57:48.792
    STEP: changing the ExternalName service to type=NodePort 01/19/23 04:57:48.796
    STEP: creating replication controller externalname-service in namespace services-3342 01/19/23 04:57:48.813
    I0119 04:57:48.820073      18 runners.go:193] Created replication controller with name: externalname-service, namespace: services-3342, replica count: 2
    I0119 04:57:51.871125      18 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 19 04:57:51.871: INFO: Creating new exec pod
    Jan 19 04:57:51.880: INFO: Waiting up to 5m0s for pod "execpodpf6lq" in namespace "services-3342" to be "running"
    Jan 19 04:57:51.884: INFO: Pod "execpodpf6lq": Phase="Pending", Reason="", readiness=false. Elapsed: 3.383311ms
    Jan 19 04:57:53.891: INFO: Pod "execpodpf6lq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01052707s
    Jan 19 04:57:55.887: INFO: Pod "execpodpf6lq": Phase="Running", Reason="", readiness=true. Elapsed: 4.007087448s
    Jan 19 04:57:55.887: INFO: Pod "execpodpf6lq" satisfied condition "running"
    Jan 19 04:57:56.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3342 exec execpodpf6lq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jan 19 04:57:57.060: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan 19 04:57:57.060: INFO: stdout: "externalname-service-tp25n"
    Jan 19 04:57:57.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3342 exec execpodpf6lq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.24.17 80'
    Jan 19 04:57:57.212: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.24.17 80\nConnection to 10.254.24.17 80 port [tcp/http] succeeded!\n"
    Jan 19 04:57:57.212: INFO: stdout: ""
    Jan 19 04:57:58.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3342 exec execpodpf6lq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.24.17 80'
    Jan 19 04:57:58.378: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.24.17 80\nConnection to 10.254.24.17 80 port [tcp/http] succeeded!\n"
    Jan 19 04:57:58.378: INFO: stdout: ""
    Jan 19 04:57:59.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3342 exec execpodpf6lq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.24.17 80'
    Jan 19 04:57:59.380: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.24.17 80\nConnection to 10.254.24.17 80 port [tcp/http] succeeded!\n"
    Jan 19 04:57:59.380: INFO: stdout: "externalname-service-stkkv"
    Jan 19 04:57:59.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3342 exec execpodpf6lq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.0.99 31530'
    Jan 19 04:57:59.543: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.0.99 31530\nConnection to 192.168.0.99 31530 port [tcp/*] succeeded!\n"
    Jan 19 04:57:59.543: INFO: stdout: "externalname-service-stkkv"
    Jan 19 04:57:59.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3342 exec execpodpf6lq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.0.78 31530'
    Jan 19 04:57:59.714: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.0.78 31530\nConnection to 192.168.0.78 31530 port [tcp/*] succeeded!\n"
    Jan 19 04:57:59.714: INFO: stdout: ""
    Jan 19 04:58:00.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3342 exec execpodpf6lq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.0.78 31530'
    Jan 19 04:58:00.891: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.0.78 31530\nConnection to 192.168.0.78 31530 port [tcp/*] succeeded!\n"
    Jan 19 04:58:00.891: INFO: stdout: "externalname-service-tp25n"
    Jan 19 04:58:00.891: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 19 04:58:00.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3342" for this suite. 01/19/23 04:58:00.935
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:58:00.941
Jan 19 04:58:00.941: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename svcaccounts 01/19/23 04:58:00.942
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:58:00.956
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:58:00.958
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
STEP: Creating a pod to test service account token:  01/19/23 04:58:00.961
Jan 19 04:58:00.970: INFO: Waiting up to 5m0s for pod "test-pod-7cfa3df1-bf41-4455-8045-fa35c0b3aa30" in namespace "svcaccounts-4954" to be "Succeeded or Failed"
Jan 19 04:58:00.978: INFO: Pod "test-pod-7cfa3df1-bf41-4455-8045-fa35c0b3aa30": Phase="Pending", Reason="", readiness=false. Elapsed: 7.860012ms
Jan 19 04:58:02.982: INFO: Pod "test-pod-7cfa3df1-bf41-4455-8045-fa35c0b3aa30": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011683491s
Jan 19 04:58:04.982: INFO: Pod "test-pod-7cfa3df1-bf41-4455-8045-fa35c0b3aa30": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011822417s
STEP: Saw pod success 01/19/23 04:58:04.982
Jan 19 04:58:04.982: INFO: Pod "test-pod-7cfa3df1-bf41-4455-8045-fa35c0b3aa30" satisfied condition "Succeeded or Failed"
Jan 19 04:58:04.985: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod test-pod-7cfa3df1-bf41-4455-8045-fa35c0b3aa30 container agnhost-container: <nil>
STEP: delete the pod 01/19/23 04:58:04.99
Jan 19 04:58:05.002: INFO: Waiting for pod test-pod-7cfa3df1-bf41-4455-8045-fa35c0b3aa30 to disappear
Jan 19 04:58:05.006: INFO: Pod test-pod-7cfa3df1-bf41-4455-8045-fa35c0b3aa30 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 19 04:58:05.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4954" for this suite. 01/19/23 04:58:05.009
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","completed":156,"skipped":2932,"failed":0}
------------------------------
â€¢ [4.074 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:58:00.941
    Jan 19 04:58:00.941: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename svcaccounts 01/19/23 04:58:00.942
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:58:00.956
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:58:00.958
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:272
    STEP: Creating a pod to test service account token:  01/19/23 04:58:00.961
    Jan 19 04:58:00.970: INFO: Waiting up to 5m0s for pod "test-pod-7cfa3df1-bf41-4455-8045-fa35c0b3aa30" in namespace "svcaccounts-4954" to be "Succeeded or Failed"
    Jan 19 04:58:00.978: INFO: Pod "test-pod-7cfa3df1-bf41-4455-8045-fa35c0b3aa30": Phase="Pending", Reason="", readiness=false. Elapsed: 7.860012ms
    Jan 19 04:58:02.982: INFO: Pod "test-pod-7cfa3df1-bf41-4455-8045-fa35c0b3aa30": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011683491s
    Jan 19 04:58:04.982: INFO: Pod "test-pod-7cfa3df1-bf41-4455-8045-fa35c0b3aa30": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011822417s
    STEP: Saw pod success 01/19/23 04:58:04.982
    Jan 19 04:58:04.982: INFO: Pod "test-pod-7cfa3df1-bf41-4455-8045-fa35c0b3aa30" satisfied condition "Succeeded or Failed"
    Jan 19 04:58:04.985: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod test-pod-7cfa3df1-bf41-4455-8045-fa35c0b3aa30 container agnhost-container: <nil>
    STEP: delete the pod 01/19/23 04:58:04.99
    Jan 19 04:58:05.002: INFO: Waiting for pod test-pod-7cfa3df1-bf41-4455-8045-fa35c0b3aa30 to disappear
    Jan 19 04:58:05.006: INFO: Pod test-pod-7cfa3df1-bf41-4455-8045-fa35c0b3aa30 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 19 04:58:05.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-4954" for this suite. 01/19/23 04:58:05.009
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:58:05.015
Jan 19 04:58:05.015: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename aggregator 01/19/23 04:58:05.016
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:58:05.031
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:58:05.033
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Jan 19 04:58:05.035: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 01/19/23 04:58:05.078
Jan 19 04:58:05.514: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jan 19 04:58:07.554: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 04:58:09.558: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 04:58:11.559: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 04:58:13.558: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 04:58:15.559: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 04:58:17.559: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 04:58:19.560: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 04:58:21.557: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 04:58:23.558: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 04:58:25.558: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 04:58:27.558: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 04:58:29.559: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 04:58:31.559: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 04:58:33.692: INFO: Waited 130.101606ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 01/19/23 04:58:33.773
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 01/19/23 04:58:33.776
STEP: List APIServices 01/19/23 04:58:33.784
Jan 19 04:58:33.790: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
Jan 19 04:58:34.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-8879" for this suite. 01/19/23 04:58:34.119
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","completed":157,"skipped":2933,"failed":0}
------------------------------
â€¢ [SLOW TEST] [29.158 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:58:05.015
    Jan 19 04:58:05.015: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename aggregator 01/19/23 04:58:05.016
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:58:05.031
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:58:05.033
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Jan 19 04:58:05.035: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 01/19/23 04:58:05.078
    Jan 19 04:58:05.514: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Jan 19 04:58:07.554: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 19 04:58:09.558: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 19 04:58:11.559: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 19 04:58:13.558: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 19 04:58:15.559: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 19 04:58:17.559: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 19 04:58:19.560: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 19 04:58:21.557: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 19 04:58:23.558: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 19 04:58:25.558: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 19 04:58:27.558: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 19 04:58:29.559: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 19 04:58:31.559: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 19 04:58:33.692: INFO: Waited 130.101606ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 01/19/23 04:58:33.773
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 01/19/23 04:58:33.776
    STEP: List APIServices 01/19/23 04:58:33.784
    Jan 19 04:58:33.790: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:187
    Jan 19 04:58:34.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "aggregator-8879" for this suite. 01/19/23 04:58:34.119
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:58:34.174
Jan 19 04:58:34.174: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename disruption 01/19/23 04:58:34.174
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:58:34.188
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:58:34.191
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:58:34.194
Jan 19 04:58:34.194: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename disruption-2 01/19/23 04:58:34.194
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:58:34.206
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:58:34.209
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
STEP: Waiting for the pdb to be processed 01/19/23 04:58:34.222
STEP: Waiting for the pdb to be processed 01/19/23 04:58:34.251
STEP: Waiting for the pdb to be processed 01/19/23 04:58:34.272
STEP: listing a collection of PDBs across all namespaces 01/19/23 04:58:36.294
STEP: listing a collection of PDBs in namespace disruption-1341 01/19/23 04:58:36.297
STEP: deleting a collection of PDBs 01/19/23 04:58:36.299
STEP: Waiting for the PDB collection to be deleted 01/19/23 04:58:36.311
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
Jan 19 04:58:36.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-1389" for this suite. 01/19/23 04:58:36.318
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan 19 04:58:36.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1341" for this suite. 01/19/23 04:58:36.327
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","completed":158,"skipped":2941,"failed":0}
------------------------------
â€¢ [2.158 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:77
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:58:34.174
    Jan 19 04:58:34.174: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename disruption 01/19/23 04:58:34.174
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:58:34.188
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:58:34.191
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:58:34.194
    Jan 19 04:58:34.194: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename disruption-2 01/19/23 04:58:34.194
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:58:34.206
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:58:34.209
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:86
    STEP: Waiting for the pdb to be processed 01/19/23 04:58:34.222
    STEP: Waiting for the pdb to be processed 01/19/23 04:58:34.251
    STEP: Waiting for the pdb to be processed 01/19/23 04:58:34.272
    STEP: listing a collection of PDBs across all namespaces 01/19/23 04:58:36.294
    STEP: listing a collection of PDBs in namespace disruption-1341 01/19/23 04:58:36.297
    STEP: deleting a collection of PDBs 01/19/23 04:58:36.299
    STEP: Waiting for the PDB collection to be deleted 01/19/23 04:58:36.311
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:187
    Jan 19 04:58:36.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2-1389" for this suite. 01/19/23 04:58:36.318
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan 19 04:58:36.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-1341" for this suite. 01/19/23 04:58:36.327
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:58:36.333
Jan 19 04:58:36.333: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename replicaset 01/19/23 04:58:36.333
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:58:36.345
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:58:36.348
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 01/19/23 04:58:36.353
STEP: Verify that the required pods have come up. 01/19/23 04:58:36.361
Jan 19 04:58:36.364: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 19 04:58:41.376: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/19/23 04:58:41.376
STEP: Getting /status 01/19/23 04:58:41.377
Jan 19 04:58:41.380: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 01/19/23 04:58:41.38
Jan 19 04:58:41.398: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 01/19/23 04:58:41.398
Jan 19 04:58:41.407: INFO: Observed &ReplicaSet event: ADDED
Jan 19 04:58:41.407: INFO: Observed &ReplicaSet event: MODIFIED
Jan 19 04:58:41.407: INFO: Observed &ReplicaSet event: MODIFIED
Jan 19 04:58:41.407: INFO: Observed &ReplicaSet event: MODIFIED
Jan 19 04:58:41.407: INFO: Found replicaset test-rs in namespace replicaset-1101 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 19 04:58:41.407: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 01/19/23 04:58:41.407
Jan 19 04:58:41.408: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 19 04:58:41.428: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 01/19/23 04:58:41.428
Jan 19 04:58:41.431: INFO: Observed &ReplicaSet event: ADDED
Jan 19 04:58:41.431: INFO: Observed &ReplicaSet event: MODIFIED
Jan 19 04:58:41.431: INFO: Observed &ReplicaSet event: MODIFIED
Jan 19 04:58:41.431: INFO: Observed &ReplicaSet event: MODIFIED
Jan 19 04:58:41.431: INFO: Observed replicaset test-rs in namespace replicaset-1101 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 19 04:58:41.431: INFO: Observed &ReplicaSet event: MODIFIED
Jan 19 04:58:41.431: INFO: Found replicaset test-rs in namespace replicaset-1101 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Jan 19 04:58:41.431: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 19 04:58:41.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1101" for this suite. 01/19/23 04:58:41.439
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","completed":159,"skipped":2965,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.113 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:58:36.333
    Jan 19 04:58:36.333: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename replicaset 01/19/23 04:58:36.333
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:58:36.345
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:58:36.348
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 01/19/23 04:58:36.353
    STEP: Verify that the required pods have come up. 01/19/23 04:58:36.361
    Jan 19 04:58:36.364: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 19 04:58:41.376: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/19/23 04:58:41.376
    STEP: Getting /status 01/19/23 04:58:41.377
    Jan 19 04:58:41.380: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 01/19/23 04:58:41.38
    Jan 19 04:58:41.398: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 01/19/23 04:58:41.398
    Jan 19 04:58:41.407: INFO: Observed &ReplicaSet event: ADDED
    Jan 19 04:58:41.407: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 19 04:58:41.407: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 19 04:58:41.407: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 19 04:58:41.407: INFO: Found replicaset test-rs in namespace replicaset-1101 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 19 04:58:41.407: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 01/19/23 04:58:41.407
    Jan 19 04:58:41.408: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 19 04:58:41.428: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 01/19/23 04:58:41.428
    Jan 19 04:58:41.431: INFO: Observed &ReplicaSet event: ADDED
    Jan 19 04:58:41.431: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 19 04:58:41.431: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 19 04:58:41.431: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 19 04:58:41.431: INFO: Observed replicaset test-rs in namespace replicaset-1101 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 19 04:58:41.431: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 19 04:58:41.431: INFO: Found replicaset test-rs in namespace replicaset-1101 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Jan 19 04:58:41.431: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 19 04:58:41.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-1101" for this suite. 01/19/23 04:58:41.439
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:58:41.447
Jan 19 04:58:41.448: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename dns 01/19/23 04:58:41.448
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:58:41.475
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:58:41.484
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-711.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-711.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 01/19/23 04:58:41.511
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-711.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-711.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 01/19/23 04:58:41.511
STEP: creating a pod to probe /etc/hosts 01/19/23 04:58:41.511
STEP: submitting the pod to kubernetes 01/19/23 04:58:41.511
Jan 19 04:58:41.538: INFO: Waiting up to 15m0s for pod "dns-test-6bc8e418-c749-4a25-9b8e-a665acea82b7" in namespace "dns-711" to be "running"
Jan 19 04:58:41.548: INFO: Pod "dns-test-6bc8e418-c749-4a25-9b8e-a665acea82b7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.9579ms
Jan 19 04:58:43.554: INFO: Pod "dns-test-6bc8e418-c749-4a25-9b8e-a665acea82b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015707134s
Jan 19 04:58:45.552: INFO: Pod "dns-test-6bc8e418-c749-4a25-9b8e-a665acea82b7": Phase="Running", Reason="", readiness=true. Elapsed: 4.013616424s
Jan 19 04:58:45.552: INFO: Pod "dns-test-6bc8e418-c749-4a25-9b8e-a665acea82b7" satisfied condition "running"
STEP: retrieving the pod 01/19/23 04:58:45.552
STEP: looking for the results for each expected name from probers 01/19/23 04:58:45.554
Jan 19 04:58:45.567: INFO: DNS probes using dns-711/dns-test-6bc8e418-c749-4a25-9b8e-a665acea82b7 succeeded

STEP: deleting the pod 01/19/23 04:58:45.567
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 19 04:58:45.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-711" for this suite. 01/19/23 04:58:45.584
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","completed":160,"skipped":3021,"failed":0}
------------------------------
â€¢ [4.145 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:58:41.447
    Jan 19 04:58:41.448: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename dns 01/19/23 04:58:41.448
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:58:41.475
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:58:41.484
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-711.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-711.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     01/19/23 04:58:41.511
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-711.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-711.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     01/19/23 04:58:41.511
    STEP: creating a pod to probe /etc/hosts 01/19/23 04:58:41.511
    STEP: submitting the pod to kubernetes 01/19/23 04:58:41.511
    Jan 19 04:58:41.538: INFO: Waiting up to 15m0s for pod "dns-test-6bc8e418-c749-4a25-9b8e-a665acea82b7" in namespace "dns-711" to be "running"
    Jan 19 04:58:41.548: INFO: Pod "dns-test-6bc8e418-c749-4a25-9b8e-a665acea82b7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.9579ms
    Jan 19 04:58:43.554: INFO: Pod "dns-test-6bc8e418-c749-4a25-9b8e-a665acea82b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015707134s
    Jan 19 04:58:45.552: INFO: Pod "dns-test-6bc8e418-c749-4a25-9b8e-a665acea82b7": Phase="Running", Reason="", readiness=true. Elapsed: 4.013616424s
    Jan 19 04:58:45.552: INFO: Pod "dns-test-6bc8e418-c749-4a25-9b8e-a665acea82b7" satisfied condition "running"
    STEP: retrieving the pod 01/19/23 04:58:45.552
    STEP: looking for the results for each expected name from probers 01/19/23 04:58:45.554
    Jan 19 04:58:45.567: INFO: DNS probes using dns-711/dns-test-6bc8e418-c749-4a25-9b8e-a665acea82b7 succeeded

    STEP: deleting the pod 01/19/23 04:58:45.567
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 19 04:58:45.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-711" for this suite. 01/19/23 04:58:45.584
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:58:45.594
Jan 19 04:58:45.594: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename webhook 01/19/23 04:58:45.594
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:58:45.607
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:58:45.61
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/19/23 04:58:45.624
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 04:58:46.224
STEP: Deploying the webhook pod 01/19/23 04:58:46.231
STEP: Wait for the deployment to be ready 01/19/23 04:58:46.247
Jan 19 04:58:46.255: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 19 04:58:48.264: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/19/23 04:58:50.268
STEP: Verifying the service has paired with the endpoint 01/19/23 04:58:50.277
Jan 19 04:58:51.277: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
STEP: Creating a validating webhook configuration 01/19/23 04:58:51.281
STEP: Creating a configMap that does not comply to the validation webhook rules 01/19/23 04:58:51.295
STEP: Updating a validating webhook configuration's rules to not include the create operation 01/19/23 04:58:51.301
STEP: Creating a configMap that does not comply to the validation webhook rules 01/19/23 04:58:51.31
STEP: Patching a validating webhook configuration's rules to include the create operation 01/19/23 04:58:51.32
STEP: Creating a configMap that does not comply to the validation webhook rules 01/19/23 04:58:51.326
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 04:58:51.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9272" for this suite. 01/19/23 04:58:51.337
STEP: Destroying namespace "webhook-9272-markers" for this suite. 01/19/23 04:58:51.343
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","completed":161,"skipped":3066,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.794 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:58:45.594
    Jan 19 04:58:45.594: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename webhook 01/19/23 04:58:45.594
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:58:45.607
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:58:45.61
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/19/23 04:58:45.624
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 04:58:46.224
    STEP: Deploying the webhook pod 01/19/23 04:58:46.231
    STEP: Wait for the deployment to be ready 01/19/23 04:58:46.247
    Jan 19 04:58:46.255: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Jan 19 04:58:48.264: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 4, 58, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 58, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/19/23 04:58:50.268
    STEP: Verifying the service has paired with the endpoint 01/19/23 04:58:50.277
    Jan 19 04:58:51.277: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:412
    STEP: Creating a validating webhook configuration 01/19/23 04:58:51.281
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/19/23 04:58:51.295
    STEP: Updating a validating webhook configuration's rules to not include the create operation 01/19/23 04:58:51.301
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/19/23 04:58:51.31
    STEP: Patching a validating webhook configuration's rules to include the create operation 01/19/23 04:58:51.32
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/19/23 04:58:51.326
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 04:58:51.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9272" for this suite. 01/19/23 04:58:51.337
    STEP: Destroying namespace "webhook-9272-markers" for this suite. 01/19/23 04:58:51.343
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:58:51.388
Jan 19 04:58:51.388: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename pod-network-test 01/19/23 04:58:51.389
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:58:51.404
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:58:51.406
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-2110 01/19/23 04:58:51.408
STEP: creating a selector 01/19/23 04:58:51.408
STEP: Creating the service pods in kubernetes 01/19/23 04:58:51.408
Jan 19 04:58:51.408: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 19 04:58:51.430: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2110" to be "running and ready"
Jan 19 04:58:51.435: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.558304ms
Jan 19 04:58:51.435: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 04:58:53.441: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010974897s
Jan 19 04:58:53.441: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 04:58:55.439: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.009661146s
Jan 19 04:58:55.439: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 04:58:57.438: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.008665967s
Jan 19 04:58:57.438: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 04:58:59.441: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.01115345s
Jan 19 04:58:59.441: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 04:59:01.439: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.009685416s
Jan 19 04:59:01.439: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 04:59:03.439: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.009498592s
Jan 19 04:59:03.439: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 04:59:05.439: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.009403425s
Jan 19 04:59:05.439: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 04:59:07.439: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.009278474s
Jan 19 04:59:07.439: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 04:59:09.439: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.009252184s
Jan 19 04:59:09.439: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 04:59:11.440: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.009962071s
Jan 19 04:59:11.440: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 04:59:13.440: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.01000139s
Jan 19 04:59:13.440: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 19 04:59:13.440: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 19 04:59:13.443: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2110" to be "running and ready"
Jan 19 04:59:13.445: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.281402ms
Jan 19 04:59:13.445: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 19 04:59:13.445: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 01/19/23 04:59:13.447
Jan 19 04:59:13.455: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2110" to be "running"
Jan 19 04:59:13.461: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.421716ms
Jan 19 04:59:15.465: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010352887s
Jan 19 04:59:15.465: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 19 04:59:15.468: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 19 04:59:15.468: INFO: Breadth first check of 10.100.24.127 on host 192.168.0.99...
Jan 19 04:59:15.475: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.70.20:9080/dial?request=hostname&protocol=udp&host=10.100.24.127&port=8081&tries=1'] Namespace:pod-network-test-2110 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 04:59:15.475: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
Jan 19 04:59:15.475: INFO: ExecWithOptions: Clientset creation
Jan 19 04:59:15.475: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-2110/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.70.20%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.100.24.127%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 19 04:59:15.740: INFO: Waiting for responses: map[]
Jan 19 04:59:15.740: INFO: reached 10.100.24.127 after 0/1 tries
Jan 19 04:59:15.740: INFO: Breadth first check of 10.100.70.19 on host 192.168.0.78...
Jan 19 04:59:15.744: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.70.20:9080/dial?request=hostname&protocol=udp&host=10.100.70.19&port=8081&tries=1'] Namespace:pod-network-test-2110 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 04:59:15.744: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
Jan 19 04:59:15.744: INFO: ExecWithOptions: Clientset creation
Jan 19 04:59:15.744: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-2110/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.70.20%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.100.70.19%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 19 04:59:15.836: INFO: Waiting for responses: map[]
Jan 19 04:59:15.836: INFO: reached 10.100.70.19 after 0/1 tries
Jan 19 04:59:15.836: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jan 19 04:59:15.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2110" for this suite. 01/19/23 04:59:15.841
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","completed":162,"skipped":3089,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.460 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:58:51.388
    Jan 19 04:58:51.388: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename pod-network-test 01/19/23 04:58:51.389
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:58:51.404
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:58:51.406
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-2110 01/19/23 04:58:51.408
    STEP: creating a selector 01/19/23 04:58:51.408
    STEP: Creating the service pods in kubernetes 01/19/23 04:58:51.408
    Jan 19 04:58:51.408: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 19 04:58:51.430: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2110" to be "running and ready"
    Jan 19 04:58:51.435: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.558304ms
    Jan 19 04:58:51.435: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 04:58:53.441: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010974897s
    Jan 19 04:58:53.441: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 04:58:55.439: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.009661146s
    Jan 19 04:58:55.439: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 04:58:57.438: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.008665967s
    Jan 19 04:58:57.438: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 04:58:59.441: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.01115345s
    Jan 19 04:58:59.441: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 04:59:01.439: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.009685416s
    Jan 19 04:59:01.439: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 04:59:03.439: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.009498592s
    Jan 19 04:59:03.439: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 04:59:05.439: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.009403425s
    Jan 19 04:59:05.439: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 04:59:07.439: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.009278474s
    Jan 19 04:59:07.439: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 04:59:09.439: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.009252184s
    Jan 19 04:59:09.439: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 04:59:11.440: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.009962071s
    Jan 19 04:59:11.440: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 04:59:13.440: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.01000139s
    Jan 19 04:59:13.440: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 19 04:59:13.440: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 19 04:59:13.443: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2110" to be "running and ready"
    Jan 19 04:59:13.445: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.281402ms
    Jan 19 04:59:13.445: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 19 04:59:13.445: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 01/19/23 04:59:13.447
    Jan 19 04:59:13.455: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2110" to be "running"
    Jan 19 04:59:13.461: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.421716ms
    Jan 19 04:59:15.465: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010352887s
    Jan 19 04:59:15.465: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 19 04:59:15.468: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jan 19 04:59:15.468: INFO: Breadth first check of 10.100.24.127 on host 192.168.0.99...
    Jan 19 04:59:15.475: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.70.20:9080/dial?request=hostname&protocol=udp&host=10.100.24.127&port=8081&tries=1'] Namespace:pod-network-test-2110 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 04:59:15.475: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    Jan 19 04:59:15.475: INFO: ExecWithOptions: Clientset creation
    Jan 19 04:59:15.475: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-2110/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.70.20%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.100.24.127%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 19 04:59:15.740: INFO: Waiting for responses: map[]
    Jan 19 04:59:15.740: INFO: reached 10.100.24.127 after 0/1 tries
    Jan 19 04:59:15.740: INFO: Breadth first check of 10.100.70.19 on host 192.168.0.78...
    Jan 19 04:59:15.744: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.70.20:9080/dial?request=hostname&protocol=udp&host=10.100.70.19&port=8081&tries=1'] Namespace:pod-network-test-2110 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 04:59:15.744: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    Jan 19 04:59:15.744: INFO: ExecWithOptions: Clientset creation
    Jan 19 04:59:15.744: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-2110/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.70.20%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.100.70.19%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 19 04:59:15.836: INFO: Waiting for responses: map[]
    Jan 19 04:59:15.836: INFO: reached 10.100.70.19 after 0/1 tries
    Jan 19 04:59:15.836: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jan 19 04:59:15.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-2110" for this suite. 01/19/23 04:59:15.841
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:59:15.848
Jan 19 04:59:15.848: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename configmap 01/19/23 04:59:15.849
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:59:15.861
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:59:15.866
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
STEP: Creating configMap with name cm-test-opt-del-4d02dc52-7709-4db5-a861-49cda0c1cb16 01/19/23 04:59:15.87
STEP: Creating configMap with name cm-test-opt-upd-6cd5b916-7405-42e8-aae7-9c00a1952129 01/19/23 04:59:15.874
STEP: Creating the pod 01/19/23 04:59:15.878
Jan 19 04:59:15.887: INFO: Waiting up to 5m0s for pod "pod-configmaps-3e45dd36-26b9-472a-a771-35c261b5223f" in namespace "configmap-3537" to be "running and ready"
Jan 19 04:59:15.892: INFO: Pod "pod-configmaps-3e45dd36-26b9-472a-a771-35c261b5223f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.055715ms
Jan 19 04:59:15.892: INFO: The phase of Pod pod-configmaps-3e45dd36-26b9-472a-a771-35c261b5223f is Pending, waiting for it to be Running (with Ready = true)
Jan 19 04:59:17.897: INFO: Pod "pod-configmaps-3e45dd36-26b9-472a-a771-35c261b5223f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009203487s
Jan 19 04:59:17.897: INFO: The phase of Pod pod-configmaps-3e45dd36-26b9-472a-a771-35c261b5223f is Pending, waiting for it to be Running (with Ready = true)
Jan 19 04:59:19.897: INFO: Pod "pod-configmaps-3e45dd36-26b9-472a-a771-35c261b5223f": Phase="Running", Reason="", readiness=true. Elapsed: 4.009235513s
Jan 19 04:59:19.897: INFO: The phase of Pod pod-configmaps-3e45dd36-26b9-472a-a771-35c261b5223f is Running (Ready = true)
Jan 19 04:59:19.897: INFO: Pod "pod-configmaps-3e45dd36-26b9-472a-a771-35c261b5223f" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-4d02dc52-7709-4db5-a861-49cda0c1cb16 01/19/23 04:59:19.914
STEP: Updating configmap cm-test-opt-upd-6cd5b916-7405-42e8-aae7-9c00a1952129 01/19/23 04:59:19.919
STEP: Creating configMap with name cm-test-opt-create-300be972-0155-4c8b-8a6e-491c46bf37b5 01/19/23 04:59:19.927
STEP: waiting to observe update in volume 01/19/23 04:59:19.93
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 19 04:59:23.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3537" for this suite. 01/19/23 04:59:23.963
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":163,"skipped":3090,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.120 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:59:15.848
    Jan 19 04:59:15.848: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename configmap 01/19/23 04:59:15.849
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:59:15.861
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:59:15.866
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:239
    STEP: Creating configMap with name cm-test-opt-del-4d02dc52-7709-4db5-a861-49cda0c1cb16 01/19/23 04:59:15.87
    STEP: Creating configMap with name cm-test-opt-upd-6cd5b916-7405-42e8-aae7-9c00a1952129 01/19/23 04:59:15.874
    STEP: Creating the pod 01/19/23 04:59:15.878
    Jan 19 04:59:15.887: INFO: Waiting up to 5m0s for pod "pod-configmaps-3e45dd36-26b9-472a-a771-35c261b5223f" in namespace "configmap-3537" to be "running and ready"
    Jan 19 04:59:15.892: INFO: Pod "pod-configmaps-3e45dd36-26b9-472a-a771-35c261b5223f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.055715ms
    Jan 19 04:59:15.892: INFO: The phase of Pod pod-configmaps-3e45dd36-26b9-472a-a771-35c261b5223f is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 04:59:17.897: INFO: Pod "pod-configmaps-3e45dd36-26b9-472a-a771-35c261b5223f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009203487s
    Jan 19 04:59:17.897: INFO: The phase of Pod pod-configmaps-3e45dd36-26b9-472a-a771-35c261b5223f is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 04:59:19.897: INFO: Pod "pod-configmaps-3e45dd36-26b9-472a-a771-35c261b5223f": Phase="Running", Reason="", readiness=true. Elapsed: 4.009235513s
    Jan 19 04:59:19.897: INFO: The phase of Pod pod-configmaps-3e45dd36-26b9-472a-a771-35c261b5223f is Running (Ready = true)
    Jan 19 04:59:19.897: INFO: Pod "pod-configmaps-3e45dd36-26b9-472a-a771-35c261b5223f" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-4d02dc52-7709-4db5-a861-49cda0c1cb16 01/19/23 04:59:19.914
    STEP: Updating configmap cm-test-opt-upd-6cd5b916-7405-42e8-aae7-9c00a1952129 01/19/23 04:59:19.919
    STEP: Creating configMap with name cm-test-opt-create-300be972-0155-4c8b-8a6e-491c46bf37b5 01/19/23 04:59:19.927
    STEP: waiting to observe update in volume 01/19/23 04:59:19.93
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 19 04:59:23.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3537" for this suite. 01/19/23 04:59:23.963
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:59:23.969
Jan 19 04:59:23.969: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename endpointslice 01/19/23 04:59:23.97
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:59:23.983
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:59:23.986
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
Jan 19 04:59:23.995: INFO: Endpoints addresses: [192.168.0.121 192.168.0.26] , ports: [6443]
Jan 19 04:59:23.995: INFO: EndpointSlices addresses: [192.168.0.121 192.168.0.26] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jan 19 04:59:23.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-6912" for this suite. 01/19/23 04:59:23.997
{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","completed":164,"skipped":3090,"failed":0}
------------------------------
â€¢ [0.033 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:59:23.969
    Jan 19 04:59:23.969: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename endpointslice 01/19/23 04:59:23.97
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:59:23.983
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:59:23.986
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:65
    Jan 19 04:59:23.995: INFO: Endpoints addresses: [192.168.0.121 192.168.0.26] , ports: [6443]
    Jan 19 04:59:23.995: INFO: EndpointSlices addresses: [192.168.0.121 192.168.0.26] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jan 19 04:59:23.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-6912" for this suite. 01/19/23 04:59:23.997
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:59:24.002
Jan 19 04:59:24.002: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename podtemplate 01/19/23 04:59:24.003
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:59:24.015
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:59:24.019
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 01/19/23 04:59:24.022
Jan 19 04:59:24.028: INFO: created test-podtemplate-1
Jan 19 04:59:24.032: INFO: created test-podtemplate-2
Jan 19 04:59:24.037: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 01/19/23 04:59:24.037
STEP: delete collection of pod templates 01/19/23 04:59:24.04
Jan 19 04:59:24.040: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 01/19/23 04:59:24.054
Jan 19 04:59:24.054: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Jan 19 04:59:24.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-4621" for this suite. 01/19/23 04:59:24.06
{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","completed":165,"skipped":3090,"failed":0}
------------------------------
â€¢ [0.063 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:59:24.002
    Jan 19 04:59:24.002: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename podtemplate 01/19/23 04:59:24.003
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:59:24.015
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:59:24.019
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 01/19/23 04:59:24.022
    Jan 19 04:59:24.028: INFO: created test-podtemplate-1
    Jan 19 04:59:24.032: INFO: created test-podtemplate-2
    Jan 19 04:59:24.037: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 01/19/23 04:59:24.037
    STEP: delete collection of pod templates 01/19/23 04:59:24.04
    Jan 19 04:59:24.040: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 01/19/23 04:59:24.054
    Jan 19 04:59:24.054: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Jan 19 04:59:24.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-4621" for this suite. 01/19/23 04:59:24.06
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:59:24.065
Jan 19 04:59:24.065: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename subpath 01/19/23 04:59:24.066
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:59:24.077
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:59:24.08
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/19/23 04:59:24.083
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-h2r4 01/19/23 04:59:24.09
STEP: Creating a pod to test atomic-volume-subpath 01/19/23 04:59:24.09
Jan 19 04:59:24.100: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-h2r4" in namespace "subpath-7214" to be "Succeeded or Failed"
Jan 19 04:59:24.105: INFO: Pod "pod-subpath-test-configmap-h2r4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.183291ms
Jan 19 04:59:26.113: INFO: Pod "pod-subpath-test-configmap-h2r4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013568388s
Jan 19 04:59:28.109: INFO: Pod "pod-subpath-test-configmap-h2r4": Phase="Running", Reason="", readiness=true. Elapsed: 4.009466043s
Jan 19 04:59:30.109: INFO: Pod "pod-subpath-test-configmap-h2r4": Phase="Running", Reason="", readiness=true. Elapsed: 6.009648009s
Jan 19 04:59:32.109: INFO: Pod "pod-subpath-test-configmap-h2r4": Phase="Running", Reason="", readiness=true. Elapsed: 8.009413119s
Jan 19 04:59:34.109: INFO: Pod "pod-subpath-test-configmap-h2r4": Phase="Running", Reason="", readiness=true. Elapsed: 10.009646029s
Jan 19 04:59:36.108: INFO: Pod "pod-subpath-test-configmap-h2r4": Phase="Running", Reason="", readiness=true. Elapsed: 12.00857129s
Jan 19 04:59:38.109: INFO: Pod "pod-subpath-test-configmap-h2r4": Phase="Running", Reason="", readiness=true. Elapsed: 14.009349432s
Jan 19 04:59:40.109: INFO: Pod "pod-subpath-test-configmap-h2r4": Phase="Running", Reason="", readiness=true. Elapsed: 16.009413276s
Jan 19 04:59:42.108: INFO: Pod "pod-subpath-test-configmap-h2r4": Phase="Running", Reason="", readiness=true. Elapsed: 18.008804241s
Jan 19 04:59:44.109: INFO: Pod "pod-subpath-test-configmap-h2r4": Phase="Running", Reason="", readiness=true. Elapsed: 20.008967317s
Jan 19 04:59:46.111: INFO: Pod "pod-subpath-test-configmap-h2r4": Phase="Running", Reason="", readiness=true. Elapsed: 22.011516236s
Jan 19 04:59:48.110: INFO: Pod "pod-subpath-test-configmap-h2r4": Phase="Running", Reason="", readiness=false. Elapsed: 24.01069485s
Jan 19 04:59:50.110: INFO: Pod "pod-subpath-test-configmap-h2r4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.010266702s
STEP: Saw pod success 01/19/23 04:59:50.11
Jan 19 04:59:50.110: INFO: Pod "pod-subpath-test-configmap-h2r4" satisfied condition "Succeeded or Failed"
Jan 19 04:59:50.114: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-subpath-test-configmap-h2r4 container test-container-subpath-configmap-h2r4: <nil>
STEP: delete the pod 01/19/23 04:59:50.12
Jan 19 04:59:50.132: INFO: Waiting for pod pod-subpath-test-configmap-h2r4 to disappear
Jan 19 04:59:50.136: INFO: Pod pod-subpath-test-configmap-h2r4 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-h2r4 01/19/23 04:59:50.136
Jan 19 04:59:50.136: INFO: Deleting pod "pod-subpath-test-configmap-h2r4" in namespace "subpath-7214"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan 19 04:59:50.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7214" for this suite. 01/19/23 04:59:50.144
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","completed":166,"skipped":3096,"failed":0}
------------------------------
â€¢ [SLOW TEST] [26.087 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:59:24.065
    Jan 19 04:59:24.065: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename subpath 01/19/23 04:59:24.066
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:59:24.077
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:59:24.08
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/19/23 04:59:24.083
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-h2r4 01/19/23 04:59:24.09
    STEP: Creating a pod to test atomic-volume-subpath 01/19/23 04:59:24.09
    Jan 19 04:59:24.100: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-h2r4" in namespace "subpath-7214" to be "Succeeded or Failed"
    Jan 19 04:59:24.105: INFO: Pod "pod-subpath-test-configmap-h2r4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.183291ms
    Jan 19 04:59:26.113: INFO: Pod "pod-subpath-test-configmap-h2r4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013568388s
    Jan 19 04:59:28.109: INFO: Pod "pod-subpath-test-configmap-h2r4": Phase="Running", Reason="", readiness=true. Elapsed: 4.009466043s
    Jan 19 04:59:30.109: INFO: Pod "pod-subpath-test-configmap-h2r4": Phase="Running", Reason="", readiness=true. Elapsed: 6.009648009s
    Jan 19 04:59:32.109: INFO: Pod "pod-subpath-test-configmap-h2r4": Phase="Running", Reason="", readiness=true. Elapsed: 8.009413119s
    Jan 19 04:59:34.109: INFO: Pod "pod-subpath-test-configmap-h2r4": Phase="Running", Reason="", readiness=true. Elapsed: 10.009646029s
    Jan 19 04:59:36.108: INFO: Pod "pod-subpath-test-configmap-h2r4": Phase="Running", Reason="", readiness=true. Elapsed: 12.00857129s
    Jan 19 04:59:38.109: INFO: Pod "pod-subpath-test-configmap-h2r4": Phase="Running", Reason="", readiness=true. Elapsed: 14.009349432s
    Jan 19 04:59:40.109: INFO: Pod "pod-subpath-test-configmap-h2r4": Phase="Running", Reason="", readiness=true. Elapsed: 16.009413276s
    Jan 19 04:59:42.108: INFO: Pod "pod-subpath-test-configmap-h2r4": Phase="Running", Reason="", readiness=true. Elapsed: 18.008804241s
    Jan 19 04:59:44.109: INFO: Pod "pod-subpath-test-configmap-h2r4": Phase="Running", Reason="", readiness=true. Elapsed: 20.008967317s
    Jan 19 04:59:46.111: INFO: Pod "pod-subpath-test-configmap-h2r4": Phase="Running", Reason="", readiness=true. Elapsed: 22.011516236s
    Jan 19 04:59:48.110: INFO: Pod "pod-subpath-test-configmap-h2r4": Phase="Running", Reason="", readiness=false. Elapsed: 24.01069485s
    Jan 19 04:59:50.110: INFO: Pod "pod-subpath-test-configmap-h2r4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.010266702s
    STEP: Saw pod success 01/19/23 04:59:50.11
    Jan 19 04:59:50.110: INFO: Pod "pod-subpath-test-configmap-h2r4" satisfied condition "Succeeded or Failed"
    Jan 19 04:59:50.114: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-subpath-test-configmap-h2r4 container test-container-subpath-configmap-h2r4: <nil>
    STEP: delete the pod 01/19/23 04:59:50.12
    Jan 19 04:59:50.132: INFO: Waiting for pod pod-subpath-test-configmap-h2r4 to disappear
    Jan 19 04:59:50.136: INFO: Pod pod-subpath-test-configmap-h2r4 no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-h2r4 01/19/23 04:59:50.136
    Jan 19 04:59:50.136: INFO: Deleting pod "pod-subpath-test-configmap-h2r4" in namespace "subpath-7214"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan 19 04:59:50.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-7214" for this suite. 01/19/23 04:59:50.144
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:59:50.156
Jan 19 04:59:50.156: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename webhook 01/19/23 04:59:50.157
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:59:50.171
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:59:50.175
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/19/23 04:59:50.191
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 04:59:50.587
STEP: Deploying the webhook pod 01/19/23 04:59:50.596
STEP: Wait for the deployment to be ready 01/19/23 04:59:50.606
Jan 19 04:59:50.613: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 19 04:59:52.621: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 4, 59, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 59, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 4, 59, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 59, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/19/23 04:59:54.626
STEP: Verifying the service has paired with the endpoint 01/19/23 04:59:54.637
Jan 19 04:59:55.638: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 01/19/23 04:59:55.641
Jan 19 04:59:55.669: INFO: Waiting for webhook configuration to be ready...
STEP: create a namespace for the webhook 01/19/23 04:59:55.778
STEP: create a configmap should be unconditionally rejected by the webhook 01/19/23 04:59:55.785
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 04:59:55.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2853" for this suite. 01/19/23 04:59:55.821
STEP: Destroying namespace "webhook-2853-markers" for this suite. 01/19/23 04:59:55.826
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","completed":167,"skipped":3149,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.737 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:59:50.156
    Jan 19 04:59:50.156: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename webhook 01/19/23 04:59:50.157
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:59:50.171
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:59:50.175
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/19/23 04:59:50.191
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 04:59:50.587
    STEP: Deploying the webhook pod 01/19/23 04:59:50.596
    STEP: Wait for the deployment to be ready 01/19/23 04:59:50.606
    Jan 19 04:59:50.613: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Jan 19 04:59:52.621: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 4, 59, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 59, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 4, 59, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 4, 59, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/19/23 04:59:54.626
    STEP: Verifying the service has paired with the endpoint 01/19/23 04:59:54.637
    Jan 19 04:59:55.638: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:238
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 01/19/23 04:59:55.641
    Jan 19 04:59:55.669: INFO: Waiting for webhook configuration to be ready...
    STEP: create a namespace for the webhook 01/19/23 04:59:55.778
    STEP: create a configmap should be unconditionally rejected by the webhook 01/19/23 04:59:55.785
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 04:59:55.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2853" for this suite. 01/19/23 04:59:55.821
    STEP: Destroying namespace "webhook-2853-markers" for this suite. 01/19/23 04:59:55.826
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:59:55.893
Jan 19 04:59:55.893: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename gc 01/19/23 04:59:55.894
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:59:55.914
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:59:55.916
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 01/19/23 04:59:55.923
STEP: Wait for the Deployment to create new ReplicaSet 01/19/23 04:59:55.93
STEP: delete the deployment 01/19/23 04:59:55.935
STEP: wait for all rs to be garbage collected 01/19/23 04:59:55.957
STEP: expected 0 rs, got 1 rs 01/19/23 04:59:55.971
STEP: expected 0 pods, got 2 pods 01/19/23 04:59:55.978
STEP: Gathering metrics 01/19/23 04:59:56.495
W0119 04:59:56.500699      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 19 04:59:56.500: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 19 04:59:56.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6994" for this suite. 01/19/23 04:59:56.503
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","completed":168,"skipped":3153,"failed":0}
------------------------------
â€¢ [0.616 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:59:55.893
    Jan 19 04:59:55.893: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename gc 01/19/23 04:59:55.894
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:59:55.914
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:59:55.916
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 01/19/23 04:59:55.923
    STEP: Wait for the Deployment to create new ReplicaSet 01/19/23 04:59:55.93
    STEP: delete the deployment 01/19/23 04:59:55.935
    STEP: wait for all rs to be garbage collected 01/19/23 04:59:55.957
    STEP: expected 0 rs, got 1 rs 01/19/23 04:59:55.971
    STEP: expected 0 pods, got 2 pods 01/19/23 04:59:55.978
    STEP: Gathering metrics 01/19/23 04:59:56.495
    W0119 04:59:56.500699      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 19 04:59:56.500: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 19 04:59:56.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-6994" for this suite. 01/19/23 04:59:56.503
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:59:56.509
Jan 19 04:59:56.509: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename ingress 01/19/23 04:59:56.51
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:59:56.522
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:59:56.525
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 01/19/23 04:59:56.529
STEP: getting /apis/networking.k8s.io 01/19/23 04:59:56.53
STEP: getting /apis/networking.k8s.iov1 01/19/23 04:59:56.531
STEP: creating 01/19/23 04:59:56.532
STEP: getting 01/19/23 04:59:56.546
STEP: listing 01/19/23 04:59:56.548
STEP: watching 01/19/23 04:59:56.551
Jan 19 04:59:56.551: INFO: starting watch
STEP: cluster-wide listing 01/19/23 04:59:56.552
STEP: cluster-wide watching 01/19/23 04:59:56.554
Jan 19 04:59:56.555: INFO: starting watch
STEP: patching 01/19/23 04:59:56.555
STEP: updating 01/19/23 04:59:56.561
Jan 19 04:59:56.568: INFO: waiting for watch events with expected annotations
Jan 19 04:59:56.568: INFO: saw patched and updated annotations
STEP: patching /status 01/19/23 04:59:56.568
STEP: updating /status 01/19/23 04:59:56.574
STEP: get /status 01/19/23 04:59:56.581
STEP: deleting 01/19/23 04:59:56.584
STEP: deleting a collection 01/19/23 04:59:56.596
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
Jan 19 04:59:56.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-5582" for this suite. 01/19/23 04:59:56.61
{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","completed":169,"skipped":3170,"failed":0}
------------------------------
â€¢ [0.106 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:59:56.509
    Jan 19 04:59:56.509: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename ingress 01/19/23 04:59:56.51
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:59:56.522
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:59:56.525
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 01/19/23 04:59:56.529
    STEP: getting /apis/networking.k8s.io 01/19/23 04:59:56.53
    STEP: getting /apis/networking.k8s.iov1 01/19/23 04:59:56.531
    STEP: creating 01/19/23 04:59:56.532
    STEP: getting 01/19/23 04:59:56.546
    STEP: listing 01/19/23 04:59:56.548
    STEP: watching 01/19/23 04:59:56.551
    Jan 19 04:59:56.551: INFO: starting watch
    STEP: cluster-wide listing 01/19/23 04:59:56.552
    STEP: cluster-wide watching 01/19/23 04:59:56.554
    Jan 19 04:59:56.555: INFO: starting watch
    STEP: patching 01/19/23 04:59:56.555
    STEP: updating 01/19/23 04:59:56.561
    Jan 19 04:59:56.568: INFO: waiting for watch events with expected annotations
    Jan 19 04:59:56.568: INFO: saw patched and updated annotations
    STEP: patching /status 01/19/23 04:59:56.568
    STEP: updating /status 01/19/23 04:59:56.574
    STEP: get /status 01/19/23 04:59:56.581
    STEP: deleting 01/19/23 04:59:56.584
    STEP: deleting a collection 01/19/23 04:59:56.596
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:187
    Jan 19 04:59:56.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingress-5582" for this suite. 01/19/23 04:59:56.61
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 04:59:56.616
Jan 19 04:59:56.616: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename downward-api 01/19/23 04:59:56.616
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:59:56.631
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:59:56.633
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
STEP: Creating a pod to test downward API volume plugin 01/19/23 04:59:56.637
Jan 19 04:59:56.647: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5942812a-32bf-4e8e-b908-03185aff8abd" in namespace "downward-api-2134" to be "Succeeded or Failed"
Jan 19 04:59:56.649: INFO: Pod "downwardapi-volume-5942812a-32bf-4e8e-b908-03185aff8abd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.476159ms
Jan 19 04:59:58.653: INFO: Pod "downwardapi-volume-5942812a-32bf-4e8e-b908-03185aff8abd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006481233s
Jan 19 05:00:00.653: INFO: Pod "downwardapi-volume-5942812a-32bf-4e8e-b908-03185aff8abd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005762426s
STEP: Saw pod success 01/19/23 05:00:00.653
Jan 19 05:00:00.653: INFO: Pod "downwardapi-volume-5942812a-32bf-4e8e-b908-03185aff8abd" satisfied condition "Succeeded or Failed"
Jan 19 05:00:00.655: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod downwardapi-volume-5942812a-32bf-4e8e-b908-03185aff8abd container client-container: <nil>
STEP: delete the pod 01/19/23 05:00:00.661
Jan 19 05:00:00.675: INFO: Waiting for pod downwardapi-volume-5942812a-32bf-4e8e-b908-03185aff8abd to disappear
Jan 19 05:00:00.678: INFO: Pod downwardapi-volume-5942812a-32bf-4e8e-b908-03185aff8abd no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 19 05:00:00.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2134" for this suite. 01/19/23 05:00:00.68
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":170,"skipped":3197,"failed":0}
------------------------------
â€¢ [4.070 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 04:59:56.616
    Jan 19 04:59:56.616: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename downward-api 01/19/23 04:59:56.616
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 04:59:56.631
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 04:59:56.633
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:260
    STEP: Creating a pod to test downward API volume plugin 01/19/23 04:59:56.637
    Jan 19 04:59:56.647: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5942812a-32bf-4e8e-b908-03185aff8abd" in namespace "downward-api-2134" to be "Succeeded or Failed"
    Jan 19 04:59:56.649: INFO: Pod "downwardapi-volume-5942812a-32bf-4e8e-b908-03185aff8abd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.476159ms
    Jan 19 04:59:58.653: INFO: Pod "downwardapi-volume-5942812a-32bf-4e8e-b908-03185aff8abd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006481233s
    Jan 19 05:00:00.653: INFO: Pod "downwardapi-volume-5942812a-32bf-4e8e-b908-03185aff8abd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005762426s
    STEP: Saw pod success 01/19/23 05:00:00.653
    Jan 19 05:00:00.653: INFO: Pod "downwardapi-volume-5942812a-32bf-4e8e-b908-03185aff8abd" satisfied condition "Succeeded or Failed"
    Jan 19 05:00:00.655: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod downwardapi-volume-5942812a-32bf-4e8e-b908-03185aff8abd container client-container: <nil>
    STEP: delete the pod 01/19/23 05:00:00.661
    Jan 19 05:00:00.675: INFO: Waiting for pod downwardapi-volume-5942812a-32bf-4e8e-b908-03185aff8abd to disappear
    Jan 19 05:00:00.678: INFO: Pod downwardapi-volume-5942812a-32bf-4e8e-b908-03185aff8abd no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 19 05:00:00.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2134" for this suite. 01/19/23 05:00:00.68
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:00:00.688
Jan 19 05:00:00.688: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename subpath 01/19/23 05:00:00.688
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:00:00.7
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:00:00.704
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/19/23 05:00:00.706
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-mqlb 01/19/23 05:00:00.715
STEP: Creating a pod to test atomic-volume-subpath 01/19/23 05:00:00.716
Jan 19 05:00:00.725: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-mqlb" in namespace "subpath-6239" to be "Succeeded or Failed"
Jan 19 05:00:00.729: INFO: Pod "pod-subpath-test-downwardapi-mqlb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.094843ms
Jan 19 05:00:02.733: INFO: Pod "pod-subpath-test-downwardapi-mqlb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007730495s
Jan 19 05:00:04.734: INFO: Pod "pod-subpath-test-downwardapi-mqlb": Phase="Running", Reason="", readiness=true. Elapsed: 4.009103199s
Jan 19 05:00:06.734: INFO: Pod "pod-subpath-test-downwardapi-mqlb": Phase="Running", Reason="", readiness=true. Elapsed: 6.008608019s
Jan 19 05:00:08.734: INFO: Pod "pod-subpath-test-downwardapi-mqlb": Phase="Running", Reason="", readiness=true. Elapsed: 8.00905981s
Jan 19 05:00:10.734: INFO: Pod "pod-subpath-test-downwardapi-mqlb": Phase="Running", Reason="", readiness=true. Elapsed: 10.008960048s
Jan 19 05:00:12.733: INFO: Pod "pod-subpath-test-downwardapi-mqlb": Phase="Running", Reason="", readiness=true. Elapsed: 12.007661194s
Jan 19 05:00:14.734: INFO: Pod "pod-subpath-test-downwardapi-mqlb": Phase="Running", Reason="", readiness=true. Elapsed: 14.009069849s
Jan 19 05:00:16.734: INFO: Pod "pod-subpath-test-downwardapi-mqlb": Phase="Running", Reason="", readiness=true. Elapsed: 16.00837854s
Jan 19 05:00:18.734: INFO: Pod "pod-subpath-test-downwardapi-mqlb": Phase="Running", Reason="", readiness=true. Elapsed: 18.008685099s
Jan 19 05:00:20.733: INFO: Pod "pod-subpath-test-downwardapi-mqlb": Phase="Running", Reason="", readiness=true. Elapsed: 20.008012638s
Jan 19 05:00:22.734: INFO: Pod "pod-subpath-test-downwardapi-mqlb": Phase="Running", Reason="", readiness=true. Elapsed: 22.008722848s
Jan 19 05:00:24.733: INFO: Pod "pod-subpath-test-downwardapi-mqlb": Phase="Running", Reason="", readiness=false. Elapsed: 24.007908749s
Jan 19 05:00:26.733: INFO: Pod "pod-subpath-test-downwardapi-mqlb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.008300947s
STEP: Saw pod success 01/19/23 05:00:26.734
Jan 19 05:00:26.734: INFO: Pod "pod-subpath-test-downwardapi-mqlb" satisfied condition "Succeeded or Failed"
Jan 19 05:00:26.737: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-subpath-test-downwardapi-mqlb container test-container-subpath-downwardapi-mqlb: <nil>
STEP: delete the pod 01/19/23 05:00:26.744
Jan 19 05:00:26.754: INFO: Waiting for pod pod-subpath-test-downwardapi-mqlb to disappear
Jan 19 05:00:26.756: INFO: Pod pod-subpath-test-downwardapi-mqlb no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-mqlb 01/19/23 05:00:26.756
Jan 19 05:00:26.756: INFO: Deleting pod "pod-subpath-test-downwardapi-mqlb" in namespace "subpath-6239"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan 19 05:00:26.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6239" for this suite. 01/19/23 05:00:26.761
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","completed":171,"skipped":3272,"failed":0}
------------------------------
â€¢ [SLOW TEST] [26.079 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:00:00.688
    Jan 19 05:00:00.688: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename subpath 01/19/23 05:00:00.688
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:00:00.7
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:00:00.704
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/19/23 05:00:00.706
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-mqlb 01/19/23 05:00:00.715
    STEP: Creating a pod to test atomic-volume-subpath 01/19/23 05:00:00.716
    Jan 19 05:00:00.725: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-mqlb" in namespace "subpath-6239" to be "Succeeded or Failed"
    Jan 19 05:00:00.729: INFO: Pod "pod-subpath-test-downwardapi-mqlb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.094843ms
    Jan 19 05:00:02.733: INFO: Pod "pod-subpath-test-downwardapi-mqlb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007730495s
    Jan 19 05:00:04.734: INFO: Pod "pod-subpath-test-downwardapi-mqlb": Phase="Running", Reason="", readiness=true. Elapsed: 4.009103199s
    Jan 19 05:00:06.734: INFO: Pod "pod-subpath-test-downwardapi-mqlb": Phase="Running", Reason="", readiness=true. Elapsed: 6.008608019s
    Jan 19 05:00:08.734: INFO: Pod "pod-subpath-test-downwardapi-mqlb": Phase="Running", Reason="", readiness=true. Elapsed: 8.00905981s
    Jan 19 05:00:10.734: INFO: Pod "pod-subpath-test-downwardapi-mqlb": Phase="Running", Reason="", readiness=true. Elapsed: 10.008960048s
    Jan 19 05:00:12.733: INFO: Pod "pod-subpath-test-downwardapi-mqlb": Phase="Running", Reason="", readiness=true. Elapsed: 12.007661194s
    Jan 19 05:00:14.734: INFO: Pod "pod-subpath-test-downwardapi-mqlb": Phase="Running", Reason="", readiness=true. Elapsed: 14.009069849s
    Jan 19 05:00:16.734: INFO: Pod "pod-subpath-test-downwardapi-mqlb": Phase="Running", Reason="", readiness=true. Elapsed: 16.00837854s
    Jan 19 05:00:18.734: INFO: Pod "pod-subpath-test-downwardapi-mqlb": Phase="Running", Reason="", readiness=true. Elapsed: 18.008685099s
    Jan 19 05:00:20.733: INFO: Pod "pod-subpath-test-downwardapi-mqlb": Phase="Running", Reason="", readiness=true. Elapsed: 20.008012638s
    Jan 19 05:00:22.734: INFO: Pod "pod-subpath-test-downwardapi-mqlb": Phase="Running", Reason="", readiness=true. Elapsed: 22.008722848s
    Jan 19 05:00:24.733: INFO: Pod "pod-subpath-test-downwardapi-mqlb": Phase="Running", Reason="", readiness=false. Elapsed: 24.007908749s
    Jan 19 05:00:26.733: INFO: Pod "pod-subpath-test-downwardapi-mqlb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.008300947s
    STEP: Saw pod success 01/19/23 05:00:26.734
    Jan 19 05:00:26.734: INFO: Pod "pod-subpath-test-downwardapi-mqlb" satisfied condition "Succeeded or Failed"
    Jan 19 05:00:26.737: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-subpath-test-downwardapi-mqlb container test-container-subpath-downwardapi-mqlb: <nil>
    STEP: delete the pod 01/19/23 05:00:26.744
    Jan 19 05:00:26.754: INFO: Waiting for pod pod-subpath-test-downwardapi-mqlb to disappear
    Jan 19 05:00:26.756: INFO: Pod pod-subpath-test-downwardapi-mqlb no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-mqlb 01/19/23 05:00:26.756
    Jan 19 05:00:26.756: INFO: Deleting pod "pod-subpath-test-downwardapi-mqlb" in namespace "subpath-6239"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan 19 05:00:26.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-6239" for this suite. 01/19/23 05:00:26.761
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:00:26.767
Jan 19 05:00:26.768: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename services 01/19/23 05:00:26.768
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:00:26.78
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:00:26.783
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-2 01/19/23 05:00:26.785
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/19/23 05:00:26.794
STEP: creating service externalsvc in namespace services-2 01/19/23 05:00:26.794
STEP: creating replication controller externalsvc in namespace services-2 01/19/23 05:00:26.814
I0119 05:00:26.820145      18 runners.go:193] Created replication controller with name: externalsvc, namespace: services-2, replica count: 2
I0119 05:00:29.870863      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 01/19/23 05:00:29.874
Jan 19 05:00:29.892: INFO: Creating new exec pod
Jan 19 05:00:29.898: INFO: Waiting up to 5m0s for pod "execpodz58gp" in namespace "services-2" to be "running"
Jan 19 05:00:29.901: INFO: Pod "execpodz58gp": Phase="Pending", Reason="", readiness=false. Elapsed: 3.046174ms
Jan 19 05:00:31.904: INFO: Pod "execpodz58gp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006416047s
Jan 19 05:00:33.904: INFO: Pod "execpodz58gp": Phase="Running", Reason="", readiness=true. Elapsed: 4.006326608s
Jan 19 05:00:33.904: INFO: Pod "execpodz58gp" satisfied condition "running"
Jan 19 05:00:33.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-2 exec execpodz58gp -- /bin/sh -x -c nslookup clusterip-service.services-2.svc.cluster.local'
Jan 19 05:00:34.444: INFO: stderr: "+ nslookup clusterip-service.services-2.svc.cluster.local\n"
Jan 19 05:00:34.444: INFO: stdout: "Server:\t\t10.254.0.10\nAddress:\t10.254.0.10#53\n\nclusterip-service.services-2.svc.cluster.local\tcanonical name = externalsvc.services-2.svc.cluster.local.\nName:\texternalsvc.services-2.svc.cluster.local\nAddress: 10.254.244.60\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-2, will wait for the garbage collector to delete the pods 01/19/23 05:00:34.444
Jan 19 05:00:34.505: INFO: Deleting ReplicationController externalsvc took: 7.076886ms
Jan 19 05:00:34.605: INFO: Terminating ReplicationController externalsvc pods took: 100.52293ms
Jan 19 05:00:36.623: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 19 05:00:36.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2" for this suite. 01/19/23 05:00:36.646
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","completed":172,"skipped":3293,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.884 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:00:26.767
    Jan 19 05:00:26.768: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename services 01/19/23 05:00:26.768
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:00:26.78
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:00:26.783
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1481
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-2 01/19/23 05:00:26.785
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/19/23 05:00:26.794
    STEP: creating service externalsvc in namespace services-2 01/19/23 05:00:26.794
    STEP: creating replication controller externalsvc in namespace services-2 01/19/23 05:00:26.814
    I0119 05:00:26.820145      18 runners.go:193] Created replication controller with name: externalsvc, namespace: services-2, replica count: 2
    I0119 05:00:29.870863      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 01/19/23 05:00:29.874
    Jan 19 05:00:29.892: INFO: Creating new exec pod
    Jan 19 05:00:29.898: INFO: Waiting up to 5m0s for pod "execpodz58gp" in namespace "services-2" to be "running"
    Jan 19 05:00:29.901: INFO: Pod "execpodz58gp": Phase="Pending", Reason="", readiness=false. Elapsed: 3.046174ms
    Jan 19 05:00:31.904: INFO: Pod "execpodz58gp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006416047s
    Jan 19 05:00:33.904: INFO: Pod "execpodz58gp": Phase="Running", Reason="", readiness=true. Elapsed: 4.006326608s
    Jan 19 05:00:33.904: INFO: Pod "execpodz58gp" satisfied condition "running"
    Jan 19 05:00:33.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-2 exec execpodz58gp -- /bin/sh -x -c nslookup clusterip-service.services-2.svc.cluster.local'
    Jan 19 05:00:34.444: INFO: stderr: "+ nslookup clusterip-service.services-2.svc.cluster.local\n"
    Jan 19 05:00:34.444: INFO: stdout: "Server:\t\t10.254.0.10\nAddress:\t10.254.0.10#53\n\nclusterip-service.services-2.svc.cluster.local\tcanonical name = externalsvc.services-2.svc.cluster.local.\nName:\texternalsvc.services-2.svc.cluster.local\nAddress: 10.254.244.60\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-2, will wait for the garbage collector to delete the pods 01/19/23 05:00:34.444
    Jan 19 05:00:34.505: INFO: Deleting ReplicationController externalsvc took: 7.076886ms
    Jan 19 05:00:34.605: INFO: Terminating ReplicationController externalsvc pods took: 100.52293ms
    Jan 19 05:00:36.623: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 19 05:00:36.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2" for this suite. 01/19/23 05:00:36.646
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:00:36.652
Jan 19 05:00:36.652: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename resourcequota 01/19/23 05:00:36.652
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:00:36.666
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:00:36.668
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
STEP: Discovering how many secrets are in namespace by default 01/19/23 05:00:36.67
STEP: Counting existing ResourceQuota 01/19/23 05:00:41.679
STEP: Creating a ResourceQuota 01/19/23 05:00:46.684
STEP: Ensuring resource quota status is calculated 01/19/23 05:00:46.692
STEP: Creating a Secret 01/19/23 05:00:48.696
STEP: Ensuring resource quota status captures secret creation 01/19/23 05:00:48.718
STEP: Deleting a secret 01/19/23 05:00:50.722
STEP: Ensuring resource quota status released usage 01/19/23 05:00:50.728
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 19 05:00:52.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9530" for this suite. 01/19/23 05:00:52.736
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","completed":173,"skipped":3308,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.091 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:00:36.652
    Jan 19 05:00:36.652: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename resourcequota 01/19/23 05:00:36.652
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:00:36.666
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:00:36.668
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:150
    STEP: Discovering how many secrets are in namespace by default 01/19/23 05:00:36.67
    STEP: Counting existing ResourceQuota 01/19/23 05:00:41.679
    STEP: Creating a ResourceQuota 01/19/23 05:00:46.684
    STEP: Ensuring resource quota status is calculated 01/19/23 05:00:46.692
    STEP: Creating a Secret 01/19/23 05:00:48.696
    STEP: Ensuring resource quota status captures secret creation 01/19/23 05:00:48.718
    STEP: Deleting a secret 01/19/23 05:00:50.722
    STEP: Ensuring resource quota status released usage 01/19/23 05:00:50.728
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 19 05:00:52.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9530" for this suite. 01/19/23 05:00:52.736
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:00:52.744
Jan 19 05:00:52.744: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename containers 01/19/23 05:00:52.744
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:00:52.756
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:00:52.759
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
STEP: Creating a pod to test override arguments 01/19/23 05:00:52.761
Jan 19 05:00:52.770: INFO: Waiting up to 5m0s for pod "client-containers-24e1c203-0123-4eed-ac88-ebb552fed151" in namespace "containers-2987" to be "Succeeded or Failed"
Jan 19 05:00:52.775: INFO: Pod "client-containers-24e1c203-0123-4eed-ac88-ebb552fed151": Phase="Pending", Reason="", readiness=false. Elapsed: 5.508175ms
Jan 19 05:00:54.780: INFO: Pod "client-containers-24e1c203-0123-4eed-ac88-ebb552fed151": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010066877s
Jan 19 05:00:56.780: INFO: Pod "client-containers-24e1c203-0123-4eed-ac88-ebb552fed151": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010169118s
Jan 19 05:00:58.778: INFO: Pod "client-containers-24e1c203-0123-4eed-ac88-ebb552fed151": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008806251s
STEP: Saw pod success 01/19/23 05:00:58.778
Jan 19 05:00:58.779: INFO: Pod "client-containers-24e1c203-0123-4eed-ac88-ebb552fed151" satisfied condition "Succeeded or Failed"
Jan 19 05:00:58.781: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod client-containers-24e1c203-0123-4eed-ac88-ebb552fed151 container agnhost-container: <nil>
STEP: delete the pod 01/19/23 05:00:58.786
Jan 19 05:00:58.795: INFO: Waiting for pod client-containers-24e1c203-0123-4eed-ac88-ebb552fed151 to disappear
Jan 19 05:00:58.797: INFO: Pod client-containers-24e1c203-0123-4eed-ac88-ebb552fed151 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jan 19 05:00:58.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2987" for this suite. 01/19/23 05:00:58.8
{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","completed":174,"skipped":3337,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.062 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:00:52.744
    Jan 19 05:00:52.744: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename containers 01/19/23 05:00:52.744
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:00:52.756
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:00:52.759
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:58
    STEP: Creating a pod to test override arguments 01/19/23 05:00:52.761
    Jan 19 05:00:52.770: INFO: Waiting up to 5m0s for pod "client-containers-24e1c203-0123-4eed-ac88-ebb552fed151" in namespace "containers-2987" to be "Succeeded or Failed"
    Jan 19 05:00:52.775: INFO: Pod "client-containers-24e1c203-0123-4eed-ac88-ebb552fed151": Phase="Pending", Reason="", readiness=false. Elapsed: 5.508175ms
    Jan 19 05:00:54.780: INFO: Pod "client-containers-24e1c203-0123-4eed-ac88-ebb552fed151": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010066877s
    Jan 19 05:00:56.780: INFO: Pod "client-containers-24e1c203-0123-4eed-ac88-ebb552fed151": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010169118s
    Jan 19 05:00:58.778: INFO: Pod "client-containers-24e1c203-0123-4eed-ac88-ebb552fed151": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008806251s
    STEP: Saw pod success 01/19/23 05:00:58.778
    Jan 19 05:00:58.779: INFO: Pod "client-containers-24e1c203-0123-4eed-ac88-ebb552fed151" satisfied condition "Succeeded or Failed"
    Jan 19 05:00:58.781: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod client-containers-24e1c203-0123-4eed-ac88-ebb552fed151 container agnhost-container: <nil>
    STEP: delete the pod 01/19/23 05:00:58.786
    Jan 19 05:00:58.795: INFO: Waiting for pod client-containers-24e1c203-0123-4eed-ac88-ebb552fed151 to disappear
    Jan 19 05:00:58.797: INFO: Pod client-containers-24e1c203-0123-4eed-ac88-ebb552fed151 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jan 19 05:00:58.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-2987" for this suite. 01/19/23 05:00:58.8
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:00:58.807
Jan 19 05:00:58.807: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename crd-publish-openapi 01/19/23 05:00:58.808
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:00:58.82
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:00:58.823
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
Jan 19 05:00:58.826: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 01/19/23 05:01:01.292
Jan 19 05:01:01.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-548 --namespace=crd-publish-openapi-548 create -f -'
Jan 19 05:01:02.113: INFO: stderr: ""
Jan 19 05:01:02.113: INFO: stdout: "e2e-test-crd-publish-openapi-9147-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 19 05:01:02.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-548 --namespace=crd-publish-openapi-548 delete e2e-test-crd-publish-openapi-9147-crds test-foo'
Jan 19 05:01:02.250: INFO: stderr: ""
Jan 19 05:01:02.250: INFO: stdout: "e2e-test-crd-publish-openapi-9147-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jan 19 05:01:02.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-548 --namespace=crd-publish-openapi-548 apply -f -'
Jan 19 05:01:02.866: INFO: stderr: ""
Jan 19 05:01:02.866: INFO: stdout: "e2e-test-crd-publish-openapi-9147-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 19 05:01:02.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-548 --namespace=crd-publish-openapi-548 delete e2e-test-crd-publish-openapi-9147-crds test-foo'
Jan 19 05:01:02.947: INFO: stderr: ""
Jan 19 05:01:02.947: INFO: stdout: "e2e-test-crd-publish-openapi-9147-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 01/19/23 05:01:02.947
Jan 19 05:01:02.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-548 --namespace=crd-publish-openapi-548 create -f -'
Jan 19 05:01:03.184: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 01/19/23 05:01:03.184
Jan 19 05:01:03.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-548 --namespace=crd-publish-openapi-548 create -f -'
Jan 19 05:01:03.388: INFO: rc: 1
Jan 19 05:01:03.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-548 --namespace=crd-publish-openapi-548 apply -f -'
Jan 19 05:01:03.603: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 01/19/23 05:01:03.603
Jan 19 05:01:03.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-548 --namespace=crd-publish-openapi-548 create -f -'
Jan 19 05:01:03.812: INFO: rc: 1
Jan 19 05:01:03.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-548 --namespace=crd-publish-openapi-548 apply -f -'
Jan 19 05:01:04.025: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 01/19/23 05:01:04.025
Jan 19 05:01:04.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-548 explain e2e-test-crd-publish-openapi-9147-crds'
Jan 19 05:01:04.229: INFO: stderr: ""
Jan 19 05:01:04.229: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9147-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 01/19/23 05:01:04.23
Jan 19 05:01:04.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-548 explain e2e-test-crd-publish-openapi-9147-crds.metadata'
Jan 19 05:01:04.425: INFO: stderr: ""
Jan 19 05:01:04.425: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9147-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jan 19 05:01:04.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-548 explain e2e-test-crd-publish-openapi-9147-crds.spec'
Jan 19 05:01:04.632: INFO: stderr: ""
Jan 19 05:01:04.632: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9147-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jan 19 05:01:04.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-548 explain e2e-test-crd-publish-openapi-9147-crds.spec.bars'
Jan 19 05:01:04.845: INFO: stderr: ""
Jan 19 05:01:04.845: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9147-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 01/19/23 05:01:04.845
Jan 19 05:01:04.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-548 explain e2e-test-crd-publish-openapi-9147-crds.spec.bars2'
Jan 19 05:01:05.053: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 05:01:07.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-548" for this suite. 01/19/23 05:01:07.541
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","completed":175,"skipped":3365,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.740 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:00:58.807
    Jan 19 05:00:58.807: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename crd-publish-openapi 01/19/23 05:00:58.808
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:00:58.82
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:00:58.823
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:68
    Jan 19 05:00:58.826: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 01/19/23 05:01:01.292
    Jan 19 05:01:01.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-548 --namespace=crd-publish-openapi-548 create -f -'
    Jan 19 05:01:02.113: INFO: stderr: ""
    Jan 19 05:01:02.113: INFO: stdout: "e2e-test-crd-publish-openapi-9147-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jan 19 05:01:02.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-548 --namespace=crd-publish-openapi-548 delete e2e-test-crd-publish-openapi-9147-crds test-foo'
    Jan 19 05:01:02.250: INFO: stderr: ""
    Jan 19 05:01:02.250: INFO: stdout: "e2e-test-crd-publish-openapi-9147-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Jan 19 05:01:02.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-548 --namespace=crd-publish-openapi-548 apply -f -'
    Jan 19 05:01:02.866: INFO: stderr: ""
    Jan 19 05:01:02.866: INFO: stdout: "e2e-test-crd-publish-openapi-9147-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jan 19 05:01:02.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-548 --namespace=crd-publish-openapi-548 delete e2e-test-crd-publish-openapi-9147-crds test-foo'
    Jan 19 05:01:02.947: INFO: stderr: ""
    Jan 19 05:01:02.947: INFO: stdout: "e2e-test-crd-publish-openapi-9147-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 01/19/23 05:01:02.947
    Jan 19 05:01:02.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-548 --namespace=crd-publish-openapi-548 create -f -'
    Jan 19 05:01:03.184: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 01/19/23 05:01:03.184
    Jan 19 05:01:03.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-548 --namespace=crd-publish-openapi-548 create -f -'
    Jan 19 05:01:03.388: INFO: rc: 1
    Jan 19 05:01:03.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-548 --namespace=crd-publish-openapi-548 apply -f -'
    Jan 19 05:01:03.603: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 01/19/23 05:01:03.603
    Jan 19 05:01:03.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-548 --namespace=crd-publish-openapi-548 create -f -'
    Jan 19 05:01:03.812: INFO: rc: 1
    Jan 19 05:01:03.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-548 --namespace=crd-publish-openapi-548 apply -f -'
    Jan 19 05:01:04.025: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 01/19/23 05:01:04.025
    Jan 19 05:01:04.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-548 explain e2e-test-crd-publish-openapi-9147-crds'
    Jan 19 05:01:04.229: INFO: stderr: ""
    Jan 19 05:01:04.229: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9147-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 01/19/23 05:01:04.23
    Jan 19 05:01:04.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-548 explain e2e-test-crd-publish-openapi-9147-crds.metadata'
    Jan 19 05:01:04.425: INFO: stderr: ""
    Jan 19 05:01:04.425: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9147-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Jan 19 05:01:04.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-548 explain e2e-test-crd-publish-openapi-9147-crds.spec'
    Jan 19 05:01:04.632: INFO: stderr: ""
    Jan 19 05:01:04.632: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9147-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Jan 19 05:01:04.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-548 explain e2e-test-crd-publish-openapi-9147-crds.spec.bars'
    Jan 19 05:01:04.845: INFO: stderr: ""
    Jan 19 05:01:04.845: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9147-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 01/19/23 05:01:04.845
    Jan 19 05:01:04.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-548 explain e2e-test-crd-publish-openapi-9147-crds.spec.bars2'
    Jan 19 05:01:05.053: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 05:01:07.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-548" for this suite. 01/19/23 05:01:07.541
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:01:07.548
Jan 19 05:01:07.549: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename var-expansion 01/19/23 05:01:07.549
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:01:07.564
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:01:07.568
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
STEP: Creating a pod to test substitution in container's command 01/19/23 05:01:07.571
Jan 19 05:01:07.581: INFO: Waiting up to 5m0s for pod "var-expansion-e9b7db6e-5231-4552-99b9-82fbb2c0410e" in namespace "var-expansion-9302" to be "Succeeded or Failed"
Jan 19 05:01:07.588: INFO: Pod "var-expansion-e9b7db6e-5231-4552-99b9-82fbb2c0410e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.551712ms
Jan 19 05:01:09.592: INFO: Pod "var-expansion-e9b7db6e-5231-4552-99b9-82fbb2c0410e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011317686s
Jan 19 05:01:11.594: INFO: Pod "var-expansion-e9b7db6e-5231-4552-99b9-82fbb2c0410e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012700944s
STEP: Saw pod success 01/19/23 05:01:11.594
Jan 19 05:01:11.594: INFO: Pod "var-expansion-e9b7db6e-5231-4552-99b9-82fbb2c0410e" satisfied condition "Succeeded or Failed"
Jan 19 05:01:11.597: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod var-expansion-e9b7db6e-5231-4552-99b9-82fbb2c0410e container dapi-container: <nil>
STEP: delete the pod 01/19/23 05:01:11.602
Jan 19 05:01:11.612: INFO: Waiting for pod var-expansion-e9b7db6e-5231-4552-99b9-82fbb2c0410e to disappear
Jan 19 05:01:11.616: INFO: Pod var-expansion-e9b7db6e-5231-4552-99b9-82fbb2c0410e no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 19 05:01:11.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9302" for this suite. 01/19/23 05:01:11.62
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","completed":176,"skipped":3404,"failed":0}
------------------------------
â€¢ [4.077 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:01:07.548
    Jan 19 05:01:07.549: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename var-expansion 01/19/23 05:01:07.549
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:01:07.564
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:01:07.568
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:72
    STEP: Creating a pod to test substitution in container's command 01/19/23 05:01:07.571
    Jan 19 05:01:07.581: INFO: Waiting up to 5m0s for pod "var-expansion-e9b7db6e-5231-4552-99b9-82fbb2c0410e" in namespace "var-expansion-9302" to be "Succeeded or Failed"
    Jan 19 05:01:07.588: INFO: Pod "var-expansion-e9b7db6e-5231-4552-99b9-82fbb2c0410e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.551712ms
    Jan 19 05:01:09.592: INFO: Pod "var-expansion-e9b7db6e-5231-4552-99b9-82fbb2c0410e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011317686s
    Jan 19 05:01:11.594: INFO: Pod "var-expansion-e9b7db6e-5231-4552-99b9-82fbb2c0410e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012700944s
    STEP: Saw pod success 01/19/23 05:01:11.594
    Jan 19 05:01:11.594: INFO: Pod "var-expansion-e9b7db6e-5231-4552-99b9-82fbb2c0410e" satisfied condition "Succeeded or Failed"
    Jan 19 05:01:11.597: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod var-expansion-e9b7db6e-5231-4552-99b9-82fbb2c0410e container dapi-container: <nil>
    STEP: delete the pod 01/19/23 05:01:11.602
    Jan 19 05:01:11.612: INFO: Waiting for pod var-expansion-e9b7db6e-5231-4552-99b9-82fbb2c0410e to disappear
    Jan 19 05:01:11.616: INFO: Pod var-expansion-e9b7db6e-5231-4552-99b9-82fbb2c0410e no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 19 05:01:11.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-9302" for this suite. 01/19/23 05:01:11.62
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:01:11.625
Jan 19 05:01:11.626: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename downward-api 01/19/23 05:01:11.626
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:01:11.637
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:01:11.639
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
STEP: Creating a pod to test downward API volume plugin 01/19/23 05:01:11.641
Jan 19 05:01:11.651: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0e7fc5dc-69f5-4aef-a479-5b995bb13b33" in namespace "downward-api-9376" to be "Succeeded or Failed"
Jan 19 05:01:11.670: INFO: Pod "downwardapi-volume-0e7fc5dc-69f5-4aef-a479-5b995bb13b33": Phase="Pending", Reason="", readiness=false. Elapsed: 18.275408ms
Jan 19 05:01:13.676: INFO: Pod "downwardapi-volume-0e7fc5dc-69f5-4aef-a479-5b995bb13b33": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02415686s
Jan 19 05:01:15.675: INFO: Pod "downwardapi-volume-0e7fc5dc-69f5-4aef-a479-5b995bb13b33": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023265726s
Jan 19 05:01:17.675: INFO: Pod "downwardapi-volume-0e7fc5dc-69f5-4aef-a479-5b995bb13b33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023390207s
STEP: Saw pod success 01/19/23 05:01:17.675
Jan 19 05:01:17.675: INFO: Pod "downwardapi-volume-0e7fc5dc-69f5-4aef-a479-5b995bb13b33" satisfied condition "Succeeded or Failed"
Jan 19 05:01:17.678: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod downwardapi-volume-0e7fc5dc-69f5-4aef-a479-5b995bb13b33 container client-container: <nil>
STEP: delete the pod 01/19/23 05:01:17.684
Jan 19 05:01:17.696: INFO: Waiting for pod downwardapi-volume-0e7fc5dc-69f5-4aef-a479-5b995bb13b33 to disappear
Jan 19 05:01:17.699: INFO: Pod downwardapi-volume-0e7fc5dc-69f5-4aef-a479-5b995bb13b33 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 19 05:01:17.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9376" for this suite. 01/19/23 05:01:17.702
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","completed":177,"skipped":3405,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.082 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:01:11.625
    Jan 19 05:01:11.626: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename downward-api 01/19/23 05:01:11.626
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:01:11.637
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:01:11.639
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:234
    STEP: Creating a pod to test downward API volume plugin 01/19/23 05:01:11.641
    Jan 19 05:01:11.651: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0e7fc5dc-69f5-4aef-a479-5b995bb13b33" in namespace "downward-api-9376" to be "Succeeded or Failed"
    Jan 19 05:01:11.670: INFO: Pod "downwardapi-volume-0e7fc5dc-69f5-4aef-a479-5b995bb13b33": Phase="Pending", Reason="", readiness=false. Elapsed: 18.275408ms
    Jan 19 05:01:13.676: INFO: Pod "downwardapi-volume-0e7fc5dc-69f5-4aef-a479-5b995bb13b33": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02415686s
    Jan 19 05:01:15.675: INFO: Pod "downwardapi-volume-0e7fc5dc-69f5-4aef-a479-5b995bb13b33": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023265726s
    Jan 19 05:01:17.675: INFO: Pod "downwardapi-volume-0e7fc5dc-69f5-4aef-a479-5b995bb13b33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023390207s
    STEP: Saw pod success 01/19/23 05:01:17.675
    Jan 19 05:01:17.675: INFO: Pod "downwardapi-volume-0e7fc5dc-69f5-4aef-a479-5b995bb13b33" satisfied condition "Succeeded or Failed"
    Jan 19 05:01:17.678: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod downwardapi-volume-0e7fc5dc-69f5-4aef-a479-5b995bb13b33 container client-container: <nil>
    STEP: delete the pod 01/19/23 05:01:17.684
    Jan 19 05:01:17.696: INFO: Waiting for pod downwardapi-volume-0e7fc5dc-69f5-4aef-a479-5b995bb13b33 to disappear
    Jan 19 05:01:17.699: INFO: Pod downwardapi-volume-0e7fc5dc-69f5-4aef-a479-5b995bb13b33 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 19 05:01:17.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9376" for this suite. 01/19/23 05:01:17.702
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:01:17.707
Jan 19 05:01:17.708: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename runtimeclass 01/19/23 05:01:17.708
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:01:17.72
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:01:17.724
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-8000-delete-me 01/19/23 05:01:17.733
STEP: Waiting for the RuntimeClass to disappear 01/19/23 05:01:17.738
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan 19 05:01:17.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-8000" for this suite. 01/19/23 05:01:17.752
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","completed":178,"skipped":3408,"failed":0}
------------------------------
â€¢ [0.050 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:01:17.707
    Jan 19 05:01:17.708: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename runtimeclass 01/19/23 05:01:17.708
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:01:17.72
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:01:17.724
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-8000-delete-me 01/19/23 05:01:17.733
    STEP: Waiting for the RuntimeClass to disappear 01/19/23 05:01:17.738
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan 19 05:01:17.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-8000" for this suite. 01/19/23 05:01:17.752
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:01:17.757
Jan 19 05:01:17.758: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename watch 01/19/23 05:01:17.758
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:01:17.771
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:01:17.774
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 01/19/23 05:01:17.776
STEP: creating a new configmap 01/19/23 05:01:17.776
STEP: modifying the configmap once 01/19/23 05:01:17.78
STEP: closing the watch once it receives two notifications 01/19/23 05:01:17.786
Jan 19 05:01:17.787: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9675  8bc7c324-cf00-4996-a156-c99f1a5eb155 20382 0 2023-01-19 05:01:17 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-19 05:01:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 19 05:01:17.787: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9675  8bc7c324-cf00-4996-a156-c99f1a5eb155 20383 0 2023-01-19 05:01:17 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-19 05:01:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 01/19/23 05:01:17.787
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 01/19/23 05:01:17.793
STEP: deleting the configmap 01/19/23 05:01:17.794
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 01/19/23 05:01:17.797
Jan 19 05:01:17.797: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9675  8bc7c324-cf00-4996-a156-c99f1a5eb155 20384 0 2023-01-19 05:01:17 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-19 05:01:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 19 05:01:17.798: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9675  8bc7c324-cf00-4996-a156-c99f1a5eb155 20385 0 2023-01-19 05:01:17 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-19 05:01:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan 19 05:01:17.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9675" for this suite. 01/19/23 05:01:17.801
{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","completed":179,"skipped":3415,"failed":0}
------------------------------
â€¢ [0.047 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:01:17.757
    Jan 19 05:01:17.758: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename watch 01/19/23 05:01:17.758
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:01:17.771
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:01:17.774
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 01/19/23 05:01:17.776
    STEP: creating a new configmap 01/19/23 05:01:17.776
    STEP: modifying the configmap once 01/19/23 05:01:17.78
    STEP: closing the watch once it receives two notifications 01/19/23 05:01:17.786
    Jan 19 05:01:17.787: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9675  8bc7c324-cf00-4996-a156-c99f1a5eb155 20382 0 2023-01-19 05:01:17 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-19 05:01:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 19 05:01:17.787: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9675  8bc7c324-cf00-4996-a156-c99f1a5eb155 20383 0 2023-01-19 05:01:17 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-19 05:01:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 01/19/23 05:01:17.787
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 01/19/23 05:01:17.793
    STEP: deleting the configmap 01/19/23 05:01:17.794
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 01/19/23 05:01:17.797
    Jan 19 05:01:17.797: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9675  8bc7c324-cf00-4996-a156-c99f1a5eb155 20384 0 2023-01-19 05:01:17 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-19 05:01:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 19 05:01:17.798: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9675  8bc7c324-cf00-4996-a156-c99f1a5eb155 20385 0 2023-01-19 05:01:17 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-19 05:01:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan 19 05:01:17.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-9675" for this suite. 01/19/23 05:01:17.801
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:01:17.805
Jan 19 05:01:17.805: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename emptydir-wrapper 01/19/23 05:01:17.806
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:01:17.817
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:01:17.82
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 01/19/23 05:01:17.822
STEP: Creating RC which spawns configmap-volume pods 01/19/23 05:01:18.06
Jan 19 05:01:18.184: INFO: Pod name wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/19/23 05:01:18.184
Jan 19 05:01:18.184: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f-49br9" in namespace "emptydir-wrapper-9513" to be "running"
Jan 19 05:01:18.210: INFO: Pod "wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f-49br9": Phase="Pending", Reason="", readiness=false. Elapsed: 26.085844ms
Jan 19 05:01:20.215: INFO: Pod "wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f-49br9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030905946s
Jan 19 05:01:22.216: INFO: Pod "wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f-49br9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031430893s
Jan 19 05:01:24.215: INFO: Pod "wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f-49br9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.030985128s
Jan 19 05:01:26.214: INFO: Pod "wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f-49br9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.029893909s
Jan 19 05:01:28.214: INFO: Pod "wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f-49br9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.029607262s
Jan 19 05:01:30.214: INFO: Pod "wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f-49br9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.029852536s
Jan 19 05:01:32.215: INFO: Pod "wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f-49br9": Phase="Pending", Reason="", readiness=false. Elapsed: 14.031037804s
Jan 19 05:01:34.215: INFO: Pod "wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f-49br9": Phase="Running", Reason="", readiness=true. Elapsed: 16.030727106s
Jan 19 05:01:34.215: INFO: Pod "wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f-49br9" satisfied condition "running"
Jan 19 05:01:34.215: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f-bmkgb" in namespace "emptydir-wrapper-9513" to be "running"
Jan 19 05:01:34.218: INFO: Pod "wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f-bmkgb": Phase="Running", Reason="", readiness=true. Elapsed: 3.003288ms
Jan 19 05:01:34.218: INFO: Pod "wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f-bmkgb" satisfied condition "running"
Jan 19 05:01:34.218: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f-dhsgv" in namespace "emptydir-wrapper-9513" to be "running"
Jan 19 05:01:34.221: INFO: Pod "wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f-dhsgv": Phase="Running", Reason="", readiness=true. Elapsed: 2.667959ms
Jan 19 05:01:34.221: INFO: Pod "wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f-dhsgv" satisfied condition "running"
Jan 19 05:01:34.221: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f-qwwft" in namespace "emptydir-wrapper-9513" to be "running"
Jan 19 05:01:34.223: INFO: Pod "wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f-qwwft": Phase="Running", Reason="", readiness=true. Elapsed: 2.592639ms
Jan 19 05:01:34.223: INFO: Pod "wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f-qwwft" satisfied condition "running"
Jan 19 05:01:34.223: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f-szvxg" in namespace "emptydir-wrapper-9513" to be "running"
Jan 19 05:01:34.226: INFO: Pod "wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f-szvxg": Phase="Running", Reason="", readiness=true. Elapsed: 2.274826ms
Jan 19 05:01:34.226: INFO: Pod "wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f-szvxg" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f in namespace emptydir-wrapper-9513, will wait for the garbage collector to delete the pods 01/19/23 05:01:34.226
Jan 19 05:01:34.285: INFO: Deleting ReplicationController wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f took: 6.55896ms
Jan 19 05:01:34.386: INFO: Terminating ReplicationController wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f pods took: 101.057569ms
STEP: Creating RC which spawns configmap-volume pods 01/19/23 05:01:36.991
Jan 19 05:01:37.010: INFO: Pod name wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04: Found 0 pods out of 5
Jan 19 05:01:42.017: INFO: Pod name wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/19/23 05:01:42.017
Jan 19 05:01:42.017: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04-2mng9" in namespace "emptydir-wrapper-9513" to be "running"
Jan 19 05:01:42.020: INFO: Pod "wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04-2mng9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.686573ms
Jan 19 05:01:44.024: INFO: Pod "wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04-2mng9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007088858s
Jan 19 05:01:46.027: INFO: Pod "wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04-2mng9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009345129s
Jan 19 05:01:48.026: INFO: Pod "wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04-2mng9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008437725s
Jan 19 05:01:50.025: INFO: Pod "wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04-2mng9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007811113s
Jan 19 05:01:52.025: INFO: Pod "wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04-2mng9": Phase="Running", Reason="", readiness=true. Elapsed: 10.007909854s
Jan 19 05:01:52.025: INFO: Pod "wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04-2mng9" satisfied condition "running"
Jan 19 05:01:52.025: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04-7zms7" in namespace "emptydir-wrapper-9513" to be "running"
Jan 19 05:01:52.029: INFO: Pod "wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04-7zms7": Phase="Running", Reason="", readiness=true. Elapsed: 3.756855ms
Jan 19 05:01:52.029: INFO: Pod "wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04-7zms7" satisfied condition "running"
Jan 19 05:01:52.029: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04-dfm4j" in namespace "emptydir-wrapper-9513" to be "running"
Jan 19 05:01:52.032: INFO: Pod "wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04-dfm4j": Phase="Running", Reason="", readiness=true. Elapsed: 2.675117ms
Jan 19 05:01:52.032: INFO: Pod "wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04-dfm4j" satisfied condition "running"
Jan 19 05:01:52.032: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04-drh4c" in namespace "emptydir-wrapper-9513" to be "running"
Jan 19 05:01:52.034: INFO: Pod "wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04-drh4c": Phase="Running", Reason="", readiness=true. Elapsed: 2.414585ms
Jan 19 05:01:52.034: INFO: Pod "wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04-drh4c" satisfied condition "running"
Jan 19 05:01:52.034: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04-m74cp" in namespace "emptydir-wrapper-9513" to be "running"
Jan 19 05:01:52.037: INFO: Pod "wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04-m74cp": Phase="Running", Reason="", readiness=true. Elapsed: 2.455608ms
Jan 19 05:01:52.037: INFO: Pod "wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04-m74cp" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04 in namespace emptydir-wrapper-9513, will wait for the garbage collector to delete the pods 01/19/23 05:01:52.037
Jan 19 05:01:52.097: INFO: Deleting ReplicationController wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04 took: 6.823734ms
Jan 19 05:01:52.197: INFO: Terminating ReplicationController wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04 pods took: 100.232009ms
STEP: Creating RC which spawns configmap-volume pods 01/19/23 05:01:56.001
Jan 19 05:01:56.020: INFO: Pod name wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe: Found 0 pods out of 5
Jan 19 05:02:01.028: INFO: Pod name wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/19/23 05:02:01.028
Jan 19 05:02:01.028: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe-742n5" in namespace "emptydir-wrapper-9513" to be "running"
Jan 19 05:02:01.031: INFO: Pod "wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe-742n5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.103449ms
Jan 19 05:02:03.039: INFO: Pod "wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe-742n5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010885935s
Jan 19 05:02:05.036: INFO: Pod "wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe-742n5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008016875s
Jan 19 05:02:07.039: INFO: Pod "wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe-742n5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010756487s
Jan 19 05:02:09.036: INFO: Pod "wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe-742n5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007656672s
Jan 19 05:02:11.037: INFO: Pod "wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe-742n5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.008541127s
Jan 19 05:02:13.037: INFO: Pod "wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe-742n5": Phase="Running", Reason="", readiness=true. Elapsed: 12.008601197s
Jan 19 05:02:13.037: INFO: Pod "wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe-742n5" satisfied condition "running"
Jan 19 05:02:13.037: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe-bd52w" in namespace "emptydir-wrapper-9513" to be "running"
Jan 19 05:02:13.040: INFO: Pod "wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe-bd52w": Phase="Running", Reason="", readiness=true. Elapsed: 2.825187ms
Jan 19 05:02:13.040: INFO: Pod "wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe-bd52w" satisfied condition "running"
Jan 19 05:02:13.040: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe-bgl7t" in namespace "emptydir-wrapper-9513" to be "running"
Jan 19 05:02:13.042: INFO: Pod "wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe-bgl7t": Phase="Running", Reason="", readiness=true. Elapsed: 2.583284ms
Jan 19 05:02:13.042: INFO: Pod "wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe-bgl7t" satisfied condition "running"
Jan 19 05:02:13.042: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe-n9jvh" in namespace "emptydir-wrapper-9513" to be "running"
Jan 19 05:02:13.045: INFO: Pod "wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe-n9jvh": Phase="Running", Reason="", readiness=true. Elapsed: 2.467811ms
Jan 19 05:02:13.045: INFO: Pod "wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe-n9jvh" satisfied condition "running"
Jan 19 05:02:13.045: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe-nxmws" in namespace "emptydir-wrapper-9513" to be "running"
Jan 19 05:02:13.048: INFO: Pod "wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe-nxmws": Phase="Running", Reason="", readiness=true. Elapsed: 3.420813ms
Jan 19 05:02:13.048: INFO: Pod "wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe-nxmws" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe in namespace emptydir-wrapper-9513, will wait for the garbage collector to delete the pods 01/19/23 05:02:13.048
Jan 19 05:02:13.113: INFO: Deleting ReplicationController wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe took: 9.954295ms
Jan 19 05:02:13.214: INFO: Terminating ReplicationController wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe pods took: 101.241204ms
STEP: Cleaning up the configMaps 01/19/23 05:02:16.415
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Jan 19 05:02:16.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9513" for this suite. 01/19/23 05:02:16.643
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","completed":180,"skipped":3416,"failed":0}
------------------------------
â€¢ [SLOW TEST] [58.843 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:01:17.805
    Jan 19 05:01:17.805: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename emptydir-wrapper 01/19/23 05:01:17.806
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:01:17.817
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:01:17.82
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 01/19/23 05:01:17.822
    STEP: Creating RC which spawns configmap-volume pods 01/19/23 05:01:18.06
    Jan 19 05:01:18.184: INFO: Pod name wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/19/23 05:01:18.184
    Jan 19 05:01:18.184: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f-49br9" in namespace "emptydir-wrapper-9513" to be "running"
    Jan 19 05:01:18.210: INFO: Pod "wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f-49br9": Phase="Pending", Reason="", readiness=false. Elapsed: 26.085844ms
    Jan 19 05:01:20.215: INFO: Pod "wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f-49br9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030905946s
    Jan 19 05:01:22.216: INFO: Pod "wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f-49br9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031430893s
    Jan 19 05:01:24.215: INFO: Pod "wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f-49br9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.030985128s
    Jan 19 05:01:26.214: INFO: Pod "wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f-49br9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.029893909s
    Jan 19 05:01:28.214: INFO: Pod "wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f-49br9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.029607262s
    Jan 19 05:01:30.214: INFO: Pod "wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f-49br9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.029852536s
    Jan 19 05:01:32.215: INFO: Pod "wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f-49br9": Phase="Pending", Reason="", readiness=false. Elapsed: 14.031037804s
    Jan 19 05:01:34.215: INFO: Pod "wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f-49br9": Phase="Running", Reason="", readiness=true. Elapsed: 16.030727106s
    Jan 19 05:01:34.215: INFO: Pod "wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f-49br9" satisfied condition "running"
    Jan 19 05:01:34.215: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f-bmkgb" in namespace "emptydir-wrapper-9513" to be "running"
    Jan 19 05:01:34.218: INFO: Pod "wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f-bmkgb": Phase="Running", Reason="", readiness=true. Elapsed: 3.003288ms
    Jan 19 05:01:34.218: INFO: Pod "wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f-bmkgb" satisfied condition "running"
    Jan 19 05:01:34.218: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f-dhsgv" in namespace "emptydir-wrapper-9513" to be "running"
    Jan 19 05:01:34.221: INFO: Pod "wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f-dhsgv": Phase="Running", Reason="", readiness=true. Elapsed: 2.667959ms
    Jan 19 05:01:34.221: INFO: Pod "wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f-dhsgv" satisfied condition "running"
    Jan 19 05:01:34.221: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f-qwwft" in namespace "emptydir-wrapper-9513" to be "running"
    Jan 19 05:01:34.223: INFO: Pod "wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f-qwwft": Phase="Running", Reason="", readiness=true. Elapsed: 2.592639ms
    Jan 19 05:01:34.223: INFO: Pod "wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f-qwwft" satisfied condition "running"
    Jan 19 05:01:34.223: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f-szvxg" in namespace "emptydir-wrapper-9513" to be "running"
    Jan 19 05:01:34.226: INFO: Pod "wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f-szvxg": Phase="Running", Reason="", readiness=true. Elapsed: 2.274826ms
    Jan 19 05:01:34.226: INFO: Pod "wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f-szvxg" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f in namespace emptydir-wrapper-9513, will wait for the garbage collector to delete the pods 01/19/23 05:01:34.226
    Jan 19 05:01:34.285: INFO: Deleting ReplicationController wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f took: 6.55896ms
    Jan 19 05:01:34.386: INFO: Terminating ReplicationController wrapped-volume-race-2aec38f2-46dc-4355-ac65-4fc919f9cc0f pods took: 101.057569ms
    STEP: Creating RC which spawns configmap-volume pods 01/19/23 05:01:36.991
    Jan 19 05:01:37.010: INFO: Pod name wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04: Found 0 pods out of 5
    Jan 19 05:01:42.017: INFO: Pod name wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/19/23 05:01:42.017
    Jan 19 05:01:42.017: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04-2mng9" in namespace "emptydir-wrapper-9513" to be "running"
    Jan 19 05:01:42.020: INFO: Pod "wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04-2mng9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.686573ms
    Jan 19 05:01:44.024: INFO: Pod "wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04-2mng9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007088858s
    Jan 19 05:01:46.027: INFO: Pod "wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04-2mng9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009345129s
    Jan 19 05:01:48.026: INFO: Pod "wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04-2mng9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008437725s
    Jan 19 05:01:50.025: INFO: Pod "wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04-2mng9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007811113s
    Jan 19 05:01:52.025: INFO: Pod "wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04-2mng9": Phase="Running", Reason="", readiness=true. Elapsed: 10.007909854s
    Jan 19 05:01:52.025: INFO: Pod "wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04-2mng9" satisfied condition "running"
    Jan 19 05:01:52.025: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04-7zms7" in namespace "emptydir-wrapper-9513" to be "running"
    Jan 19 05:01:52.029: INFO: Pod "wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04-7zms7": Phase="Running", Reason="", readiness=true. Elapsed: 3.756855ms
    Jan 19 05:01:52.029: INFO: Pod "wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04-7zms7" satisfied condition "running"
    Jan 19 05:01:52.029: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04-dfm4j" in namespace "emptydir-wrapper-9513" to be "running"
    Jan 19 05:01:52.032: INFO: Pod "wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04-dfm4j": Phase="Running", Reason="", readiness=true. Elapsed: 2.675117ms
    Jan 19 05:01:52.032: INFO: Pod "wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04-dfm4j" satisfied condition "running"
    Jan 19 05:01:52.032: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04-drh4c" in namespace "emptydir-wrapper-9513" to be "running"
    Jan 19 05:01:52.034: INFO: Pod "wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04-drh4c": Phase="Running", Reason="", readiness=true. Elapsed: 2.414585ms
    Jan 19 05:01:52.034: INFO: Pod "wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04-drh4c" satisfied condition "running"
    Jan 19 05:01:52.034: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04-m74cp" in namespace "emptydir-wrapper-9513" to be "running"
    Jan 19 05:01:52.037: INFO: Pod "wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04-m74cp": Phase="Running", Reason="", readiness=true. Elapsed: 2.455608ms
    Jan 19 05:01:52.037: INFO: Pod "wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04-m74cp" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04 in namespace emptydir-wrapper-9513, will wait for the garbage collector to delete the pods 01/19/23 05:01:52.037
    Jan 19 05:01:52.097: INFO: Deleting ReplicationController wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04 took: 6.823734ms
    Jan 19 05:01:52.197: INFO: Terminating ReplicationController wrapped-volume-race-ae6ab86d-24c6-4ab8-8430-5eb407f00c04 pods took: 100.232009ms
    STEP: Creating RC which spawns configmap-volume pods 01/19/23 05:01:56.001
    Jan 19 05:01:56.020: INFO: Pod name wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe: Found 0 pods out of 5
    Jan 19 05:02:01.028: INFO: Pod name wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/19/23 05:02:01.028
    Jan 19 05:02:01.028: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe-742n5" in namespace "emptydir-wrapper-9513" to be "running"
    Jan 19 05:02:01.031: INFO: Pod "wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe-742n5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.103449ms
    Jan 19 05:02:03.039: INFO: Pod "wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe-742n5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010885935s
    Jan 19 05:02:05.036: INFO: Pod "wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe-742n5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008016875s
    Jan 19 05:02:07.039: INFO: Pod "wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe-742n5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010756487s
    Jan 19 05:02:09.036: INFO: Pod "wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe-742n5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007656672s
    Jan 19 05:02:11.037: INFO: Pod "wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe-742n5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.008541127s
    Jan 19 05:02:13.037: INFO: Pod "wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe-742n5": Phase="Running", Reason="", readiness=true. Elapsed: 12.008601197s
    Jan 19 05:02:13.037: INFO: Pod "wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe-742n5" satisfied condition "running"
    Jan 19 05:02:13.037: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe-bd52w" in namespace "emptydir-wrapper-9513" to be "running"
    Jan 19 05:02:13.040: INFO: Pod "wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe-bd52w": Phase="Running", Reason="", readiness=true. Elapsed: 2.825187ms
    Jan 19 05:02:13.040: INFO: Pod "wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe-bd52w" satisfied condition "running"
    Jan 19 05:02:13.040: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe-bgl7t" in namespace "emptydir-wrapper-9513" to be "running"
    Jan 19 05:02:13.042: INFO: Pod "wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe-bgl7t": Phase="Running", Reason="", readiness=true. Elapsed: 2.583284ms
    Jan 19 05:02:13.042: INFO: Pod "wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe-bgl7t" satisfied condition "running"
    Jan 19 05:02:13.042: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe-n9jvh" in namespace "emptydir-wrapper-9513" to be "running"
    Jan 19 05:02:13.045: INFO: Pod "wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe-n9jvh": Phase="Running", Reason="", readiness=true. Elapsed: 2.467811ms
    Jan 19 05:02:13.045: INFO: Pod "wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe-n9jvh" satisfied condition "running"
    Jan 19 05:02:13.045: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe-nxmws" in namespace "emptydir-wrapper-9513" to be "running"
    Jan 19 05:02:13.048: INFO: Pod "wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe-nxmws": Phase="Running", Reason="", readiness=true. Elapsed: 3.420813ms
    Jan 19 05:02:13.048: INFO: Pod "wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe-nxmws" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe in namespace emptydir-wrapper-9513, will wait for the garbage collector to delete the pods 01/19/23 05:02:13.048
    Jan 19 05:02:13.113: INFO: Deleting ReplicationController wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe took: 9.954295ms
    Jan 19 05:02:13.214: INFO: Terminating ReplicationController wrapped-volume-race-d37c75be-5fdf-4c16-80f8-053c1d0d9cbe pods took: 101.241204ms
    STEP: Cleaning up the configMaps 01/19/23 05:02:16.415
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Jan 19 05:02:16.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-9513" for this suite. 01/19/23 05:02:16.643
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:02:16.648
Jan 19 05:02:16.649: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename var-expansion 01/19/23 05:02:16.649
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:02:16.66
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:02:16.663
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
Jan 19 05:02:16.672: INFO: Waiting up to 2m0s for pod "var-expansion-6a505a90-0875-4932-a425-4ece228d1265" in namespace "var-expansion-6846" to be "container 0 failed with reason CreateContainerConfigError"
Jan 19 05:02:16.676: INFO: Pod "var-expansion-6a505a90-0875-4932-a425-4ece228d1265": Phase="Pending", Reason="", readiness=false. Elapsed: 3.76812ms
Jan 19 05:02:18.680: INFO: Pod "var-expansion-6a505a90-0875-4932-a425-4ece228d1265": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007885106s
Jan 19 05:02:20.680: INFO: Pod "var-expansion-6a505a90-0875-4932-a425-4ece228d1265": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007513627s
Jan 19 05:02:20.680: INFO: Pod "var-expansion-6a505a90-0875-4932-a425-4ece228d1265" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jan 19 05:02:20.680: INFO: Deleting pod "var-expansion-6a505a90-0875-4932-a425-4ece228d1265" in namespace "var-expansion-6846"
Jan 19 05:02:20.686: INFO: Wait up to 5m0s for pod "var-expansion-6a505a90-0875-4932-a425-4ece228d1265" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 19 05:02:22.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6846" for this suite. 01/19/23 05:02:22.697
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","completed":181,"skipped":3417,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.056 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:02:16.648
    Jan 19 05:02:16.649: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename var-expansion 01/19/23 05:02:16.649
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:02:16.66
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:02:16.663
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:151
    Jan 19 05:02:16.672: INFO: Waiting up to 2m0s for pod "var-expansion-6a505a90-0875-4932-a425-4ece228d1265" in namespace "var-expansion-6846" to be "container 0 failed with reason CreateContainerConfigError"
    Jan 19 05:02:16.676: INFO: Pod "var-expansion-6a505a90-0875-4932-a425-4ece228d1265": Phase="Pending", Reason="", readiness=false. Elapsed: 3.76812ms
    Jan 19 05:02:18.680: INFO: Pod "var-expansion-6a505a90-0875-4932-a425-4ece228d1265": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007885106s
    Jan 19 05:02:20.680: INFO: Pod "var-expansion-6a505a90-0875-4932-a425-4ece228d1265": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007513627s
    Jan 19 05:02:20.680: INFO: Pod "var-expansion-6a505a90-0875-4932-a425-4ece228d1265" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jan 19 05:02:20.680: INFO: Deleting pod "var-expansion-6a505a90-0875-4932-a425-4ece228d1265" in namespace "var-expansion-6846"
    Jan 19 05:02:20.686: INFO: Wait up to 5m0s for pod "var-expansion-6a505a90-0875-4932-a425-4ece228d1265" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 19 05:02:22.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-6846" for this suite. 01/19/23 05:02:22.697
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:02:22.706
Jan 19 05:02:22.706: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename kubectl 01/19/23 05:02:22.707
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:02:22.721
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:02:22.724
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
STEP: create deployment with httpd image 01/19/23 05:02:22.729
Jan 19 05:02:22.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-7537 create -f -'
Jan 19 05:02:23.304: INFO: stderr: ""
Jan 19 05:02:23.304: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 01/19/23 05:02:23.304
Jan 19 05:02:23.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-7537 diff -f -'
Jan 19 05:02:23.968: INFO: rc: 1
Jan 19 05:02:23.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-7537 delete -f -'
Jan 19 05:02:24.048: INFO: stderr: ""
Jan 19 05:02:24.048: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 19 05:02:24.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7537" for this suite. 01/19/23 05:02:24.053
{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","completed":182,"skipped":3453,"failed":0}
------------------------------
â€¢ [1.354 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:923
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:929

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:02:22.706
    Jan 19 05:02:22.706: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename kubectl 01/19/23 05:02:22.707
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:02:22.721
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:02:22.724
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:929
    STEP: create deployment with httpd image 01/19/23 05:02:22.729
    Jan 19 05:02:22.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-7537 create -f -'
    Jan 19 05:02:23.304: INFO: stderr: ""
    Jan 19 05:02:23.304: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 01/19/23 05:02:23.304
    Jan 19 05:02:23.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-7537 diff -f -'
    Jan 19 05:02:23.968: INFO: rc: 1
    Jan 19 05:02:23.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-7537 delete -f -'
    Jan 19 05:02:24.048: INFO: stderr: ""
    Jan 19 05:02:24.048: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 19 05:02:24.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7537" for this suite. 01/19/23 05:02:24.053
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:02:24.061
Jan 19 05:02:24.061: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename crd-publish-openapi 01/19/23 05:02:24.061
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:02:24.071
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:02:24.073
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
Jan 19 05:02:24.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/19/23 05:02:26.479
Jan 19 05:02:26.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-6933 --namespace=crd-publish-openapi-6933 create -f -'
Jan 19 05:02:27.159: INFO: stderr: ""
Jan 19 05:02:27.159: INFO: stdout: "e2e-test-crd-publish-openapi-8899-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 19 05:02:27.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-6933 --namespace=crd-publish-openapi-6933 delete e2e-test-crd-publish-openapi-8899-crds test-cr'
Jan 19 05:02:27.291: INFO: stderr: ""
Jan 19 05:02:27.291: INFO: stdout: "e2e-test-crd-publish-openapi-8899-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jan 19 05:02:27.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-6933 --namespace=crd-publish-openapi-6933 apply -f -'
Jan 19 05:02:27.900: INFO: stderr: ""
Jan 19 05:02:27.900: INFO: stdout: "e2e-test-crd-publish-openapi-8899-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 19 05:02:27.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-6933 --namespace=crd-publish-openapi-6933 delete e2e-test-crd-publish-openapi-8899-crds test-cr'
Jan 19 05:02:27.981: INFO: stderr: ""
Jan 19 05:02:27.981: INFO: stdout: "e2e-test-crd-publish-openapi-8899-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 01/19/23 05:02:27.981
Jan 19 05:02:27.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-6933 explain e2e-test-crd-publish-openapi-8899-crds'
Jan 19 05:02:28.179: INFO: stderr: ""
Jan 19 05:02:28.179: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8899-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 05:02:30.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6933" for this suite. 01/19/23 05:02:30.612
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","completed":183,"skipped":3473,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.558 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:02:24.061
    Jan 19 05:02:24.061: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename crd-publish-openapi 01/19/23 05:02:24.061
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:02:24.071
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:02:24.073
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:152
    Jan 19 05:02:24.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/19/23 05:02:26.479
    Jan 19 05:02:26.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-6933 --namespace=crd-publish-openapi-6933 create -f -'
    Jan 19 05:02:27.159: INFO: stderr: ""
    Jan 19 05:02:27.159: INFO: stdout: "e2e-test-crd-publish-openapi-8899-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jan 19 05:02:27.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-6933 --namespace=crd-publish-openapi-6933 delete e2e-test-crd-publish-openapi-8899-crds test-cr'
    Jan 19 05:02:27.291: INFO: stderr: ""
    Jan 19 05:02:27.291: INFO: stdout: "e2e-test-crd-publish-openapi-8899-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Jan 19 05:02:27.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-6933 --namespace=crd-publish-openapi-6933 apply -f -'
    Jan 19 05:02:27.900: INFO: stderr: ""
    Jan 19 05:02:27.900: INFO: stdout: "e2e-test-crd-publish-openapi-8899-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jan 19 05:02:27.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-6933 --namespace=crd-publish-openapi-6933 delete e2e-test-crd-publish-openapi-8899-crds test-cr'
    Jan 19 05:02:27.981: INFO: stderr: ""
    Jan 19 05:02:27.981: INFO: stdout: "e2e-test-crd-publish-openapi-8899-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 01/19/23 05:02:27.981
    Jan 19 05:02:27.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-6933 explain e2e-test-crd-publish-openapi-8899-crds'
    Jan 19 05:02:28.179: INFO: stderr: ""
    Jan 19 05:02:28.179: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8899-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 05:02:30.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-6933" for this suite. 01/19/23 05:02:30.612
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:02:30.62
Jan 19 05:02:30.620: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename replicaset 01/19/23 05:02:30.62
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:02:30.636
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:02:30.639
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Jan 19 05:02:30.653: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 19 05:02:35.658: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/19/23 05:02:35.658
STEP: Scaling up "test-rs" replicaset  01/19/23 05:02:35.658
Jan 19 05:02:35.675: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 01/19/23 05:02:35.675
W0119 05:02:35.681205      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 19 05:02:35.682: INFO: observed ReplicaSet test-rs in namespace replicaset-1606 with ReadyReplicas 1, AvailableReplicas 1
Jan 19 05:02:35.701: INFO: observed ReplicaSet test-rs in namespace replicaset-1606 with ReadyReplicas 1, AvailableReplicas 1
Jan 19 05:02:35.721: INFO: observed ReplicaSet test-rs in namespace replicaset-1606 with ReadyReplicas 1, AvailableReplicas 1
Jan 19 05:02:35.726: INFO: observed ReplicaSet test-rs in namespace replicaset-1606 with ReadyReplicas 1, AvailableReplicas 1
Jan 19 05:02:38.498: INFO: observed ReplicaSet test-rs in namespace replicaset-1606 with ReadyReplicas 2, AvailableReplicas 2
Jan 19 05:02:38.883: INFO: observed Replicaset test-rs in namespace replicaset-1606 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 19 05:02:38.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1606" for this suite. 01/19/23 05:02:38.886
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","completed":184,"skipped":3489,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.270 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:02:30.62
    Jan 19 05:02:30.620: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename replicaset 01/19/23 05:02:30.62
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:02:30.636
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:02:30.639
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Jan 19 05:02:30.653: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 19 05:02:35.658: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/19/23 05:02:35.658
    STEP: Scaling up "test-rs" replicaset  01/19/23 05:02:35.658
    Jan 19 05:02:35.675: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 01/19/23 05:02:35.675
    W0119 05:02:35.681205      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 19 05:02:35.682: INFO: observed ReplicaSet test-rs in namespace replicaset-1606 with ReadyReplicas 1, AvailableReplicas 1
    Jan 19 05:02:35.701: INFO: observed ReplicaSet test-rs in namespace replicaset-1606 with ReadyReplicas 1, AvailableReplicas 1
    Jan 19 05:02:35.721: INFO: observed ReplicaSet test-rs in namespace replicaset-1606 with ReadyReplicas 1, AvailableReplicas 1
    Jan 19 05:02:35.726: INFO: observed ReplicaSet test-rs in namespace replicaset-1606 with ReadyReplicas 1, AvailableReplicas 1
    Jan 19 05:02:38.498: INFO: observed ReplicaSet test-rs in namespace replicaset-1606 with ReadyReplicas 2, AvailableReplicas 2
    Jan 19 05:02:38.883: INFO: observed Replicaset test-rs in namespace replicaset-1606 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 19 05:02:38.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-1606" for this suite. 01/19/23 05:02:38.886
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:02:38.891
Jan 19 05:02:38.891: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename container-runtime 01/19/23 05:02:38.892
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:02:38.905
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:02:38.906
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
STEP: create the container 01/19/23 05:02:38.908
STEP: wait for the container to reach Succeeded 01/19/23 05:02:38.915
STEP: get the container status 01/19/23 05:02:43.941
STEP: the container should be terminated 01/19/23 05:02:43.945
STEP: the termination message should be set 01/19/23 05:02:43.946
Jan 19 05:02:43.946: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 01/19/23 05:02:43.946
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan 19 05:02:43.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3058" for this suite. 01/19/23 05:02:43.978
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":185,"skipped":3510,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.098 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:02:38.891
    Jan 19 05:02:38.891: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename container-runtime 01/19/23 05:02:38.892
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:02:38.905
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:02:38.906
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231
    STEP: create the container 01/19/23 05:02:38.908
    STEP: wait for the container to reach Succeeded 01/19/23 05:02:38.915
    STEP: get the container status 01/19/23 05:02:43.941
    STEP: the container should be terminated 01/19/23 05:02:43.945
    STEP: the termination message should be set 01/19/23 05:02:43.946
    Jan 19 05:02:43.946: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 01/19/23 05:02:43.946
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan 19 05:02:43.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-3058" for this suite. 01/19/23 05:02:43.978
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:02:43.989
Jan 19 05:02:43.989: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename kubectl 01/19/23 05:02:43.989
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:02:44.005
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:02:44.007
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1732
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/19/23 05:02:44.01
Jan 19 05:02:44.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-3444 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan 19 05:02:44.103: INFO: stderr: ""
Jan 19 05:02:44.103: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 01/19/23 05:02:44.103
STEP: verifying the pod e2e-test-httpd-pod was created 01/19/23 05:02:49.153
Jan 19 05:02:49.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-3444 get pod e2e-test-httpd-pod -o json'
Jan 19 05:02:49.229: INFO: stderr: ""
Jan 19 05:02:49.229: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-01-19T05:02:44Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-3444\",\n        \"resourceVersion\": \"21567\",\n        \"uid\": \"4fc33ce4-bf75-43e4-9fbb-3992c83e69f1\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-fwqjm\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ckcp-nks-default-worker-node-1\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-fwqjm\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-19T05:02:44Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-19T05:02:46Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-19T05:02:46Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-19T05:02:44Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://ea3b0e424a63aa0a60ffa64661c9a088980d609d95a94ada6e08c2eafb0c6ad6\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-01-19T05:02:46Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.0.78\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.100.70.47\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.100.70.47\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-01-19T05:02:44Z\"\n    }\n}\n"
STEP: replace the image in the pod 01/19/23 05:02:49.229
Jan 19 05:02:49.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-3444 replace -f -'
Jan 19 05:02:49.789: INFO: stderr: ""
Jan 19 05:02:49.789: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 01/19/23 05:02:49.789
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1736
Jan 19 05:02:49.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-3444 delete pods e2e-test-httpd-pod'
Jan 19 05:02:51.933: INFO: stderr: ""
Jan 19 05:02:51.933: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 19 05:02:51.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3444" for this suite. 01/19/23 05:02:51.937
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","completed":186,"skipped":3517,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.954 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1729
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1745

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:02:43.989
    Jan 19 05:02:43.989: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename kubectl 01/19/23 05:02:43.989
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:02:44.005
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:02:44.007
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1732
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1745
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/19/23 05:02:44.01
    Jan 19 05:02:44.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-3444 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jan 19 05:02:44.103: INFO: stderr: ""
    Jan 19 05:02:44.103: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 01/19/23 05:02:44.103
    STEP: verifying the pod e2e-test-httpd-pod was created 01/19/23 05:02:49.153
    Jan 19 05:02:49.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-3444 get pod e2e-test-httpd-pod -o json'
    Jan 19 05:02:49.229: INFO: stderr: ""
    Jan 19 05:02:49.229: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-01-19T05:02:44Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-3444\",\n        \"resourceVersion\": \"21567\",\n        \"uid\": \"4fc33ce4-bf75-43e4-9fbb-3992c83e69f1\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-fwqjm\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ckcp-nks-default-worker-node-1\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-fwqjm\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-19T05:02:44Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-19T05:02:46Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-19T05:02:46Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-19T05:02:44Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://ea3b0e424a63aa0a60ffa64661c9a088980d609d95a94ada6e08c2eafb0c6ad6\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-01-19T05:02:46Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.0.78\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.100.70.47\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.100.70.47\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-01-19T05:02:44Z\"\n    }\n}\n"
    STEP: replace the image in the pod 01/19/23 05:02:49.229
    Jan 19 05:02:49.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-3444 replace -f -'
    Jan 19 05:02:49.789: INFO: stderr: ""
    Jan 19 05:02:49.789: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 01/19/23 05:02:49.789
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1736
    Jan 19 05:02:49.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-3444 delete pods e2e-test-httpd-pod'
    Jan 19 05:02:51.933: INFO: stderr: ""
    Jan 19 05:02:51.933: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 19 05:02:51.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3444" for this suite. 01/19/23 05:02:51.937
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:02:51.943
Jan 19 05:02:51.943: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename webhook 01/19/23 05:02:51.944
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:02:51.953
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:02:51.956
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/19/23 05:02:51.968
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 05:02:52.516
STEP: Deploying the webhook pod 01/19/23 05:02:52.526
STEP: Wait for the deployment to be ready 01/19/23 05:02:52.538
Jan 19 05:02:52.550: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 19 05:02:54.561: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 5, 2, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 2, 52, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 2, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 2, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/19/23 05:02:56.565
STEP: Verifying the service has paired with the endpoint 01/19/23 05:02:56.577
Jan 19 05:02:57.577: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/19/23 05:02:57.581
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/19/23 05:02:57.596
STEP: Creating a dummy validating-webhook-configuration object 01/19/23 05:02:57.607
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 01/19/23 05:02:57.615
STEP: Creating a dummy mutating-webhook-configuration object 01/19/23 05:02:57.619
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 01/19/23 05:02:57.628
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 05:02:57.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6085" for this suite. 01/19/23 05:02:57.654
STEP: Destroying namespace "webhook-6085-markers" for this suite. 01/19/23 05:02:57.659
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","completed":187,"skipped":3529,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.768 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:02:51.943
    Jan 19 05:02:51.943: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename webhook 01/19/23 05:02:51.944
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:02:51.953
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:02:51.956
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/19/23 05:02:51.968
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 05:02:52.516
    STEP: Deploying the webhook pod 01/19/23 05:02:52.526
    STEP: Wait for the deployment to be ready 01/19/23 05:02:52.538
    Jan 19 05:02:52.550: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 19 05:02:54.561: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 5, 2, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 2, 52, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 2, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 2, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/19/23 05:02:56.565
    STEP: Verifying the service has paired with the endpoint 01/19/23 05:02:56.577
    Jan 19 05:02:57.577: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:276
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/19/23 05:02:57.581
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/19/23 05:02:57.596
    STEP: Creating a dummy validating-webhook-configuration object 01/19/23 05:02:57.607
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 01/19/23 05:02:57.615
    STEP: Creating a dummy mutating-webhook-configuration object 01/19/23 05:02:57.619
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 01/19/23 05:02:57.628
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 05:02:57.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6085" for this suite. 01/19/23 05:02:57.654
    STEP: Destroying namespace "webhook-6085-markers" for this suite. 01/19/23 05:02:57.659
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:02:57.712
Jan 19 05:02:57.712: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename projected 01/19/23 05:02:57.712
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:02:57.727
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:02:57.73
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
STEP: Creating projection with secret that has name projected-secret-test-map-38f1bead-b0c9-47a9-9670-6938b874b84d 01/19/23 05:02:57.732
STEP: Creating a pod to test consume secrets 01/19/23 05:02:57.737
Jan 19 05:02:57.745: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-efb63551-c9ab-43e0-9b63-08258ddabcff" in namespace "projected-105" to be "Succeeded or Failed"
Jan 19 05:02:57.748: INFO: Pod "pod-projected-secrets-efb63551-c9ab-43e0-9b63-08258ddabcff": Phase="Pending", Reason="", readiness=false. Elapsed: 3.383984ms
Jan 19 05:02:59.755: INFO: Pod "pod-projected-secrets-efb63551-c9ab-43e0-9b63-08258ddabcff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009712546s
Jan 19 05:03:01.754: INFO: Pod "pod-projected-secrets-efb63551-c9ab-43e0-9b63-08258ddabcff": Phase="Running", Reason="", readiness=false. Elapsed: 4.009210497s
Jan 19 05:03:03.752: INFO: Pod "pod-projected-secrets-efb63551-c9ab-43e0-9b63-08258ddabcff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007230148s
STEP: Saw pod success 01/19/23 05:03:03.752
Jan 19 05:03:03.752: INFO: Pod "pod-projected-secrets-efb63551-c9ab-43e0-9b63-08258ddabcff" satisfied condition "Succeeded or Failed"
Jan 19 05:03:03.755: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-projected-secrets-efb63551-c9ab-43e0-9b63-08258ddabcff container projected-secret-volume-test: <nil>
STEP: delete the pod 01/19/23 05:03:03.787
Jan 19 05:03:03.800: INFO: Waiting for pod pod-projected-secrets-efb63551-c9ab-43e0-9b63-08258ddabcff to disappear
Jan 19 05:03:03.802: INFO: Pod pod-projected-secrets-efb63551-c9ab-43e0-9b63-08258ddabcff no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 19 05:03:03.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-105" for this suite. 01/19/23 05:03:03.805
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":188,"skipped":3533,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.098 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:02:57.712
    Jan 19 05:02:57.712: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename projected 01/19/23 05:02:57.712
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:02:57.727
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:02:57.73
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:77
    STEP: Creating projection with secret that has name projected-secret-test-map-38f1bead-b0c9-47a9-9670-6938b874b84d 01/19/23 05:02:57.732
    STEP: Creating a pod to test consume secrets 01/19/23 05:02:57.737
    Jan 19 05:02:57.745: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-efb63551-c9ab-43e0-9b63-08258ddabcff" in namespace "projected-105" to be "Succeeded or Failed"
    Jan 19 05:02:57.748: INFO: Pod "pod-projected-secrets-efb63551-c9ab-43e0-9b63-08258ddabcff": Phase="Pending", Reason="", readiness=false. Elapsed: 3.383984ms
    Jan 19 05:02:59.755: INFO: Pod "pod-projected-secrets-efb63551-c9ab-43e0-9b63-08258ddabcff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009712546s
    Jan 19 05:03:01.754: INFO: Pod "pod-projected-secrets-efb63551-c9ab-43e0-9b63-08258ddabcff": Phase="Running", Reason="", readiness=false. Elapsed: 4.009210497s
    Jan 19 05:03:03.752: INFO: Pod "pod-projected-secrets-efb63551-c9ab-43e0-9b63-08258ddabcff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007230148s
    STEP: Saw pod success 01/19/23 05:03:03.752
    Jan 19 05:03:03.752: INFO: Pod "pod-projected-secrets-efb63551-c9ab-43e0-9b63-08258ddabcff" satisfied condition "Succeeded or Failed"
    Jan 19 05:03:03.755: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-projected-secrets-efb63551-c9ab-43e0-9b63-08258ddabcff container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/19/23 05:03:03.787
    Jan 19 05:03:03.800: INFO: Waiting for pod pod-projected-secrets-efb63551-c9ab-43e0-9b63-08258ddabcff to disappear
    Jan 19 05:03:03.802: INFO: Pod pod-projected-secrets-efb63551-c9ab-43e0-9b63-08258ddabcff no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 19 05:03:03.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-105" for this suite. 01/19/23 05:03:03.805
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:03:03.811
Jan 19 05:03:03.811: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename container-lifecycle-hook 01/19/23 05:03:03.812
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:03:03.825
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:03:03.828
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 01/19/23 05:03:03.833
Jan 19 05:03:03.843: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7830" to be "running and ready"
Jan 19 05:03:03.845: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.257836ms
Jan 19 05:03:03.845: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 19 05:03:05.851: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007722593s
Jan 19 05:03:05.851: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 19 05:03:07.851: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.007719004s
Jan 19 05:03:07.851: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 19 05:03:07.851: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
STEP: create the pod with lifecycle hook 01/19/23 05:03:07.854
Jan 19 05:03:07.861: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-7830" to be "running and ready"
Jan 19 05:03:07.870: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 8.990017ms
Jan 19 05:03:07.870: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 19 05:03:09.874: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013461991s
Jan 19 05:03:09.874: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 19 05:03:11.873: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.012488207s
Jan 19 05:03:11.873: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Jan 19 05:03:11.873: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 01/19/23 05:03:11.876
STEP: delete the pod with lifecycle hook 01/19/23 05:03:11.914
Jan 19 05:03:11.921: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 19 05:03:11.924: INFO: Pod pod-with-poststart-http-hook still exists
Jan 19 05:03:13.924: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 19 05:03:13.927: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jan 19 05:03:13.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7830" for this suite. 01/19/23 05:03:13.93
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","completed":189,"skipped":3558,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.124 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:03:03.811
    Jan 19 05:03:03.811: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/19/23 05:03:03.812
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:03:03.825
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:03:03.828
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 01/19/23 05:03:03.833
    Jan 19 05:03:03.843: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7830" to be "running and ready"
    Jan 19 05:03:03.845: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.257836ms
    Jan 19 05:03:03.845: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 05:03:05.851: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007722593s
    Jan 19 05:03:05.851: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 05:03:07.851: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.007719004s
    Jan 19 05:03:07.851: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 19 05:03:07.851: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:130
    STEP: create the pod with lifecycle hook 01/19/23 05:03:07.854
    Jan 19 05:03:07.861: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-7830" to be "running and ready"
    Jan 19 05:03:07.870: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 8.990017ms
    Jan 19 05:03:07.870: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 05:03:09.874: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013461991s
    Jan 19 05:03:09.874: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 05:03:11.873: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.012488207s
    Jan 19 05:03:11.873: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Jan 19 05:03:11.873: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 01/19/23 05:03:11.876
    STEP: delete the pod with lifecycle hook 01/19/23 05:03:11.914
    Jan 19 05:03:11.921: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan 19 05:03:11.924: INFO: Pod pod-with-poststart-http-hook still exists
    Jan 19 05:03:13.924: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan 19 05:03:13.927: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jan 19 05:03:13.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-7830" for this suite. 01/19/23 05:03:13.93
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:03:13.937
Jan 19 05:03:13.937: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename services 01/19/23 05:03:13.937
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:03:13.948
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:03:13.951
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
STEP: creating service in namespace services-6155 01/19/23 05:03:13.953
STEP: creating service affinity-nodeport-transition in namespace services-6155 01/19/23 05:03:13.953
STEP: creating replication controller affinity-nodeport-transition in namespace services-6155 01/19/23 05:03:13.965
I0119 05:03:13.971647      18 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-6155, replica count: 3
I0119 05:03:17.022427      18 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 19 05:03:17.030: INFO: Creating new exec pod
Jan 19 05:03:17.036: INFO: Waiting up to 5m0s for pod "execpod-affinityzfd4c" in namespace "services-6155" to be "running"
Jan 19 05:03:17.041: INFO: Pod "execpod-affinityzfd4c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.141609ms
Jan 19 05:03:19.044: INFO: Pod "execpod-affinityzfd4c": Phase="Running", Reason="", readiness=true. Elapsed: 2.008139756s
Jan 19 05:03:19.045: INFO: Pod "execpod-affinityzfd4c" satisfied condition "running"
Jan 19 05:03:20.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-6155 exec execpod-affinityzfd4c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Jan 19 05:03:20.282: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jan 19 05:03:20.282: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 05:03:20.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-6155 exec execpod-affinityzfd4c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.225.34 80'
Jan 19 05:03:20.449: INFO: stderr: "+ nc -v -t -w 2 10.254.225.34 80\n+ echo hostName\nConnection to 10.254.225.34 80 port [tcp/http] succeeded!\n"
Jan 19 05:03:20.449: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 05:03:20.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-6155 exec execpod-affinityzfd4c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.0.99 32135'
Jan 19 05:03:20.606: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.0.99 32135\nConnection to 192.168.0.99 32135 port [tcp/*] succeeded!\n"
Jan 19 05:03:20.606: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 05:03:20.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-6155 exec execpod-affinityzfd4c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.0.78 32135'
Jan 19 05:03:20.761: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.0.78 32135\nConnection to 192.168.0.78 32135 port [tcp/*] succeeded!\n"
Jan 19 05:03:20.761: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 05:03:20.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-6155 exec execpod-affinityzfd4c -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.0.99:32135/ ; done'
Jan 19 05:03:21.015: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n"
Jan 19 05:03:21.015: INFO: stdout: "\naffinity-nodeport-transition-7fp2m\naffinity-nodeport-transition-7fp2m\naffinity-nodeport-transition-7fp2m\naffinity-nodeport-transition-7fp2m\naffinity-nodeport-transition-m4qfj\naffinity-nodeport-transition-m4qfj\naffinity-nodeport-transition-8nrtm\naffinity-nodeport-transition-8nrtm\naffinity-nodeport-transition-m4qfj\naffinity-nodeport-transition-m4qfj\naffinity-nodeport-transition-m4qfj\naffinity-nodeport-transition-7fp2m\naffinity-nodeport-transition-m4qfj\naffinity-nodeport-transition-7fp2m\naffinity-nodeport-transition-m4qfj\naffinity-nodeport-transition-8nrtm"
Jan 19 05:03:21.015: INFO: Received response from host: affinity-nodeport-transition-7fp2m
Jan 19 05:03:21.015: INFO: Received response from host: affinity-nodeport-transition-7fp2m
Jan 19 05:03:21.015: INFO: Received response from host: affinity-nodeport-transition-7fp2m
Jan 19 05:03:21.015: INFO: Received response from host: affinity-nodeport-transition-7fp2m
Jan 19 05:03:21.015: INFO: Received response from host: affinity-nodeport-transition-m4qfj
Jan 19 05:03:21.015: INFO: Received response from host: affinity-nodeport-transition-m4qfj
Jan 19 05:03:21.015: INFO: Received response from host: affinity-nodeport-transition-8nrtm
Jan 19 05:03:21.015: INFO: Received response from host: affinity-nodeport-transition-8nrtm
Jan 19 05:03:21.015: INFO: Received response from host: affinity-nodeport-transition-m4qfj
Jan 19 05:03:21.015: INFO: Received response from host: affinity-nodeport-transition-m4qfj
Jan 19 05:03:21.015: INFO: Received response from host: affinity-nodeport-transition-m4qfj
Jan 19 05:03:21.015: INFO: Received response from host: affinity-nodeport-transition-7fp2m
Jan 19 05:03:21.015: INFO: Received response from host: affinity-nodeport-transition-m4qfj
Jan 19 05:03:21.015: INFO: Received response from host: affinity-nodeport-transition-7fp2m
Jan 19 05:03:21.015: INFO: Received response from host: affinity-nodeport-transition-m4qfj
Jan 19 05:03:21.015: INFO: Received response from host: affinity-nodeport-transition-8nrtm
Jan 19 05:03:21.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-6155 exec execpod-affinityzfd4c -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.0.99:32135/ ; done'
Jan 19 05:03:21.273: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n"
Jan 19 05:03:21.273: INFO: stdout: "\naffinity-nodeport-transition-8nrtm\naffinity-nodeport-transition-8nrtm\naffinity-nodeport-transition-8nrtm\naffinity-nodeport-transition-8nrtm\naffinity-nodeport-transition-8nrtm\naffinity-nodeport-transition-8nrtm\naffinity-nodeport-transition-8nrtm\naffinity-nodeport-transition-8nrtm\naffinity-nodeport-transition-8nrtm\naffinity-nodeport-transition-8nrtm\naffinity-nodeport-transition-8nrtm\naffinity-nodeport-transition-8nrtm\naffinity-nodeport-transition-8nrtm\naffinity-nodeport-transition-8nrtm\naffinity-nodeport-transition-8nrtm\naffinity-nodeport-transition-8nrtm"
Jan 19 05:03:21.274: INFO: Received response from host: affinity-nodeport-transition-8nrtm
Jan 19 05:03:21.274: INFO: Received response from host: affinity-nodeport-transition-8nrtm
Jan 19 05:03:21.274: INFO: Received response from host: affinity-nodeport-transition-8nrtm
Jan 19 05:03:21.274: INFO: Received response from host: affinity-nodeport-transition-8nrtm
Jan 19 05:03:21.274: INFO: Received response from host: affinity-nodeport-transition-8nrtm
Jan 19 05:03:21.274: INFO: Received response from host: affinity-nodeport-transition-8nrtm
Jan 19 05:03:21.274: INFO: Received response from host: affinity-nodeport-transition-8nrtm
Jan 19 05:03:21.274: INFO: Received response from host: affinity-nodeport-transition-8nrtm
Jan 19 05:03:21.274: INFO: Received response from host: affinity-nodeport-transition-8nrtm
Jan 19 05:03:21.274: INFO: Received response from host: affinity-nodeport-transition-8nrtm
Jan 19 05:03:21.274: INFO: Received response from host: affinity-nodeport-transition-8nrtm
Jan 19 05:03:21.274: INFO: Received response from host: affinity-nodeport-transition-8nrtm
Jan 19 05:03:21.274: INFO: Received response from host: affinity-nodeport-transition-8nrtm
Jan 19 05:03:21.274: INFO: Received response from host: affinity-nodeport-transition-8nrtm
Jan 19 05:03:21.274: INFO: Received response from host: affinity-nodeport-transition-8nrtm
Jan 19 05:03:21.274: INFO: Received response from host: affinity-nodeport-transition-8nrtm
Jan 19 05:03:21.274: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-6155, will wait for the garbage collector to delete the pods 01/19/23 05:03:21.284
Jan 19 05:03:21.361: INFO: Deleting ReplicationController affinity-nodeport-transition took: 21.897904ms
Jan 19 05:03:21.462: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.095555ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 19 05:03:23.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6155" for this suite. 01/19/23 05:03:23.682
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","completed":190,"skipped":3605,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.750 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:03:13.937
    Jan 19 05:03:13.937: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename services 01/19/23 05:03:13.937
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:03:13.948
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:03:13.951
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2237
    STEP: creating service in namespace services-6155 01/19/23 05:03:13.953
    STEP: creating service affinity-nodeport-transition in namespace services-6155 01/19/23 05:03:13.953
    STEP: creating replication controller affinity-nodeport-transition in namespace services-6155 01/19/23 05:03:13.965
    I0119 05:03:13.971647      18 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-6155, replica count: 3
    I0119 05:03:17.022427      18 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 19 05:03:17.030: INFO: Creating new exec pod
    Jan 19 05:03:17.036: INFO: Waiting up to 5m0s for pod "execpod-affinityzfd4c" in namespace "services-6155" to be "running"
    Jan 19 05:03:17.041: INFO: Pod "execpod-affinityzfd4c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.141609ms
    Jan 19 05:03:19.044: INFO: Pod "execpod-affinityzfd4c": Phase="Running", Reason="", readiness=true. Elapsed: 2.008139756s
    Jan 19 05:03:19.045: INFO: Pod "execpod-affinityzfd4c" satisfied condition "running"
    Jan 19 05:03:20.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-6155 exec execpod-affinityzfd4c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
    Jan 19 05:03:20.282: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Jan 19 05:03:20.282: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 19 05:03:20.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-6155 exec execpod-affinityzfd4c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.225.34 80'
    Jan 19 05:03:20.449: INFO: stderr: "+ nc -v -t -w 2 10.254.225.34 80\n+ echo hostName\nConnection to 10.254.225.34 80 port [tcp/http] succeeded!\n"
    Jan 19 05:03:20.449: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 19 05:03:20.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-6155 exec execpod-affinityzfd4c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.0.99 32135'
    Jan 19 05:03:20.606: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.0.99 32135\nConnection to 192.168.0.99 32135 port [tcp/*] succeeded!\n"
    Jan 19 05:03:20.606: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 19 05:03:20.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-6155 exec execpod-affinityzfd4c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.0.78 32135'
    Jan 19 05:03:20.761: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.0.78 32135\nConnection to 192.168.0.78 32135 port [tcp/*] succeeded!\n"
    Jan 19 05:03:20.761: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 19 05:03:20.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-6155 exec execpod-affinityzfd4c -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.0.99:32135/ ; done'
    Jan 19 05:03:21.015: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n"
    Jan 19 05:03:21.015: INFO: stdout: "\naffinity-nodeport-transition-7fp2m\naffinity-nodeport-transition-7fp2m\naffinity-nodeport-transition-7fp2m\naffinity-nodeport-transition-7fp2m\naffinity-nodeport-transition-m4qfj\naffinity-nodeport-transition-m4qfj\naffinity-nodeport-transition-8nrtm\naffinity-nodeport-transition-8nrtm\naffinity-nodeport-transition-m4qfj\naffinity-nodeport-transition-m4qfj\naffinity-nodeport-transition-m4qfj\naffinity-nodeport-transition-7fp2m\naffinity-nodeport-transition-m4qfj\naffinity-nodeport-transition-7fp2m\naffinity-nodeport-transition-m4qfj\naffinity-nodeport-transition-8nrtm"
    Jan 19 05:03:21.015: INFO: Received response from host: affinity-nodeport-transition-7fp2m
    Jan 19 05:03:21.015: INFO: Received response from host: affinity-nodeport-transition-7fp2m
    Jan 19 05:03:21.015: INFO: Received response from host: affinity-nodeport-transition-7fp2m
    Jan 19 05:03:21.015: INFO: Received response from host: affinity-nodeport-transition-7fp2m
    Jan 19 05:03:21.015: INFO: Received response from host: affinity-nodeport-transition-m4qfj
    Jan 19 05:03:21.015: INFO: Received response from host: affinity-nodeport-transition-m4qfj
    Jan 19 05:03:21.015: INFO: Received response from host: affinity-nodeport-transition-8nrtm
    Jan 19 05:03:21.015: INFO: Received response from host: affinity-nodeport-transition-8nrtm
    Jan 19 05:03:21.015: INFO: Received response from host: affinity-nodeport-transition-m4qfj
    Jan 19 05:03:21.015: INFO: Received response from host: affinity-nodeport-transition-m4qfj
    Jan 19 05:03:21.015: INFO: Received response from host: affinity-nodeport-transition-m4qfj
    Jan 19 05:03:21.015: INFO: Received response from host: affinity-nodeport-transition-7fp2m
    Jan 19 05:03:21.015: INFO: Received response from host: affinity-nodeport-transition-m4qfj
    Jan 19 05:03:21.015: INFO: Received response from host: affinity-nodeport-transition-7fp2m
    Jan 19 05:03:21.015: INFO: Received response from host: affinity-nodeport-transition-m4qfj
    Jan 19 05:03:21.015: INFO: Received response from host: affinity-nodeport-transition-8nrtm
    Jan 19 05:03:21.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-6155 exec execpod-affinityzfd4c -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.0.99:32135/ ; done'
    Jan 19 05:03:21.273: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32135/\n"
    Jan 19 05:03:21.273: INFO: stdout: "\naffinity-nodeport-transition-8nrtm\naffinity-nodeport-transition-8nrtm\naffinity-nodeport-transition-8nrtm\naffinity-nodeport-transition-8nrtm\naffinity-nodeport-transition-8nrtm\naffinity-nodeport-transition-8nrtm\naffinity-nodeport-transition-8nrtm\naffinity-nodeport-transition-8nrtm\naffinity-nodeport-transition-8nrtm\naffinity-nodeport-transition-8nrtm\naffinity-nodeport-transition-8nrtm\naffinity-nodeport-transition-8nrtm\naffinity-nodeport-transition-8nrtm\naffinity-nodeport-transition-8nrtm\naffinity-nodeport-transition-8nrtm\naffinity-nodeport-transition-8nrtm"
    Jan 19 05:03:21.274: INFO: Received response from host: affinity-nodeport-transition-8nrtm
    Jan 19 05:03:21.274: INFO: Received response from host: affinity-nodeport-transition-8nrtm
    Jan 19 05:03:21.274: INFO: Received response from host: affinity-nodeport-transition-8nrtm
    Jan 19 05:03:21.274: INFO: Received response from host: affinity-nodeport-transition-8nrtm
    Jan 19 05:03:21.274: INFO: Received response from host: affinity-nodeport-transition-8nrtm
    Jan 19 05:03:21.274: INFO: Received response from host: affinity-nodeport-transition-8nrtm
    Jan 19 05:03:21.274: INFO: Received response from host: affinity-nodeport-transition-8nrtm
    Jan 19 05:03:21.274: INFO: Received response from host: affinity-nodeport-transition-8nrtm
    Jan 19 05:03:21.274: INFO: Received response from host: affinity-nodeport-transition-8nrtm
    Jan 19 05:03:21.274: INFO: Received response from host: affinity-nodeport-transition-8nrtm
    Jan 19 05:03:21.274: INFO: Received response from host: affinity-nodeport-transition-8nrtm
    Jan 19 05:03:21.274: INFO: Received response from host: affinity-nodeport-transition-8nrtm
    Jan 19 05:03:21.274: INFO: Received response from host: affinity-nodeport-transition-8nrtm
    Jan 19 05:03:21.274: INFO: Received response from host: affinity-nodeport-transition-8nrtm
    Jan 19 05:03:21.274: INFO: Received response from host: affinity-nodeport-transition-8nrtm
    Jan 19 05:03:21.274: INFO: Received response from host: affinity-nodeport-transition-8nrtm
    Jan 19 05:03:21.274: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-6155, will wait for the garbage collector to delete the pods 01/19/23 05:03:21.284
    Jan 19 05:03:21.361: INFO: Deleting ReplicationController affinity-nodeport-transition took: 21.897904ms
    Jan 19 05:03:21.462: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.095555ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 19 05:03:23.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6155" for this suite. 01/19/23 05:03:23.682
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:03:23.688
Jan 19 05:03:23.688: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename webhook 01/19/23 05:03:23.688
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:03:23.751
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:03:23.754
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/19/23 05:03:23.766
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 05:03:24.361
STEP: Deploying the webhook pod 01/19/23 05:03:24.37
STEP: Wait for the deployment to be ready 01/19/23 05:03:24.381
Jan 19 05:03:24.387: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 19 05:03:26.396: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 5, 3, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 3, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 3, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 3, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/19/23 05:03:28.4
STEP: Verifying the service has paired with the endpoint 01/19/23 05:03:28.411
Jan 19 05:03:29.411: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
STEP: Listing all of the created validation webhooks 01/19/23 05:03:29.467
STEP: Creating a configMap that does not comply to the validation webhook rules 01/19/23 05:03:29.501
STEP: Deleting the collection of validation webhooks 01/19/23 05:03:29.539
STEP: Creating a configMap that does not comply to the validation webhook rules 01/19/23 05:03:29.597
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 05:03:29.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7327" for this suite. 01/19/23 05:03:29.608
STEP: Destroying namespace "webhook-7327-markers" for this suite. 01/19/23 05:03:29.614
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","completed":191,"skipped":3615,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.989 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:03:23.688
    Jan 19 05:03:23.688: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename webhook 01/19/23 05:03:23.688
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:03:23.751
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:03:23.754
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/19/23 05:03:23.766
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 05:03:24.361
    STEP: Deploying the webhook pod 01/19/23 05:03:24.37
    STEP: Wait for the deployment to be ready 01/19/23 05:03:24.381
    Jan 19 05:03:24.387: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Jan 19 05:03:26.396: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 5, 3, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 3, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 3, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 3, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/19/23 05:03:28.4
    STEP: Verifying the service has paired with the endpoint 01/19/23 05:03:28.411
    Jan 19 05:03:29.411: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:581
    STEP: Listing all of the created validation webhooks 01/19/23 05:03:29.467
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/19/23 05:03:29.501
    STEP: Deleting the collection of validation webhooks 01/19/23 05:03:29.539
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/19/23 05:03:29.597
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 05:03:29.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7327" for this suite. 01/19/23 05:03:29.608
    STEP: Destroying namespace "webhook-7327-markers" for this suite. 01/19/23 05:03:29.614
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:03:29.679
Jan 19 05:03:29.679: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename secrets 01/19/23 05:03:29.68
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:03:29.694
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:03:29.697
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
STEP: creating secret secrets-149/secret-test-0a2fe137-539a-49ac-b51c-1471615e8f3d 01/19/23 05:03:29.699
STEP: Creating a pod to test consume secrets 01/19/23 05:03:29.703
Jan 19 05:03:29.710: INFO: Waiting up to 5m0s for pod "pod-configmaps-fde02080-87c4-4e09-bd13-0c123488af64" in namespace "secrets-149" to be "Succeeded or Failed"
Jan 19 05:03:29.714: INFO: Pod "pod-configmaps-fde02080-87c4-4e09-bd13-0c123488af64": Phase="Pending", Reason="", readiness=false. Elapsed: 3.823639ms
Jan 19 05:03:31.721: INFO: Pod "pod-configmaps-fde02080-87c4-4e09-bd13-0c123488af64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010291631s
Jan 19 05:03:33.719: INFO: Pod "pod-configmaps-fde02080-87c4-4e09-bd13-0c123488af64": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00825172s
Jan 19 05:03:35.719: INFO: Pod "pod-configmaps-fde02080-87c4-4e09-bd13-0c123488af64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008324217s
STEP: Saw pod success 01/19/23 05:03:35.719
Jan 19 05:03:35.719: INFO: Pod "pod-configmaps-fde02080-87c4-4e09-bd13-0c123488af64" satisfied condition "Succeeded or Failed"
Jan 19 05:03:35.722: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-configmaps-fde02080-87c4-4e09-bd13-0c123488af64 container env-test: <nil>
STEP: delete the pod 01/19/23 05:03:35.727
Jan 19 05:03:35.737: INFO: Waiting for pod pod-configmaps-fde02080-87c4-4e09-bd13-0c123488af64 to disappear
Jan 19 05:03:35.739: INFO: Pod pod-configmaps-fde02080-87c4-4e09-bd13-0c123488af64 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jan 19 05:03:35.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-149" for this suite. 01/19/23 05:03:35.743
{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","completed":192,"skipped":3670,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.069 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:03:29.679
    Jan 19 05:03:29.679: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename secrets 01/19/23 05:03:29.68
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:03:29.694
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:03:29.697
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:94
    STEP: creating secret secrets-149/secret-test-0a2fe137-539a-49ac-b51c-1471615e8f3d 01/19/23 05:03:29.699
    STEP: Creating a pod to test consume secrets 01/19/23 05:03:29.703
    Jan 19 05:03:29.710: INFO: Waiting up to 5m0s for pod "pod-configmaps-fde02080-87c4-4e09-bd13-0c123488af64" in namespace "secrets-149" to be "Succeeded or Failed"
    Jan 19 05:03:29.714: INFO: Pod "pod-configmaps-fde02080-87c4-4e09-bd13-0c123488af64": Phase="Pending", Reason="", readiness=false. Elapsed: 3.823639ms
    Jan 19 05:03:31.721: INFO: Pod "pod-configmaps-fde02080-87c4-4e09-bd13-0c123488af64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010291631s
    Jan 19 05:03:33.719: INFO: Pod "pod-configmaps-fde02080-87c4-4e09-bd13-0c123488af64": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00825172s
    Jan 19 05:03:35.719: INFO: Pod "pod-configmaps-fde02080-87c4-4e09-bd13-0c123488af64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008324217s
    STEP: Saw pod success 01/19/23 05:03:35.719
    Jan 19 05:03:35.719: INFO: Pod "pod-configmaps-fde02080-87c4-4e09-bd13-0c123488af64" satisfied condition "Succeeded or Failed"
    Jan 19 05:03:35.722: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-configmaps-fde02080-87c4-4e09-bd13-0c123488af64 container env-test: <nil>
    STEP: delete the pod 01/19/23 05:03:35.727
    Jan 19 05:03:35.737: INFO: Waiting for pod pod-configmaps-fde02080-87c4-4e09-bd13-0c123488af64 to disappear
    Jan 19 05:03:35.739: INFO: Pod pod-configmaps-fde02080-87c4-4e09-bd13-0c123488af64 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jan 19 05:03:35.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-149" for this suite. 01/19/23 05:03:35.743
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:03:35.748
Jan 19 05:03:35.748: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename dns 01/19/23 05:03:35.749
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:03:35.761
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:03:35.764
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 01/19/23 05:03:35.765
Jan 19 05:03:35.772: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-6983  5894ea1a-0f24-4b07-9e59-f6b28d13f76b 22057 0 2023-01-19 05:03:35 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-01-19 05:03:35 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dknld,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dknld,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 05:03:35.772: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-6983" to be "running and ready"
Jan 19 05:03:35.776: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 3.728427ms
Jan 19 05:03:35.776: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jan 19 05:03:37.780: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007900509s
Jan 19 05:03:37.780: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jan 19 05:03:39.780: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 4.008089422s
Jan 19 05:03:39.780: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Jan 19 05:03:39.780: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 01/19/23 05:03:39.78
Jan 19 05:03:39.780: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-6983 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 05:03:39.780: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
Jan 19 05:03:39.780: INFO: ExecWithOptions: Clientset creation
Jan 19 05:03:39.781: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/dns-6983/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 01/19/23 05:03:39.873
Jan 19 05:03:39.873: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-6983 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 05:03:39.873: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
Jan 19 05:03:39.873: INFO: ExecWithOptions: Clientset creation
Jan 19 05:03:39.874: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/dns-6983/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 19 05:03:39.966: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 19 05:03:39.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6983" for this suite. 01/19/23 05:03:39.984
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","completed":193,"skipped":3671,"failed":0}
------------------------------
â€¢ [4.242 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:03:35.748
    Jan 19 05:03:35.748: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename dns 01/19/23 05:03:35.749
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:03:35.761
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:03:35.764
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 01/19/23 05:03:35.765
    Jan 19 05:03:35.772: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-6983  5894ea1a-0f24-4b07-9e59-f6b28d13f76b 22057 0 2023-01-19 05:03:35 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-01-19 05:03:35 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dknld,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dknld,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 19 05:03:35.772: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-6983" to be "running and ready"
    Jan 19 05:03:35.776: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 3.728427ms
    Jan 19 05:03:35.776: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 05:03:37.780: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007900509s
    Jan 19 05:03:37.780: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 05:03:39.780: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 4.008089422s
    Jan 19 05:03:39.780: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Jan 19 05:03:39.780: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 01/19/23 05:03:39.78
    Jan 19 05:03:39.780: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-6983 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 05:03:39.780: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    Jan 19 05:03:39.780: INFO: ExecWithOptions: Clientset creation
    Jan 19 05:03:39.781: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/dns-6983/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 01/19/23 05:03:39.873
    Jan 19 05:03:39.873: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-6983 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 05:03:39.873: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    Jan 19 05:03:39.873: INFO: ExecWithOptions: Clientset creation
    Jan 19 05:03:39.874: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/dns-6983/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 19 05:03:39.966: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 19 05:03:39.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-6983" for this suite. 01/19/23 05:03:39.984
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:03:39.991
Jan 19 05:03:39.991: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename job 01/19/23 05:03:39.991
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:03:40.006
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:03:40.009
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
STEP: Creating a suspended job 01/19/23 05:03:40.014
STEP: Patching the Job 01/19/23 05:03:40.02
STEP: Watching for Job to be patched 01/19/23 05:03:40.039
Jan 19 05:03:40.041: INFO: Event ADDED observed for Job e2e-p6v9q in namespace job-4165 with labels: map[e2e-job-label:e2e-p6v9q] and annotations: map[batch.kubernetes.io/job-tracking:]
Jan 19 05:03:40.041: INFO: Event MODIFIED observed for Job e2e-p6v9q in namespace job-4165 with labels: map[e2e-job-label:e2e-p6v9q] and annotations: map[batch.kubernetes.io/job-tracking:]
Jan 19 05:03:40.041: INFO: Event MODIFIED found for Job e2e-p6v9q in namespace job-4165 with labels: map[e2e-job-label:e2e-p6v9q e2e-p6v9q:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 01/19/23 05:03:40.041
STEP: Watching for Job to be updated 01/19/23 05:03:40.053
Jan 19 05:03:40.054: INFO: Event MODIFIED found for Job e2e-p6v9q in namespace job-4165 with labels: map[e2e-job-label:e2e-p6v9q e2e-p6v9q:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 19 05:03:40.054: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 01/19/23 05:03:40.054
Jan 19 05:03:40.059: INFO: Job: e2e-p6v9q as labels: map[e2e-job-label:e2e-p6v9q e2e-p6v9q:patched]
STEP: Waiting for job to complete 01/19/23 05:03:40.059
STEP: Delete a job collection with a labelselector 01/19/23 05:03:50.063
STEP: Watching for Job to be deleted 01/19/23 05:03:50.069
Jan 19 05:03:50.072: INFO: Event MODIFIED observed for Job e2e-p6v9q in namespace job-4165 with labels: map[e2e-job-label:e2e-p6v9q e2e-p6v9q:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 19 05:03:50.072: INFO: Event MODIFIED observed for Job e2e-p6v9q in namespace job-4165 with labels: map[e2e-job-label:e2e-p6v9q e2e-p6v9q:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 19 05:03:50.072: INFO: Event MODIFIED observed for Job e2e-p6v9q in namespace job-4165 with labels: map[e2e-job-label:e2e-p6v9q e2e-p6v9q:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 19 05:03:50.072: INFO: Event MODIFIED observed for Job e2e-p6v9q in namespace job-4165 with labels: map[e2e-job-label:e2e-p6v9q e2e-p6v9q:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 19 05:03:50.072: INFO: Event MODIFIED observed for Job e2e-p6v9q in namespace job-4165 with labels: map[e2e-job-label:e2e-p6v9q e2e-p6v9q:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 19 05:03:50.072: INFO: Event MODIFIED observed for Job e2e-p6v9q in namespace job-4165 with labels: map[e2e-job-label:e2e-p6v9q e2e-p6v9q:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 19 05:03:50.072: INFO: Event MODIFIED observed for Job e2e-p6v9q in namespace job-4165 with labels: map[e2e-job-label:e2e-p6v9q e2e-p6v9q:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 19 05:03:50.072: INFO: Event DELETED found for Job e2e-p6v9q in namespace job-4165 with labels: map[e2e-job-label:e2e-p6v9q e2e-p6v9q:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 01/19/23 05:03:50.072
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 19 05:03:50.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4165" for this suite. 01/19/23 05:03:50.086
{"msg":"PASSED [sig-apps] Job should manage the lifecycle of a job [Conformance]","completed":194,"skipped":3686,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.107 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:03:39.991
    Jan 19 05:03:39.991: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename job 01/19/23 05:03:39.991
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:03:40.006
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:03:40.009
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:531
    STEP: Creating a suspended job 01/19/23 05:03:40.014
    STEP: Patching the Job 01/19/23 05:03:40.02
    STEP: Watching for Job to be patched 01/19/23 05:03:40.039
    Jan 19 05:03:40.041: INFO: Event ADDED observed for Job e2e-p6v9q in namespace job-4165 with labels: map[e2e-job-label:e2e-p6v9q] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jan 19 05:03:40.041: INFO: Event MODIFIED observed for Job e2e-p6v9q in namespace job-4165 with labels: map[e2e-job-label:e2e-p6v9q] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jan 19 05:03:40.041: INFO: Event MODIFIED found for Job e2e-p6v9q in namespace job-4165 with labels: map[e2e-job-label:e2e-p6v9q e2e-p6v9q:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 01/19/23 05:03:40.041
    STEP: Watching for Job to be updated 01/19/23 05:03:40.053
    Jan 19 05:03:40.054: INFO: Event MODIFIED found for Job e2e-p6v9q in namespace job-4165 with labels: map[e2e-job-label:e2e-p6v9q e2e-p6v9q:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 19 05:03:40.054: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 01/19/23 05:03:40.054
    Jan 19 05:03:40.059: INFO: Job: e2e-p6v9q as labels: map[e2e-job-label:e2e-p6v9q e2e-p6v9q:patched]
    STEP: Waiting for job to complete 01/19/23 05:03:40.059
    STEP: Delete a job collection with a labelselector 01/19/23 05:03:50.063
    STEP: Watching for Job to be deleted 01/19/23 05:03:50.069
    Jan 19 05:03:50.072: INFO: Event MODIFIED observed for Job e2e-p6v9q in namespace job-4165 with labels: map[e2e-job-label:e2e-p6v9q e2e-p6v9q:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 19 05:03:50.072: INFO: Event MODIFIED observed for Job e2e-p6v9q in namespace job-4165 with labels: map[e2e-job-label:e2e-p6v9q e2e-p6v9q:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 19 05:03:50.072: INFO: Event MODIFIED observed for Job e2e-p6v9q in namespace job-4165 with labels: map[e2e-job-label:e2e-p6v9q e2e-p6v9q:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 19 05:03:50.072: INFO: Event MODIFIED observed for Job e2e-p6v9q in namespace job-4165 with labels: map[e2e-job-label:e2e-p6v9q e2e-p6v9q:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 19 05:03:50.072: INFO: Event MODIFIED observed for Job e2e-p6v9q in namespace job-4165 with labels: map[e2e-job-label:e2e-p6v9q e2e-p6v9q:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 19 05:03:50.072: INFO: Event MODIFIED observed for Job e2e-p6v9q in namespace job-4165 with labels: map[e2e-job-label:e2e-p6v9q e2e-p6v9q:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 19 05:03:50.072: INFO: Event MODIFIED observed for Job e2e-p6v9q in namespace job-4165 with labels: map[e2e-job-label:e2e-p6v9q e2e-p6v9q:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 19 05:03:50.072: INFO: Event DELETED found for Job e2e-p6v9q in namespace job-4165 with labels: map[e2e-job-label:e2e-p6v9q e2e-p6v9q:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 01/19/23 05:03:50.072
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 19 05:03:50.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-4165" for this suite. 01/19/23 05:03:50.086
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:03:50.098
Jan 19 05:03:50.098: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename webhook 01/19/23 05:03:50.099
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:03:50.109
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:03:50.111
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/19/23 05:03:50.124
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 05:03:50.429
STEP: Deploying the webhook pod 01/19/23 05:03:50.436
STEP: Wait for the deployment to be ready 01/19/23 05:03:50.446
Jan 19 05:03:50.455: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 19 05:03:52.467: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 5, 3, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 3, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 3, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 3, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/19/23 05:03:54.47
STEP: Verifying the service has paired with the endpoint 01/19/23 05:03:54.481
Jan 19 05:03:55.481: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
STEP: Registering the crd webhook via the AdmissionRegistration API 01/19/23 05:03:55.485
STEP: Creating a custom resource definition that should be denied by the webhook 01/19/23 05:03:55.501
Jan 19 05:03:55.501: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 05:03:55.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7277" for this suite. 01/19/23 05:03:55.55
STEP: Destroying namespace "webhook-7277-markers" for this suite. 01/19/23 05:03:55.558
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","completed":195,"skipped":3721,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.499 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:03:50.098
    Jan 19 05:03:50.098: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename webhook 01/19/23 05:03:50.099
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:03:50.109
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:03:50.111
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/19/23 05:03:50.124
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 05:03:50.429
    STEP: Deploying the webhook pod 01/19/23 05:03:50.436
    STEP: Wait for the deployment to be ready 01/19/23 05:03:50.446
    Jan 19 05:03:50.455: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 19 05:03:52.467: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 5, 3, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 3, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 3, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 3, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/19/23 05:03:54.47
    STEP: Verifying the service has paired with the endpoint 01/19/23 05:03:54.481
    Jan 19 05:03:55.481: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:307
    STEP: Registering the crd webhook via the AdmissionRegistration API 01/19/23 05:03:55.485
    STEP: Creating a custom resource definition that should be denied by the webhook 01/19/23 05:03:55.501
    Jan 19 05:03:55.501: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 05:03:55.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7277" for this suite. 01/19/23 05:03:55.55
    STEP: Destroying namespace "webhook-7277-markers" for this suite. 01/19/23 05:03:55.558
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:03:55.599
Jan 19 05:03:55.599: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename daemonsets 01/19/23 05:03:55.6
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:03:55.615
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:03:55.618
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
STEP: Creating simple DaemonSet "daemon-set" 01/19/23 05:03:55.634
STEP: Check that daemon pods launch on every node of the cluster. 01/19/23 05:03:55.639
Jan 19 05:03:55.644: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 05:03:55.644: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
Jan 19 05:03:56.652: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 05:03:56.652: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
Jan 19 05:03:57.653: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 05:03:57.653: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
Jan 19 05:03:58.659: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 19 05:03:58.659: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
Jan 19 05:03:59.652: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 19 05:03:59.652: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Getting /status 01/19/23 05:03:59.655
Jan 19 05:03:59.658: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 01/19/23 05:03:59.658
Jan 19 05:03:59.666: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 01/19/23 05:03:59.666
Jan 19 05:03:59.669: INFO: Observed &DaemonSet event: ADDED
Jan 19 05:03:59.669: INFO: Observed &DaemonSet event: MODIFIED
Jan 19 05:03:59.669: INFO: Observed &DaemonSet event: MODIFIED
Jan 19 05:03:59.669: INFO: Observed &DaemonSet event: MODIFIED
Jan 19 05:03:59.669: INFO: Found daemon set daemon-set in namespace daemonsets-1525 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 19 05:03:59.669: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 01/19/23 05:03:59.669
STEP: watching for the daemon set status to be patched 01/19/23 05:03:59.678
Jan 19 05:03:59.680: INFO: Observed &DaemonSet event: ADDED
Jan 19 05:03:59.680: INFO: Observed &DaemonSet event: MODIFIED
Jan 19 05:03:59.680: INFO: Observed &DaemonSet event: MODIFIED
Jan 19 05:03:59.680: INFO: Observed &DaemonSet event: MODIFIED
Jan 19 05:03:59.680: INFO: Observed daemon set daemon-set in namespace daemonsets-1525 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 19 05:03:59.680: INFO: Observed &DaemonSet event: MODIFIED
Jan 19 05:03:59.680: INFO: Found daemon set daemon-set in namespace daemonsets-1525 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Jan 19 05:03:59.680: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/19/23 05:03:59.685
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1525, will wait for the garbage collector to delete the pods 01/19/23 05:03:59.685
Jan 19 05:03:59.744: INFO: Deleting DaemonSet.extensions daemon-set took: 5.64044ms
Jan 19 05:03:59.844: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.756074ms
Jan 19 05:04:01.748: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 05:04:01.748: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 19 05:04:01.751: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"22343"},"items":null}

Jan 19 05:04:01.753: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"22343"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 19 05:04:01.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1525" for this suite. 01/19/23 05:04:01.762
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","completed":196,"skipped":3759,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.168 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:03:55.599
    Jan 19 05:03:55.599: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename daemonsets 01/19/23 05:03:55.6
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:03:55.615
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:03:55.618
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:861
    STEP: Creating simple DaemonSet "daemon-set" 01/19/23 05:03:55.634
    STEP: Check that daemon pods launch on every node of the cluster. 01/19/23 05:03:55.639
    Jan 19 05:03:55.644: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 05:03:55.644: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
    Jan 19 05:03:56.652: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 05:03:56.652: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
    Jan 19 05:03:57.653: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 05:03:57.653: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
    Jan 19 05:03:58.659: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 19 05:03:58.659: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
    Jan 19 05:03:59.652: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 19 05:03:59.652: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Getting /status 01/19/23 05:03:59.655
    Jan 19 05:03:59.658: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 01/19/23 05:03:59.658
    Jan 19 05:03:59.666: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 01/19/23 05:03:59.666
    Jan 19 05:03:59.669: INFO: Observed &DaemonSet event: ADDED
    Jan 19 05:03:59.669: INFO: Observed &DaemonSet event: MODIFIED
    Jan 19 05:03:59.669: INFO: Observed &DaemonSet event: MODIFIED
    Jan 19 05:03:59.669: INFO: Observed &DaemonSet event: MODIFIED
    Jan 19 05:03:59.669: INFO: Found daemon set daemon-set in namespace daemonsets-1525 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 19 05:03:59.669: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 01/19/23 05:03:59.669
    STEP: watching for the daemon set status to be patched 01/19/23 05:03:59.678
    Jan 19 05:03:59.680: INFO: Observed &DaemonSet event: ADDED
    Jan 19 05:03:59.680: INFO: Observed &DaemonSet event: MODIFIED
    Jan 19 05:03:59.680: INFO: Observed &DaemonSet event: MODIFIED
    Jan 19 05:03:59.680: INFO: Observed &DaemonSet event: MODIFIED
    Jan 19 05:03:59.680: INFO: Observed daemon set daemon-set in namespace daemonsets-1525 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 19 05:03:59.680: INFO: Observed &DaemonSet event: MODIFIED
    Jan 19 05:03:59.680: INFO: Found daemon set daemon-set in namespace daemonsets-1525 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Jan 19 05:03:59.680: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/19/23 05:03:59.685
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1525, will wait for the garbage collector to delete the pods 01/19/23 05:03:59.685
    Jan 19 05:03:59.744: INFO: Deleting DaemonSet.extensions daemon-set took: 5.64044ms
    Jan 19 05:03:59.844: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.756074ms
    Jan 19 05:04:01.748: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 05:04:01.748: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 19 05:04:01.751: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"22343"},"items":null}

    Jan 19 05:04:01.753: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"22343"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 19 05:04:01.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-1525" for this suite. 01/19/23 05:04:01.762
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:04:01.768
Jan 19 05:04:01.768: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename projected 01/19/23 05:04:01.768
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:04:01.783
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:04:01.786
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
STEP: Creating configMap with name projected-configmap-test-volume-e987d3d2-1a44-4a59-9d50-21b525b10c5a 01/19/23 05:04:01.788
STEP: Creating a pod to test consume configMaps 01/19/23 05:04:01.792
Jan 19 05:04:01.802: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c27437be-ee7d-4c85-9818-3c94135cf371" in namespace "projected-756" to be "Succeeded or Failed"
Jan 19 05:04:01.808: INFO: Pod "pod-projected-configmaps-c27437be-ee7d-4c85-9818-3c94135cf371": Phase="Pending", Reason="", readiness=false. Elapsed: 6.068378ms
Jan 19 05:04:03.812: INFO: Pod "pod-projected-configmaps-c27437be-ee7d-4c85-9818-3c94135cf371": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010666849s
Jan 19 05:04:05.812: INFO: Pod "pod-projected-configmaps-c27437be-ee7d-4c85-9818-3c94135cf371": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010644624s
Jan 19 05:04:07.812: INFO: Pod "pod-projected-configmaps-c27437be-ee7d-4c85-9818-3c94135cf371": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009994934s
STEP: Saw pod success 01/19/23 05:04:07.812
Jan 19 05:04:07.812: INFO: Pod "pod-projected-configmaps-c27437be-ee7d-4c85-9818-3c94135cf371" satisfied condition "Succeeded or Failed"
Jan 19 05:04:07.814: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-projected-configmaps-c27437be-ee7d-4c85-9818-3c94135cf371 container agnhost-container: <nil>
STEP: delete the pod 01/19/23 05:04:07.819
Jan 19 05:04:07.829: INFO: Waiting for pod pod-projected-configmaps-c27437be-ee7d-4c85-9818-3c94135cf371 to disappear
Jan 19 05:04:07.831: INFO: Pod pod-projected-configmaps-c27437be-ee7d-4c85-9818-3c94135cf371 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 19 05:04:07.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-756" for this suite. 01/19/23 05:04:07.834
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":197,"skipped":3760,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.072 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:04:01.768
    Jan 19 05:04:01.768: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename projected 01/19/23 05:04:01.768
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:04:01.783
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:04:01.786
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:56
    STEP: Creating configMap with name projected-configmap-test-volume-e987d3d2-1a44-4a59-9d50-21b525b10c5a 01/19/23 05:04:01.788
    STEP: Creating a pod to test consume configMaps 01/19/23 05:04:01.792
    Jan 19 05:04:01.802: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c27437be-ee7d-4c85-9818-3c94135cf371" in namespace "projected-756" to be "Succeeded or Failed"
    Jan 19 05:04:01.808: INFO: Pod "pod-projected-configmaps-c27437be-ee7d-4c85-9818-3c94135cf371": Phase="Pending", Reason="", readiness=false. Elapsed: 6.068378ms
    Jan 19 05:04:03.812: INFO: Pod "pod-projected-configmaps-c27437be-ee7d-4c85-9818-3c94135cf371": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010666849s
    Jan 19 05:04:05.812: INFO: Pod "pod-projected-configmaps-c27437be-ee7d-4c85-9818-3c94135cf371": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010644624s
    Jan 19 05:04:07.812: INFO: Pod "pod-projected-configmaps-c27437be-ee7d-4c85-9818-3c94135cf371": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009994934s
    STEP: Saw pod success 01/19/23 05:04:07.812
    Jan 19 05:04:07.812: INFO: Pod "pod-projected-configmaps-c27437be-ee7d-4c85-9818-3c94135cf371" satisfied condition "Succeeded or Failed"
    Jan 19 05:04:07.814: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-projected-configmaps-c27437be-ee7d-4c85-9818-3c94135cf371 container agnhost-container: <nil>
    STEP: delete the pod 01/19/23 05:04:07.819
    Jan 19 05:04:07.829: INFO: Waiting for pod pod-projected-configmaps-c27437be-ee7d-4c85-9818-3c94135cf371 to disappear
    Jan 19 05:04:07.831: INFO: Pod pod-projected-configmaps-c27437be-ee7d-4c85-9818-3c94135cf371 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 19 05:04:07.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-756" for this suite. 01/19/23 05:04:07.834
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:04:07.841
Jan 19 05:04:07.841: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename sched-preemption 01/19/23 05:04:07.842
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:04:07.856
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:04:07.859
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 19 05:04:07.873: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 19 05:05:07.892: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
STEP: Create pods that use 4/5 of node resources. 01/19/23 05:05:07.894
Jan 19 05:05:07.915: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan 19 05:05:07.923: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan 19 05:05:07.958: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan 19 05:05:07.967: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 01/19/23 05:05:07.967
Jan 19 05:05:07.967: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-5918" to be "running"
Jan 19 05:05:07.973: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.195824ms
Jan 19 05:05:09.977: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009791991s
Jan 19 05:05:11.977: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.009766279s
Jan 19 05:05:11.977: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jan 19 05:05:11.977: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-5918" to be "running"
Jan 19 05:05:11.980: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.05684ms
Jan 19 05:05:11.980: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 19 05:05:11.980: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-5918" to be "running"
Jan 19 05:05:11.982: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.628127ms
Jan 19 05:05:11.982: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 19 05:05:11.982: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-5918" to be "running"
Jan 19 05:05:11.985: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.340088ms
Jan 19 05:05:11.985: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 01/19/23 05:05:11.985
Jan 19 05:05:11.994: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Jan 19 05:05:11.998: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.355961ms
Jan 19 05:05:14.002: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008401871s
Jan 19 05:05:16.003: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008707542s
Jan 19 05:05:18.004: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.010289713s
Jan 19 05:05:18.004: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jan 19 05:05:18.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5918" for this suite. 01/19/23 05:05:18.035
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","completed":198,"skipped":3780,"failed":0}
------------------------------
â€¢ [SLOW TEST] [70.236 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:04:07.841
    Jan 19 05:04:07.841: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename sched-preemption 01/19/23 05:04:07.842
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:04:07.856
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:04:07.859
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jan 19 05:04:07.873: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 19 05:05:07.892: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:218
    STEP: Create pods that use 4/5 of node resources. 01/19/23 05:05:07.894
    Jan 19 05:05:07.915: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jan 19 05:05:07.923: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jan 19 05:05:07.958: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jan 19 05:05:07.967: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 01/19/23 05:05:07.967
    Jan 19 05:05:07.967: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-5918" to be "running"
    Jan 19 05:05:07.973: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.195824ms
    Jan 19 05:05:09.977: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009791991s
    Jan 19 05:05:11.977: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.009766279s
    Jan 19 05:05:11.977: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jan 19 05:05:11.977: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-5918" to be "running"
    Jan 19 05:05:11.980: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.05684ms
    Jan 19 05:05:11.980: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 19 05:05:11.980: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-5918" to be "running"
    Jan 19 05:05:11.982: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.628127ms
    Jan 19 05:05:11.982: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 19 05:05:11.982: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-5918" to be "running"
    Jan 19 05:05:11.985: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.340088ms
    Jan 19 05:05:11.985: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 01/19/23 05:05:11.985
    Jan 19 05:05:11.994: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Jan 19 05:05:11.998: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.355961ms
    Jan 19 05:05:14.002: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008401871s
    Jan 19 05:05:16.003: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008707542s
    Jan 19 05:05:18.004: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.010289713s
    Jan 19 05:05:18.004: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jan 19 05:05:18.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-5918" for this suite. 01/19/23 05:05:18.035
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:05:18.078
Jan 19 05:05:18.078: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename statefulset 01/19/23 05:05:18.078
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:05:18.093
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:05:18.096
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-4389 01/19/23 05:05:18.098
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
STEP: Initializing watcher for selector baz=blah,foo=bar 01/19/23 05:05:18.103
STEP: Creating stateful set ss in namespace statefulset-4389 01/19/23 05:05:18.109
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4389 01/19/23 05:05:18.114
Jan 19 05:05:18.116: INFO: Found 0 stateful pods, waiting for 1
Jan 19 05:05:28.121: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 01/19/23 05:05:28.121
Jan 19 05:05:28.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=statefulset-4389 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 19 05:05:28.301: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 19 05:05:28.301: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 19 05:05:28.301: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 19 05:05:28.304: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 19 05:05:38.308: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 19 05:05:38.308: INFO: Waiting for statefulset status.replicas updated to 0
Jan 19 05:05:38.324: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999781s
Jan 19 05:05:39.329: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997536719s
Jan 19 05:05:40.332: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.992391559s
Jan 19 05:05:41.336: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.988867566s
Jan 19 05:05:42.339: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.985361547s
Jan 19 05:05:43.344: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.982261337s
Jan 19 05:05:44.348: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.977528701s
Jan 19 05:05:45.351: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.974138242s
Jan 19 05:05:46.354: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.970832431s
Jan 19 05:05:47.358: INFO: Verifying statefulset ss doesn't scale past 1 for another 967.406917ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4389 01/19/23 05:05:48.359
Jan 19 05:05:48.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=statefulset-4389 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 19 05:05:48.530: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 19 05:05:48.530: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 19 05:05:48.530: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 19 05:05:48.534: INFO: Found 1 stateful pods, waiting for 3
Jan 19 05:05:58.541: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 05:05:58.541: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 05:05:58.541: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 01/19/23 05:05:58.541
STEP: Scale down will halt with unhealthy stateful pod 01/19/23 05:05:58.541
Jan 19 05:05:58.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=statefulset-4389 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 19 05:05:58.704: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 19 05:05:58.704: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 19 05:05:58.704: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 19 05:05:58.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=statefulset-4389 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 19 05:05:58.864: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 19 05:05:58.864: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 19 05:05:58.864: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 19 05:05:58.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=statefulset-4389 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 19 05:05:59.071: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 19 05:05:59.071: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 19 05:05:59.071: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 19 05:05:59.071: INFO: Waiting for statefulset status.replicas updated to 0
Jan 19 05:05:59.075: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jan 19 05:06:09.085: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 19 05:06:09.085: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 19 05:06:09.085: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 19 05:06:09.105: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999671s
Jan 19 05:06:10.110: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993552188s
Jan 19 05:06:11.114: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988702676s
Jan 19 05:06:12.118: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.984457437s
Jan 19 05:06:13.123: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.980253123s
Jan 19 05:06:14.127: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.975737955s
Jan 19 05:06:15.133: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.970497565s
Jan 19 05:06:16.138: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.965481523s
Jan 19 05:06:17.141: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.960840131s
Jan 19 05:06:18.145: INFO: Verifying statefulset ss doesn't scale past 3 for another 957.413086ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4389 01/19/23 05:06:19.145
Jan 19 05:06:19.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=statefulset-4389 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 19 05:06:19.309: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 19 05:06:19.309: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 19 05:06:19.309: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 19 05:06:19.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=statefulset-4389 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 19 05:06:19.471: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 19 05:06:19.471: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 19 05:06:19.471: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 19 05:06:19.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=statefulset-4389 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 19 05:06:19.634: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 19 05:06:19.634: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 19 05:06:19.634: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 19 05:06:19.634: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 01/19/23 05:06:29.65
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 19 05:06:29.650: INFO: Deleting all statefulset in ns statefulset-4389
Jan 19 05:06:29.652: INFO: Scaling statefulset ss to 0
Jan 19 05:06:29.662: INFO: Waiting for statefulset status.replicas updated to 0
Jan 19 05:06:29.664: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 19 05:06:29.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4389" for this suite. 01/19/23 05:06:29.687
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","completed":199,"skipped":3798,"failed":0}
------------------------------
â€¢ [SLOW TEST] [71.616 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:585

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:05:18.078
    Jan 19 05:05:18.078: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename statefulset 01/19/23 05:05:18.078
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:05:18.093
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:05:18.096
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-4389 01/19/23 05:05:18.098
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:585
    STEP: Initializing watcher for selector baz=blah,foo=bar 01/19/23 05:05:18.103
    STEP: Creating stateful set ss in namespace statefulset-4389 01/19/23 05:05:18.109
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4389 01/19/23 05:05:18.114
    Jan 19 05:05:18.116: INFO: Found 0 stateful pods, waiting for 1
    Jan 19 05:05:28.121: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 01/19/23 05:05:28.121
    Jan 19 05:05:28.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=statefulset-4389 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 19 05:05:28.301: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 19 05:05:28.301: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 19 05:05:28.301: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 19 05:05:28.304: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jan 19 05:05:38.308: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 19 05:05:38.308: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 19 05:05:38.324: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999781s
    Jan 19 05:05:39.329: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997536719s
    Jan 19 05:05:40.332: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.992391559s
    Jan 19 05:05:41.336: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.988867566s
    Jan 19 05:05:42.339: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.985361547s
    Jan 19 05:05:43.344: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.982261337s
    Jan 19 05:05:44.348: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.977528701s
    Jan 19 05:05:45.351: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.974138242s
    Jan 19 05:05:46.354: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.970832431s
    Jan 19 05:05:47.358: INFO: Verifying statefulset ss doesn't scale past 1 for another 967.406917ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4389 01/19/23 05:05:48.359
    Jan 19 05:05:48.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=statefulset-4389 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 19 05:05:48.530: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 19 05:05:48.530: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 19 05:05:48.530: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 19 05:05:48.534: INFO: Found 1 stateful pods, waiting for 3
    Jan 19 05:05:58.541: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 19 05:05:58.541: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 19 05:05:58.541: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 01/19/23 05:05:58.541
    STEP: Scale down will halt with unhealthy stateful pod 01/19/23 05:05:58.541
    Jan 19 05:05:58.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=statefulset-4389 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 19 05:05:58.704: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 19 05:05:58.704: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 19 05:05:58.704: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 19 05:05:58.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=statefulset-4389 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 19 05:05:58.864: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 19 05:05:58.864: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 19 05:05:58.864: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 19 05:05:58.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=statefulset-4389 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 19 05:05:59.071: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 19 05:05:59.071: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 19 05:05:59.071: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 19 05:05:59.071: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 19 05:05:59.075: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Jan 19 05:06:09.085: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 19 05:06:09.085: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jan 19 05:06:09.085: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jan 19 05:06:09.105: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999671s
    Jan 19 05:06:10.110: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993552188s
    Jan 19 05:06:11.114: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988702676s
    Jan 19 05:06:12.118: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.984457437s
    Jan 19 05:06:13.123: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.980253123s
    Jan 19 05:06:14.127: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.975737955s
    Jan 19 05:06:15.133: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.970497565s
    Jan 19 05:06:16.138: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.965481523s
    Jan 19 05:06:17.141: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.960840131s
    Jan 19 05:06:18.145: INFO: Verifying statefulset ss doesn't scale past 3 for another 957.413086ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4389 01/19/23 05:06:19.145
    Jan 19 05:06:19.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=statefulset-4389 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 19 05:06:19.309: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 19 05:06:19.309: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 19 05:06:19.309: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 19 05:06:19.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=statefulset-4389 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 19 05:06:19.471: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 19 05:06:19.471: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 19 05:06:19.471: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 19 05:06:19.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=statefulset-4389 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 19 05:06:19.634: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 19 05:06:19.634: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 19 05:06:19.634: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 19 05:06:19.634: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 01/19/23 05:06:29.65
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 19 05:06:29.650: INFO: Deleting all statefulset in ns statefulset-4389
    Jan 19 05:06:29.652: INFO: Scaling statefulset ss to 0
    Jan 19 05:06:29.662: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 19 05:06:29.664: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 19 05:06:29.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-4389" for this suite. 01/19/23 05:06:29.687
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:06:29.694
Jan 19 05:06:29.694: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename crd-webhook 01/19/23 05:06:29.695
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:06:29.714
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:06:29.716
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 01/19/23 05:06:29.718
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/19/23 05:06:30.489
STEP: Deploying the custom resource conversion webhook pod 01/19/23 05:06:30.497
STEP: Wait for the deployment to be ready 01/19/23 05:06:30.512
Jan 19 05:06:30.521: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jan 19 05:06:32.530: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 5, 6, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 6, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 6, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 6, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/19/23 05:06:34.535
STEP: Verifying the service has paired with the endpoint 01/19/23 05:06:34.545
Jan 19 05:06:35.546: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Jan 19 05:06:35.550: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Creating a v1 custom resource 01/19/23 05:06:38.245
STEP: Create a v2 custom resource 01/19/23 05:06:38.26
STEP: List CRs in v1 01/19/23 05:06:38.272
STEP: List CRs in v2 01/19/23 05:06:38.354
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 05:06:38.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5658" for this suite. 01/19/23 05:06:38.881
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","completed":200,"skipped":3800,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.251 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:06:29.694
    Jan 19 05:06:29.694: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename crd-webhook 01/19/23 05:06:29.695
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:06:29.714
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:06:29.716
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 01/19/23 05:06:29.718
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/19/23 05:06:30.489
    STEP: Deploying the custom resource conversion webhook pod 01/19/23 05:06:30.497
    STEP: Wait for the deployment to be ready 01/19/23 05:06:30.512
    Jan 19 05:06:30.521: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    Jan 19 05:06:32.530: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 5, 6, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 6, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 6, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 6, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/19/23 05:06:34.535
    STEP: Verifying the service has paired with the endpoint 01/19/23 05:06:34.545
    Jan 19 05:06:35.546: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Jan 19 05:06:35.550: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Creating a v1 custom resource 01/19/23 05:06:38.245
    STEP: Create a v2 custom resource 01/19/23 05:06:38.26
    STEP: List CRs in v1 01/19/23 05:06:38.272
    STEP: List CRs in v2 01/19/23 05:06:38.354
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 05:06:38.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-5658" for this suite. 01/19/23 05:06:38.881
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:06:38.946
Jan 19 05:06:38.946: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename statefulset 01/19/23 05:06:38.947
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:06:39.018
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:06:39.02
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1030 01/19/23 05:06:39.023
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
STEP: Creating a new StatefulSet 01/19/23 05:06:39.028
Jan 19 05:06:39.040: INFO: Found 0 stateful pods, waiting for 3
Jan 19 05:06:49.045: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 05:06:49.045: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 05:06:49.045: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 01/19/23 05:06:49.052
Jan 19 05:06:49.071: INFO: Updating stateful set ss2
STEP: Creating a new revision 01/19/23 05:06:49.071
STEP: Not applying an update when the partition is greater than the number of replicas 01/19/23 05:06:59.089
STEP: Performing a canary update 01/19/23 05:06:59.089
Jan 19 05:06:59.108: INFO: Updating stateful set ss2
Jan 19 05:06:59.114: INFO: Waiting for Pod statefulset-1030/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
STEP: Restoring Pods to the correct revision when they are deleted 01/19/23 05:07:09.124
Jan 19 05:07:09.165: INFO: Found 1 stateful pods, waiting for 3
Jan 19 05:07:19.174: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 05:07:19.174: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 05:07:19.174: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 01/19/23 05:07:19.184
Jan 19 05:07:19.203: INFO: Updating stateful set ss2
Jan 19 05:07:19.208: INFO: Waiting for Pod statefulset-1030/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Jan 19 05:07:29.239: INFO: Updating stateful set ss2
Jan 19 05:07:29.245: INFO: Waiting for StatefulSet statefulset-1030/ss2 to complete update
Jan 19 05:07:29.245: INFO: Waiting for Pod statefulset-1030/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 19 05:07:39.252: INFO: Deleting all statefulset in ns statefulset-1030
Jan 19 05:07:39.254: INFO: Scaling statefulset ss2 to 0
Jan 19 05:07:49.270: INFO: Waiting for statefulset status.replicas updated to 0
Jan 19 05:07:49.273: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 19 05:07:49.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1030" for this suite. 01/19/23 05:07:49.299
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","completed":201,"skipped":3806,"failed":0}
------------------------------
â€¢ [SLOW TEST] [70.361 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:315

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:06:38.946
    Jan 19 05:06:38.946: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename statefulset 01/19/23 05:06:38.947
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:06:39.018
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:06:39.02
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-1030 01/19/23 05:06:39.023
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:315
    STEP: Creating a new StatefulSet 01/19/23 05:06:39.028
    Jan 19 05:06:39.040: INFO: Found 0 stateful pods, waiting for 3
    Jan 19 05:06:49.045: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 19 05:06:49.045: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 19 05:06:49.045: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 01/19/23 05:06:49.052
    Jan 19 05:06:49.071: INFO: Updating stateful set ss2
    STEP: Creating a new revision 01/19/23 05:06:49.071
    STEP: Not applying an update when the partition is greater than the number of replicas 01/19/23 05:06:59.089
    STEP: Performing a canary update 01/19/23 05:06:59.089
    Jan 19 05:06:59.108: INFO: Updating stateful set ss2
    Jan 19 05:06:59.114: INFO: Waiting for Pod statefulset-1030/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    STEP: Restoring Pods to the correct revision when they are deleted 01/19/23 05:07:09.124
    Jan 19 05:07:09.165: INFO: Found 1 stateful pods, waiting for 3
    Jan 19 05:07:19.174: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 19 05:07:19.174: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 19 05:07:19.174: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 01/19/23 05:07:19.184
    Jan 19 05:07:19.203: INFO: Updating stateful set ss2
    Jan 19 05:07:19.208: INFO: Waiting for Pod statefulset-1030/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Jan 19 05:07:29.239: INFO: Updating stateful set ss2
    Jan 19 05:07:29.245: INFO: Waiting for StatefulSet statefulset-1030/ss2 to complete update
    Jan 19 05:07:29.245: INFO: Waiting for Pod statefulset-1030/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 19 05:07:39.252: INFO: Deleting all statefulset in ns statefulset-1030
    Jan 19 05:07:39.254: INFO: Scaling statefulset ss2 to 0
    Jan 19 05:07:49.270: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 19 05:07:49.273: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 19 05:07:49.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-1030" for this suite. 01/19/23 05:07:49.299
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:07:49.308
Jan 19 05:07:49.308: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename events 01/19/23 05:07:49.309
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:07:49.321
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:07:49.323
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 01/19/23 05:07:49.326
STEP: get a list of Events with a label in the current namespace 01/19/23 05:07:49.371
STEP: delete a list of events 01/19/23 05:07:49.43
Jan 19 05:07:49.430: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 01/19/23 05:07:49.445
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Jan 19 05:07:49.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-184" for this suite. 01/19/23 05:07:49.451
{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","completed":202,"skipped":3812,"failed":0}
------------------------------
â€¢ [0.149 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:07:49.308
    Jan 19 05:07:49.308: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename events 01/19/23 05:07:49.309
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:07:49.321
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:07:49.323
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 01/19/23 05:07:49.326
    STEP: get a list of Events with a label in the current namespace 01/19/23 05:07:49.371
    STEP: delete a list of events 01/19/23 05:07:49.43
    Jan 19 05:07:49.430: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 01/19/23 05:07:49.445
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Jan 19 05:07:49.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-184" for this suite. 01/19/23 05:07:49.451
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:07:49.457
Jan 19 05:07:49.457: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename crd-publish-openapi 01/19/23 05:07:49.458
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:07:49.47
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:07:49.472
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 01/19/23 05:07:49.474
Jan 19 05:07:49.474: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 01/19/23 05:08:00.223
Jan 19 05:08:00.224: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
Jan 19 05:08:02.629: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 05:08:13.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5384" for this suite. 01/19/23 05:08:13.884
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","completed":203,"skipped":3830,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.433 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:07:49.457
    Jan 19 05:07:49.457: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename crd-publish-openapi 01/19/23 05:07:49.458
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:07:49.47
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:07:49.472
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:308
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 01/19/23 05:07:49.474
    Jan 19 05:07:49.474: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 01/19/23 05:08:00.223
    Jan 19 05:08:00.224: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    Jan 19 05:08:02.629: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 05:08:13.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-5384" for this suite. 01/19/23 05:08:13.884
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:08:13.892
Jan 19 05:08:13.892: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename projected 01/19/23 05:08:13.892
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:08:13.907
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:08:13.91
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
STEP: Creating projection with secret that has name projected-secret-test-c6349f2e-e30f-4c72-b914-89ef0969f3db 01/19/23 05:08:13.912
STEP: Creating a pod to test consume secrets 01/19/23 05:08:13.916
Jan 19 05:08:13.924: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d864eb3b-60a9-4436-97b3-437e68c29516" in namespace "projected-1168" to be "Succeeded or Failed"
Jan 19 05:08:13.929: INFO: Pod "pod-projected-secrets-d864eb3b-60a9-4436-97b3-437e68c29516": Phase="Pending", Reason="", readiness=false. Elapsed: 4.822282ms
Jan 19 05:08:15.933: INFO: Pod "pod-projected-secrets-d864eb3b-60a9-4436-97b3-437e68c29516": Phase="Running", Reason="", readiness=true. Elapsed: 2.008963157s
Jan 19 05:08:17.934: INFO: Pod "pod-projected-secrets-d864eb3b-60a9-4436-97b3-437e68c29516": Phase="Running", Reason="", readiness=false. Elapsed: 4.009734486s
Jan 19 05:08:19.935: INFO: Pod "pod-projected-secrets-d864eb3b-60a9-4436-97b3-437e68c29516": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010991466s
STEP: Saw pod success 01/19/23 05:08:19.935
Jan 19 05:08:19.936: INFO: Pod "pod-projected-secrets-d864eb3b-60a9-4436-97b3-437e68c29516" satisfied condition "Succeeded or Failed"
Jan 19 05:08:19.939: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-projected-secrets-d864eb3b-60a9-4436-97b3-437e68c29516 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/19/23 05:08:19.972
Jan 19 05:08:19.984: INFO: Waiting for pod pod-projected-secrets-d864eb3b-60a9-4436-97b3-437e68c29516 to disappear
Jan 19 05:08:19.986: INFO: Pod pod-projected-secrets-d864eb3b-60a9-4436-97b3-437e68c29516 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 19 05:08:19.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1168" for this suite. 01/19/23 05:08:19.989
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":204,"skipped":3871,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.103 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:08:13.892
    Jan 19 05:08:13.892: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename projected 01/19/23 05:08:13.892
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:08:13.907
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:08:13.91
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:66
    STEP: Creating projection with secret that has name projected-secret-test-c6349f2e-e30f-4c72-b914-89ef0969f3db 01/19/23 05:08:13.912
    STEP: Creating a pod to test consume secrets 01/19/23 05:08:13.916
    Jan 19 05:08:13.924: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d864eb3b-60a9-4436-97b3-437e68c29516" in namespace "projected-1168" to be "Succeeded or Failed"
    Jan 19 05:08:13.929: INFO: Pod "pod-projected-secrets-d864eb3b-60a9-4436-97b3-437e68c29516": Phase="Pending", Reason="", readiness=false. Elapsed: 4.822282ms
    Jan 19 05:08:15.933: INFO: Pod "pod-projected-secrets-d864eb3b-60a9-4436-97b3-437e68c29516": Phase="Running", Reason="", readiness=true. Elapsed: 2.008963157s
    Jan 19 05:08:17.934: INFO: Pod "pod-projected-secrets-d864eb3b-60a9-4436-97b3-437e68c29516": Phase="Running", Reason="", readiness=false. Elapsed: 4.009734486s
    Jan 19 05:08:19.935: INFO: Pod "pod-projected-secrets-d864eb3b-60a9-4436-97b3-437e68c29516": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010991466s
    STEP: Saw pod success 01/19/23 05:08:19.935
    Jan 19 05:08:19.936: INFO: Pod "pod-projected-secrets-d864eb3b-60a9-4436-97b3-437e68c29516" satisfied condition "Succeeded or Failed"
    Jan 19 05:08:19.939: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-projected-secrets-d864eb3b-60a9-4436-97b3-437e68c29516 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/19/23 05:08:19.972
    Jan 19 05:08:19.984: INFO: Waiting for pod pod-projected-secrets-d864eb3b-60a9-4436-97b3-437e68c29516 to disappear
    Jan 19 05:08:19.986: INFO: Pod pod-projected-secrets-d864eb3b-60a9-4436-97b3-437e68c29516 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 19 05:08:19.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1168" for this suite. 01/19/23 05:08:19.989
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:08:19.995
Jan 19 05:08:19.995: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename crd-webhook 01/19/23 05:08:19.996
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:08:20.062
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:08:20.065
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 01/19/23 05:08:20.067
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/19/23 05:08:20.669
STEP: Deploying the custom resource conversion webhook pod 01/19/23 05:08:20.682
STEP: Wait for the deployment to be ready 01/19/23 05:08:20.695
Jan 19 05:08:20.706: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jan 19 05:08:22.716: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 5, 8, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 8, 20, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 8, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 8, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/19/23 05:08:24.721
STEP: Verifying the service has paired with the endpoint 01/19/23 05:08:24.735
Jan 19 05:08:25.736: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Jan 19 05:08:25.740: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Creating a v1 custom resource 01/19/23 05:08:28.357
STEP: v2 custom resource should be converted 01/19/23 05:08:28.364
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 05:08:28.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-7188" for this suite. 01/19/23 05:08:28.888
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","completed":205,"skipped":3871,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.936 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:08:19.995
    Jan 19 05:08:19.995: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename crd-webhook 01/19/23 05:08:19.996
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:08:20.062
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:08:20.065
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 01/19/23 05:08:20.067
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/19/23 05:08:20.669
    STEP: Deploying the custom resource conversion webhook pod 01/19/23 05:08:20.682
    STEP: Wait for the deployment to be ready 01/19/23 05:08:20.695
    Jan 19 05:08:20.706: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    Jan 19 05:08:22.716: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 5, 8, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 8, 20, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 8, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 8, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/19/23 05:08:24.721
    STEP: Verifying the service has paired with the endpoint 01/19/23 05:08:24.735
    Jan 19 05:08:25.736: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Jan 19 05:08:25.740: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Creating a v1 custom resource 01/19/23 05:08:28.357
    STEP: v2 custom resource should be converted 01/19/23 05:08:28.364
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 05:08:28.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-7188" for this suite. 01/19/23 05:08:28.888
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:08:28.932
Jan 19 05:08:28.932: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename emptydir 01/19/23 05:08:28.932
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:08:28.955
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:08:28.958
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
STEP: Creating a pod to test emptydir 0777 on tmpfs 01/19/23 05:08:28.961
Jan 19 05:08:28.974: INFO: Waiting up to 5m0s for pod "pod-9f2621da-f2c4-402e-b4d7-d4e089b0fd49" in namespace "emptydir-9329" to be "Succeeded or Failed"
Jan 19 05:08:28.976: INFO: Pod "pod-9f2621da-f2c4-402e-b4d7-d4e089b0fd49": Phase="Pending", Reason="", readiness=false. Elapsed: 2.2446ms
Jan 19 05:08:30.981: INFO: Pod "pod-9f2621da-f2c4-402e-b4d7-d4e089b0fd49": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006885355s
Jan 19 05:08:32.983: INFO: Pod "pod-9f2621da-f2c4-402e-b4d7-d4e089b0fd49": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008826186s
Jan 19 05:08:34.980: INFO: Pod "pod-9f2621da-f2c4-402e-b4d7-d4e089b0fd49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006620922s
STEP: Saw pod success 01/19/23 05:08:34.98
Jan 19 05:08:34.981: INFO: Pod "pod-9f2621da-f2c4-402e-b4d7-d4e089b0fd49" satisfied condition "Succeeded or Failed"
Jan 19 05:08:34.983: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-9f2621da-f2c4-402e-b4d7-d4e089b0fd49 container test-container: <nil>
STEP: delete the pod 01/19/23 05:08:34.989
Jan 19 05:08:34.999: INFO: Waiting for pod pod-9f2621da-f2c4-402e-b4d7-d4e089b0fd49 to disappear
Jan 19 05:08:35.001: INFO: Pod pod-9f2621da-f2c4-402e-b4d7-d4e089b0fd49 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 19 05:08:35.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9329" for this suite. 01/19/23 05:08:35.004
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":206,"skipped":3905,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.079 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:08:28.932
    Jan 19 05:08:28.932: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename emptydir 01/19/23 05:08:28.932
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:08:28.955
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:08:28.958
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:116
    STEP: Creating a pod to test emptydir 0777 on tmpfs 01/19/23 05:08:28.961
    Jan 19 05:08:28.974: INFO: Waiting up to 5m0s for pod "pod-9f2621da-f2c4-402e-b4d7-d4e089b0fd49" in namespace "emptydir-9329" to be "Succeeded or Failed"
    Jan 19 05:08:28.976: INFO: Pod "pod-9f2621da-f2c4-402e-b4d7-d4e089b0fd49": Phase="Pending", Reason="", readiness=false. Elapsed: 2.2446ms
    Jan 19 05:08:30.981: INFO: Pod "pod-9f2621da-f2c4-402e-b4d7-d4e089b0fd49": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006885355s
    Jan 19 05:08:32.983: INFO: Pod "pod-9f2621da-f2c4-402e-b4d7-d4e089b0fd49": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008826186s
    Jan 19 05:08:34.980: INFO: Pod "pod-9f2621da-f2c4-402e-b4d7-d4e089b0fd49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006620922s
    STEP: Saw pod success 01/19/23 05:08:34.98
    Jan 19 05:08:34.981: INFO: Pod "pod-9f2621da-f2c4-402e-b4d7-d4e089b0fd49" satisfied condition "Succeeded or Failed"
    Jan 19 05:08:34.983: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-9f2621da-f2c4-402e-b4d7-d4e089b0fd49 container test-container: <nil>
    STEP: delete the pod 01/19/23 05:08:34.989
    Jan 19 05:08:34.999: INFO: Waiting for pod pod-9f2621da-f2c4-402e-b4d7-d4e089b0fd49 to disappear
    Jan 19 05:08:35.001: INFO: Pod pod-9f2621da-f2c4-402e-b4d7-d4e089b0fd49 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 19 05:08:35.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9329" for this suite. 01/19/23 05:08:35.004
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:08:35.011
Jan 19 05:08:35.011: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename crd-publish-openapi 01/19/23 05:08:35.012
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:08:35.024
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:08:35.025
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
STEP: set up a multi version CRD 01/19/23 05:08:35.027
Jan 19 05:08:35.028: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: mark a version not serverd 01/19/23 05:08:41.783
STEP: check the unserved version gets removed 01/19/23 05:08:41.81
STEP: check the other version is not changed 01/19/23 05:08:44.66
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 05:08:49.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1022" for this suite. 01/19/23 05:08:49.986
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","completed":207,"skipped":3911,"failed":0}
------------------------------
â€¢ [SLOW TEST] [14.981 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:08:35.011
    Jan 19 05:08:35.011: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename crd-publish-openapi 01/19/23 05:08:35.012
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:08:35.024
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:08:35.025
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:441
    STEP: set up a multi version CRD 01/19/23 05:08:35.027
    Jan 19 05:08:35.028: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: mark a version not serverd 01/19/23 05:08:41.783
    STEP: check the unserved version gets removed 01/19/23 05:08:41.81
    STEP: check the other version is not changed 01/19/23 05:08:44.66
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 05:08:49.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-1022" for this suite. 01/19/23 05:08:49.986
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:08:49.992
Jan 19 05:08:49.992: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename init-container 01/19/23 05:08:49.993
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:08:50.006
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:08:50.009
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
STEP: creating the pod 01/19/23 05:08:50.012
Jan 19 05:08:50.012: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 19 05:08:55.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2857" for this suite. 01/19/23 05:08:55.535
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","completed":208,"skipped":3912,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.548 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:08:49.992
    Jan 19 05:08:49.992: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename init-container 01/19/23 05:08:49.993
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:08:50.006
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:08:50.009
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:457
    STEP: creating the pod 01/19/23 05:08:50.012
    Jan 19 05:08:50.012: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 19 05:08:55.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-2857" for this suite. 01/19/23 05:08:55.535
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:08:55.541
Jan 19 05:08:55.541: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename controllerrevisions 01/19/23 05:08:55.541
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:08:55.552
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:08:55.555
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-cqpbn-daemon-set" 01/19/23 05:08:55.569
STEP: Check that daemon pods launch on every node of the cluster. 01/19/23 05:08:55.574
Jan 19 05:08:55.582: INFO: Number of nodes with available pods controlled by daemonset e2e-cqpbn-daemon-set: 0
Jan 19 05:08:55.582: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
Jan 19 05:08:56.589: INFO: Number of nodes with available pods controlled by daemonset e2e-cqpbn-daemon-set: 0
Jan 19 05:08:56.589: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
Jan 19 05:08:57.588: INFO: Number of nodes with available pods controlled by daemonset e2e-cqpbn-daemon-set: 0
Jan 19 05:08:57.588: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
Jan 19 05:08:58.589: INFO: Number of nodes with available pods controlled by daemonset e2e-cqpbn-daemon-set: 1
Jan 19 05:08:58.589: INFO: Node ckcp-nks-default-worker-node-1 is running 0 daemon pod, expected 1
Jan 19 05:08:59.590: INFO: Number of nodes with available pods controlled by daemonset e2e-cqpbn-daemon-set: 2
Jan 19 05:08:59.590: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-cqpbn-daemon-set
STEP: Confirm DaemonSet "e2e-cqpbn-daemon-set" successfully created with "daemonset-name=e2e-cqpbn-daemon-set" label 01/19/23 05:08:59.592
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-cqpbn-daemon-set" 01/19/23 05:08:59.597
Jan 19 05:08:59.599: INFO: Located ControllerRevision: "e2e-cqpbn-daemon-set-6bf65cd459"
STEP: Patching ControllerRevision "e2e-cqpbn-daemon-set-6bf65cd459" 01/19/23 05:08:59.601
Jan 19 05:08:59.608: INFO: e2e-cqpbn-daemon-set-6bf65cd459 has been patched
STEP: Create a new ControllerRevision 01/19/23 05:08:59.608
Jan 19 05:08:59.613: INFO: Created ControllerRevision: e2e-cqpbn-daemon-set-5956fcfb99
STEP: Confirm that there are two ControllerRevisions 01/19/23 05:08:59.613
Jan 19 05:08:59.613: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 19 05:08:59.615: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-cqpbn-daemon-set-6bf65cd459" 01/19/23 05:08:59.615
STEP: Confirm that there is only one ControllerRevision 01/19/23 05:08:59.621
Jan 19 05:08:59.621: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 19 05:08:59.624: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-cqpbn-daemon-set-5956fcfb99" 01/19/23 05:08:59.625
Jan 19 05:08:59.632: INFO: e2e-cqpbn-daemon-set-5956fcfb99 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 01/19/23 05:08:59.632
W0119 05:08:59.643301      18 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 01/19/23 05:08:59.643
Jan 19 05:08:59.643: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 19 05:09:00.649: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 19 05:09:00.653: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-cqpbn-daemon-set-5956fcfb99=updated" 01/19/23 05:09:00.653
STEP: Confirm that there is only one ControllerRevision 01/19/23 05:09:00.661
Jan 19 05:09:00.661: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 19 05:09:00.664: INFO: Found 1 ControllerRevisions
Jan 19 05:09:00.667: INFO: ControllerRevision "e2e-cqpbn-daemon-set-75cfdf6b58" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-cqpbn-daemon-set" 01/19/23 05:09:00.67
STEP: deleting DaemonSet.extensions e2e-cqpbn-daemon-set in namespace controllerrevisions-6431, will wait for the garbage collector to delete the pods 01/19/23 05:09:00.67
Jan 19 05:09:00.733: INFO: Deleting DaemonSet.extensions e2e-cqpbn-daemon-set took: 8.519684ms
Jan 19 05:09:00.834: INFO: Terminating DaemonSet.extensions e2e-cqpbn-daemon-set pods took: 100.142035ms
Jan 19 05:09:01.738: INFO: Number of nodes with available pods controlled by daemonset e2e-cqpbn-daemon-set: 0
Jan 19 05:09:01.738: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-cqpbn-daemon-set
Jan 19 05:09:01.740: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"23980"},"items":null}

Jan 19 05:09:01.742: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"23980"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:187
Jan 19 05:09:01.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "controllerrevisions-6431" for this suite. 01/19/23 05:09:01.752
{"msg":"PASSED [sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]","completed":209,"skipped":3923,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.216 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:08:55.541
    Jan 19 05:08:55.541: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename controllerrevisions 01/19/23 05:08:55.541
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:08:55.552
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:08:55.555
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-cqpbn-daemon-set" 01/19/23 05:08:55.569
    STEP: Check that daemon pods launch on every node of the cluster. 01/19/23 05:08:55.574
    Jan 19 05:08:55.582: INFO: Number of nodes with available pods controlled by daemonset e2e-cqpbn-daemon-set: 0
    Jan 19 05:08:55.582: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
    Jan 19 05:08:56.589: INFO: Number of nodes with available pods controlled by daemonset e2e-cqpbn-daemon-set: 0
    Jan 19 05:08:56.589: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
    Jan 19 05:08:57.588: INFO: Number of nodes with available pods controlled by daemonset e2e-cqpbn-daemon-set: 0
    Jan 19 05:08:57.588: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
    Jan 19 05:08:58.589: INFO: Number of nodes with available pods controlled by daemonset e2e-cqpbn-daemon-set: 1
    Jan 19 05:08:58.589: INFO: Node ckcp-nks-default-worker-node-1 is running 0 daemon pod, expected 1
    Jan 19 05:08:59.590: INFO: Number of nodes with available pods controlled by daemonset e2e-cqpbn-daemon-set: 2
    Jan 19 05:08:59.590: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-cqpbn-daemon-set
    STEP: Confirm DaemonSet "e2e-cqpbn-daemon-set" successfully created with "daemonset-name=e2e-cqpbn-daemon-set" label 01/19/23 05:08:59.592
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-cqpbn-daemon-set" 01/19/23 05:08:59.597
    Jan 19 05:08:59.599: INFO: Located ControllerRevision: "e2e-cqpbn-daemon-set-6bf65cd459"
    STEP: Patching ControllerRevision "e2e-cqpbn-daemon-set-6bf65cd459" 01/19/23 05:08:59.601
    Jan 19 05:08:59.608: INFO: e2e-cqpbn-daemon-set-6bf65cd459 has been patched
    STEP: Create a new ControllerRevision 01/19/23 05:08:59.608
    Jan 19 05:08:59.613: INFO: Created ControllerRevision: e2e-cqpbn-daemon-set-5956fcfb99
    STEP: Confirm that there are two ControllerRevisions 01/19/23 05:08:59.613
    Jan 19 05:08:59.613: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 19 05:08:59.615: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-cqpbn-daemon-set-6bf65cd459" 01/19/23 05:08:59.615
    STEP: Confirm that there is only one ControllerRevision 01/19/23 05:08:59.621
    Jan 19 05:08:59.621: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 19 05:08:59.624: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-cqpbn-daemon-set-5956fcfb99" 01/19/23 05:08:59.625
    Jan 19 05:08:59.632: INFO: e2e-cqpbn-daemon-set-5956fcfb99 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 01/19/23 05:08:59.632
    W0119 05:08:59.643301      18 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 01/19/23 05:08:59.643
    Jan 19 05:08:59.643: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 19 05:09:00.649: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 19 05:09:00.653: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-cqpbn-daemon-set-5956fcfb99=updated" 01/19/23 05:09:00.653
    STEP: Confirm that there is only one ControllerRevision 01/19/23 05:09:00.661
    Jan 19 05:09:00.661: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 19 05:09:00.664: INFO: Found 1 ControllerRevisions
    Jan 19 05:09:00.667: INFO: ControllerRevision "e2e-cqpbn-daemon-set-75cfdf6b58" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-cqpbn-daemon-set" 01/19/23 05:09:00.67
    STEP: deleting DaemonSet.extensions e2e-cqpbn-daemon-set in namespace controllerrevisions-6431, will wait for the garbage collector to delete the pods 01/19/23 05:09:00.67
    Jan 19 05:09:00.733: INFO: Deleting DaemonSet.extensions e2e-cqpbn-daemon-set took: 8.519684ms
    Jan 19 05:09:00.834: INFO: Terminating DaemonSet.extensions e2e-cqpbn-daemon-set pods took: 100.142035ms
    Jan 19 05:09:01.738: INFO: Number of nodes with available pods controlled by daemonset e2e-cqpbn-daemon-set: 0
    Jan 19 05:09:01.738: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-cqpbn-daemon-set
    Jan 19 05:09:01.740: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"23980"},"items":null}

    Jan 19 05:09:01.742: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"23980"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:187
    Jan 19 05:09:01.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "controllerrevisions-6431" for this suite. 01/19/23 05:09:01.752
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:09:01.758
Jan 19 05:09:01.758: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename webhook 01/19/23 05:09:01.758
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:09:01.771
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:09:01.773
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/19/23 05:09:01.784
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 05:09:02.049
STEP: Deploying the webhook pod 01/19/23 05:09:02.057
STEP: Wait for the deployment to be ready 01/19/23 05:09:02.069
Jan 19 05:09:02.077: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 19 05:09:04.087: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 5, 9, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 9, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 9, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 9, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/19/23 05:09:06.091
STEP: Verifying the service has paired with the endpoint 01/19/23 05:09:06.101
Jan 19 05:09:07.101: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
STEP: fetching the /apis discovery document 01/19/23 05:09:07.105
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 01/19/23 05:09:07.106
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 01/19/23 05:09:07.106
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 01/19/23 05:09:07.106
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 01/19/23 05:09:07.107
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 01/19/23 05:09:07.107
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 01/19/23 05:09:07.108
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 05:09:07.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7918" for this suite. 01/19/23 05:09:07.111
STEP: Destroying namespace "webhook-7918-markers" for this suite. 01/19/23 05:09:07.117
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","completed":210,"skipped":3933,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.398 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:09:01.758
    Jan 19 05:09:01.758: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename webhook 01/19/23 05:09:01.758
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:09:01.771
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:09:01.773
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/19/23 05:09:01.784
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 05:09:02.049
    STEP: Deploying the webhook pod 01/19/23 05:09:02.057
    STEP: Wait for the deployment to be ready 01/19/23 05:09:02.069
    Jan 19 05:09:02.077: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 19 05:09:04.087: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 5, 9, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 9, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 9, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 9, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/19/23 05:09:06.091
    STEP: Verifying the service has paired with the endpoint 01/19/23 05:09:06.101
    Jan 19 05:09:07.101: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:116
    STEP: fetching the /apis discovery document 01/19/23 05:09:07.105
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 01/19/23 05:09:07.106
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 01/19/23 05:09:07.106
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 01/19/23 05:09:07.106
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 01/19/23 05:09:07.107
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 01/19/23 05:09:07.107
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 01/19/23 05:09:07.108
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 05:09:07.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7918" for this suite. 01/19/23 05:09:07.111
    STEP: Destroying namespace "webhook-7918-markers" for this suite. 01/19/23 05:09:07.117
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:09:07.156
Jan 19 05:09:07.157: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename kubectl 01/19/23 05:09:07.157
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:09:07.172
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:09:07.175
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
STEP: creating Agnhost RC 01/19/23 05:09:07.177
Jan 19 05:09:07.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-3621 create -f -'
Jan 19 05:09:07.684: INFO: stderr: ""
Jan 19 05:09:07.684: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/19/23 05:09:07.684
Jan 19 05:09:08.690: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 19 05:09:08.690: INFO: Found 0 / 1
Jan 19 05:09:09.688: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 19 05:09:09.688: INFO: Found 0 / 1
Jan 19 05:09:10.688: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 19 05:09:10.688: INFO: Found 1 / 1
Jan 19 05:09:10.688: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 01/19/23 05:09:10.688
Jan 19 05:09:10.692: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 19 05:09:10.692: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 19 05:09:10.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-3621 patch pod agnhost-primary-vdllc -p {"metadata":{"annotations":{"x":"y"}}}'
Jan 19 05:09:10.837: INFO: stderr: ""
Jan 19 05:09:10.837: INFO: stdout: "pod/agnhost-primary-vdllc patched\n"
STEP: checking annotations 01/19/23 05:09:10.837
Jan 19 05:09:10.841: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 19 05:09:10.841: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 19 05:09:10.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3621" for this suite. 01/19/23 05:09:10.845
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","completed":211,"skipped":3956,"failed":0}
------------------------------
â€¢ [3.693 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1644
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:09:07.156
    Jan 19 05:09:07.157: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename kubectl 01/19/23 05:09:07.157
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:09:07.172
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:09:07.175
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1650
    STEP: creating Agnhost RC 01/19/23 05:09:07.177
    Jan 19 05:09:07.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-3621 create -f -'
    Jan 19 05:09:07.684: INFO: stderr: ""
    Jan 19 05:09:07.684: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/19/23 05:09:07.684
    Jan 19 05:09:08.690: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 19 05:09:08.690: INFO: Found 0 / 1
    Jan 19 05:09:09.688: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 19 05:09:09.688: INFO: Found 0 / 1
    Jan 19 05:09:10.688: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 19 05:09:10.688: INFO: Found 1 / 1
    Jan 19 05:09:10.688: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 01/19/23 05:09:10.688
    Jan 19 05:09:10.692: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 19 05:09:10.692: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 19 05:09:10.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-3621 patch pod agnhost-primary-vdllc -p {"metadata":{"annotations":{"x":"y"}}}'
    Jan 19 05:09:10.837: INFO: stderr: ""
    Jan 19 05:09:10.837: INFO: stdout: "pod/agnhost-primary-vdllc patched\n"
    STEP: checking annotations 01/19/23 05:09:10.837
    Jan 19 05:09:10.841: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 19 05:09:10.841: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 19 05:09:10.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3621" for this suite. 01/19/23 05:09:10.845
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:09:10.85
Jan 19 05:09:10.850: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename subpath 01/19/23 05:09:10.851
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:09:10.87
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:09:10.875
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/19/23 05:09:10.876
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-nvkv 01/19/23 05:09:10.885
STEP: Creating a pod to test atomic-volume-subpath 01/19/23 05:09:10.885
Jan 19 05:09:10.893: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-nvkv" in namespace "subpath-860" to be "Succeeded or Failed"
Jan 19 05:09:10.898: INFO: Pod "pod-subpath-test-projected-nvkv": Phase="Pending", Reason="", readiness=false. Elapsed: 5.294619ms
Jan 19 05:09:12.903: INFO: Pod "pod-subpath-test-projected-nvkv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009997476s
Jan 19 05:09:14.903: INFO: Pod "pod-subpath-test-projected-nvkv": Phase="Running", Reason="", readiness=true. Elapsed: 4.009678859s
Jan 19 05:09:16.903: INFO: Pod "pod-subpath-test-projected-nvkv": Phase="Running", Reason="", readiness=true. Elapsed: 6.009812255s
Jan 19 05:09:18.904: INFO: Pod "pod-subpath-test-projected-nvkv": Phase="Running", Reason="", readiness=true. Elapsed: 8.010759356s
Jan 19 05:09:20.902: INFO: Pod "pod-subpath-test-projected-nvkv": Phase="Running", Reason="", readiness=true. Elapsed: 10.009390882s
Jan 19 05:09:22.903: INFO: Pod "pod-subpath-test-projected-nvkv": Phase="Running", Reason="", readiness=true. Elapsed: 12.009635304s
Jan 19 05:09:24.902: INFO: Pod "pod-subpath-test-projected-nvkv": Phase="Running", Reason="", readiness=true. Elapsed: 14.008649294s
Jan 19 05:09:26.902: INFO: Pod "pod-subpath-test-projected-nvkv": Phase="Running", Reason="", readiness=true. Elapsed: 16.009419393s
Jan 19 05:09:28.902: INFO: Pod "pod-subpath-test-projected-nvkv": Phase="Running", Reason="", readiness=true. Elapsed: 18.008992337s
Jan 19 05:09:30.903: INFO: Pod "pod-subpath-test-projected-nvkv": Phase="Running", Reason="", readiness=true. Elapsed: 20.00960091s
Jan 19 05:09:32.902: INFO: Pod "pod-subpath-test-projected-nvkv": Phase="Running", Reason="", readiness=true. Elapsed: 22.008658397s
Jan 19 05:09:34.903: INFO: Pod "pod-subpath-test-projected-nvkv": Phase="Running", Reason="", readiness=false. Elapsed: 24.009471158s
Jan 19 05:09:36.903: INFO: Pod "pod-subpath-test-projected-nvkv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.009983022s
STEP: Saw pod success 01/19/23 05:09:36.903
Jan 19 05:09:36.903: INFO: Pod "pod-subpath-test-projected-nvkv" satisfied condition "Succeeded or Failed"
Jan 19 05:09:36.907: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-subpath-test-projected-nvkv container test-container-subpath-projected-nvkv: <nil>
STEP: delete the pod 01/19/23 05:09:36.939
Jan 19 05:09:36.952: INFO: Waiting for pod pod-subpath-test-projected-nvkv to disappear
Jan 19 05:09:36.955: INFO: Pod pod-subpath-test-projected-nvkv no longer exists
STEP: Deleting pod pod-subpath-test-projected-nvkv 01/19/23 05:09:36.955
Jan 19 05:09:36.955: INFO: Deleting pod "pod-subpath-test-projected-nvkv" in namespace "subpath-860"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan 19 05:09:36.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-860" for this suite. 01/19/23 05:09:36.961
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","completed":212,"skipped":3967,"failed":0}
------------------------------
â€¢ [SLOW TEST] [26.117 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:09:10.85
    Jan 19 05:09:10.850: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename subpath 01/19/23 05:09:10.851
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:09:10.87
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:09:10.875
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/19/23 05:09:10.876
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-nvkv 01/19/23 05:09:10.885
    STEP: Creating a pod to test atomic-volume-subpath 01/19/23 05:09:10.885
    Jan 19 05:09:10.893: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-nvkv" in namespace "subpath-860" to be "Succeeded or Failed"
    Jan 19 05:09:10.898: INFO: Pod "pod-subpath-test-projected-nvkv": Phase="Pending", Reason="", readiness=false. Elapsed: 5.294619ms
    Jan 19 05:09:12.903: INFO: Pod "pod-subpath-test-projected-nvkv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009997476s
    Jan 19 05:09:14.903: INFO: Pod "pod-subpath-test-projected-nvkv": Phase="Running", Reason="", readiness=true. Elapsed: 4.009678859s
    Jan 19 05:09:16.903: INFO: Pod "pod-subpath-test-projected-nvkv": Phase="Running", Reason="", readiness=true. Elapsed: 6.009812255s
    Jan 19 05:09:18.904: INFO: Pod "pod-subpath-test-projected-nvkv": Phase="Running", Reason="", readiness=true. Elapsed: 8.010759356s
    Jan 19 05:09:20.902: INFO: Pod "pod-subpath-test-projected-nvkv": Phase="Running", Reason="", readiness=true. Elapsed: 10.009390882s
    Jan 19 05:09:22.903: INFO: Pod "pod-subpath-test-projected-nvkv": Phase="Running", Reason="", readiness=true. Elapsed: 12.009635304s
    Jan 19 05:09:24.902: INFO: Pod "pod-subpath-test-projected-nvkv": Phase="Running", Reason="", readiness=true. Elapsed: 14.008649294s
    Jan 19 05:09:26.902: INFO: Pod "pod-subpath-test-projected-nvkv": Phase="Running", Reason="", readiness=true. Elapsed: 16.009419393s
    Jan 19 05:09:28.902: INFO: Pod "pod-subpath-test-projected-nvkv": Phase="Running", Reason="", readiness=true. Elapsed: 18.008992337s
    Jan 19 05:09:30.903: INFO: Pod "pod-subpath-test-projected-nvkv": Phase="Running", Reason="", readiness=true. Elapsed: 20.00960091s
    Jan 19 05:09:32.902: INFO: Pod "pod-subpath-test-projected-nvkv": Phase="Running", Reason="", readiness=true. Elapsed: 22.008658397s
    Jan 19 05:09:34.903: INFO: Pod "pod-subpath-test-projected-nvkv": Phase="Running", Reason="", readiness=false. Elapsed: 24.009471158s
    Jan 19 05:09:36.903: INFO: Pod "pod-subpath-test-projected-nvkv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.009983022s
    STEP: Saw pod success 01/19/23 05:09:36.903
    Jan 19 05:09:36.903: INFO: Pod "pod-subpath-test-projected-nvkv" satisfied condition "Succeeded or Failed"
    Jan 19 05:09:36.907: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-subpath-test-projected-nvkv container test-container-subpath-projected-nvkv: <nil>
    STEP: delete the pod 01/19/23 05:09:36.939
    Jan 19 05:09:36.952: INFO: Waiting for pod pod-subpath-test-projected-nvkv to disappear
    Jan 19 05:09:36.955: INFO: Pod pod-subpath-test-projected-nvkv no longer exists
    STEP: Deleting pod pod-subpath-test-projected-nvkv 01/19/23 05:09:36.955
    Jan 19 05:09:36.955: INFO: Deleting pod "pod-subpath-test-projected-nvkv" in namespace "subpath-860"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan 19 05:09:36.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-860" for this suite. 01/19/23 05:09:36.961
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:09:36.967
Jan 19 05:09:36.967: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename dns 01/19/23 05:09:36.968
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:09:36.98
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:09:36.983
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 01/19/23 05:09:36.985
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 01/19/23 05:09:36.986
STEP: creating a pod to probe DNS 01/19/23 05:09:36.986
STEP: submitting the pod to kubernetes 01/19/23 05:09:36.986
Jan 19 05:09:36.995: INFO: Waiting up to 15m0s for pod "dns-test-23ef2dfd-fbac-493c-9436-4107508fe7e8" in namespace "dns-4064" to be "running"
Jan 19 05:09:36.999: INFO: Pod "dns-test-23ef2dfd-fbac-493c-9436-4107508fe7e8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028746ms
Jan 19 05:09:39.003: INFO: Pod "dns-test-23ef2dfd-fbac-493c-9436-4107508fe7e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008822331s
Jan 19 05:09:41.004: INFO: Pod "dns-test-23ef2dfd-fbac-493c-9436-4107508fe7e8": Phase="Running", Reason="", readiness=true. Elapsed: 4.009136017s
Jan 19 05:09:41.004: INFO: Pod "dns-test-23ef2dfd-fbac-493c-9436-4107508fe7e8" satisfied condition "running"
STEP: retrieving the pod 01/19/23 05:09:41.004
STEP: looking for the results for each expected name from probers 01/19/23 05:09:41.007
Jan 19 05:09:41.015: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod dns-4064/dns-test-23ef2dfd-fbac-493c-9436-4107508fe7e8: the server could not find the requested resource (get pods dns-test-23ef2dfd-fbac-493c-9436-4107508fe7e8)
Jan 19 05:09:41.021: INFO: Unable to read jessie_tcp@kubernetes.default.svc.cluster.local from pod dns-4064/dns-test-23ef2dfd-fbac-493c-9436-4107508fe7e8: the server could not find the requested resource (get pods dns-test-23ef2dfd-fbac-493c-9436-4107508fe7e8)
Jan 19 05:09:41.021: INFO: Lookups using dns-4064/dns-test-23ef2dfd-fbac-493c-9436-4107508fe7e8 failed for: [wheezy_tcp@kubernetes.default.svc.cluster.local jessie_tcp@kubernetes.default.svc.cluster.local]

Jan 19 05:09:46.029: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod dns-4064/dns-test-23ef2dfd-fbac-493c-9436-4107508fe7e8: the server could not find the requested resource (get pods dns-test-23ef2dfd-fbac-493c-9436-4107508fe7e8)
Jan 19 05:09:46.034: INFO: Lookups using dns-4064/dns-test-23ef2dfd-fbac-493c-9436-4107508fe7e8 failed for: [wheezy_tcp@kubernetes.default.svc.cluster.local]

Jan 19 05:09:51.034: INFO: DNS probes using dns-4064/dns-test-23ef2dfd-fbac-493c-9436-4107508fe7e8 succeeded

STEP: deleting the pod 01/19/23 05:09:51.034
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 19 05:09:51.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4064" for this suite. 01/19/23 05:09:51.051
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","completed":213,"skipped":3967,"failed":0}
------------------------------
â€¢ [SLOW TEST] [14.091 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:09:36.967
    Jan 19 05:09:36.967: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename dns 01/19/23 05:09:36.968
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:09:36.98
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:09:36.983
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     01/19/23 05:09:36.985
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     01/19/23 05:09:36.986
    STEP: creating a pod to probe DNS 01/19/23 05:09:36.986
    STEP: submitting the pod to kubernetes 01/19/23 05:09:36.986
    Jan 19 05:09:36.995: INFO: Waiting up to 15m0s for pod "dns-test-23ef2dfd-fbac-493c-9436-4107508fe7e8" in namespace "dns-4064" to be "running"
    Jan 19 05:09:36.999: INFO: Pod "dns-test-23ef2dfd-fbac-493c-9436-4107508fe7e8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028746ms
    Jan 19 05:09:39.003: INFO: Pod "dns-test-23ef2dfd-fbac-493c-9436-4107508fe7e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008822331s
    Jan 19 05:09:41.004: INFO: Pod "dns-test-23ef2dfd-fbac-493c-9436-4107508fe7e8": Phase="Running", Reason="", readiness=true. Elapsed: 4.009136017s
    Jan 19 05:09:41.004: INFO: Pod "dns-test-23ef2dfd-fbac-493c-9436-4107508fe7e8" satisfied condition "running"
    STEP: retrieving the pod 01/19/23 05:09:41.004
    STEP: looking for the results for each expected name from probers 01/19/23 05:09:41.007
    Jan 19 05:09:41.015: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod dns-4064/dns-test-23ef2dfd-fbac-493c-9436-4107508fe7e8: the server could not find the requested resource (get pods dns-test-23ef2dfd-fbac-493c-9436-4107508fe7e8)
    Jan 19 05:09:41.021: INFO: Unable to read jessie_tcp@kubernetes.default.svc.cluster.local from pod dns-4064/dns-test-23ef2dfd-fbac-493c-9436-4107508fe7e8: the server could not find the requested resource (get pods dns-test-23ef2dfd-fbac-493c-9436-4107508fe7e8)
    Jan 19 05:09:41.021: INFO: Lookups using dns-4064/dns-test-23ef2dfd-fbac-493c-9436-4107508fe7e8 failed for: [wheezy_tcp@kubernetes.default.svc.cluster.local jessie_tcp@kubernetes.default.svc.cluster.local]

    Jan 19 05:09:46.029: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod dns-4064/dns-test-23ef2dfd-fbac-493c-9436-4107508fe7e8: the server could not find the requested resource (get pods dns-test-23ef2dfd-fbac-493c-9436-4107508fe7e8)
    Jan 19 05:09:46.034: INFO: Lookups using dns-4064/dns-test-23ef2dfd-fbac-493c-9436-4107508fe7e8 failed for: [wheezy_tcp@kubernetes.default.svc.cluster.local]

    Jan 19 05:09:51.034: INFO: DNS probes using dns-4064/dns-test-23ef2dfd-fbac-493c-9436-4107508fe7e8 succeeded

    STEP: deleting the pod 01/19/23 05:09:51.034
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 19 05:09:51.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-4064" for this suite. 01/19/23 05:09:51.051
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:09:51.059
Jan 19 05:09:51.059: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename replicaset 01/19/23 05:09:51.06
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:09:51.075
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:09:51.077
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 01/19/23 05:09:51.081
STEP: Verify that the required pods have come up 01/19/23 05:09:51.088
Jan 19 05:09:51.090: INFO: Pod name sample-pod: Found 0 pods out of 3
Jan 19 05:09:56.093: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 01/19/23 05:09:56.093
Jan 19 05:09:56.096: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 01/19/23 05:09:56.096
STEP: DeleteCollection of the ReplicaSets 01/19/23 05:09:56.099
STEP: After DeleteCollection verify that ReplicaSets have been deleted 01/19/23 05:09:56.106
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 19 05:09:56.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8069" for this suite. 01/19/23 05:09:56.14
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","completed":214,"skipped":3996,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.096 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:09:51.059
    Jan 19 05:09:51.059: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename replicaset 01/19/23 05:09:51.06
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:09:51.075
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:09:51.077
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 01/19/23 05:09:51.081
    STEP: Verify that the required pods have come up 01/19/23 05:09:51.088
    Jan 19 05:09:51.090: INFO: Pod name sample-pod: Found 0 pods out of 3
    Jan 19 05:09:56.093: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 01/19/23 05:09:56.093
    Jan 19 05:09:56.096: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 01/19/23 05:09:56.096
    STEP: DeleteCollection of the ReplicaSets 01/19/23 05:09:56.099
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 01/19/23 05:09:56.106
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 19 05:09:56.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-8069" for this suite. 01/19/23 05:09:56.14
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:09:56.155
Jan 19 05:09:56.155: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename limitrange 01/19/23 05:09:56.156
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:09:56.18
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:09:56.184
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
STEP: Creating a LimitRange 01/19/23 05:09:56.187
STEP: Setting up watch 01/19/23 05:09:56.187
STEP: Submitting a LimitRange 01/19/23 05:09:56.29
STEP: Verifying LimitRange creation was observed 01/19/23 05:09:56.308
STEP: Fetching the LimitRange to ensure it has proper values 01/19/23 05:09:56.308
Jan 19 05:09:56.312: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 19 05:09:56.312: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 01/19/23 05:09:56.312
STEP: Ensuring Pod has resource requirements applied from LimitRange 01/19/23 05:09:56.32
Jan 19 05:09:56.328: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 19 05:09:56.328: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 01/19/23 05:09:56.328
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 01/19/23 05:09:56.334
Jan 19 05:09:56.341: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jan 19 05:09:56.341: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 01/19/23 05:09:56.341
STEP: Failing to create a Pod with more than max resources 01/19/23 05:09:56.346
STEP: Updating a LimitRange 01/19/23 05:09:56.349
STEP: Verifying LimitRange updating is effective 01/19/23 05:09:56.354
STEP: Creating a Pod with less than former min resources 01/19/23 05:09:58.358
STEP: Failing to create a Pod with more than max resources 01/19/23 05:09:58.367
STEP: Deleting a LimitRange 01/19/23 05:09:58.369
STEP: Verifying the LimitRange was deleted 01/19/23 05:09:58.379
Jan 19 05:10:03.384: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 01/19/23 05:10:03.384
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
Jan 19 05:10:03.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-1786" for this suite. 01/19/23 05:10:03.402
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","completed":215,"skipped":4018,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.251 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:09:56.155
    Jan 19 05:09:56.155: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename limitrange 01/19/23 05:09:56.156
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:09:56.18
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:09:56.184
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:57
    STEP: Creating a LimitRange 01/19/23 05:09:56.187
    STEP: Setting up watch 01/19/23 05:09:56.187
    STEP: Submitting a LimitRange 01/19/23 05:09:56.29
    STEP: Verifying LimitRange creation was observed 01/19/23 05:09:56.308
    STEP: Fetching the LimitRange to ensure it has proper values 01/19/23 05:09:56.308
    Jan 19 05:09:56.312: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jan 19 05:09:56.312: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 01/19/23 05:09:56.312
    STEP: Ensuring Pod has resource requirements applied from LimitRange 01/19/23 05:09:56.32
    Jan 19 05:09:56.328: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jan 19 05:09:56.328: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 01/19/23 05:09:56.328
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 01/19/23 05:09:56.334
    Jan 19 05:09:56.341: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Jan 19 05:09:56.341: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 01/19/23 05:09:56.341
    STEP: Failing to create a Pod with more than max resources 01/19/23 05:09:56.346
    STEP: Updating a LimitRange 01/19/23 05:09:56.349
    STEP: Verifying LimitRange updating is effective 01/19/23 05:09:56.354
    STEP: Creating a Pod with less than former min resources 01/19/23 05:09:58.358
    STEP: Failing to create a Pod with more than max resources 01/19/23 05:09:58.367
    STEP: Deleting a LimitRange 01/19/23 05:09:58.369
    STEP: Verifying the LimitRange was deleted 01/19/23 05:09:58.379
    Jan 19 05:10:03.384: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 01/19/23 05:10:03.384
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:187
    Jan 19 05:10:03.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "limitrange-1786" for this suite. 01/19/23 05:10:03.402
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:10:03.407
Jan 19 05:10:03.407: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename runtimeclass 01/19/23 05:10:03.408
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:10:03.419
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:10:03.421
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Jan 19 05:10:03.433: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-8403 to be scheduled
Jan 19 05:10:03.436: INFO: 1 pods are not scheduled: [runtimeclass-8403/test-runtimeclass-runtimeclass-8403-preconfigured-handler-p44dw(e02b31e1-6b9b-4373-bf1b-83fa7e9651f1)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan 19 05:10:05.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-8403" for this suite. 01/19/23 05:10:05.45
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","completed":216,"skipped":4043,"failed":0}
------------------------------
â€¢ [2.048 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:10:03.407
    Jan 19 05:10:03.407: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename runtimeclass 01/19/23 05:10:03.408
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:10:03.419
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:10:03.421
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Jan 19 05:10:03.433: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-8403 to be scheduled
    Jan 19 05:10:03.436: INFO: 1 pods are not scheduled: [runtimeclass-8403/test-runtimeclass-runtimeclass-8403-preconfigured-handler-p44dw(e02b31e1-6b9b-4373-bf1b-83fa7e9651f1)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan 19 05:10:05.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-8403" for this suite. 01/19/23 05:10:05.45
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:10:05.455
Jan 19 05:10:05.455: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename deployment 01/19/23 05:10:05.456
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:10:05.469
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:10:05.474
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Jan 19 05:10:05.478: INFO: Creating deployment "webserver-deployment"
Jan 19 05:10:05.485: INFO: Waiting for observed generation 1
Jan 19 05:10:07.494: INFO: Waiting for all required pods to come up
Jan 19 05:10:07.502: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 01/19/23 05:10:07.502
Jan 19 05:10:07.502: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-xx2vf" in namespace "deployment-3693" to be "running"
Jan 19 05:10:07.502: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-228sr" in namespace "deployment-3693" to be "running"
Jan 19 05:10:07.502: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-2kfxc" in namespace "deployment-3693" to be "running"
Jan 19 05:10:07.502: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-2m4nq" in namespace "deployment-3693" to be "running"
Jan 19 05:10:07.502: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-7jvv4" in namespace "deployment-3693" to be "running"
Jan 19 05:10:07.503: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-88s5j" in namespace "deployment-3693" to be "running"
Jan 19 05:10:07.503: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-9zckc" in namespace "deployment-3693" to be "running"
Jan 19 05:10:07.503: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-cltzq" in namespace "deployment-3693" to be "running"
Jan 19 05:10:07.503: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-grjrz" in namespace "deployment-3693" to be "running"
Jan 19 05:10:07.503: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-qhcn4" in namespace "deployment-3693" to be "running"
Jan 19 05:10:07.508: INFO: Pod "webserver-deployment-845c8977d9-xx2vf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.443291ms
Jan 19 05:10:07.508: INFO: Pod "webserver-deployment-845c8977d9-2kfxc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.51402ms
Jan 19 05:10:07.510: INFO: Pod "webserver-deployment-845c8977d9-9zckc": Phase="Pending", Reason="", readiness=false. Elapsed: 7.005968ms
Jan 19 05:10:07.510: INFO: Pod "webserver-deployment-845c8977d9-cltzq": Phase="Pending", Reason="", readiness=false. Elapsed: 7.056774ms
Jan 19 05:10:07.511: INFO: Pod "webserver-deployment-845c8977d9-grjrz": Phase="Pending", Reason="", readiness=false. Elapsed: 8.435992ms
Jan 19 05:10:07.511: INFO: Pod "webserver-deployment-845c8977d9-7jvv4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.765345ms
Jan 19 05:10:07.511: INFO: Pod "webserver-deployment-845c8977d9-228sr": Phase="Pending", Reason="", readiness=false. Elapsed: 9.049864ms
Jan 19 05:10:07.511: INFO: Pod "webserver-deployment-845c8977d9-88s5j": Phase="Pending", Reason="", readiness=false. Elapsed: 8.797277ms
Jan 19 05:10:07.511: INFO: Pod "webserver-deployment-845c8977d9-qhcn4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.579129ms
Jan 19 05:10:07.511: INFO: Pod "webserver-deployment-845c8977d9-2m4nq": Phase="Pending", Reason="", readiness=false. Elapsed: 9.048889ms
Jan 19 05:10:09.524: INFO: Pod "webserver-deployment-845c8977d9-xx2vf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022313935s
Jan 19 05:10:09.524: INFO: Pod "webserver-deployment-845c8977d9-2kfxc": Phase="Running", Reason="", readiness=true. Elapsed: 2.022150193s
Jan 19 05:10:09.525: INFO: Pod "webserver-deployment-845c8977d9-2kfxc" satisfied condition "running"
Jan 19 05:10:09.528: INFO: Pod "webserver-deployment-845c8977d9-cltzq": Phase="Running", Reason="", readiness=true. Elapsed: 2.025090623s
Jan 19 05:10:09.528: INFO: Pod "webserver-deployment-845c8977d9-cltzq" satisfied condition "running"
Jan 19 05:10:09.529: INFO: Pod "webserver-deployment-845c8977d9-qhcn4": Phase="Running", Reason="", readiness=true. Elapsed: 2.02608792s
Jan 19 05:10:09.529: INFO: Pod "webserver-deployment-845c8977d9-qhcn4" satisfied condition "running"
Jan 19 05:10:09.530: INFO: Pod "webserver-deployment-845c8977d9-9zckc": Phase="Running", Reason="", readiness=true. Elapsed: 2.027884475s
Jan 19 05:10:09.531: INFO: Pod "webserver-deployment-845c8977d9-9zckc" satisfied condition "running"
Jan 19 05:10:09.531: INFO: Pod "webserver-deployment-845c8977d9-88s5j": Phase="Running", Reason="", readiness=true. Elapsed: 2.028171984s
Jan 19 05:10:09.531: INFO: Pod "webserver-deployment-845c8977d9-88s5j" satisfied condition "running"
Jan 19 05:10:09.531: INFO: Pod "webserver-deployment-845c8977d9-7jvv4": Phase="Running", Reason="", readiness=true. Elapsed: 2.028300151s
Jan 19 05:10:09.531: INFO: Pod "webserver-deployment-845c8977d9-7jvv4" satisfied condition "running"
Jan 19 05:10:09.531: INFO: Pod "webserver-deployment-845c8977d9-grjrz": Phase="Running", Reason="", readiness=true. Elapsed: 2.02807196s
Jan 19 05:10:09.531: INFO: Pod "webserver-deployment-845c8977d9-grjrz" satisfied condition "running"
Jan 19 05:10:09.531: INFO: Pod "webserver-deployment-845c8977d9-228sr": Phase="Running", Reason="", readiness=true. Elapsed: 2.028595324s
Jan 19 05:10:09.531: INFO: Pod "webserver-deployment-845c8977d9-228sr" satisfied condition "running"
Jan 19 05:10:09.531: INFO: Pod "webserver-deployment-845c8977d9-2m4nq": Phase="Running", Reason="", readiness=true. Elapsed: 2.02848032s
Jan 19 05:10:09.531: INFO: Pod "webserver-deployment-845c8977d9-2m4nq" satisfied condition "running"
Jan 19 05:10:11.512: INFO: Pod "webserver-deployment-845c8977d9-xx2vf": Phase="Running", Reason="", readiness=true. Elapsed: 4.009586419s
Jan 19 05:10:11.512: INFO: Pod "webserver-deployment-845c8977d9-xx2vf" satisfied condition "running"
Jan 19 05:10:11.512: INFO: Waiting for deployment "webserver-deployment" to complete
Jan 19 05:10:11.516: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jan 19 05:10:11.526: INFO: Updating deployment webserver-deployment
Jan 19 05:10:11.526: INFO: Waiting for observed generation 2
Jan 19 05:10:13.534: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jan 19 05:10:13.548: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jan 19 05:10:13.551: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 19 05:10:13.564: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jan 19 05:10:13.564: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jan 19 05:10:13.567: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 19 05:10:13.572: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jan 19 05:10:13.572: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jan 19 05:10:13.585: INFO: Updating deployment webserver-deployment
Jan 19 05:10:13.585: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jan 19 05:10:13.604: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jan 19 05:10:15.617: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 19 05:10:15.623: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-3693  5e4ac302-4e1c-4bad-b033-326e9a10e571 24689 3 2023-01-19 05:10:05 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e8e4b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-19 05:10:13 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-01-19 05:10:13 +0000 UTC,LastTransitionTime:2023-01-19 05:10:05 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jan 19 05:10:15.626: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-3693  9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00 24688 3 2023-01-19 05:10:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 5e4ac302-4e1c-4bad-b033-326e9a10e571 0xc003e55677 0xc003e55678}] [] [{kube-controller-manager Update apps/v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e4ac302-4e1c-4bad-b033-326e9a10e571\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e55718 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 19 05:10:15.626: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jan 19 05:10:15.626: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-3693  6153ac80-678f-4cdc-a866-74114655f488 24671 3 2023-01-19 05:10:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 5e4ac302-4e1c-4bad-b033-326e9a10e571 0xc003e557c7 0xc003e557c8}] [] [{kube-controller-manager Update apps/v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e4ac302-4e1c-4bad-b033-326e9a10e571\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e55868 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jan 19 05:10:15.632: INFO: Pod "webserver-deployment-69b7448995-4cxgq" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-4cxgq webserver-deployment-69b7448995- deployment-3693  8a04257f-dc14-486b-a6b9-98eacb5360a4 24563 0 2023-01-19 05:10:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00 0xc003e55e37 0xc003e55e38}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 05:10:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9vzp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9vzp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.99,PodIP:,StartTime:2023-01-19 05:10:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 05:10:15.632: INFO: Pod "webserver-deployment-69b7448995-4gfzc" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-4gfzc webserver-deployment-69b7448995- deployment-3693  ceba659d-51b3-4b85-a1e7-fd3b29e57e8f 24655 0 2023-01-19 05:10:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00 0xc003f06027 0xc003f06028}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d9szn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d9szn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 05:10:15.632: INFO: Pod "webserver-deployment-69b7448995-5x8zw" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-5x8zw webserver-deployment-69b7448995- deployment-3693  cc7e8709-8a9f-40b6-ac8b-61b1effb9750 24670 0 2023-01-19 05:10:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00 0xc003f061c0 0xc003f061c1}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-474cn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-474cn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 05:10:15.633: INFO: Pod "webserver-deployment-69b7448995-6k4p4" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-6k4p4 webserver-deployment-69b7448995- deployment-3693  c864f935-cf23-4f8d-82aa-02244fdf8252 24631 0 2023-01-19 05:10:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00 0xc003f063d0 0xc003f063d1}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xdh5h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xdh5h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 05:10:15.633: INFO: Pod "webserver-deployment-69b7448995-77tl6" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-77tl6 webserver-deployment-69b7448995- deployment-3693  279ac440-c9f3-4721-a5f3-6a2bbbb3211e 24649 0 2023-01-19 05:10:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00 0xc003f065c0 0xc003f065c1}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pxddk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pxddk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.99,PodIP:,StartTime:2023-01-19 05:10:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 05:10:15.633: INFO: Pod "webserver-deployment-69b7448995-7m9cq" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-7m9cq webserver-deployment-69b7448995- deployment-3693  74ceeb0a-7a18-44ed-96a8-5ebff8b60929 24651 0 2023-01-19 05:10:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00 0xc003f06857 0xc003f06858}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nfqfj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nfqfj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.99,PodIP:,StartTime:2023-01-19 05:10:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 05:10:15.633: INFO: Pod "webserver-deployment-69b7448995-8h5gz" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-8h5gz webserver-deployment-69b7448995- deployment-3693  eec161d0-c5b9-4851-b2a0-6388c332e53a 24656 0 2023-01-19 05:10:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00 0xc003f06a67 0xc003f06a68}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fffqq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fffqq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 05:10:15.633: INFO: Pod "webserver-deployment-69b7448995-8k2wc" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-8k2wc webserver-deployment-69b7448995- deployment-3693  5956db5e-615a-4b62-a372-b672469936a2 24645 0 2023-01-19 05:10:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00 0xc003f06bd0 0xc003f06bd1}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ljbnf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ljbnf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 05:10:15.633: INFO: Pod "webserver-deployment-69b7448995-bnnqr" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-bnnqr webserver-deployment-69b7448995- deployment-3693  697d8afa-17ae-45db-8809-893df864a3c7 24561 0 2023-01-19 05:10:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00 0xc003f06d30 0xc003f06d31}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 05:10:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cb24x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cb24x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.78,PodIP:,StartTime:2023-01-19 05:10:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 05:10:15.633: INFO: Pod "webserver-deployment-69b7448995-hqvd6" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-hqvd6 webserver-deployment-69b7448995- deployment-3693  73d3ec4f-7cbe-4871-8dcb-66bcd9a99fd5 24552 0 2023-01-19 05:10:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00 0xc003f06f07 0xc003f06f08}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 05:10:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zv5nx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zv5nx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.78,PodIP:,StartTime:2023-01-19 05:10:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 05:10:15.634: INFO: Pod "webserver-deployment-69b7448995-rbbk8" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-rbbk8 webserver-deployment-69b7448995- deployment-3693  55b88ebd-4197-4f7a-b684-78039e118c3b 24595 0 2023-01-19 05:10:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00 0xc003f070e7 0xc003f070e8}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 05:10:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t8dsq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t8dsq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.99,PodIP:,StartTime:2023-01-19 05:10:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 05:10:15.634: INFO: Pod "webserver-deployment-69b7448995-sg9tz" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-sg9tz webserver-deployment-69b7448995- deployment-3693  a3be4393-6c21-4ee2-9358-26bc431dc2f6 24598 0 2023-01-19 05:10:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00 0xc003f072c7 0xc003f072c8}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 05:10:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2fnlb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2fnlb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.78,PodIP:,StartTime:2023-01-19 05:10:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 05:10:15.634: INFO: Pod "webserver-deployment-69b7448995-zwhf7" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-zwhf7 webserver-deployment-69b7448995- deployment-3693  048e8e0f-27e9-49f1-90ab-f8f0a9ce0a7b 24653 0 2023-01-19 05:10:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00 0xc003f074a7 0xc003f074a8}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q7jt9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q7jt9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 05:10:15.634: INFO: Pod "webserver-deployment-845c8977d9-228sr" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-228sr webserver-deployment-845c8977d9- deployment-3693  ed8508b9-beaf-4178-8198-69e194e696f2 24521 0 2023-01-19 05:10:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6153ac80-678f-4cdc-a866-74114655f488 0xc003f07650 0xc003f07651}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6153ac80-678f-4cdc-a866-74114655f488\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 05:10:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.70.84\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vr9ms,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vr9ms,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.78,PodIP:10.100.70.84,StartTime:2023-01-19 05:10:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 05:10:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://f5d28daa7c252395491852642c2b458c7cabcc2a43b9c980f32fcb1b3fa8d945,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.70.84,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 05:10:15.634: INFO: Pod "webserver-deployment-845c8977d9-2kfxc" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-2kfxc webserver-deployment-845c8977d9- deployment-3693  48a3193d-34e9-4d8c-b8e4-5d0c450d24d3 24469 0 2023-01-19 05:10:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6153ac80-678f-4cdc-a866-74114655f488 0xc003f07837 0xc003f07838}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6153ac80-678f-4cdc-a866-74114655f488\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 05:10:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.70.87\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x5jht,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x5jht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.78,PodIP:10.100.70.87,StartTime:2023-01-19 05:10:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 05:10:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://0b4fd17115558e4ebff7ef69667a9767289489d34b6245281df693e03ebefad7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.70.87,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 05:10:15.634: INFO: Pod "webserver-deployment-845c8977d9-2m4nq" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-2m4nq webserver-deployment-845c8977d9- deployment-3693  3d8db484-3b81-4b96-931a-9b0ab566df37 24493 0 2023-01-19 05:10:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6153ac80-678f-4cdc-a866-74114655f488 0xc003f07a97 0xc003f07a98}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6153ac80-678f-4cdc-a866-74114655f488\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 05:10:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.24.150\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z2vlw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z2vlw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.99,PodIP:10.100.24.150,StartTime:2023-01-19 05:10:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 05:10:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://492c484255108e82066fe5bc2dea26985041a8816256f1c9b6bb082aadac8bff,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.24.150,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 05:10:15.634: INFO: Pod "webserver-deployment-845c8977d9-7955f" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-7955f webserver-deployment-845c8977d9- deployment-3693  fe32279e-6d01-49d6-b2df-ea2fe260894d 24706 0 2023-01-19 05:10:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6153ac80-678f-4cdc-a866-74114655f488 0xc003f07cc7 0xc003f07cc8}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6153ac80-678f-4cdc-a866-74114655f488\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 05:10:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rndl2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rndl2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.99,PodIP:,StartTime:2023-01-19 05:10:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 05:10:15.634: INFO: Pod "webserver-deployment-845c8977d9-88s5j" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-88s5j webserver-deployment-845c8977d9- deployment-3693  193761da-155d-463f-8fad-3a66f21f31a8 24478 0 2023-01-19 05:10:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6153ac80-678f-4cdc-a866-74114655f488 0xc003f07f27 0xc003f07f28}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6153ac80-678f-4cdc-a866-74114655f488\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 05:10:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.70.88\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-97f9b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-97f9b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.78,PodIP:10.100.70.88,StartTime:2023-01-19 05:10:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 05:10:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://09cd817b183b360cf9195fd5d4851a9bbfec5a5c879726b4bef43aafdf4b8b6c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.70.88,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 05:10:15.635: INFO: Pod "webserver-deployment-845c8977d9-8g8sz" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-8g8sz webserver-deployment-845c8977d9- deployment-3693  09120396-aea1-4c92-94b1-83c567956807 24664 0 2023-01-19 05:10:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6153ac80-678f-4cdc-a866-74114655f488 0xc003f52167 0xc003f52168}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6153ac80-678f-4cdc-a866-74114655f488\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w2s5m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w2s5m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.99,PodIP:,StartTime:2023-01-19 05:10:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 05:10:15.635: INFO: Pod "webserver-deployment-845c8977d9-9vbpx" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-9vbpx webserver-deployment-845c8977d9- deployment-3693  21a9e0e8-81fb-4e9c-a289-667207516f5b 24693 0 2023-01-19 05:10:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6153ac80-678f-4cdc-a866-74114655f488 0xc003f523f7 0xc003f523f8}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6153ac80-678f-4cdc-a866-74114655f488\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hkwgn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hkwgn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.78,PodIP:,StartTime:2023-01-19 05:10:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 05:10:15.635: INFO: Pod "webserver-deployment-845c8977d9-9zckc" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-9zckc webserver-deployment-845c8977d9- deployment-3693  57da30d3-ceb9-44df-bb64-2c1460177a76 24489 0 2023-01-19 05:10:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6153ac80-678f-4cdc-a866-74114655f488 0xc003f52687 0xc003f52688}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6153ac80-678f-4cdc-a866-74114655f488\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 05:10:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.24.148\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dpw7w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dpw7w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.99,PodIP:10.100.24.148,StartTime:2023-01-19 05:10:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 05:10:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://132fff4ac88e5661c7663922c8a68e28643fde4d9b9ae66f6a1de31e36c4a6a6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.24.148,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 05:10:15.635: INFO: Pod "webserver-deployment-845c8977d9-bt2lp" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-bt2lp webserver-deployment-845c8977d9- deployment-3693  2e6761ce-5e38-437a-aad0-43a2c0bec032 24668 0 2023-01-19 05:10:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6153ac80-678f-4cdc-a866-74114655f488 0xc003f52907 0xc003f52908}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6153ac80-678f-4cdc-a866-74114655f488\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vtx2c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vtx2c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 05:10:15.635: INFO: Pod "webserver-deployment-845c8977d9-grjrz" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-grjrz webserver-deployment-845c8977d9- deployment-3693  708e6669-a177-4eb1-96ce-8e85a27fee51 24487 0 2023-01-19 05:10:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6153ac80-678f-4cdc-a866-74114655f488 0xc003f52a70 0xc003f52a71}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6153ac80-678f-4cdc-a866-74114655f488\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 05:10:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.24.149\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-59s99,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-59s99,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.99,PodIP:10.100.24.149,StartTime:2023-01-19 05:10:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 05:10:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://00576d8f32509ec82f1338a72c6d1bff8b86a564c9b99f42976edc07f3a53a48,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.24.149,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 05:10:15.635: INFO: Pod "webserver-deployment-845c8977d9-kmnnc" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-kmnnc webserver-deployment-845c8977d9- deployment-3693  aa686e7b-8634-4334-a430-bbcb3869a348 24634 0 2023-01-19 05:10:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6153ac80-678f-4cdc-a866-74114655f488 0xc003f52c57 0xc003f52c58}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6153ac80-678f-4cdc-a866-74114655f488\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tkx4l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tkx4l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 05:10:15.635: INFO: Pod "webserver-deployment-845c8977d9-n4ltk" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-n4ltk webserver-deployment-845c8977d9- deployment-3693  e6470b88-ae86-430e-9962-c5cbb42be2a5 24665 0 2023-01-19 05:10:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6153ac80-678f-4cdc-a866-74114655f488 0xc003f52e40 0xc003f52e41}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6153ac80-678f-4cdc-a866-74114655f488\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7ljrr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7ljrr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 05:10:15.636: INFO: Pod "webserver-deployment-845c8977d9-nckw9" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-nckw9 webserver-deployment-845c8977d9- deployment-3693  90157060-6b93-4f9b-bb62-90c43940d319 24632 0 2023-01-19 05:10:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6153ac80-678f-4cdc-a866-74114655f488 0xc003f53000 0xc003f53001}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6153ac80-678f-4cdc-a866-74114655f488\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h6hdz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h6hdz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.99,PodIP:,StartTime:2023-01-19 05:10:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 05:10:15.636: INFO: Pod "webserver-deployment-845c8977d9-njwxj" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-njwxj webserver-deployment-845c8977d9- deployment-3693  9fe5474c-4780-41ae-b11f-5c098d5b8867 24646 0 2023-01-19 05:10:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6153ac80-678f-4cdc-a866-74114655f488 0xc003f53257 0xc003f53258}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6153ac80-678f-4cdc-a866-74114655f488\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xj679,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xj679,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 05:10:15.636: INFO: Pod "webserver-deployment-845c8977d9-qhcn4" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-qhcn4 webserver-deployment-845c8977d9- deployment-3693  b6702120-2ea6-40bf-a968-e1c74456484d 24492 0 2023-01-19 05:10:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6153ac80-678f-4cdc-a866-74114655f488 0xc003f533d0 0xc003f533d1}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6153ac80-678f-4cdc-a866-74114655f488\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 05:10:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.24.147\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ns4g8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ns4g8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.99,PodIP:10.100.24.147,StartTime:2023-01-19 05:10:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 05:10:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://6cd8b8811ee9ef8c828496fa3694ce79978a8fb80acee891760f56b4598731f7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.24.147,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 05:10:15.636: INFO: Pod "webserver-deployment-845c8977d9-trp2b" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-trp2b webserver-deployment-845c8977d9- deployment-3693  c6ef31f2-884f-4bf1-9b33-20e69db5c351 24666 0 2023-01-19 05:10:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6153ac80-678f-4cdc-a866-74114655f488 0xc003f535c7 0xc003f535c8}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6153ac80-678f-4cdc-a866-74114655f488\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n8hf7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n8hf7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 05:10:15.636: INFO: Pod "webserver-deployment-845c8977d9-vpd78" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-vpd78 webserver-deployment-845c8977d9- deployment-3693  55764534-77c8-494e-be90-386ea1981f26 24644 0 2023-01-19 05:10:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6153ac80-678f-4cdc-a866-74114655f488 0xc003f53720 0xc003f53721}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6153ac80-678f-4cdc-a866-74114655f488\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rd7kb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rd7kb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.99,PodIP:,StartTime:2023-01-19 05:10:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 05:10:15.637: INFO: Pod "webserver-deployment-845c8977d9-xg78l" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-xg78l webserver-deployment-845c8977d9- deployment-3693  324755fe-88b6-4b60-83ee-d7406c78cd85 24667 0 2023-01-19 05:10:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6153ac80-678f-4cdc-a866-74114655f488 0xc003f53957 0xc003f53958}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6153ac80-678f-4cdc-a866-74114655f488\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kfwn2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kfwn2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 05:10:15.637: INFO: Pod "webserver-deployment-845c8977d9-xx2vf" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-xx2vf webserver-deployment-845c8977d9- deployment-3693  f6c6d9d7-3550-42da-addd-bf32bcfbf1df 24530 0 2023-01-19 05:10:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6153ac80-678f-4cdc-a866-74114655f488 0xc003f53b10 0xc003f53b11}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6153ac80-678f-4cdc-a866-74114655f488\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 05:10:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.70.89\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fh6cr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fh6cr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.78,PodIP:10.100.70.89,StartTime:2023-01-19 05:10:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 05:10:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://efa2dfe858a048a965533e6bd129d3452278dca141114b67e7a140f6c8031266,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.70.89,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 05:10:15.637: INFO: Pod "webserver-deployment-845c8977d9-zcp8l" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-zcp8l webserver-deployment-845c8977d9- deployment-3693  d0d2b680-5f89-4e2c-b025-577c40987f5a 24630 0 2023-01-19 05:10:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6153ac80-678f-4cdc-a866-74114655f488 0xc003f53d97 0xc003f53d98}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6153ac80-678f-4cdc-a866-74114655f488\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gcn4j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gcn4j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 19 05:10:15.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3693" for this suite. 01/19/23 05:10:15.642
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","completed":217,"skipped":4051,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.198 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:10:05.455
    Jan 19 05:10:05.455: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename deployment 01/19/23 05:10:05.456
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:10:05.469
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:10:05.474
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Jan 19 05:10:05.478: INFO: Creating deployment "webserver-deployment"
    Jan 19 05:10:05.485: INFO: Waiting for observed generation 1
    Jan 19 05:10:07.494: INFO: Waiting for all required pods to come up
    Jan 19 05:10:07.502: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 01/19/23 05:10:07.502
    Jan 19 05:10:07.502: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-xx2vf" in namespace "deployment-3693" to be "running"
    Jan 19 05:10:07.502: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-228sr" in namespace "deployment-3693" to be "running"
    Jan 19 05:10:07.502: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-2kfxc" in namespace "deployment-3693" to be "running"
    Jan 19 05:10:07.502: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-2m4nq" in namespace "deployment-3693" to be "running"
    Jan 19 05:10:07.502: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-7jvv4" in namespace "deployment-3693" to be "running"
    Jan 19 05:10:07.503: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-88s5j" in namespace "deployment-3693" to be "running"
    Jan 19 05:10:07.503: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-9zckc" in namespace "deployment-3693" to be "running"
    Jan 19 05:10:07.503: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-cltzq" in namespace "deployment-3693" to be "running"
    Jan 19 05:10:07.503: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-grjrz" in namespace "deployment-3693" to be "running"
    Jan 19 05:10:07.503: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-qhcn4" in namespace "deployment-3693" to be "running"
    Jan 19 05:10:07.508: INFO: Pod "webserver-deployment-845c8977d9-xx2vf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.443291ms
    Jan 19 05:10:07.508: INFO: Pod "webserver-deployment-845c8977d9-2kfxc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.51402ms
    Jan 19 05:10:07.510: INFO: Pod "webserver-deployment-845c8977d9-9zckc": Phase="Pending", Reason="", readiness=false. Elapsed: 7.005968ms
    Jan 19 05:10:07.510: INFO: Pod "webserver-deployment-845c8977d9-cltzq": Phase="Pending", Reason="", readiness=false. Elapsed: 7.056774ms
    Jan 19 05:10:07.511: INFO: Pod "webserver-deployment-845c8977d9-grjrz": Phase="Pending", Reason="", readiness=false. Elapsed: 8.435992ms
    Jan 19 05:10:07.511: INFO: Pod "webserver-deployment-845c8977d9-7jvv4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.765345ms
    Jan 19 05:10:07.511: INFO: Pod "webserver-deployment-845c8977d9-228sr": Phase="Pending", Reason="", readiness=false. Elapsed: 9.049864ms
    Jan 19 05:10:07.511: INFO: Pod "webserver-deployment-845c8977d9-88s5j": Phase="Pending", Reason="", readiness=false. Elapsed: 8.797277ms
    Jan 19 05:10:07.511: INFO: Pod "webserver-deployment-845c8977d9-qhcn4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.579129ms
    Jan 19 05:10:07.511: INFO: Pod "webserver-deployment-845c8977d9-2m4nq": Phase="Pending", Reason="", readiness=false. Elapsed: 9.048889ms
    Jan 19 05:10:09.524: INFO: Pod "webserver-deployment-845c8977d9-xx2vf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022313935s
    Jan 19 05:10:09.524: INFO: Pod "webserver-deployment-845c8977d9-2kfxc": Phase="Running", Reason="", readiness=true. Elapsed: 2.022150193s
    Jan 19 05:10:09.525: INFO: Pod "webserver-deployment-845c8977d9-2kfxc" satisfied condition "running"
    Jan 19 05:10:09.528: INFO: Pod "webserver-deployment-845c8977d9-cltzq": Phase="Running", Reason="", readiness=true. Elapsed: 2.025090623s
    Jan 19 05:10:09.528: INFO: Pod "webserver-deployment-845c8977d9-cltzq" satisfied condition "running"
    Jan 19 05:10:09.529: INFO: Pod "webserver-deployment-845c8977d9-qhcn4": Phase="Running", Reason="", readiness=true. Elapsed: 2.02608792s
    Jan 19 05:10:09.529: INFO: Pod "webserver-deployment-845c8977d9-qhcn4" satisfied condition "running"
    Jan 19 05:10:09.530: INFO: Pod "webserver-deployment-845c8977d9-9zckc": Phase="Running", Reason="", readiness=true. Elapsed: 2.027884475s
    Jan 19 05:10:09.531: INFO: Pod "webserver-deployment-845c8977d9-9zckc" satisfied condition "running"
    Jan 19 05:10:09.531: INFO: Pod "webserver-deployment-845c8977d9-88s5j": Phase="Running", Reason="", readiness=true. Elapsed: 2.028171984s
    Jan 19 05:10:09.531: INFO: Pod "webserver-deployment-845c8977d9-88s5j" satisfied condition "running"
    Jan 19 05:10:09.531: INFO: Pod "webserver-deployment-845c8977d9-7jvv4": Phase="Running", Reason="", readiness=true. Elapsed: 2.028300151s
    Jan 19 05:10:09.531: INFO: Pod "webserver-deployment-845c8977d9-7jvv4" satisfied condition "running"
    Jan 19 05:10:09.531: INFO: Pod "webserver-deployment-845c8977d9-grjrz": Phase="Running", Reason="", readiness=true. Elapsed: 2.02807196s
    Jan 19 05:10:09.531: INFO: Pod "webserver-deployment-845c8977d9-grjrz" satisfied condition "running"
    Jan 19 05:10:09.531: INFO: Pod "webserver-deployment-845c8977d9-228sr": Phase="Running", Reason="", readiness=true. Elapsed: 2.028595324s
    Jan 19 05:10:09.531: INFO: Pod "webserver-deployment-845c8977d9-228sr" satisfied condition "running"
    Jan 19 05:10:09.531: INFO: Pod "webserver-deployment-845c8977d9-2m4nq": Phase="Running", Reason="", readiness=true. Elapsed: 2.02848032s
    Jan 19 05:10:09.531: INFO: Pod "webserver-deployment-845c8977d9-2m4nq" satisfied condition "running"
    Jan 19 05:10:11.512: INFO: Pod "webserver-deployment-845c8977d9-xx2vf": Phase="Running", Reason="", readiness=true. Elapsed: 4.009586419s
    Jan 19 05:10:11.512: INFO: Pod "webserver-deployment-845c8977d9-xx2vf" satisfied condition "running"
    Jan 19 05:10:11.512: INFO: Waiting for deployment "webserver-deployment" to complete
    Jan 19 05:10:11.516: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Jan 19 05:10:11.526: INFO: Updating deployment webserver-deployment
    Jan 19 05:10:11.526: INFO: Waiting for observed generation 2
    Jan 19 05:10:13.534: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Jan 19 05:10:13.548: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Jan 19 05:10:13.551: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jan 19 05:10:13.564: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Jan 19 05:10:13.564: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Jan 19 05:10:13.567: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jan 19 05:10:13.572: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Jan 19 05:10:13.572: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Jan 19 05:10:13.585: INFO: Updating deployment webserver-deployment
    Jan 19 05:10:13.585: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Jan 19 05:10:13.604: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Jan 19 05:10:15.617: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 19 05:10:15.623: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-3693  5e4ac302-4e1c-4bad-b033-326e9a10e571 24689 3 2023-01-19 05:10:05 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e8e4b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-19 05:10:13 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-01-19 05:10:13 +0000 UTC,LastTransitionTime:2023-01-19 05:10:05 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Jan 19 05:10:15.626: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-3693  9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00 24688 3 2023-01-19 05:10:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 5e4ac302-4e1c-4bad-b033-326e9a10e571 0xc003e55677 0xc003e55678}] [] [{kube-controller-manager Update apps/v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e4ac302-4e1c-4bad-b033-326e9a10e571\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e55718 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 19 05:10:15.626: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Jan 19 05:10:15.626: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-3693  6153ac80-678f-4cdc-a866-74114655f488 24671 3 2023-01-19 05:10:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 5e4ac302-4e1c-4bad-b033-326e9a10e571 0xc003e557c7 0xc003e557c8}] [] [{kube-controller-manager Update apps/v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e4ac302-4e1c-4bad-b033-326e9a10e571\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e55868 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Jan 19 05:10:15.632: INFO: Pod "webserver-deployment-69b7448995-4cxgq" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-4cxgq webserver-deployment-69b7448995- deployment-3693  8a04257f-dc14-486b-a6b9-98eacb5360a4 24563 0 2023-01-19 05:10:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00 0xc003e55e37 0xc003e55e38}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 05:10:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9vzp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9vzp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.99,PodIP:,StartTime:2023-01-19 05:10:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 19 05:10:15.632: INFO: Pod "webserver-deployment-69b7448995-4gfzc" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-4gfzc webserver-deployment-69b7448995- deployment-3693  ceba659d-51b3-4b85-a1e7-fd3b29e57e8f 24655 0 2023-01-19 05:10:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00 0xc003f06027 0xc003f06028}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d9szn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d9szn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 19 05:10:15.632: INFO: Pod "webserver-deployment-69b7448995-5x8zw" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-5x8zw webserver-deployment-69b7448995- deployment-3693  cc7e8709-8a9f-40b6-ac8b-61b1effb9750 24670 0 2023-01-19 05:10:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00 0xc003f061c0 0xc003f061c1}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-474cn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-474cn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 19 05:10:15.633: INFO: Pod "webserver-deployment-69b7448995-6k4p4" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-6k4p4 webserver-deployment-69b7448995- deployment-3693  c864f935-cf23-4f8d-82aa-02244fdf8252 24631 0 2023-01-19 05:10:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00 0xc003f063d0 0xc003f063d1}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xdh5h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xdh5h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 19 05:10:15.633: INFO: Pod "webserver-deployment-69b7448995-77tl6" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-77tl6 webserver-deployment-69b7448995- deployment-3693  279ac440-c9f3-4721-a5f3-6a2bbbb3211e 24649 0 2023-01-19 05:10:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00 0xc003f065c0 0xc003f065c1}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pxddk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pxddk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.99,PodIP:,StartTime:2023-01-19 05:10:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 19 05:10:15.633: INFO: Pod "webserver-deployment-69b7448995-7m9cq" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-7m9cq webserver-deployment-69b7448995- deployment-3693  74ceeb0a-7a18-44ed-96a8-5ebff8b60929 24651 0 2023-01-19 05:10:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00 0xc003f06857 0xc003f06858}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nfqfj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nfqfj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.99,PodIP:,StartTime:2023-01-19 05:10:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 19 05:10:15.633: INFO: Pod "webserver-deployment-69b7448995-8h5gz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-8h5gz webserver-deployment-69b7448995- deployment-3693  eec161d0-c5b9-4851-b2a0-6388c332e53a 24656 0 2023-01-19 05:10:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00 0xc003f06a67 0xc003f06a68}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fffqq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fffqq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 19 05:10:15.633: INFO: Pod "webserver-deployment-69b7448995-8k2wc" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-8k2wc webserver-deployment-69b7448995- deployment-3693  5956db5e-615a-4b62-a372-b672469936a2 24645 0 2023-01-19 05:10:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00 0xc003f06bd0 0xc003f06bd1}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ljbnf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ljbnf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 19 05:10:15.633: INFO: Pod "webserver-deployment-69b7448995-bnnqr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-bnnqr webserver-deployment-69b7448995- deployment-3693  697d8afa-17ae-45db-8809-893df864a3c7 24561 0 2023-01-19 05:10:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00 0xc003f06d30 0xc003f06d31}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 05:10:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cb24x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cb24x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.78,PodIP:,StartTime:2023-01-19 05:10:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 19 05:10:15.633: INFO: Pod "webserver-deployment-69b7448995-hqvd6" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-hqvd6 webserver-deployment-69b7448995- deployment-3693  73d3ec4f-7cbe-4871-8dcb-66bcd9a99fd5 24552 0 2023-01-19 05:10:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00 0xc003f06f07 0xc003f06f08}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 05:10:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zv5nx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zv5nx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.78,PodIP:,StartTime:2023-01-19 05:10:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 19 05:10:15.634: INFO: Pod "webserver-deployment-69b7448995-rbbk8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-rbbk8 webserver-deployment-69b7448995- deployment-3693  55b88ebd-4197-4f7a-b684-78039e118c3b 24595 0 2023-01-19 05:10:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00 0xc003f070e7 0xc003f070e8}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 05:10:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t8dsq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t8dsq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.99,PodIP:,StartTime:2023-01-19 05:10:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 19 05:10:15.634: INFO: Pod "webserver-deployment-69b7448995-sg9tz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-sg9tz webserver-deployment-69b7448995- deployment-3693  a3be4393-6c21-4ee2-9358-26bc431dc2f6 24598 0 2023-01-19 05:10:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00 0xc003f072c7 0xc003f072c8}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 05:10:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2fnlb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2fnlb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.78,PodIP:,StartTime:2023-01-19 05:10:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 19 05:10:15.634: INFO: Pod "webserver-deployment-69b7448995-zwhf7" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-zwhf7 webserver-deployment-69b7448995- deployment-3693  048e8e0f-27e9-49f1-90ab-f8f0a9ce0a7b 24653 0 2023-01-19 05:10:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00 0xc003f074a7 0xc003f074a8}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9d4c8370-8f41-4fbd-8e11-c6d0a5e50b00\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q7jt9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q7jt9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 19 05:10:15.634: INFO: Pod "webserver-deployment-845c8977d9-228sr" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-228sr webserver-deployment-845c8977d9- deployment-3693  ed8508b9-beaf-4178-8198-69e194e696f2 24521 0 2023-01-19 05:10:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6153ac80-678f-4cdc-a866-74114655f488 0xc003f07650 0xc003f07651}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6153ac80-678f-4cdc-a866-74114655f488\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 05:10:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.70.84\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vr9ms,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vr9ms,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.78,PodIP:10.100.70.84,StartTime:2023-01-19 05:10:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 05:10:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://f5d28daa7c252395491852642c2b458c7cabcc2a43b9c980f32fcb1b3fa8d945,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.70.84,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 19 05:10:15.634: INFO: Pod "webserver-deployment-845c8977d9-2kfxc" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-2kfxc webserver-deployment-845c8977d9- deployment-3693  48a3193d-34e9-4d8c-b8e4-5d0c450d24d3 24469 0 2023-01-19 05:10:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6153ac80-678f-4cdc-a866-74114655f488 0xc003f07837 0xc003f07838}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6153ac80-678f-4cdc-a866-74114655f488\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 05:10:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.70.87\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x5jht,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x5jht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.78,PodIP:10.100.70.87,StartTime:2023-01-19 05:10:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 05:10:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://0b4fd17115558e4ebff7ef69667a9767289489d34b6245281df693e03ebefad7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.70.87,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 19 05:10:15.634: INFO: Pod "webserver-deployment-845c8977d9-2m4nq" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-2m4nq webserver-deployment-845c8977d9- deployment-3693  3d8db484-3b81-4b96-931a-9b0ab566df37 24493 0 2023-01-19 05:10:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6153ac80-678f-4cdc-a866-74114655f488 0xc003f07a97 0xc003f07a98}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6153ac80-678f-4cdc-a866-74114655f488\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 05:10:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.24.150\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z2vlw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z2vlw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.99,PodIP:10.100.24.150,StartTime:2023-01-19 05:10:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 05:10:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://492c484255108e82066fe5bc2dea26985041a8816256f1c9b6bb082aadac8bff,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.24.150,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 19 05:10:15.634: INFO: Pod "webserver-deployment-845c8977d9-7955f" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-7955f webserver-deployment-845c8977d9- deployment-3693  fe32279e-6d01-49d6-b2df-ea2fe260894d 24706 0 2023-01-19 05:10:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6153ac80-678f-4cdc-a866-74114655f488 0xc003f07cc7 0xc003f07cc8}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6153ac80-678f-4cdc-a866-74114655f488\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 05:10:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rndl2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rndl2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.99,PodIP:,StartTime:2023-01-19 05:10:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 19 05:10:15.634: INFO: Pod "webserver-deployment-845c8977d9-88s5j" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-88s5j webserver-deployment-845c8977d9- deployment-3693  193761da-155d-463f-8fad-3a66f21f31a8 24478 0 2023-01-19 05:10:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6153ac80-678f-4cdc-a866-74114655f488 0xc003f07f27 0xc003f07f28}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6153ac80-678f-4cdc-a866-74114655f488\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 05:10:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.70.88\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-97f9b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-97f9b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.78,PodIP:10.100.70.88,StartTime:2023-01-19 05:10:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 05:10:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://09cd817b183b360cf9195fd5d4851a9bbfec5a5c879726b4bef43aafdf4b8b6c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.70.88,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 19 05:10:15.635: INFO: Pod "webserver-deployment-845c8977d9-8g8sz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-8g8sz webserver-deployment-845c8977d9- deployment-3693  09120396-aea1-4c92-94b1-83c567956807 24664 0 2023-01-19 05:10:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6153ac80-678f-4cdc-a866-74114655f488 0xc003f52167 0xc003f52168}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6153ac80-678f-4cdc-a866-74114655f488\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w2s5m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w2s5m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.99,PodIP:,StartTime:2023-01-19 05:10:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 19 05:10:15.635: INFO: Pod "webserver-deployment-845c8977d9-9vbpx" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-9vbpx webserver-deployment-845c8977d9- deployment-3693  21a9e0e8-81fb-4e9c-a289-667207516f5b 24693 0 2023-01-19 05:10:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6153ac80-678f-4cdc-a866-74114655f488 0xc003f523f7 0xc003f523f8}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6153ac80-678f-4cdc-a866-74114655f488\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hkwgn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hkwgn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.78,PodIP:,StartTime:2023-01-19 05:10:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 19 05:10:15.635: INFO: Pod "webserver-deployment-845c8977d9-9zckc" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-9zckc webserver-deployment-845c8977d9- deployment-3693  57da30d3-ceb9-44df-bb64-2c1460177a76 24489 0 2023-01-19 05:10:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6153ac80-678f-4cdc-a866-74114655f488 0xc003f52687 0xc003f52688}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6153ac80-678f-4cdc-a866-74114655f488\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 05:10:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.24.148\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dpw7w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dpw7w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.99,PodIP:10.100.24.148,StartTime:2023-01-19 05:10:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 05:10:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://132fff4ac88e5661c7663922c8a68e28643fde4d9b9ae66f6a1de31e36c4a6a6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.24.148,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 19 05:10:15.635: INFO: Pod "webserver-deployment-845c8977d9-bt2lp" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-bt2lp webserver-deployment-845c8977d9- deployment-3693  2e6761ce-5e38-437a-aad0-43a2c0bec032 24668 0 2023-01-19 05:10:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6153ac80-678f-4cdc-a866-74114655f488 0xc003f52907 0xc003f52908}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6153ac80-678f-4cdc-a866-74114655f488\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vtx2c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vtx2c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 19 05:10:15.635: INFO: Pod "webserver-deployment-845c8977d9-grjrz" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-grjrz webserver-deployment-845c8977d9- deployment-3693  708e6669-a177-4eb1-96ce-8e85a27fee51 24487 0 2023-01-19 05:10:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6153ac80-678f-4cdc-a866-74114655f488 0xc003f52a70 0xc003f52a71}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6153ac80-678f-4cdc-a866-74114655f488\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 05:10:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.24.149\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-59s99,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-59s99,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.99,PodIP:10.100.24.149,StartTime:2023-01-19 05:10:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 05:10:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://00576d8f32509ec82f1338a72c6d1bff8b86a564c9b99f42976edc07f3a53a48,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.24.149,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 19 05:10:15.635: INFO: Pod "webserver-deployment-845c8977d9-kmnnc" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-kmnnc webserver-deployment-845c8977d9- deployment-3693  aa686e7b-8634-4334-a430-bbcb3869a348 24634 0 2023-01-19 05:10:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6153ac80-678f-4cdc-a866-74114655f488 0xc003f52c57 0xc003f52c58}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6153ac80-678f-4cdc-a866-74114655f488\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tkx4l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tkx4l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 19 05:10:15.635: INFO: Pod "webserver-deployment-845c8977d9-n4ltk" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-n4ltk webserver-deployment-845c8977d9- deployment-3693  e6470b88-ae86-430e-9962-c5cbb42be2a5 24665 0 2023-01-19 05:10:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6153ac80-678f-4cdc-a866-74114655f488 0xc003f52e40 0xc003f52e41}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6153ac80-678f-4cdc-a866-74114655f488\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7ljrr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7ljrr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 19 05:10:15.636: INFO: Pod "webserver-deployment-845c8977d9-nckw9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-nckw9 webserver-deployment-845c8977d9- deployment-3693  90157060-6b93-4f9b-bb62-90c43940d319 24632 0 2023-01-19 05:10:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6153ac80-678f-4cdc-a866-74114655f488 0xc003f53000 0xc003f53001}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6153ac80-678f-4cdc-a866-74114655f488\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h6hdz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h6hdz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.99,PodIP:,StartTime:2023-01-19 05:10:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 19 05:10:15.636: INFO: Pod "webserver-deployment-845c8977d9-njwxj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-njwxj webserver-deployment-845c8977d9- deployment-3693  9fe5474c-4780-41ae-b11f-5c098d5b8867 24646 0 2023-01-19 05:10:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6153ac80-678f-4cdc-a866-74114655f488 0xc003f53257 0xc003f53258}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6153ac80-678f-4cdc-a866-74114655f488\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xj679,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xj679,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 19 05:10:15.636: INFO: Pod "webserver-deployment-845c8977d9-qhcn4" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-qhcn4 webserver-deployment-845c8977d9- deployment-3693  b6702120-2ea6-40bf-a968-e1c74456484d 24492 0 2023-01-19 05:10:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6153ac80-678f-4cdc-a866-74114655f488 0xc003f533d0 0xc003f533d1}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6153ac80-678f-4cdc-a866-74114655f488\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 05:10:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.24.147\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ns4g8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ns4g8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.99,PodIP:10.100.24.147,StartTime:2023-01-19 05:10:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 05:10:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://6cd8b8811ee9ef8c828496fa3694ce79978a8fb80acee891760f56b4598731f7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.24.147,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 19 05:10:15.636: INFO: Pod "webserver-deployment-845c8977d9-trp2b" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-trp2b webserver-deployment-845c8977d9- deployment-3693  c6ef31f2-884f-4bf1-9b33-20e69db5c351 24666 0 2023-01-19 05:10:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6153ac80-678f-4cdc-a866-74114655f488 0xc003f535c7 0xc003f535c8}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6153ac80-678f-4cdc-a866-74114655f488\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n8hf7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n8hf7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 19 05:10:15.636: INFO: Pod "webserver-deployment-845c8977d9-vpd78" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-vpd78 webserver-deployment-845c8977d9- deployment-3693  55764534-77c8-494e-be90-386ea1981f26 24644 0 2023-01-19 05:10:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6153ac80-678f-4cdc-a866-74114655f488 0xc003f53720 0xc003f53721}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6153ac80-678f-4cdc-a866-74114655f488\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rd7kb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rd7kb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.99,PodIP:,StartTime:2023-01-19 05:10:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 19 05:10:15.637: INFO: Pod "webserver-deployment-845c8977d9-xg78l" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-xg78l webserver-deployment-845c8977d9- deployment-3693  324755fe-88b6-4b60-83ee-d7406c78cd85 24667 0 2023-01-19 05:10:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6153ac80-678f-4cdc-a866-74114655f488 0xc003f53957 0xc003f53958}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6153ac80-678f-4cdc-a866-74114655f488\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kfwn2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kfwn2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 19 05:10:15.637: INFO: Pod "webserver-deployment-845c8977d9-xx2vf" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-xx2vf webserver-deployment-845c8977d9- deployment-3693  f6c6d9d7-3550-42da-addd-bf32bcfbf1df 24530 0 2023-01-19 05:10:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6153ac80-678f-4cdc-a866-74114655f488 0xc003f53b10 0xc003f53b11}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6153ac80-678f-4cdc-a866-74114655f488\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 05:10:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.70.89\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fh6cr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fh6cr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.78,PodIP:10.100.70.89,StartTime:2023-01-19 05:10:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 05:10:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://efa2dfe858a048a965533e6bd129d3452278dca141114b67e7a140f6c8031266,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.70.89,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 19 05:10:15.637: INFO: Pod "webserver-deployment-845c8977d9-zcp8l" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-zcp8l webserver-deployment-845c8977d9- deployment-3693  d0d2b680-5f89-4e2c-b025-577c40987f5a 24630 0 2023-01-19 05:10:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6153ac80-678f-4cdc-a866-74114655f488 0xc003f53d97 0xc003f53d98}] [] [{kube-controller-manager Update v1 2023-01-19 05:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6153ac80-678f-4cdc-a866-74114655f488\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gcn4j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gcn4j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:10:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 19 05:10:15.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-3693" for this suite. 01/19/23 05:10:15.642
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:10:15.66
Jan 19 05:10:15.661: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename security-context-test 01/19/23 05:10:15.661
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:10:15.686
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:10:15.694
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
Jan 19 05:10:15.710: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-063adf34-0c78-4393-8d72-25a0951d8623" in namespace "security-context-test-4558" to be "Succeeded or Failed"
Jan 19 05:10:15.715: INFO: Pod "busybox-privileged-false-063adf34-0c78-4393-8d72-25a0951d8623": Phase="Pending", Reason="", readiness=false. Elapsed: 4.55481ms
Jan 19 05:10:17.719: INFO: Pod "busybox-privileged-false-063adf34-0c78-4393-8d72-25a0951d8623": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009171522s
Jan 19 05:10:19.719: INFO: Pod "busybox-privileged-false-063adf34-0c78-4393-8d72-25a0951d8623": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008653513s
Jan 19 05:10:21.720: INFO: Pod "busybox-privileged-false-063adf34-0c78-4393-8d72-25a0951d8623": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009707312s
Jan 19 05:10:23.719: INFO: Pod "busybox-privileged-false-063adf34-0c78-4393-8d72-25a0951d8623": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.009077109s
Jan 19 05:10:23.719: INFO: Pod "busybox-privileged-false-063adf34-0c78-4393-8d72-25a0951d8623" satisfied condition "Succeeded or Failed"
Jan 19 05:10:23.724: INFO: Got logs for pod "busybox-privileged-false-063adf34-0c78-4393-8d72-25a0951d8623": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 19 05:10:23.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4558" for this suite. 01/19/23 05:10:23.728
{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","completed":218,"skipped":4181,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.073 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:490
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:527

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:10:15.66
    Jan 19 05:10:15.661: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename security-context-test 01/19/23 05:10:15.661
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:10:15.686
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:10:15.694
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:527
    Jan 19 05:10:15.710: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-063adf34-0c78-4393-8d72-25a0951d8623" in namespace "security-context-test-4558" to be "Succeeded or Failed"
    Jan 19 05:10:15.715: INFO: Pod "busybox-privileged-false-063adf34-0c78-4393-8d72-25a0951d8623": Phase="Pending", Reason="", readiness=false. Elapsed: 4.55481ms
    Jan 19 05:10:17.719: INFO: Pod "busybox-privileged-false-063adf34-0c78-4393-8d72-25a0951d8623": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009171522s
    Jan 19 05:10:19.719: INFO: Pod "busybox-privileged-false-063adf34-0c78-4393-8d72-25a0951d8623": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008653513s
    Jan 19 05:10:21.720: INFO: Pod "busybox-privileged-false-063adf34-0c78-4393-8d72-25a0951d8623": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009707312s
    Jan 19 05:10:23.719: INFO: Pod "busybox-privileged-false-063adf34-0c78-4393-8d72-25a0951d8623": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.009077109s
    Jan 19 05:10:23.719: INFO: Pod "busybox-privileged-false-063adf34-0c78-4393-8d72-25a0951d8623" satisfied condition "Succeeded or Failed"
    Jan 19 05:10:23.724: INFO: Got logs for pod "busybox-privileged-false-063adf34-0c78-4393-8d72-25a0951d8623": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 19 05:10:23.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-4558" for this suite. 01/19/23 05:10:23.728
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:10:23.734
Jan 19 05:10:23.734: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 01/19/23 05:10:23.735
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:10:23.748
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:10:23.751
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 01/19/23 05:10:23.754
STEP: Creating hostNetwork=false pod 01/19/23 05:10:23.754
Jan 19 05:10:23.762: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-2389" to be "running and ready"
Jan 19 05:10:23.769: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.389376ms
Jan 19 05:10:23.769: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 19 05:10:25.772: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009558827s
Jan 19 05:10:25.772: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 19 05:10:27.772: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.009800244s
Jan 19 05:10:27.772: INFO: The phase of Pod test-pod is Running (Ready = true)
Jan 19 05:10:27.772: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 01/19/23 05:10:27.775
Jan 19 05:10:27.780: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-2389" to be "running and ready"
Jan 19 05:10:27.783: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.382276ms
Jan 19 05:10:27.783: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 19 05:10:29.788: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00800947s
Jan 19 05:10:29.788: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Jan 19 05:10:29.788: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 01/19/23 05:10:29.791
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 01/19/23 05:10:29.791
Jan 19 05:10:29.791: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2389 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 05:10:29.791: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
Jan 19 05:10:29.791: INFO: ExecWithOptions: Clientset creation
Jan 19 05:10:29.791: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2389/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 19 05:10:29.885: INFO: Exec stderr: ""
Jan 19 05:10:29.885: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2389 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 05:10:29.885: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
Jan 19 05:10:29.885: INFO: ExecWithOptions: Clientset creation
Jan 19 05:10:29.885: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2389/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 19 05:10:29.972: INFO: Exec stderr: ""
Jan 19 05:10:29.972: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2389 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 05:10:29.972: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
Jan 19 05:10:29.972: INFO: ExecWithOptions: Clientset creation
Jan 19 05:10:29.972: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2389/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 19 05:10:30.061: INFO: Exec stderr: ""
Jan 19 05:10:30.061: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2389 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 05:10:30.061: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
Jan 19 05:10:30.061: INFO: ExecWithOptions: Clientset creation
Jan 19 05:10:30.061: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2389/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 19 05:10:30.140: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 01/19/23 05:10:30.14
Jan 19 05:10:30.140: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2389 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 05:10:30.140: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
Jan 19 05:10:30.141: INFO: ExecWithOptions: Clientset creation
Jan 19 05:10:30.141: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2389/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan 19 05:10:30.217: INFO: Exec stderr: ""
Jan 19 05:10:30.217: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2389 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 05:10:30.218: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
Jan 19 05:10:30.218: INFO: ExecWithOptions: Clientset creation
Jan 19 05:10:30.218: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2389/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan 19 05:10:30.299: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 01/19/23 05:10:30.299
Jan 19 05:10:30.299: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2389 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 05:10:30.299: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
Jan 19 05:10:30.300: INFO: ExecWithOptions: Clientset creation
Jan 19 05:10:30.300: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2389/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 19 05:10:30.376: INFO: Exec stderr: ""
Jan 19 05:10:30.376: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2389 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 05:10:30.376: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
Jan 19 05:10:30.376: INFO: ExecWithOptions: Clientset creation
Jan 19 05:10:30.376: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2389/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 19 05:10:30.453: INFO: Exec stderr: ""
Jan 19 05:10:30.453: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2389 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 05:10:30.453: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
Jan 19 05:10:30.453: INFO: ExecWithOptions: Clientset creation
Jan 19 05:10:30.453: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2389/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 19 05:10:30.533: INFO: Exec stderr: ""
Jan 19 05:10:30.533: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2389 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 05:10:30.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
Jan 19 05:10:30.533: INFO: ExecWithOptions: Clientset creation
Jan 19 05:10:30.533: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2389/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 19 05:10:30.616: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
Jan 19 05:10:30.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-2389" for this suite. 01/19/23 05:10:30.62
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","completed":219,"skipped":4186,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.892 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:10:23.734
    Jan 19 05:10:23.734: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 01/19/23 05:10:23.735
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:10:23.748
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:10:23.751
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 01/19/23 05:10:23.754
    STEP: Creating hostNetwork=false pod 01/19/23 05:10:23.754
    Jan 19 05:10:23.762: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-2389" to be "running and ready"
    Jan 19 05:10:23.769: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.389376ms
    Jan 19 05:10:23.769: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 05:10:25.772: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009558827s
    Jan 19 05:10:25.772: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 05:10:27.772: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.009800244s
    Jan 19 05:10:27.772: INFO: The phase of Pod test-pod is Running (Ready = true)
    Jan 19 05:10:27.772: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 01/19/23 05:10:27.775
    Jan 19 05:10:27.780: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-2389" to be "running and ready"
    Jan 19 05:10:27.783: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.382276ms
    Jan 19 05:10:27.783: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 05:10:29.788: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00800947s
    Jan 19 05:10:29.788: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Jan 19 05:10:29.788: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 01/19/23 05:10:29.791
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 01/19/23 05:10:29.791
    Jan 19 05:10:29.791: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2389 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 05:10:29.791: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    Jan 19 05:10:29.791: INFO: ExecWithOptions: Clientset creation
    Jan 19 05:10:29.791: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2389/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 19 05:10:29.885: INFO: Exec stderr: ""
    Jan 19 05:10:29.885: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2389 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 05:10:29.885: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    Jan 19 05:10:29.885: INFO: ExecWithOptions: Clientset creation
    Jan 19 05:10:29.885: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2389/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 19 05:10:29.972: INFO: Exec stderr: ""
    Jan 19 05:10:29.972: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2389 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 05:10:29.972: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    Jan 19 05:10:29.972: INFO: ExecWithOptions: Clientset creation
    Jan 19 05:10:29.972: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2389/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 19 05:10:30.061: INFO: Exec stderr: ""
    Jan 19 05:10:30.061: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2389 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 05:10:30.061: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    Jan 19 05:10:30.061: INFO: ExecWithOptions: Clientset creation
    Jan 19 05:10:30.061: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2389/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 19 05:10:30.140: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 01/19/23 05:10:30.14
    Jan 19 05:10:30.140: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2389 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 05:10:30.140: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    Jan 19 05:10:30.141: INFO: ExecWithOptions: Clientset creation
    Jan 19 05:10:30.141: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2389/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jan 19 05:10:30.217: INFO: Exec stderr: ""
    Jan 19 05:10:30.217: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2389 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 05:10:30.218: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    Jan 19 05:10:30.218: INFO: ExecWithOptions: Clientset creation
    Jan 19 05:10:30.218: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2389/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jan 19 05:10:30.299: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 01/19/23 05:10:30.299
    Jan 19 05:10:30.299: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2389 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 05:10:30.299: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    Jan 19 05:10:30.300: INFO: ExecWithOptions: Clientset creation
    Jan 19 05:10:30.300: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2389/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 19 05:10:30.376: INFO: Exec stderr: ""
    Jan 19 05:10:30.376: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2389 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 05:10:30.376: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    Jan 19 05:10:30.376: INFO: ExecWithOptions: Clientset creation
    Jan 19 05:10:30.376: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2389/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 19 05:10:30.453: INFO: Exec stderr: ""
    Jan 19 05:10:30.453: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2389 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 05:10:30.453: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    Jan 19 05:10:30.453: INFO: ExecWithOptions: Clientset creation
    Jan 19 05:10:30.453: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2389/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 19 05:10:30.533: INFO: Exec stderr: ""
    Jan 19 05:10:30.533: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2389 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 05:10:30.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    Jan 19 05:10:30.533: INFO: ExecWithOptions: Clientset creation
    Jan 19 05:10:30.533: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2389/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 19 05:10:30.616: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:187
    Jan 19 05:10:30.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-2389" for this suite. 01/19/23 05:10:30.62
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:10:30.626
Jan 19 05:10:30.627: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename secrets 01/19/23 05:10:30.627
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:10:30.641
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:10:30.643
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
STEP: Creating secret with name secret-test-97908a08-e6fc-42ad-8b3f-2b4ee4153173 01/19/23 05:10:30.645
STEP: Creating a pod to test consume secrets 01/19/23 05:10:30.649
Jan 19 05:10:30.655: INFO: Waiting up to 5m0s for pod "pod-secrets-1c7065b9-f9d8-409b-bec0-92b2dabaea95" in namespace "secrets-7735" to be "Succeeded or Failed"
Jan 19 05:10:30.660: INFO: Pod "pod-secrets-1c7065b9-f9d8-409b-bec0-92b2dabaea95": Phase="Pending", Reason="", readiness=false. Elapsed: 4.658102ms
Jan 19 05:10:32.664: INFO: Pod "pod-secrets-1c7065b9-f9d8-409b-bec0-92b2dabaea95": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009232991s
Jan 19 05:10:34.664: INFO: Pod "pod-secrets-1c7065b9-f9d8-409b-bec0-92b2dabaea95": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00935612s
Jan 19 05:10:36.667: INFO: Pod "pod-secrets-1c7065b9-f9d8-409b-bec0-92b2dabaea95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011741693s
STEP: Saw pod success 01/19/23 05:10:36.667
Jan 19 05:10:36.667: INFO: Pod "pod-secrets-1c7065b9-f9d8-409b-bec0-92b2dabaea95" satisfied condition "Succeeded or Failed"
Jan 19 05:10:36.670: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-secrets-1c7065b9-f9d8-409b-bec0-92b2dabaea95 container secret-volume-test: <nil>
STEP: delete the pod 01/19/23 05:10:36.676
Jan 19 05:10:36.687: INFO: Waiting for pod pod-secrets-1c7065b9-f9d8-409b-bec0-92b2dabaea95 to disappear
Jan 19 05:10:36.689: INFO: Pod pod-secrets-1c7065b9-f9d8-409b-bec0-92b2dabaea95 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 19 05:10:36.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7735" for this suite. 01/19/23 05:10:36.691
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":220,"skipped":4210,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.070 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:10:30.626
    Jan 19 05:10:30.627: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename secrets 01/19/23 05:10:30.627
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:10:30.641
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:10:30.643
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:56
    STEP: Creating secret with name secret-test-97908a08-e6fc-42ad-8b3f-2b4ee4153173 01/19/23 05:10:30.645
    STEP: Creating a pod to test consume secrets 01/19/23 05:10:30.649
    Jan 19 05:10:30.655: INFO: Waiting up to 5m0s for pod "pod-secrets-1c7065b9-f9d8-409b-bec0-92b2dabaea95" in namespace "secrets-7735" to be "Succeeded or Failed"
    Jan 19 05:10:30.660: INFO: Pod "pod-secrets-1c7065b9-f9d8-409b-bec0-92b2dabaea95": Phase="Pending", Reason="", readiness=false. Elapsed: 4.658102ms
    Jan 19 05:10:32.664: INFO: Pod "pod-secrets-1c7065b9-f9d8-409b-bec0-92b2dabaea95": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009232991s
    Jan 19 05:10:34.664: INFO: Pod "pod-secrets-1c7065b9-f9d8-409b-bec0-92b2dabaea95": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00935612s
    Jan 19 05:10:36.667: INFO: Pod "pod-secrets-1c7065b9-f9d8-409b-bec0-92b2dabaea95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011741693s
    STEP: Saw pod success 01/19/23 05:10:36.667
    Jan 19 05:10:36.667: INFO: Pod "pod-secrets-1c7065b9-f9d8-409b-bec0-92b2dabaea95" satisfied condition "Succeeded or Failed"
    Jan 19 05:10:36.670: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-secrets-1c7065b9-f9d8-409b-bec0-92b2dabaea95 container secret-volume-test: <nil>
    STEP: delete the pod 01/19/23 05:10:36.676
    Jan 19 05:10:36.687: INFO: Waiting for pod pod-secrets-1c7065b9-f9d8-409b-bec0-92b2dabaea95 to disappear
    Jan 19 05:10:36.689: INFO: Pod pod-secrets-1c7065b9-f9d8-409b-bec0-92b2dabaea95 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 19 05:10:36.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7735" for this suite. 01/19/23 05:10:36.691
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:10:36.697
Jan 19 05:10:36.697: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename job 01/19/23 05:10:36.698
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:10:36.711
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:10:36.715
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
STEP: Creating a job 01/19/23 05:10:36.717
STEP: Ensuring active pods == parallelism 01/19/23 05:10:36.721
STEP: Orphaning one of the Job's Pods 01/19/23 05:10:40.727
Jan 19 05:10:41.240: INFO: Successfully updated pod "adopt-release-9vpjp"
STEP: Checking that the Job readopts the Pod 01/19/23 05:10:41.24
Jan 19 05:10:41.240: INFO: Waiting up to 15m0s for pod "adopt-release-9vpjp" in namespace "job-7748" to be "adopted"
Jan 19 05:10:41.260: INFO: Pod "adopt-release-9vpjp": Phase="Running", Reason="", readiness=true. Elapsed: 19.957088ms
Jan 19 05:10:43.265: INFO: Pod "adopt-release-9vpjp": Phase="Running", Reason="", readiness=true. Elapsed: 2.024532971s
Jan 19 05:10:43.265: INFO: Pod "adopt-release-9vpjp" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 01/19/23 05:10:43.265
Jan 19 05:10:43.780: INFO: Successfully updated pod "adopt-release-9vpjp"
STEP: Checking that the Job releases the Pod 01/19/23 05:10:43.78
Jan 19 05:10:43.780: INFO: Waiting up to 15m0s for pod "adopt-release-9vpjp" in namespace "job-7748" to be "released"
Jan 19 05:10:43.783: INFO: Pod "adopt-release-9vpjp": Phase="Running", Reason="", readiness=true. Elapsed: 3.657896ms
Jan 19 05:10:45.788: INFO: Pod "adopt-release-9vpjp": Phase="Running", Reason="", readiness=true. Elapsed: 2.008015608s
Jan 19 05:10:45.788: INFO: Pod "adopt-release-9vpjp" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 19 05:10:45.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7748" for this suite. 01/19/23 05:10:45.791
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","completed":221,"skipped":4250,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.101 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:10:36.697
    Jan 19 05:10:36.697: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename job 01/19/23 05:10:36.698
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:10:36.711
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:10:36.715
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:335
    STEP: Creating a job 01/19/23 05:10:36.717
    STEP: Ensuring active pods == parallelism 01/19/23 05:10:36.721
    STEP: Orphaning one of the Job's Pods 01/19/23 05:10:40.727
    Jan 19 05:10:41.240: INFO: Successfully updated pod "adopt-release-9vpjp"
    STEP: Checking that the Job readopts the Pod 01/19/23 05:10:41.24
    Jan 19 05:10:41.240: INFO: Waiting up to 15m0s for pod "adopt-release-9vpjp" in namespace "job-7748" to be "adopted"
    Jan 19 05:10:41.260: INFO: Pod "adopt-release-9vpjp": Phase="Running", Reason="", readiness=true. Elapsed: 19.957088ms
    Jan 19 05:10:43.265: INFO: Pod "adopt-release-9vpjp": Phase="Running", Reason="", readiness=true. Elapsed: 2.024532971s
    Jan 19 05:10:43.265: INFO: Pod "adopt-release-9vpjp" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 01/19/23 05:10:43.265
    Jan 19 05:10:43.780: INFO: Successfully updated pod "adopt-release-9vpjp"
    STEP: Checking that the Job releases the Pod 01/19/23 05:10:43.78
    Jan 19 05:10:43.780: INFO: Waiting up to 15m0s for pod "adopt-release-9vpjp" in namespace "job-7748" to be "released"
    Jan 19 05:10:43.783: INFO: Pod "adopt-release-9vpjp": Phase="Running", Reason="", readiness=true. Elapsed: 3.657896ms
    Jan 19 05:10:45.788: INFO: Pod "adopt-release-9vpjp": Phase="Running", Reason="", readiness=true. Elapsed: 2.008015608s
    Jan 19 05:10:45.788: INFO: Pod "adopt-release-9vpjp" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 19 05:10:45.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-7748" for this suite. 01/19/23 05:10:45.791
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:10:45.798
Jan 19 05:10:45.799: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename container-runtime 01/19/23 05:10:45.799
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:10:45.812
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:10:45.816
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
STEP: create the container 01/19/23 05:10:45.818
STEP: wait for the container to reach Succeeded 01/19/23 05:10:45.827
STEP: get the container status 01/19/23 05:10:50.855
STEP: the container should be terminated 01/19/23 05:10:50.858
STEP: the termination message should be set 01/19/23 05:10:50.858
Jan 19 05:10:50.858: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 01/19/23 05:10:50.858
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan 19 05:10:50.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-791" for this suite. 01/19/23 05:10:50.874
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":222,"skipped":4255,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.083 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:10:45.798
    Jan 19 05:10:45.799: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename container-runtime 01/19/23 05:10:45.799
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:10:45.812
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:10:45.816
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247
    STEP: create the container 01/19/23 05:10:45.818
    STEP: wait for the container to reach Succeeded 01/19/23 05:10:45.827
    STEP: get the container status 01/19/23 05:10:50.855
    STEP: the container should be terminated 01/19/23 05:10:50.858
    STEP: the termination message should be set 01/19/23 05:10:50.858
    Jan 19 05:10:50.858: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 01/19/23 05:10:50.858
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan 19 05:10:50.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-791" for this suite. 01/19/23 05:10:50.874
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:10:50.882
Jan 19 05:10:50.882: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename webhook 01/19/23 05:10:50.883
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:10:50.9
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:10:50.906
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/19/23 05:10:50.923
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 05:10:51.095
STEP: Deploying the webhook pod 01/19/23 05:10:51.103
STEP: Wait for the deployment to be ready 01/19/23 05:10:51.114
Jan 19 05:10:51.120: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 19 05:10:53.130: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 5, 10, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 10, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 10, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 10, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/19/23 05:10:55.136
STEP: Verifying the service has paired with the endpoint 01/19/23 05:10:55.15
Jan 19 05:10:56.151: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
STEP: Registering the webhook via the AdmissionRegistration API 01/19/23 05:10:56.155
STEP: create a pod that should be denied by the webhook 01/19/23 05:10:56.172
STEP: create a pod that causes the webhook to hang 01/19/23 05:10:56.182
STEP: create a configmap that should be denied by the webhook 01/19/23 05:11:06.201
STEP: create a configmap that should be admitted by the webhook 01/19/23 05:11:06.208
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 01/19/23 05:11:06.22
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 01/19/23 05:11:06.229
STEP: create a namespace that bypass the webhook 01/19/23 05:11:06.234
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 01/19/23 05:11:06.24
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 05:11:06.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5415" for this suite. 01/19/23 05:11:06.264
STEP: Destroying namespace "webhook-5415-markers" for this suite. 01/19/23 05:11:06.268
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","completed":223,"skipped":4277,"failed":0}
------------------------------
â€¢ [SLOW TEST] [15.442 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:10:50.882
    Jan 19 05:10:50.882: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename webhook 01/19/23 05:10:50.883
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:10:50.9
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:10:50.906
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/19/23 05:10:50.923
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 05:10:51.095
    STEP: Deploying the webhook pod 01/19/23 05:10:51.103
    STEP: Wait for the deployment to be ready 01/19/23 05:10:51.114
    Jan 19 05:10:51.120: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Jan 19 05:10:53.130: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 5, 10, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 10, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 10, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 10, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/19/23 05:10:55.136
    STEP: Verifying the service has paired with the endpoint 01/19/23 05:10:55.15
    Jan 19 05:10:56.151: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:196
    STEP: Registering the webhook via the AdmissionRegistration API 01/19/23 05:10:56.155
    STEP: create a pod that should be denied by the webhook 01/19/23 05:10:56.172
    STEP: create a pod that causes the webhook to hang 01/19/23 05:10:56.182
    STEP: create a configmap that should be denied by the webhook 01/19/23 05:11:06.201
    STEP: create a configmap that should be admitted by the webhook 01/19/23 05:11:06.208
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 01/19/23 05:11:06.22
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 01/19/23 05:11:06.229
    STEP: create a namespace that bypass the webhook 01/19/23 05:11:06.234
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 01/19/23 05:11:06.24
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 05:11:06.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5415" for this suite. 01/19/23 05:11:06.264
    STEP: Destroying namespace "webhook-5415-markers" for this suite. 01/19/23 05:11:06.268
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:11:06.324
Jan 19 05:11:06.324: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename webhook 01/19/23 05:11:06.325
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:11:06.342
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:11:06.346
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/19/23 05:11:06.362
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 05:11:06.765
STEP: Deploying the webhook pod 01/19/23 05:11:06.769
STEP: Wait for the deployment to be ready 01/19/23 05:11:06.779
Jan 19 05:11:06.783: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 19 05:11:08.793: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 5, 11, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 11, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 11, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 11, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/19/23 05:11:10.798
STEP: Verifying the service has paired with the endpoint 01/19/23 05:11:10.807
Jan 19 05:11:11.807: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
Jan 19 05:11:11.813: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8817-crds.webhook.example.com via the AdmissionRegistration API 01/19/23 05:11:12.324
STEP: Creating a custom resource that should be mutated by the webhook 01/19/23 05:11:12.345
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 05:11:14.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8620" for this suite. 01/19/23 05:11:14.933
STEP: Destroying namespace "webhook-8620-markers" for this suite. 01/19/23 05:11:14.938
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","completed":224,"skipped":4278,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.654 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:11:06.324
    Jan 19 05:11:06.324: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename webhook 01/19/23 05:11:06.325
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:11:06.342
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:11:06.346
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/19/23 05:11:06.362
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 05:11:06.765
    STEP: Deploying the webhook pod 01/19/23 05:11:06.769
    STEP: Wait for the deployment to be ready 01/19/23 05:11:06.779
    Jan 19 05:11:06.783: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Jan 19 05:11:08.793: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 5, 11, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 11, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 11, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 11, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/19/23 05:11:10.798
    STEP: Verifying the service has paired with the endpoint 01/19/23 05:11:10.807
    Jan 19 05:11:11.807: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:290
    Jan 19 05:11:11.813: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8817-crds.webhook.example.com via the AdmissionRegistration API 01/19/23 05:11:12.324
    STEP: Creating a custom resource that should be mutated by the webhook 01/19/23 05:11:12.345
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 05:11:14.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8620" for this suite. 01/19/23 05:11:14.933
    STEP: Destroying namespace "webhook-8620-markers" for this suite. 01/19/23 05:11:14.938
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:11:14.979
Jan 19 05:11:14.979: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename gc 01/19/23 05:11:14.979
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:11:14.993
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:11:14.999
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 01/19/23 05:11:15.003
STEP: Wait for the Deployment to create new ReplicaSet 01/19/23 05:11:15.009
STEP: delete the deployment 01/19/23 05:11:15.123
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 01/19/23 05:11:15.131
STEP: Gathering metrics 01/19/23 05:11:15.657
W0119 05:11:15.661280      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 19 05:11:15.661: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 19 05:11:15.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4497" for this suite. 01/19/23 05:11:15.663
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","completed":225,"skipped":4285,"failed":0}
------------------------------
â€¢ [0.691 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:11:14.979
    Jan 19 05:11:14.979: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename gc 01/19/23 05:11:14.979
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:11:14.993
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:11:14.999
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 01/19/23 05:11:15.003
    STEP: Wait for the Deployment to create new ReplicaSet 01/19/23 05:11:15.009
    STEP: delete the deployment 01/19/23 05:11:15.123
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 01/19/23 05:11:15.131
    STEP: Gathering metrics 01/19/23 05:11:15.657
    W0119 05:11:15.661280      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 19 05:11:15.661: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 19 05:11:15.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-4497" for this suite. 01/19/23 05:11:15.663
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:11:15.67
Jan 19 05:11:15.670: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename configmap 01/19/23 05:11:15.67
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:11:15.683
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:11:15.686
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
STEP: Creating configMap with name configmap-test-upd-c63d8bfb-8c40-4ca1-9764-e476824a8ab9 01/19/23 05:11:15.691
STEP: Creating the pod 01/19/23 05:11:15.695
Jan 19 05:11:15.703: INFO: Waiting up to 5m0s for pod "pod-configmaps-0ae106ae-6c69-49b5-80f6-9802a994d5ee" in namespace "configmap-5145" to be "running and ready"
Jan 19 05:11:15.706: INFO: Pod "pod-configmaps-0ae106ae-6c69-49b5-80f6-9802a994d5ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.774184ms
Jan 19 05:11:15.706: INFO: The phase of Pod pod-configmaps-0ae106ae-6c69-49b5-80f6-9802a994d5ee is Pending, waiting for it to be Running (with Ready = true)
Jan 19 05:11:17.711: INFO: Pod "pod-configmaps-0ae106ae-6c69-49b5-80f6-9802a994d5ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007741164s
Jan 19 05:11:17.711: INFO: The phase of Pod pod-configmaps-0ae106ae-6c69-49b5-80f6-9802a994d5ee is Pending, waiting for it to be Running (with Ready = true)
Jan 19 05:11:19.710: INFO: Pod "pod-configmaps-0ae106ae-6c69-49b5-80f6-9802a994d5ee": Phase="Running", Reason="", readiness=true. Elapsed: 4.006703036s
Jan 19 05:11:19.710: INFO: The phase of Pod pod-configmaps-0ae106ae-6c69-49b5-80f6-9802a994d5ee is Running (Ready = true)
Jan 19 05:11:19.710: INFO: Pod "pod-configmaps-0ae106ae-6c69-49b5-80f6-9802a994d5ee" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-c63d8bfb-8c40-4ca1-9764-e476824a8ab9 01/19/23 05:11:19.719
STEP: waiting to observe update in volume 01/19/23 05:11:19.73
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 19 05:11:21.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5145" for this suite. 01/19/23 05:11:21.744
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":226,"skipped":4286,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.081 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:11:15.67
    Jan 19 05:11:15.670: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename configmap 01/19/23 05:11:15.67
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:11:15.683
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:11:15.686
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:123
    STEP: Creating configMap with name configmap-test-upd-c63d8bfb-8c40-4ca1-9764-e476824a8ab9 01/19/23 05:11:15.691
    STEP: Creating the pod 01/19/23 05:11:15.695
    Jan 19 05:11:15.703: INFO: Waiting up to 5m0s for pod "pod-configmaps-0ae106ae-6c69-49b5-80f6-9802a994d5ee" in namespace "configmap-5145" to be "running and ready"
    Jan 19 05:11:15.706: INFO: Pod "pod-configmaps-0ae106ae-6c69-49b5-80f6-9802a994d5ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.774184ms
    Jan 19 05:11:15.706: INFO: The phase of Pod pod-configmaps-0ae106ae-6c69-49b5-80f6-9802a994d5ee is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 05:11:17.711: INFO: Pod "pod-configmaps-0ae106ae-6c69-49b5-80f6-9802a994d5ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007741164s
    Jan 19 05:11:17.711: INFO: The phase of Pod pod-configmaps-0ae106ae-6c69-49b5-80f6-9802a994d5ee is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 05:11:19.710: INFO: Pod "pod-configmaps-0ae106ae-6c69-49b5-80f6-9802a994d5ee": Phase="Running", Reason="", readiness=true. Elapsed: 4.006703036s
    Jan 19 05:11:19.710: INFO: The phase of Pod pod-configmaps-0ae106ae-6c69-49b5-80f6-9802a994d5ee is Running (Ready = true)
    Jan 19 05:11:19.710: INFO: Pod "pod-configmaps-0ae106ae-6c69-49b5-80f6-9802a994d5ee" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-c63d8bfb-8c40-4ca1-9764-e476824a8ab9 01/19/23 05:11:19.719
    STEP: waiting to observe update in volume 01/19/23 05:11:19.73
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 19 05:11:21.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-5145" for this suite. 01/19/23 05:11:21.744
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:11:21.751
Jan 19 05:11:21.751: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename resourcequota 01/19/23 05:11:21.751
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:11:21.763
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:11:21.766
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
STEP: Creating a ResourceQuota 01/19/23 05:11:21.767
STEP: Getting a ResourceQuota 01/19/23 05:11:21.771
STEP: Updating a ResourceQuota 01/19/23 05:11:21.774
STEP: Verifying a ResourceQuota was modified 01/19/23 05:11:21.782
STEP: Deleting a ResourceQuota 01/19/23 05:11:21.785
STEP: Verifying the deleted ResourceQuota 01/19/23 05:11:21.79
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 19 05:11:21.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2090" for this suite. 01/19/23 05:11:21.795
{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","completed":227,"skipped":4303,"failed":0}
------------------------------
â€¢ [0.049 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:11:21.751
    Jan 19 05:11:21.751: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename resourcequota 01/19/23 05:11:21.751
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:11:21.763
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:11:21.766
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:874
    STEP: Creating a ResourceQuota 01/19/23 05:11:21.767
    STEP: Getting a ResourceQuota 01/19/23 05:11:21.771
    STEP: Updating a ResourceQuota 01/19/23 05:11:21.774
    STEP: Verifying a ResourceQuota was modified 01/19/23 05:11:21.782
    STEP: Deleting a ResourceQuota 01/19/23 05:11:21.785
    STEP: Verifying the deleted ResourceQuota 01/19/23 05:11:21.79
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 19 05:11:21.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-2090" for this suite. 01/19/23 05:11:21.795
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:11:21.801
Jan 19 05:11:21.801: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename sysctl 01/19/23 05:11:21.801
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:11:21.812
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:11:21.815
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 01/19/23 05:11:21.817
STEP: Watching for error events or started pod 01/19/23 05:11:21.824
STEP: Waiting for pod completion 01/19/23 05:11:25.828
Jan 19 05:11:25.828: INFO: Waiting up to 3m0s for pod "sysctl-3aabef07-03ae-47f8-b9a2-29b2703d7782" in namespace "sysctl-3157" to be "completed"
Jan 19 05:11:25.831: INFO: Pod "sysctl-3aabef07-03ae-47f8-b9a2-29b2703d7782": Phase="Pending", Reason="", readiness=false. Elapsed: 3.094785ms
Jan 19 05:11:27.837: INFO: Pod "sysctl-3aabef07-03ae-47f8-b9a2-29b2703d7782": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009434514s
Jan 19 05:11:27.837: INFO: Pod "sysctl-3aabef07-03ae-47f8-b9a2-29b2703d7782" satisfied condition "completed"
STEP: Checking that the pod succeeded 01/19/23 05:11:27.84
STEP: Getting logs from the pod 01/19/23 05:11:27.84
STEP: Checking that the sysctl is actually updated 01/19/23 05:11:27.846
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 19 05:11:27.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-3157" for this suite. 01/19/23 05:11:27.849
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":228,"skipped":4328,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.053 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:11:21.801
    Jan 19 05:11:21.801: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename sysctl 01/19/23 05:11:21.801
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:11:21.812
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:11:21.815
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 01/19/23 05:11:21.817
    STEP: Watching for error events or started pod 01/19/23 05:11:21.824
    STEP: Waiting for pod completion 01/19/23 05:11:25.828
    Jan 19 05:11:25.828: INFO: Waiting up to 3m0s for pod "sysctl-3aabef07-03ae-47f8-b9a2-29b2703d7782" in namespace "sysctl-3157" to be "completed"
    Jan 19 05:11:25.831: INFO: Pod "sysctl-3aabef07-03ae-47f8-b9a2-29b2703d7782": Phase="Pending", Reason="", readiness=false. Elapsed: 3.094785ms
    Jan 19 05:11:27.837: INFO: Pod "sysctl-3aabef07-03ae-47f8-b9a2-29b2703d7782": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009434514s
    Jan 19 05:11:27.837: INFO: Pod "sysctl-3aabef07-03ae-47f8-b9a2-29b2703d7782" satisfied condition "completed"
    STEP: Checking that the pod succeeded 01/19/23 05:11:27.84
    STEP: Getting logs from the pod 01/19/23 05:11:27.84
    STEP: Checking that the sysctl is actually updated 01/19/23 05:11:27.846
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 19 05:11:27.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-3157" for this suite. 01/19/23 05:11:27.849
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:11:27.854
Jan 19 05:11:27.854: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename webhook 01/19/23 05:11:27.855
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:11:27.866
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:11:27.869
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/19/23 05:11:27.887
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 05:11:28.387
STEP: Deploying the webhook pod 01/19/23 05:11:28.395
STEP: Wait for the deployment to be ready 01/19/23 05:11:28.407
Jan 19 05:11:28.418: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 19 05:11:30.428: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 5, 11, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 11, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 11, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 11, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/19/23 05:11:32.433
STEP: Verifying the service has paired with the endpoint 01/19/23 05:11:32.443
Jan 19 05:11:33.444: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
STEP: Setting timeout (1s) shorter than webhook latency (5s) 01/19/23 05:11:33.447
STEP: Registering slow webhook via the AdmissionRegistration API 01/19/23 05:11:33.447
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 01/19/23 05:11:33.462
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 01/19/23 05:11:34.474
STEP: Registering slow webhook via the AdmissionRegistration API 01/19/23 05:11:34.474
STEP: Having no error when timeout is longer than webhook latency 01/19/23 05:11:35.506
STEP: Registering slow webhook via the AdmissionRegistration API 01/19/23 05:11:35.506
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 01/19/23 05:11:40.536
STEP: Registering slow webhook via the AdmissionRegistration API 01/19/23 05:11:40.536
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 05:11:45.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-598" for this suite. 01/19/23 05:11:45.572
STEP: Destroying namespace "webhook-598-markers" for this suite. 01/19/23 05:11:45.578
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","completed":229,"skipped":4335,"failed":0}
------------------------------
â€¢ [SLOW TEST] [17.793 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:11:27.854
    Jan 19 05:11:27.854: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename webhook 01/19/23 05:11:27.855
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:11:27.866
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:11:27.869
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/19/23 05:11:27.887
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 05:11:28.387
    STEP: Deploying the webhook pod 01/19/23 05:11:28.395
    STEP: Wait for the deployment to be ready 01/19/23 05:11:28.407
    Jan 19 05:11:28.418: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 19 05:11:30.428: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 5, 11, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 11, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 11, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 11, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/19/23 05:11:32.433
    STEP: Verifying the service has paired with the endpoint 01/19/23 05:11:32.443
    Jan 19 05:11:33.444: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:380
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 01/19/23 05:11:33.447
    STEP: Registering slow webhook via the AdmissionRegistration API 01/19/23 05:11:33.447
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 01/19/23 05:11:33.462
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 01/19/23 05:11:34.474
    STEP: Registering slow webhook via the AdmissionRegistration API 01/19/23 05:11:34.474
    STEP: Having no error when timeout is longer than webhook latency 01/19/23 05:11:35.506
    STEP: Registering slow webhook via the AdmissionRegistration API 01/19/23 05:11:35.506
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 01/19/23 05:11:40.536
    STEP: Registering slow webhook via the AdmissionRegistration API 01/19/23 05:11:40.536
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 05:11:45.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-598" for this suite. 01/19/23 05:11:45.572
    STEP: Destroying namespace "webhook-598-markers" for this suite. 01/19/23 05:11:45.578
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:11:45.648
Jan 19 05:11:45.648: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename endpointslicemirroring 01/19/23 05:11:45.649
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:11:45.667
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:11:45.67
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 01/19/23 05:11:45.684
STEP: mirroring an update to a custom Endpoint 01/19/23 05:11:45.694
Jan 19 05:11:45.702: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 01/19/23 05:11:47.707
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
Jan 19 05:11:47.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-3199" for this suite. 01/19/23 05:11:47.727
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","completed":230,"skipped":4358,"failed":0}
------------------------------
â€¢ [2.084 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:11:45.648
    Jan 19 05:11:45.648: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename endpointslicemirroring 01/19/23 05:11:45.649
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:11:45.667
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:11:45.67
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 01/19/23 05:11:45.684
    STEP: mirroring an update to a custom Endpoint 01/19/23 05:11:45.694
    Jan 19 05:11:45.702: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 01/19/23 05:11:47.707
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:187
    Jan 19 05:11:47.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslicemirroring-3199" for this suite. 01/19/23 05:11:47.727
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:11:47.733
Jan 19 05:11:47.733: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename daemonsets 01/19/23 05:11:47.734
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:11:47.746
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:11:47.748
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
Jan 19 05:11:47.764: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 01/19/23 05:11:47.769
Jan 19 05:11:47.776: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 05:11:47.776: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
Jan 19 05:11:48.788: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 05:11:48.788: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
Jan 19 05:11:49.784: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 05:11:49.784: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
Jan 19 05:11:50.789: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 19 05:11:50.789: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Update daemon pods image. 01/19/23 05:11:50.819
STEP: Check that daemon pods images are updated. 01/19/23 05:11:50.842
Jan 19 05:11:50.853: INFO: Wrong image for pod: daemon-set-m8d74. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 05:11:51.861: INFO: Wrong image for pod: daemon-set-m8d74. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 05:11:52.862: INFO: Pod daemon-set-cptjm is not available
Jan 19 05:11:52.862: INFO: Wrong image for pod: daemon-set-m8d74. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 05:11:53.861: INFO: Pod daemon-set-cptjm is not available
Jan 19 05:11:53.861: INFO: Wrong image for pod: daemon-set-m8d74. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 05:11:55.862: INFO: Pod daemon-set-tktfp is not available
STEP: Check that daemon pods are still running on every node of the cluster. 01/19/23 05:11:55.865
Jan 19 05:11:55.872: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 19 05:11:55.872: INFO: Node ckcp-nks-default-worker-node-1 is running 0 daemon pod, expected 1
Jan 19 05:11:56.880: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 19 05:11:56.880: INFO: Node ckcp-nks-default-worker-node-1 is running 0 daemon pod, expected 1
Jan 19 05:11:57.880: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 19 05:11:57.880: INFO: Node ckcp-nks-default-worker-node-1 is running 0 daemon pod, expected 1
Jan 19 05:11:58.878: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 19 05:11:58.878: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/19/23 05:11:58.89
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-822, will wait for the garbage collector to delete the pods 01/19/23 05:11:58.89
Jan 19 05:11:58.951: INFO: Deleting DaemonSet.extensions daemon-set took: 7.277134ms
Jan 19 05:11:59.051: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.686488ms
Jan 19 05:12:01.356: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 05:12:01.356: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 19 05:12:01.358: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"25770"},"items":null}

Jan 19 05:12:01.361: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"25770"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 19 05:12:01.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-822" for this suite. 01/19/23 05:12:01.37
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","completed":231,"skipped":4381,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.643 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:11:47.733
    Jan 19 05:11:47.733: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename daemonsets 01/19/23 05:11:47.734
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:11:47.746
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:11:47.748
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:373
    Jan 19 05:11:47.764: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 01/19/23 05:11:47.769
    Jan 19 05:11:47.776: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 05:11:47.776: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
    Jan 19 05:11:48.788: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 05:11:48.788: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
    Jan 19 05:11:49.784: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 05:11:49.784: INFO: Node ckcp-nks-default-worker-node-0 is running 0 daemon pod, expected 1
    Jan 19 05:11:50.789: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 19 05:11:50.789: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Update daemon pods image. 01/19/23 05:11:50.819
    STEP: Check that daemon pods images are updated. 01/19/23 05:11:50.842
    Jan 19 05:11:50.853: INFO: Wrong image for pod: daemon-set-m8d74. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 19 05:11:51.861: INFO: Wrong image for pod: daemon-set-m8d74. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 19 05:11:52.862: INFO: Pod daemon-set-cptjm is not available
    Jan 19 05:11:52.862: INFO: Wrong image for pod: daemon-set-m8d74. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 19 05:11:53.861: INFO: Pod daemon-set-cptjm is not available
    Jan 19 05:11:53.861: INFO: Wrong image for pod: daemon-set-m8d74. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 19 05:11:55.862: INFO: Pod daemon-set-tktfp is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 01/19/23 05:11:55.865
    Jan 19 05:11:55.872: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 19 05:11:55.872: INFO: Node ckcp-nks-default-worker-node-1 is running 0 daemon pod, expected 1
    Jan 19 05:11:56.880: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 19 05:11:56.880: INFO: Node ckcp-nks-default-worker-node-1 is running 0 daemon pod, expected 1
    Jan 19 05:11:57.880: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 19 05:11:57.880: INFO: Node ckcp-nks-default-worker-node-1 is running 0 daemon pod, expected 1
    Jan 19 05:11:58.878: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 19 05:11:58.878: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/19/23 05:11:58.89
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-822, will wait for the garbage collector to delete the pods 01/19/23 05:11:58.89
    Jan 19 05:11:58.951: INFO: Deleting DaemonSet.extensions daemon-set took: 7.277134ms
    Jan 19 05:11:59.051: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.686488ms
    Jan 19 05:12:01.356: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 05:12:01.356: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 19 05:12:01.358: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"25770"},"items":null}

    Jan 19 05:12:01.361: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"25770"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 19 05:12:01.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-822" for this suite. 01/19/23 05:12:01.37
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:12:01.377
Jan 19 05:12:01.377: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename crd-publish-openapi 01/19/23 05:12:01.377
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:12:01.389
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:12:01.39
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
Jan 19 05:12:01.393: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/19/23 05:12:03.755
Jan 19 05:12:03.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-9686 --namespace=crd-publish-openapi-9686 create -f -'
Jan 19 05:12:04.449: INFO: stderr: ""
Jan 19 05:12:04.449: INFO: stdout: "e2e-test-crd-publish-openapi-8354-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 19 05:12:04.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-9686 --namespace=crd-publish-openapi-9686 delete e2e-test-crd-publish-openapi-8354-crds test-cr'
Jan 19 05:12:04.531: INFO: stderr: ""
Jan 19 05:12:04.531: INFO: stdout: "e2e-test-crd-publish-openapi-8354-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jan 19 05:12:04.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-9686 --namespace=crd-publish-openapi-9686 apply -f -'
Jan 19 05:12:05.187: INFO: stderr: ""
Jan 19 05:12:05.187: INFO: stdout: "e2e-test-crd-publish-openapi-8354-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 19 05:12:05.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-9686 --namespace=crd-publish-openapi-9686 delete e2e-test-crd-publish-openapi-8354-crds test-cr'
Jan 19 05:12:05.267: INFO: stderr: ""
Jan 19 05:12:05.267: INFO: stdout: "e2e-test-crd-publish-openapi-8354-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 01/19/23 05:12:05.267
Jan 19 05:12:05.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-9686 explain e2e-test-crd-publish-openapi-8354-crds'
Jan 19 05:12:05.473: INFO: stderr: ""
Jan 19 05:12:05.473: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8354-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 05:12:08.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9686" for this suite. 01/19/23 05:12:08.86
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","completed":232,"skipped":4385,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.489 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:12:01.377
    Jan 19 05:12:01.377: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename crd-publish-openapi 01/19/23 05:12:01.377
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:12:01.389
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:12:01.39
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:235
    Jan 19 05:12:01.393: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/19/23 05:12:03.755
    Jan 19 05:12:03.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-9686 --namespace=crd-publish-openapi-9686 create -f -'
    Jan 19 05:12:04.449: INFO: stderr: ""
    Jan 19 05:12:04.449: INFO: stdout: "e2e-test-crd-publish-openapi-8354-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jan 19 05:12:04.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-9686 --namespace=crd-publish-openapi-9686 delete e2e-test-crd-publish-openapi-8354-crds test-cr'
    Jan 19 05:12:04.531: INFO: stderr: ""
    Jan 19 05:12:04.531: INFO: stdout: "e2e-test-crd-publish-openapi-8354-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Jan 19 05:12:04.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-9686 --namespace=crd-publish-openapi-9686 apply -f -'
    Jan 19 05:12:05.187: INFO: stderr: ""
    Jan 19 05:12:05.187: INFO: stdout: "e2e-test-crd-publish-openapi-8354-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jan 19 05:12:05.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-9686 --namespace=crd-publish-openapi-9686 delete e2e-test-crd-publish-openapi-8354-crds test-cr'
    Jan 19 05:12:05.267: INFO: stderr: ""
    Jan 19 05:12:05.267: INFO: stdout: "e2e-test-crd-publish-openapi-8354-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 01/19/23 05:12:05.267
    Jan 19 05:12:05.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-9686 explain e2e-test-crd-publish-openapi-8354-crds'
    Jan 19 05:12:05.473: INFO: stderr: ""
    Jan 19 05:12:05.473: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8354-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 05:12:08.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-9686" for this suite. 01/19/23 05:12:08.86
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:12:08.866
Jan 19 05:12:08.866: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename container-probe 01/19/23 05:12:08.867
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:12:08.878
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:12:08.881
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
STEP: Creating pod busybox-55af74c1-c2ca-4bf4-a283-49348a17fdae in namespace container-probe-385 01/19/23 05:12:08.883
Jan 19 05:12:08.890: INFO: Waiting up to 5m0s for pod "busybox-55af74c1-c2ca-4bf4-a283-49348a17fdae" in namespace "container-probe-385" to be "not pending"
Jan 19 05:12:08.893: INFO: Pod "busybox-55af74c1-c2ca-4bf4-a283-49348a17fdae": Phase="Pending", Reason="", readiness=false. Elapsed: 3.506345ms
Jan 19 05:12:10.898: INFO: Pod "busybox-55af74c1-c2ca-4bf4-a283-49348a17fdae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00789447s
Jan 19 05:12:12.898: INFO: Pod "busybox-55af74c1-c2ca-4bf4-a283-49348a17fdae": Phase="Running", Reason="", readiness=true. Elapsed: 4.008516014s
Jan 19 05:12:12.898: INFO: Pod "busybox-55af74c1-c2ca-4bf4-a283-49348a17fdae" satisfied condition "not pending"
Jan 19 05:12:12.898: INFO: Started pod busybox-55af74c1-c2ca-4bf4-a283-49348a17fdae in namespace container-probe-385
STEP: checking the pod's current state and verifying that restartCount is present 01/19/23 05:12:12.898
Jan 19 05:12:12.902: INFO: Initial restart count of pod busybox-55af74c1-c2ca-4bf4-a283-49348a17fdae is 0
STEP: deleting the pod 01/19/23 05:16:13.443
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 19 05:16:13.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-385" for this suite. 01/19/23 05:16:13.463
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":233,"skipped":4402,"failed":0}
------------------------------
â€¢ [SLOW TEST] [244.601 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:12:08.866
    Jan 19 05:12:08.866: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename container-probe 01/19/23 05:12:08.867
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:12:08.878
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:12:08.881
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:148
    STEP: Creating pod busybox-55af74c1-c2ca-4bf4-a283-49348a17fdae in namespace container-probe-385 01/19/23 05:12:08.883
    Jan 19 05:12:08.890: INFO: Waiting up to 5m0s for pod "busybox-55af74c1-c2ca-4bf4-a283-49348a17fdae" in namespace "container-probe-385" to be "not pending"
    Jan 19 05:12:08.893: INFO: Pod "busybox-55af74c1-c2ca-4bf4-a283-49348a17fdae": Phase="Pending", Reason="", readiness=false. Elapsed: 3.506345ms
    Jan 19 05:12:10.898: INFO: Pod "busybox-55af74c1-c2ca-4bf4-a283-49348a17fdae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00789447s
    Jan 19 05:12:12.898: INFO: Pod "busybox-55af74c1-c2ca-4bf4-a283-49348a17fdae": Phase="Running", Reason="", readiness=true. Elapsed: 4.008516014s
    Jan 19 05:12:12.898: INFO: Pod "busybox-55af74c1-c2ca-4bf4-a283-49348a17fdae" satisfied condition "not pending"
    Jan 19 05:12:12.898: INFO: Started pod busybox-55af74c1-c2ca-4bf4-a283-49348a17fdae in namespace container-probe-385
    STEP: checking the pod's current state and verifying that restartCount is present 01/19/23 05:12:12.898
    Jan 19 05:12:12.902: INFO: Initial restart count of pod busybox-55af74c1-c2ca-4bf4-a283-49348a17fdae is 0
    STEP: deleting the pod 01/19/23 05:16:13.443
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 19 05:16:13.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-385" for this suite. 01/19/23 05:16:13.463
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:16:13.468
Jan 19 05:16:13.468: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename services 01/19/23 05:16:13.469
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:16:13.481
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:16:13.484
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
STEP: creating service in namespace services-9748 01/19/23 05:16:13.487
STEP: creating service affinity-nodeport in namespace services-9748 01/19/23 05:16:13.487
STEP: creating replication controller affinity-nodeport in namespace services-9748 01/19/23 05:16:13.5
I0119 05:16:13.508096      18 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-9748, replica count: 3
I0119 05:16:16.559262      18 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 19 05:16:16.569: INFO: Creating new exec pod
Jan 19 05:16:16.576: INFO: Waiting up to 5m0s for pod "execpod-affinity7wcnt" in namespace "services-9748" to be "running"
Jan 19 05:16:16.581: INFO: Pod "execpod-affinity7wcnt": Phase="Pending", Reason="", readiness=false. Elapsed: 4.97908ms
Jan 19 05:16:18.584: INFO: Pod "execpod-affinity7wcnt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007961686s
Jan 19 05:16:20.584: INFO: Pod "execpod-affinity7wcnt": Phase="Running", Reason="", readiness=true. Elapsed: 4.008432774s
Jan 19 05:16:20.584: INFO: Pod "execpod-affinity7wcnt" satisfied condition "running"
Jan 19 05:16:21.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-9748 exec execpod-affinity7wcnt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Jan 19 05:16:21.789: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jan 19 05:16:21.789: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 05:16:21.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-9748 exec execpod-affinity7wcnt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.242.124 80'
Jan 19 05:16:21.957: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.242.124 80\nConnection to 10.254.242.124 80 port [tcp/http] succeeded!\n"
Jan 19 05:16:21.957: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 05:16:21.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-9748 exec execpod-affinity7wcnt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.0.99 32673'
Jan 19 05:16:22.119: INFO: stderr: "+ nc -v -t -w 2 192.168.0.99 32673\n+ echo hostName\nConnection to 192.168.0.99 32673 port [tcp/*] succeeded!\n"
Jan 19 05:16:22.119: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 05:16:22.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-9748 exec execpod-affinity7wcnt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.0.78 32673'
Jan 19 05:16:22.275: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.0.78 32673\nConnection to 192.168.0.78 32673 port [tcp/*] succeeded!\n"
Jan 19 05:16:22.275: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 05:16:22.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-9748 exec execpod-affinity7wcnt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.0.99:32673/ ; done'
Jan 19 05:16:22.511: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32673/\n"
Jan 19 05:16:22.511: INFO: stdout: "\naffinity-nodeport-qdmwf\naffinity-nodeport-qdmwf\naffinity-nodeport-qdmwf\naffinity-nodeport-qdmwf\naffinity-nodeport-qdmwf\naffinity-nodeport-qdmwf\naffinity-nodeport-qdmwf\naffinity-nodeport-qdmwf\naffinity-nodeport-qdmwf\naffinity-nodeport-qdmwf\naffinity-nodeport-qdmwf\naffinity-nodeport-qdmwf\naffinity-nodeport-qdmwf\naffinity-nodeport-qdmwf\naffinity-nodeport-qdmwf\naffinity-nodeport-qdmwf"
Jan 19 05:16:22.511: INFO: Received response from host: affinity-nodeport-qdmwf
Jan 19 05:16:22.511: INFO: Received response from host: affinity-nodeport-qdmwf
Jan 19 05:16:22.511: INFO: Received response from host: affinity-nodeport-qdmwf
Jan 19 05:16:22.511: INFO: Received response from host: affinity-nodeport-qdmwf
Jan 19 05:16:22.511: INFO: Received response from host: affinity-nodeport-qdmwf
Jan 19 05:16:22.511: INFO: Received response from host: affinity-nodeport-qdmwf
Jan 19 05:16:22.511: INFO: Received response from host: affinity-nodeport-qdmwf
Jan 19 05:16:22.511: INFO: Received response from host: affinity-nodeport-qdmwf
Jan 19 05:16:22.511: INFO: Received response from host: affinity-nodeport-qdmwf
Jan 19 05:16:22.511: INFO: Received response from host: affinity-nodeport-qdmwf
Jan 19 05:16:22.511: INFO: Received response from host: affinity-nodeport-qdmwf
Jan 19 05:16:22.511: INFO: Received response from host: affinity-nodeport-qdmwf
Jan 19 05:16:22.511: INFO: Received response from host: affinity-nodeport-qdmwf
Jan 19 05:16:22.511: INFO: Received response from host: affinity-nodeport-qdmwf
Jan 19 05:16:22.511: INFO: Received response from host: affinity-nodeport-qdmwf
Jan 19 05:16:22.511: INFO: Received response from host: affinity-nodeport-qdmwf
Jan 19 05:16:22.511: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-9748, will wait for the garbage collector to delete the pods 01/19/23 05:16:22.525
Jan 19 05:16:22.585: INFO: Deleting ReplicationController affinity-nodeport took: 6.547509ms
Jan 19 05:16:22.686: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.19345ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 19 05:16:24.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9748" for this suite. 01/19/23 05:16:24.911
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","completed":234,"skipped":4404,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.448 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:16:13.468
    Jan 19 05:16:13.468: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename services 01/19/23 05:16:13.469
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:16:13.481
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:16:13.484
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2204
    STEP: creating service in namespace services-9748 01/19/23 05:16:13.487
    STEP: creating service affinity-nodeport in namespace services-9748 01/19/23 05:16:13.487
    STEP: creating replication controller affinity-nodeport in namespace services-9748 01/19/23 05:16:13.5
    I0119 05:16:13.508096      18 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-9748, replica count: 3
    I0119 05:16:16.559262      18 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 19 05:16:16.569: INFO: Creating new exec pod
    Jan 19 05:16:16.576: INFO: Waiting up to 5m0s for pod "execpod-affinity7wcnt" in namespace "services-9748" to be "running"
    Jan 19 05:16:16.581: INFO: Pod "execpod-affinity7wcnt": Phase="Pending", Reason="", readiness=false. Elapsed: 4.97908ms
    Jan 19 05:16:18.584: INFO: Pod "execpod-affinity7wcnt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007961686s
    Jan 19 05:16:20.584: INFO: Pod "execpod-affinity7wcnt": Phase="Running", Reason="", readiness=true. Elapsed: 4.008432774s
    Jan 19 05:16:20.584: INFO: Pod "execpod-affinity7wcnt" satisfied condition "running"
    Jan 19 05:16:21.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-9748 exec execpod-affinity7wcnt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
    Jan 19 05:16:21.789: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Jan 19 05:16:21.789: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 19 05:16:21.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-9748 exec execpod-affinity7wcnt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.242.124 80'
    Jan 19 05:16:21.957: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.242.124 80\nConnection to 10.254.242.124 80 port [tcp/http] succeeded!\n"
    Jan 19 05:16:21.957: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 19 05:16:21.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-9748 exec execpod-affinity7wcnt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.0.99 32673'
    Jan 19 05:16:22.119: INFO: stderr: "+ nc -v -t -w 2 192.168.0.99 32673\n+ echo hostName\nConnection to 192.168.0.99 32673 port [tcp/*] succeeded!\n"
    Jan 19 05:16:22.119: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 19 05:16:22.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-9748 exec execpod-affinity7wcnt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.0.78 32673'
    Jan 19 05:16:22.275: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.0.78 32673\nConnection to 192.168.0.78 32673 port [tcp/*] succeeded!\n"
    Jan 19 05:16:22.275: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 19 05:16:22.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-9748 exec execpod-affinity7wcnt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.0.99:32673/ ; done'
    Jan 19 05:16:22.511: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.99:32673/\n"
    Jan 19 05:16:22.511: INFO: stdout: "\naffinity-nodeport-qdmwf\naffinity-nodeport-qdmwf\naffinity-nodeport-qdmwf\naffinity-nodeport-qdmwf\naffinity-nodeport-qdmwf\naffinity-nodeport-qdmwf\naffinity-nodeport-qdmwf\naffinity-nodeport-qdmwf\naffinity-nodeport-qdmwf\naffinity-nodeport-qdmwf\naffinity-nodeport-qdmwf\naffinity-nodeport-qdmwf\naffinity-nodeport-qdmwf\naffinity-nodeport-qdmwf\naffinity-nodeport-qdmwf\naffinity-nodeport-qdmwf"
    Jan 19 05:16:22.511: INFO: Received response from host: affinity-nodeport-qdmwf
    Jan 19 05:16:22.511: INFO: Received response from host: affinity-nodeport-qdmwf
    Jan 19 05:16:22.511: INFO: Received response from host: affinity-nodeport-qdmwf
    Jan 19 05:16:22.511: INFO: Received response from host: affinity-nodeport-qdmwf
    Jan 19 05:16:22.511: INFO: Received response from host: affinity-nodeport-qdmwf
    Jan 19 05:16:22.511: INFO: Received response from host: affinity-nodeport-qdmwf
    Jan 19 05:16:22.511: INFO: Received response from host: affinity-nodeport-qdmwf
    Jan 19 05:16:22.511: INFO: Received response from host: affinity-nodeport-qdmwf
    Jan 19 05:16:22.511: INFO: Received response from host: affinity-nodeport-qdmwf
    Jan 19 05:16:22.511: INFO: Received response from host: affinity-nodeport-qdmwf
    Jan 19 05:16:22.511: INFO: Received response from host: affinity-nodeport-qdmwf
    Jan 19 05:16:22.511: INFO: Received response from host: affinity-nodeport-qdmwf
    Jan 19 05:16:22.511: INFO: Received response from host: affinity-nodeport-qdmwf
    Jan 19 05:16:22.511: INFO: Received response from host: affinity-nodeport-qdmwf
    Jan 19 05:16:22.511: INFO: Received response from host: affinity-nodeport-qdmwf
    Jan 19 05:16:22.511: INFO: Received response from host: affinity-nodeport-qdmwf
    Jan 19 05:16:22.511: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-9748, will wait for the garbage collector to delete the pods 01/19/23 05:16:22.525
    Jan 19 05:16:22.585: INFO: Deleting ReplicationController affinity-nodeport took: 6.547509ms
    Jan 19 05:16:22.686: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.19345ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 19 05:16:24.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9748" for this suite. 01/19/23 05:16:24.911
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:16:24.917
Jan 19 05:16:24.917: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename services 01/19/23 05:16:24.917
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:16:24.929
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:16:24.932
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
STEP: creating service in namespace services-4068 01/19/23 05:16:24.934
STEP: creating service affinity-clusterip in namespace services-4068 01/19/23 05:16:24.934
STEP: creating replication controller affinity-clusterip in namespace services-4068 01/19/23 05:16:24.946
I0119 05:16:24.953649      18 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-4068, replica count: 3
I0119 05:16:28.004564      18 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0119 05:16:31.004774      18 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 19 05:16:31.012: INFO: Creating new exec pod
Jan 19 05:16:31.021: INFO: Waiting up to 5m0s for pod "execpod-affinitylz6vl" in namespace "services-4068" to be "running"
Jan 19 05:16:31.023: INFO: Pod "execpod-affinitylz6vl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.838966ms
Jan 19 05:16:33.028: INFO: Pod "execpod-affinitylz6vl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007285828s
Jan 19 05:16:35.028: INFO: Pod "execpod-affinitylz6vl": Phase="Running", Reason="", readiness=true. Elapsed: 4.007783953s
Jan 19 05:16:35.028: INFO: Pod "execpod-affinitylz6vl" satisfied condition "running"
Jan 19 05:16:36.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-4068 exec execpod-affinitylz6vl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Jan 19 05:16:36.213: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jan 19 05:16:36.213: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 05:16:36.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-4068 exec execpod-affinitylz6vl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.157 80'
Jan 19 05:16:36.384: INFO: stderr: "+ nc -v -t -w 2 10.254.243.157 80\n+ echo hostName\nConnection to 10.254.243.157 80 port [tcp/http] succeeded!\n"
Jan 19 05:16:36.384: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 05:16:36.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-4068 exec execpod-affinitylz6vl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.243.157:80/ ; done'
Jan 19 05:16:36.615: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.243.157:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.243.157:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.243.157:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.243.157:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.243.157:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.243.157:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.243.157:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.243.157:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.243.157:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.243.157:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.243.157:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.243.157:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.243.157:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.243.157:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.243.157:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.243.157:80/\n"
Jan 19 05:16:36.615: INFO: stdout: "\naffinity-clusterip-k8mmf\naffinity-clusterip-k8mmf\naffinity-clusterip-k8mmf\naffinity-clusterip-k8mmf\naffinity-clusterip-k8mmf\naffinity-clusterip-k8mmf\naffinity-clusterip-k8mmf\naffinity-clusterip-k8mmf\naffinity-clusterip-k8mmf\naffinity-clusterip-k8mmf\naffinity-clusterip-k8mmf\naffinity-clusterip-k8mmf\naffinity-clusterip-k8mmf\naffinity-clusterip-k8mmf\naffinity-clusterip-k8mmf\naffinity-clusterip-k8mmf"
Jan 19 05:16:36.615: INFO: Received response from host: affinity-clusterip-k8mmf
Jan 19 05:16:36.615: INFO: Received response from host: affinity-clusterip-k8mmf
Jan 19 05:16:36.615: INFO: Received response from host: affinity-clusterip-k8mmf
Jan 19 05:16:36.615: INFO: Received response from host: affinity-clusterip-k8mmf
Jan 19 05:16:36.615: INFO: Received response from host: affinity-clusterip-k8mmf
Jan 19 05:16:36.615: INFO: Received response from host: affinity-clusterip-k8mmf
Jan 19 05:16:36.615: INFO: Received response from host: affinity-clusterip-k8mmf
Jan 19 05:16:36.615: INFO: Received response from host: affinity-clusterip-k8mmf
Jan 19 05:16:36.615: INFO: Received response from host: affinity-clusterip-k8mmf
Jan 19 05:16:36.615: INFO: Received response from host: affinity-clusterip-k8mmf
Jan 19 05:16:36.615: INFO: Received response from host: affinity-clusterip-k8mmf
Jan 19 05:16:36.615: INFO: Received response from host: affinity-clusterip-k8mmf
Jan 19 05:16:36.615: INFO: Received response from host: affinity-clusterip-k8mmf
Jan 19 05:16:36.615: INFO: Received response from host: affinity-clusterip-k8mmf
Jan 19 05:16:36.615: INFO: Received response from host: affinity-clusterip-k8mmf
Jan 19 05:16:36.615: INFO: Received response from host: affinity-clusterip-k8mmf
Jan 19 05:16:36.615: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-4068, will wait for the garbage collector to delete the pods 01/19/23 05:16:36.626
Jan 19 05:16:36.689: INFO: Deleting ReplicationController affinity-clusterip took: 8.795095ms
Jan 19 05:16:36.789: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.568386ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 19 05:16:38.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4068" for this suite. 01/19/23 05:16:38.91
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","completed":235,"skipped":4419,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.999 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:16:24.917
    Jan 19 05:16:24.917: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename services 01/19/23 05:16:24.917
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:16:24.929
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:16:24.932
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2157
    STEP: creating service in namespace services-4068 01/19/23 05:16:24.934
    STEP: creating service affinity-clusterip in namespace services-4068 01/19/23 05:16:24.934
    STEP: creating replication controller affinity-clusterip in namespace services-4068 01/19/23 05:16:24.946
    I0119 05:16:24.953649      18 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-4068, replica count: 3
    I0119 05:16:28.004564      18 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0119 05:16:31.004774      18 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 19 05:16:31.012: INFO: Creating new exec pod
    Jan 19 05:16:31.021: INFO: Waiting up to 5m0s for pod "execpod-affinitylz6vl" in namespace "services-4068" to be "running"
    Jan 19 05:16:31.023: INFO: Pod "execpod-affinitylz6vl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.838966ms
    Jan 19 05:16:33.028: INFO: Pod "execpod-affinitylz6vl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007285828s
    Jan 19 05:16:35.028: INFO: Pod "execpod-affinitylz6vl": Phase="Running", Reason="", readiness=true. Elapsed: 4.007783953s
    Jan 19 05:16:35.028: INFO: Pod "execpod-affinitylz6vl" satisfied condition "running"
    Jan 19 05:16:36.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-4068 exec execpod-affinitylz6vl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
    Jan 19 05:16:36.213: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Jan 19 05:16:36.213: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 19 05:16:36.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-4068 exec execpod-affinitylz6vl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.157 80'
    Jan 19 05:16:36.384: INFO: stderr: "+ nc -v -t -w 2 10.254.243.157 80\n+ echo hostName\nConnection to 10.254.243.157 80 port [tcp/http] succeeded!\n"
    Jan 19 05:16:36.384: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 19 05:16:36.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-4068 exec execpod-affinitylz6vl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.243.157:80/ ; done'
    Jan 19 05:16:36.615: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.243.157:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.243.157:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.243.157:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.243.157:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.243.157:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.243.157:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.243.157:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.243.157:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.243.157:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.243.157:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.243.157:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.243.157:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.243.157:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.243.157:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.243.157:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.243.157:80/\n"
    Jan 19 05:16:36.615: INFO: stdout: "\naffinity-clusterip-k8mmf\naffinity-clusterip-k8mmf\naffinity-clusterip-k8mmf\naffinity-clusterip-k8mmf\naffinity-clusterip-k8mmf\naffinity-clusterip-k8mmf\naffinity-clusterip-k8mmf\naffinity-clusterip-k8mmf\naffinity-clusterip-k8mmf\naffinity-clusterip-k8mmf\naffinity-clusterip-k8mmf\naffinity-clusterip-k8mmf\naffinity-clusterip-k8mmf\naffinity-clusterip-k8mmf\naffinity-clusterip-k8mmf\naffinity-clusterip-k8mmf"
    Jan 19 05:16:36.615: INFO: Received response from host: affinity-clusterip-k8mmf
    Jan 19 05:16:36.615: INFO: Received response from host: affinity-clusterip-k8mmf
    Jan 19 05:16:36.615: INFO: Received response from host: affinity-clusterip-k8mmf
    Jan 19 05:16:36.615: INFO: Received response from host: affinity-clusterip-k8mmf
    Jan 19 05:16:36.615: INFO: Received response from host: affinity-clusterip-k8mmf
    Jan 19 05:16:36.615: INFO: Received response from host: affinity-clusterip-k8mmf
    Jan 19 05:16:36.615: INFO: Received response from host: affinity-clusterip-k8mmf
    Jan 19 05:16:36.615: INFO: Received response from host: affinity-clusterip-k8mmf
    Jan 19 05:16:36.615: INFO: Received response from host: affinity-clusterip-k8mmf
    Jan 19 05:16:36.615: INFO: Received response from host: affinity-clusterip-k8mmf
    Jan 19 05:16:36.615: INFO: Received response from host: affinity-clusterip-k8mmf
    Jan 19 05:16:36.615: INFO: Received response from host: affinity-clusterip-k8mmf
    Jan 19 05:16:36.615: INFO: Received response from host: affinity-clusterip-k8mmf
    Jan 19 05:16:36.615: INFO: Received response from host: affinity-clusterip-k8mmf
    Jan 19 05:16:36.615: INFO: Received response from host: affinity-clusterip-k8mmf
    Jan 19 05:16:36.615: INFO: Received response from host: affinity-clusterip-k8mmf
    Jan 19 05:16:36.615: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-4068, will wait for the garbage collector to delete the pods 01/19/23 05:16:36.626
    Jan 19 05:16:36.689: INFO: Deleting ReplicationController affinity-clusterip took: 8.795095ms
    Jan 19 05:16:36.789: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.568386ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 19 05:16:38.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4068" for this suite. 01/19/23 05:16:38.91
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:16:38.916
Jan 19 05:16:38.916: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename svcaccounts 01/19/23 05:16:38.917
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:16:38.926
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:16:38.928
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
Jan 19 05:16:38.943: INFO: created pod
Jan 19 05:16:38.943: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-7558" to be "Succeeded or Failed"
Jan 19 05:16:38.955: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 11.846419ms
Jan 19 05:16:40.960: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016426467s
Jan 19 05:16:42.961: INFO: Pod "oidc-discovery-validator": Phase="Running", Reason="", readiness=false. Elapsed: 4.017080321s
Jan 19 05:16:44.960: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016888982s
STEP: Saw pod success 01/19/23 05:16:44.96
Jan 19 05:16:44.961: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Jan 19 05:17:14.962: INFO: polling logs
Jan 19 05:17:14.997: INFO: Pod logs: 
I0119 05:16:41.687007       1 log.go:195] OK: Got token
I0119 05:16:41.687040       1 log.go:195] validating with in-cluster discovery
I0119 05:16:42.037632       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
I0119 05:16:42.037671       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-7558:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1674105999, NotBefore:1674105399, IssuedAt:1674105399, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-7558", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"461bca3b-6861-4544-88c1-8e2699a5417a"}}}
I0119 05:16:42.123888       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0119 05:16:42.139599       1 log.go:195] OK: Validated signature on JWT
I0119 05:16:42.139669       1 log.go:195] OK: Got valid claims from token!
I0119 05:16:42.139688       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-7558:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1674105999, NotBefore:1674105399, IssuedAt:1674105399, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-7558", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"461bca3b-6861-4544-88c1-8e2699a5417a"}}}

Jan 19 05:17:14.997: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 19 05:17:15.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7558" for this suite. 01/19/23 05:17:15.007
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","completed":236,"skipped":4448,"failed":0}
------------------------------
â€¢ [SLOW TEST] [36.096 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:16:38.916
    Jan 19 05:16:38.916: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename svcaccounts 01/19/23 05:16:38.917
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:16:38.926
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:16:38.928
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:528
    Jan 19 05:16:38.943: INFO: created pod
    Jan 19 05:16:38.943: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-7558" to be "Succeeded or Failed"
    Jan 19 05:16:38.955: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 11.846419ms
    Jan 19 05:16:40.960: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016426467s
    Jan 19 05:16:42.961: INFO: Pod "oidc-discovery-validator": Phase="Running", Reason="", readiness=false. Elapsed: 4.017080321s
    Jan 19 05:16:44.960: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016888982s
    STEP: Saw pod success 01/19/23 05:16:44.96
    Jan 19 05:16:44.961: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Jan 19 05:17:14.962: INFO: polling logs
    Jan 19 05:17:14.997: INFO: Pod logs: 
    I0119 05:16:41.687007       1 log.go:195] OK: Got token
    I0119 05:16:41.687040       1 log.go:195] validating with in-cluster discovery
    I0119 05:16:42.037632       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0119 05:16:42.037671       1 log.go:195] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-7558:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1674105999, NotBefore:1674105399, IssuedAt:1674105399, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-7558", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"461bca3b-6861-4544-88c1-8e2699a5417a"}}}
    I0119 05:16:42.123888       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0119 05:16:42.139599       1 log.go:195] OK: Validated signature on JWT
    I0119 05:16:42.139669       1 log.go:195] OK: Got valid claims from token!
    I0119 05:16:42.139688       1 log.go:195] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-7558:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1674105999, NotBefore:1674105399, IssuedAt:1674105399, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-7558", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"461bca3b-6861-4544-88c1-8e2699a5417a"}}}

    Jan 19 05:17:14.997: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 19 05:17:15.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-7558" for this suite. 01/19/23 05:17:15.007
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:17:15.013
Jan 19 05:17:15.013: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename cronjob 01/19/23 05:17:15.014
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:17:15.026
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:17:15.028
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 01/19/23 05:17:15.031
STEP: Ensuring no jobs are scheduled 01/19/23 05:17:15.037
STEP: Ensuring no job exists by listing jobs explicitly 01/19/23 05:22:15.045
STEP: Removing cronjob 01/19/23 05:22:15.048
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan 19 05:22:15.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-333" for this suite. 01/19/23 05:22:15.056
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","completed":237,"skipped":4454,"failed":0}
------------------------------
â€¢ [SLOW TEST] [300.048 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:17:15.013
    Jan 19 05:17:15.013: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename cronjob 01/19/23 05:17:15.014
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:17:15.026
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:17:15.028
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 01/19/23 05:17:15.031
    STEP: Ensuring no jobs are scheduled 01/19/23 05:17:15.037
    STEP: Ensuring no job exists by listing jobs explicitly 01/19/23 05:22:15.045
    STEP: Removing cronjob 01/19/23 05:22:15.048
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan 19 05:22:15.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-333" for this suite. 01/19/23 05:22:15.056
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:22:15.061
Jan 19 05:22:15.061: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename secrets 01/19/23 05:22:15.062
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:22:15.076
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:22:15.078
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 19 05:22:15.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2721" for this suite. 01/19/23 05:22:15.113
{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","completed":238,"skipped":4466,"failed":0}
------------------------------
â€¢ [0.057 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:22:15.061
    Jan 19 05:22:15.061: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename secrets 01/19/23 05:22:15.062
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:22:15.076
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:22:15.078
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:385
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 19 05:22:15.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-2721" for this suite. 01/19/23 05:22:15.113
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:22:15.118
Jan 19 05:22:15.118: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename projected 01/19/23 05:22:15.119
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:22:15.13
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:22:15.132
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
STEP: Creating a pod to test downward API volume plugin 01/19/23 05:22:15.135
Jan 19 05:22:15.142: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0e2de796-9280-4553-98fb-5a38e8fb596c" in namespace "projected-5864" to be "Succeeded or Failed"
Jan 19 05:22:15.147: INFO: Pod "downwardapi-volume-0e2de796-9280-4553-98fb-5a38e8fb596c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.028605ms
Jan 19 05:22:17.152: INFO: Pod "downwardapi-volume-0e2de796-9280-4553-98fb-5a38e8fb596c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009744421s
Jan 19 05:22:19.151: INFO: Pod "downwardapi-volume-0e2de796-9280-4553-98fb-5a38e8fb596c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008862106s
Jan 19 05:22:21.151: INFO: Pod "downwardapi-volume-0e2de796-9280-4553-98fb-5a38e8fb596c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009134386s
STEP: Saw pod success 01/19/23 05:22:21.151
Jan 19 05:22:21.151: INFO: Pod "downwardapi-volume-0e2de796-9280-4553-98fb-5a38e8fb596c" satisfied condition "Succeeded or Failed"
Jan 19 05:22:21.155: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod downwardapi-volume-0e2de796-9280-4553-98fb-5a38e8fb596c container client-container: <nil>
STEP: delete the pod 01/19/23 05:22:21.19
Jan 19 05:22:21.203: INFO: Waiting for pod downwardapi-volume-0e2de796-9280-4553-98fb-5a38e8fb596c to disappear
Jan 19 05:22:21.207: INFO: Pod downwardapi-volume-0e2de796-9280-4553-98fb-5a38e8fb596c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 19 05:22:21.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5864" for this suite. 01/19/23 05:22:21.21
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":239,"skipped":4467,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.096 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:22:15.118
    Jan 19 05:22:15.118: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename projected 01/19/23 05:22:15.119
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:22:15.13
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:22:15.132
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:67
    STEP: Creating a pod to test downward API volume plugin 01/19/23 05:22:15.135
    Jan 19 05:22:15.142: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0e2de796-9280-4553-98fb-5a38e8fb596c" in namespace "projected-5864" to be "Succeeded or Failed"
    Jan 19 05:22:15.147: INFO: Pod "downwardapi-volume-0e2de796-9280-4553-98fb-5a38e8fb596c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.028605ms
    Jan 19 05:22:17.152: INFO: Pod "downwardapi-volume-0e2de796-9280-4553-98fb-5a38e8fb596c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009744421s
    Jan 19 05:22:19.151: INFO: Pod "downwardapi-volume-0e2de796-9280-4553-98fb-5a38e8fb596c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008862106s
    Jan 19 05:22:21.151: INFO: Pod "downwardapi-volume-0e2de796-9280-4553-98fb-5a38e8fb596c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009134386s
    STEP: Saw pod success 01/19/23 05:22:21.151
    Jan 19 05:22:21.151: INFO: Pod "downwardapi-volume-0e2de796-9280-4553-98fb-5a38e8fb596c" satisfied condition "Succeeded or Failed"
    Jan 19 05:22:21.155: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod downwardapi-volume-0e2de796-9280-4553-98fb-5a38e8fb596c container client-container: <nil>
    STEP: delete the pod 01/19/23 05:22:21.19
    Jan 19 05:22:21.203: INFO: Waiting for pod downwardapi-volume-0e2de796-9280-4553-98fb-5a38e8fb596c to disappear
    Jan 19 05:22:21.207: INFO: Pod downwardapi-volume-0e2de796-9280-4553-98fb-5a38e8fb596c no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 19 05:22:21.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5864" for this suite. 01/19/23 05:22:21.21
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:22:21.215
Jan 19 05:22:21.215: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename secrets 01/19/23 05:22:21.215
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:22:21.227
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:22:21.231
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
STEP: Creating secret with name secret-test-aeebc577-a91a-4fb9-90b6-c4a62418f348 01/19/23 05:22:21.233
STEP: Creating a pod to test consume secrets 01/19/23 05:22:21.237
Jan 19 05:22:21.245: INFO: Waiting up to 5m0s for pod "pod-secrets-c97902bd-0df6-45df-b936-0474b4690f03" in namespace "secrets-139" to be "Succeeded or Failed"
Jan 19 05:22:21.249: INFO: Pod "pod-secrets-c97902bd-0df6-45df-b936-0474b4690f03": Phase="Pending", Reason="", readiness=false. Elapsed: 4.18431ms
Jan 19 05:22:23.253: INFO: Pod "pod-secrets-c97902bd-0df6-45df-b936-0474b4690f03": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008356335s
Jan 19 05:22:25.255: INFO: Pod "pod-secrets-c97902bd-0df6-45df-b936-0474b4690f03": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009929749s
Jan 19 05:22:27.254: INFO: Pod "pod-secrets-c97902bd-0df6-45df-b936-0474b4690f03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008788052s
STEP: Saw pod success 01/19/23 05:22:27.254
Jan 19 05:22:27.254: INFO: Pod "pod-secrets-c97902bd-0df6-45df-b936-0474b4690f03" satisfied condition "Succeeded or Failed"
Jan 19 05:22:27.256: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-secrets-c97902bd-0df6-45df-b936-0474b4690f03 container secret-env-test: <nil>
STEP: delete the pod 01/19/23 05:22:27.262
Jan 19 05:22:27.275: INFO: Waiting for pod pod-secrets-c97902bd-0df6-45df-b936-0474b4690f03 to disappear
Jan 19 05:22:27.278: INFO: Pod pod-secrets-c97902bd-0df6-45df-b936-0474b4690f03 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jan 19 05:22:27.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-139" for this suite. 01/19/23 05:22:27.281
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","completed":240,"skipped":4473,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.072 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:22:21.215
    Jan 19 05:22:21.215: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename secrets 01/19/23 05:22:21.215
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:22:21.227
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:22:21.231
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:45
    STEP: Creating secret with name secret-test-aeebc577-a91a-4fb9-90b6-c4a62418f348 01/19/23 05:22:21.233
    STEP: Creating a pod to test consume secrets 01/19/23 05:22:21.237
    Jan 19 05:22:21.245: INFO: Waiting up to 5m0s for pod "pod-secrets-c97902bd-0df6-45df-b936-0474b4690f03" in namespace "secrets-139" to be "Succeeded or Failed"
    Jan 19 05:22:21.249: INFO: Pod "pod-secrets-c97902bd-0df6-45df-b936-0474b4690f03": Phase="Pending", Reason="", readiness=false. Elapsed: 4.18431ms
    Jan 19 05:22:23.253: INFO: Pod "pod-secrets-c97902bd-0df6-45df-b936-0474b4690f03": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008356335s
    Jan 19 05:22:25.255: INFO: Pod "pod-secrets-c97902bd-0df6-45df-b936-0474b4690f03": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009929749s
    Jan 19 05:22:27.254: INFO: Pod "pod-secrets-c97902bd-0df6-45df-b936-0474b4690f03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008788052s
    STEP: Saw pod success 01/19/23 05:22:27.254
    Jan 19 05:22:27.254: INFO: Pod "pod-secrets-c97902bd-0df6-45df-b936-0474b4690f03" satisfied condition "Succeeded or Failed"
    Jan 19 05:22:27.256: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-secrets-c97902bd-0df6-45df-b936-0474b4690f03 container secret-env-test: <nil>
    STEP: delete the pod 01/19/23 05:22:27.262
    Jan 19 05:22:27.275: INFO: Waiting for pod pod-secrets-c97902bd-0df6-45df-b936-0474b4690f03 to disappear
    Jan 19 05:22:27.278: INFO: Pod pod-secrets-c97902bd-0df6-45df-b936-0474b4690f03 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jan 19 05:22:27.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-139" for this suite. 01/19/23 05:22:27.281
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:22:27.287
Jan 19 05:22:27.287: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename pods 01/19/23 05:22:27.288
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:22:27.301
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:22:27.303
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
STEP: creating the pod 01/19/23 05:22:27.306
STEP: submitting the pod to kubernetes 01/19/23 05:22:27.306
Jan 19 05:22:27.313: INFO: Waiting up to 5m0s for pod "pod-update-e5ad1c7d-de36-4fc7-924e-d7f5bf1e4544" in namespace "pods-3064" to be "running and ready"
Jan 19 05:22:27.318: INFO: Pod "pod-update-e5ad1c7d-de36-4fc7-924e-d7f5bf1e4544": Phase="Pending", Reason="", readiness=false. Elapsed: 5.742995ms
Jan 19 05:22:27.318: INFO: The phase of Pod pod-update-e5ad1c7d-de36-4fc7-924e-d7f5bf1e4544 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 05:22:29.323: INFO: Pod "pod-update-e5ad1c7d-de36-4fc7-924e-d7f5bf1e4544": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009995299s
Jan 19 05:22:29.323: INFO: The phase of Pod pod-update-e5ad1c7d-de36-4fc7-924e-d7f5bf1e4544 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 05:22:31.322: INFO: Pod "pod-update-e5ad1c7d-de36-4fc7-924e-d7f5bf1e4544": Phase="Running", Reason="", readiness=true. Elapsed: 4.009610367s
Jan 19 05:22:31.322: INFO: The phase of Pod pod-update-e5ad1c7d-de36-4fc7-924e-d7f5bf1e4544 is Running (Ready = true)
Jan 19 05:22:31.322: INFO: Pod "pod-update-e5ad1c7d-de36-4fc7-924e-d7f5bf1e4544" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 01/19/23 05:22:31.325
STEP: updating the pod 01/19/23 05:22:31.327
Jan 19 05:22:31.841: INFO: Successfully updated pod "pod-update-e5ad1c7d-de36-4fc7-924e-d7f5bf1e4544"
Jan 19 05:22:31.841: INFO: Waiting up to 5m0s for pod "pod-update-e5ad1c7d-de36-4fc7-924e-d7f5bf1e4544" in namespace "pods-3064" to be "running"
Jan 19 05:22:31.844: INFO: Pod "pod-update-e5ad1c7d-de36-4fc7-924e-d7f5bf1e4544": Phase="Running", Reason="", readiness=true. Elapsed: 2.889027ms
Jan 19 05:22:31.844: INFO: Pod "pod-update-e5ad1c7d-de36-4fc7-924e-d7f5bf1e4544" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 01/19/23 05:22:31.844
Jan 19 05:22:31.846: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 19 05:22:31.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3064" for this suite. 01/19/23 05:22:31.849
{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","completed":241,"skipped":4485,"failed":0}
------------------------------
â€¢ [4.566 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:22:27.287
    Jan 19 05:22:27.287: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename pods 01/19/23 05:22:27.288
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:22:27.301
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:22:27.303
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:343
    STEP: creating the pod 01/19/23 05:22:27.306
    STEP: submitting the pod to kubernetes 01/19/23 05:22:27.306
    Jan 19 05:22:27.313: INFO: Waiting up to 5m0s for pod "pod-update-e5ad1c7d-de36-4fc7-924e-d7f5bf1e4544" in namespace "pods-3064" to be "running and ready"
    Jan 19 05:22:27.318: INFO: Pod "pod-update-e5ad1c7d-de36-4fc7-924e-d7f5bf1e4544": Phase="Pending", Reason="", readiness=false. Elapsed: 5.742995ms
    Jan 19 05:22:27.318: INFO: The phase of Pod pod-update-e5ad1c7d-de36-4fc7-924e-d7f5bf1e4544 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 05:22:29.323: INFO: Pod "pod-update-e5ad1c7d-de36-4fc7-924e-d7f5bf1e4544": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009995299s
    Jan 19 05:22:29.323: INFO: The phase of Pod pod-update-e5ad1c7d-de36-4fc7-924e-d7f5bf1e4544 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 05:22:31.322: INFO: Pod "pod-update-e5ad1c7d-de36-4fc7-924e-d7f5bf1e4544": Phase="Running", Reason="", readiness=true. Elapsed: 4.009610367s
    Jan 19 05:22:31.322: INFO: The phase of Pod pod-update-e5ad1c7d-de36-4fc7-924e-d7f5bf1e4544 is Running (Ready = true)
    Jan 19 05:22:31.322: INFO: Pod "pod-update-e5ad1c7d-de36-4fc7-924e-d7f5bf1e4544" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 01/19/23 05:22:31.325
    STEP: updating the pod 01/19/23 05:22:31.327
    Jan 19 05:22:31.841: INFO: Successfully updated pod "pod-update-e5ad1c7d-de36-4fc7-924e-d7f5bf1e4544"
    Jan 19 05:22:31.841: INFO: Waiting up to 5m0s for pod "pod-update-e5ad1c7d-de36-4fc7-924e-d7f5bf1e4544" in namespace "pods-3064" to be "running"
    Jan 19 05:22:31.844: INFO: Pod "pod-update-e5ad1c7d-de36-4fc7-924e-d7f5bf1e4544": Phase="Running", Reason="", readiness=true. Elapsed: 2.889027ms
    Jan 19 05:22:31.844: INFO: Pod "pod-update-e5ad1c7d-de36-4fc7-924e-d7f5bf1e4544" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 01/19/23 05:22:31.844
    Jan 19 05:22:31.846: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 19 05:22:31.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-3064" for this suite. 01/19/23 05:22:31.849
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:22:31.854
Jan 19 05:22:31.855: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename configmap 01/19/23 05:22:31.855
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:22:31.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:22:31.873
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
STEP: creating a ConfigMap 01/19/23 05:22:31.875
STEP: fetching the ConfigMap 01/19/23 05:22:31.879
STEP: patching the ConfigMap 01/19/23 05:22:31.881
STEP: listing all ConfigMaps in all namespaces with a label selector 01/19/23 05:22:31.885
STEP: deleting the ConfigMap by collection with a label selector 01/19/23 05:22:31.888
STEP: listing all ConfigMaps in test namespace 01/19/23 05:22:31.893
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jan 19 05:22:31.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9509" for this suite. 01/19/23 05:22:31.897
{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","completed":242,"skipped":4503,"failed":0}
------------------------------
â€¢ [0.048 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:22:31.854
    Jan 19 05:22:31.855: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename configmap 01/19/23 05:22:31.855
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:22:31.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:22:31.873
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:168
    STEP: creating a ConfigMap 01/19/23 05:22:31.875
    STEP: fetching the ConfigMap 01/19/23 05:22:31.879
    STEP: patching the ConfigMap 01/19/23 05:22:31.881
    STEP: listing all ConfigMaps in all namespaces with a label selector 01/19/23 05:22:31.885
    STEP: deleting the ConfigMap by collection with a label selector 01/19/23 05:22:31.888
    STEP: listing all ConfigMaps in test namespace 01/19/23 05:22:31.893
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 19 05:22:31.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9509" for this suite. 01/19/23 05:22:31.897
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:22:31.903
Jan 19 05:22:31.903: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename security-context-test 01/19/23 05:22:31.904
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:22:31.917
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:22:31.919
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
Jan 19 05:22:31.927: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-4ca3aacc-fc3f-4c65-9815-fd9de12aa046" in namespace "security-context-test-2950" to be "Succeeded or Failed"
Jan 19 05:22:31.933: INFO: Pod "alpine-nnp-false-4ca3aacc-fc3f-4c65-9815-fd9de12aa046": Phase="Pending", Reason="", readiness=false. Elapsed: 6.459183ms
Jan 19 05:22:33.942: INFO: Pod "alpine-nnp-false-4ca3aacc-fc3f-4c65-9815-fd9de12aa046": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015155049s
Jan 19 05:22:35.938: INFO: Pod "alpine-nnp-false-4ca3aacc-fc3f-4c65-9815-fd9de12aa046": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010976146s
Jan 19 05:22:37.937: INFO: Pod "alpine-nnp-false-4ca3aacc-fc3f-4c65-9815-fd9de12aa046": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010317643s
Jan 19 05:22:39.937: INFO: Pod "alpine-nnp-false-4ca3aacc-fc3f-4c65-9815-fd9de12aa046": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010033215s
Jan 19 05:22:41.938: INFO: Pod "alpine-nnp-false-4ca3aacc-fc3f-4c65-9815-fd9de12aa046": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.011153217s
Jan 19 05:22:41.938: INFO: Pod "alpine-nnp-false-4ca3aacc-fc3f-4c65-9815-fd9de12aa046" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 19 05:22:41.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2950" for this suite. 01/19/23 05:22:41.948
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","completed":243,"skipped":4519,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.051 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:554
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:608

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:22:31.903
    Jan 19 05:22:31.903: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename security-context-test 01/19/23 05:22:31.904
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:22:31.917
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:22:31.919
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:608
    Jan 19 05:22:31.927: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-4ca3aacc-fc3f-4c65-9815-fd9de12aa046" in namespace "security-context-test-2950" to be "Succeeded or Failed"
    Jan 19 05:22:31.933: INFO: Pod "alpine-nnp-false-4ca3aacc-fc3f-4c65-9815-fd9de12aa046": Phase="Pending", Reason="", readiness=false. Elapsed: 6.459183ms
    Jan 19 05:22:33.942: INFO: Pod "alpine-nnp-false-4ca3aacc-fc3f-4c65-9815-fd9de12aa046": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015155049s
    Jan 19 05:22:35.938: INFO: Pod "alpine-nnp-false-4ca3aacc-fc3f-4c65-9815-fd9de12aa046": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010976146s
    Jan 19 05:22:37.937: INFO: Pod "alpine-nnp-false-4ca3aacc-fc3f-4c65-9815-fd9de12aa046": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010317643s
    Jan 19 05:22:39.937: INFO: Pod "alpine-nnp-false-4ca3aacc-fc3f-4c65-9815-fd9de12aa046": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010033215s
    Jan 19 05:22:41.938: INFO: Pod "alpine-nnp-false-4ca3aacc-fc3f-4c65-9815-fd9de12aa046": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.011153217s
    Jan 19 05:22:41.938: INFO: Pod "alpine-nnp-false-4ca3aacc-fc3f-4c65-9815-fd9de12aa046" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 19 05:22:41.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-2950" for this suite. 01/19/23 05:22:41.948
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:22:41.954
Jan 19 05:22:41.955: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename secrets 01/19/23 05:22:41.955
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:22:41.967
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:22:41.969
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
STEP: Creating secret with name secret-test-map-1c7ab97a-0a27-4b91-a0f1-d7460bc16824 01/19/23 05:22:41.972
STEP: Creating a pod to test consume secrets 01/19/23 05:22:41.976
Jan 19 05:22:41.985: INFO: Waiting up to 5m0s for pod "pod-secrets-efe3a988-9245-4795-b5e6-aeb91118b134" in namespace "secrets-9406" to be "Succeeded or Failed"
Jan 19 05:22:41.989: INFO: Pod "pod-secrets-efe3a988-9245-4795-b5e6-aeb91118b134": Phase="Pending", Reason="", readiness=false. Elapsed: 3.641418ms
Jan 19 05:22:43.993: INFO: Pod "pod-secrets-efe3a988-9245-4795-b5e6-aeb91118b134": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007578399s
Jan 19 05:22:45.992: INFO: Pod "pod-secrets-efe3a988-9245-4795-b5e6-aeb91118b134": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007317976s
Jan 19 05:22:47.992: INFO: Pod "pod-secrets-efe3a988-9245-4795-b5e6-aeb91118b134": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007097383s
STEP: Saw pod success 01/19/23 05:22:47.992
Jan 19 05:22:47.992: INFO: Pod "pod-secrets-efe3a988-9245-4795-b5e6-aeb91118b134" satisfied condition "Succeeded or Failed"
Jan 19 05:22:47.996: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-secrets-efe3a988-9245-4795-b5e6-aeb91118b134 container secret-volume-test: <nil>
STEP: delete the pod 01/19/23 05:22:48.001
Jan 19 05:22:48.013: INFO: Waiting for pod pod-secrets-efe3a988-9245-4795-b5e6-aeb91118b134 to disappear
Jan 19 05:22:48.016: INFO: Pod pod-secrets-efe3a988-9245-4795-b5e6-aeb91118b134 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 19 05:22:48.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9406" for this suite. 01/19/23 05:22:48.02
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":244,"skipped":4521,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.070 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:22:41.954
    Jan 19 05:22:41.955: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename secrets 01/19/23 05:22:41.955
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:22:41.967
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:22:41.969
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:78
    STEP: Creating secret with name secret-test-map-1c7ab97a-0a27-4b91-a0f1-d7460bc16824 01/19/23 05:22:41.972
    STEP: Creating a pod to test consume secrets 01/19/23 05:22:41.976
    Jan 19 05:22:41.985: INFO: Waiting up to 5m0s for pod "pod-secrets-efe3a988-9245-4795-b5e6-aeb91118b134" in namespace "secrets-9406" to be "Succeeded or Failed"
    Jan 19 05:22:41.989: INFO: Pod "pod-secrets-efe3a988-9245-4795-b5e6-aeb91118b134": Phase="Pending", Reason="", readiness=false. Elapsed: 3.641418ms
    Jan 19 05:22:43.993: INFO: Pod "pod-secrets-efe3a988-9245-4795-b5e6-aeb91118b134": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007578399s
    Jan 19 05:22:45.992: INFO: Pod "pod-secrets-efe3a988-9245-4795-b5e6-aeb91118b134": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007317976s
    Jan 19 05:22:47.992: INFO: Pod "pod-secrets-efe3a988-9245-4795-b5e6-aeb91118b134": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007097383s
    STEP: Saw pod success 01/19/23 05:22:47.992
    Jan 19 05:22:47.992: INFO: Pod "pod-secrets-efe3a988-9245-4795-b5e6-aeb91118b134" satisfied condition "Succeeded or Failed"
    Jan 19 05:22:47.996: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-secrets-efe3a988-9245-4795-b5e6-aeb91118b134 container secret-volume-test: <nil>
    STEP: delete the pod 01/19/23 05:22:48.001
    Jan 19 05:22:48.013: INFO: Waiting for pod pod-secrets-efe3a988-9245-4795-b5e6-aeb91118b134 to disappear
    Jan 19 05:22:48.016: INFO: Pod pod-secrets-efe3a988-9245-4795-b5e6-aeb91118b134 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 19 05:22:48.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-9406" for this suite. 01/19/23 05:22:48.02
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:22:48.025
Jan 19 05:22:48.025: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename configmap 01/19/23 05:22:48.026
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:22:48.037
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:22:48.04
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
STEP: Creating configMap with name configmap-test-upd-6171bb50-ede8-44a9-a020-1f24206b66cc 01/19/23 05:22:48.045
STEP: Creating the pod 01/19/23 05:22:48.048
Jan 19 05:22:48.055: INFO: Waiting up to 5m0s for pod "pod-configmaps-cf60100c-53ad-4ec4-be3b-6df4c4954763" in namespace "configmap-6401" to be "running"
Jan 19 05:22:48.058: INFO: Pod "pod-configmaps-cf60100c-53ad-4ec4-be3b-6df4c4954763": Phase="Pending", Reason="", readiness=false. Elapsed: 2.546699ms
Jan 19 05:22:50.063: INFO: Pod "pod-configmaps-cf60100c-53ad-4ec4-be3b-6df4c4954763": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007192757s
Jan 19 05:22:52.063: INFO: Pod "pod-configmaps-cf60100c-53ad-4ec4-be3b-6df4c4954763": Phase="Running", Reason="", readiness=false. Elapsed: 4.007272371s
Jan 19 05:22:52.063: INFO: Pod "pod-configmaps-cf60100c-53ad-4ec4-be3b-6df4c4954763" satisfied condition "running"
STEP: Waiting for pod with text data 01/19/23 05:22:52.063
STEP: Waiting for pod with binary data 01/19/23 05:22:52.068
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 19 05:22:52.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6401" for this suite. 01/19/23 05:22:52.076
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","completed":245,"skipped":4524,"failed":0}
------------------------------
â€¢ [4.056 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:22:48.025
    Jan 19 05:22:48.025: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename configmap 01/19/23 05:22:48.026
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:22:48.037
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:22:48.04
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:174
    STEP: Creating configMap with name configmap-test-upd-6171bb50-ede8-44a9-a020-1f24206b66cc 01/19/23 05:22:48.045
    STEP: Creating the pod 01/19/23 05:22:48.048
    Jan 19 05:22:48.055: INFO: Waiting up to 5m0s for pod "pod-configmaps-cf60100c-53ad-4ec4-be3b-6df4c4954763" in namespace "configmap-6401" to be "running"
    Jan 19 05:22:48.058: INFO: Pod "pod-configmaps-cf60100c-53ad-4ec4-be3b-6df4c4954763": Phase="Pending", Reason="", readiness=false. Elapsed: 2.546699ms
    Jan 19 05:22:50.063: INFO: Pod "pod-configmaps-cf60100c-53ad-4ec4-be3b-6df4c4954763": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007192757s
    Jan 19 05:22:52.063: INFO: Pod "pod-configmaps-cf60100c-53ad-4ec4-be3b-6df4c4954763": Phase="Running", Reason="", readiness=false. Elapsed: 4.007272371s
    Jan 19 05:22:52.063: INFO: Pod "pod-configmaps-cf60100c-53ad-4ec4-be3b-6df4c4954763" satisfied condition "running"
    STEP: Waiting for pod with text data 01/19/23 05:22:52.063
    STEP: Waiting for pod with binary data 01/19/23 05:22:52.068
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 19 05:22:52.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6401" for this suite. 01/19/23 05:22:52.076
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:22:52.082
Jan 19 05:22:52.082: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename server-version 01/19/23 05:22:52.082
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:22:52.093
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:22:52.095
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 01/19/23 05:22:52.098
STEP: Confirm major version 01/19/23 05:22:52.098
Jan 19 05:22:52.098: INFO: Major version: 1
STEP: Confirm minor version 01/19/23 05:22:52.098
Jan 19 05:22:52.098: INFO: cleanMinorVersion: 25
Jan 19 05:22:52.098: INFO: Minor version: 25
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
Jan 19 05:22:52.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-3229" for this suite. 01/19/23 05:22:52.102
{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","completed":246,"skipped":4530,"failed":0}
------------------------------
â€¢ [0.025 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:22:52.082
    Jan 19 05:22:52.082: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename server-version 01/19/23 05:22:52.082
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:22:52.093
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:22:52.095
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 01/19/23 05:22:52.098
    STEP: Confirm major version 01/19/23 05:22:52.098
    Jan 19 05:22:52.098: INFO: Major version: 1
    STEP: Confirm minor version 01/19/23 05:22:52.098
    Jan 19 05:22:52.098: INFO: cleanMinorVersion: 25
    Jan 19 05:22:52.098: INFO: Minor version: 25
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:187
    Jan 19 05:22:52.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "server-version-3229" for this suite. 01/19/23 05:22:52.102
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:22:52.108
Jan 19 05:22:52.108: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename container-lifecycle-hook 01/19/23 05:22:52.109
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:22:52.12
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:22:52.124
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 01/19/23 05:22:52.129
Jan 19 05:22:52.140: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7578" to be "running and ready"
Jan 19 05:22:52.146: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.987495ms
Jan 19 05:22:52.146: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 19 05:22:54.149: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009354815s
Jan 19 05:22:54.149: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 19 05:22:56.150: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.01065609s
Jan 19 05:22:56.150: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 19 05:22:56.150: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
STEP: create the pod with lifecycle hook 01/19/23 05:22:56.153
Jan 19 05:22:56.158: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-7578" to be "running and ready"
Jan 19 05:22:56.162: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.104552ms
Jan 19 05:22:56.162: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 19 05:22:58.167: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008935358s
Jan 19 05:22:58.167: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 19 05:23:00.166: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.008446443s
Jan 19 05:23:00.166: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Jan 19 05:23:00.166: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 01/19/23 05:23:00.169
Jan 19 05:23:00.177: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 19 05:23:00.179: INFO: Pod pod-with-prestop-http-hook still exists
Jan 19 05:23:02.180: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 19 05:23:02.184: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 01/19/23 05:23:02.184
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jan 19 05:23:02.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7578" for this suite. 01/19/23 05:23:02.193
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","completed":247,"skipped":4559,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.093 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:22:52.108
    Jan 19 05:22:52.108: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/19/23 05:22:52.109
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:22:52.12
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:22:52.124
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 01/19/23 05:22:52.129
    Jan 19 05:22:52.140: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7578" to be "running and ready"
    Jan 19 05:22:52.146: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.987495ms
    Jan 19 05:22:52.146: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 05:22:54.149: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009354815s
    Jan 19 05:22:54.149: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 05:22:56.150: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.01065609s
    Jan 19 05:22:56.150: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 19 05:22:56.150: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:152
    STEP: create the pod with lifecycle hook 01/19/23 05:22:56.153
    Jan 19 05:22:56.158: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-7578" to be "running and ready"
    Jan 19 05:22:56.162: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.104552ms
    Jan 19 05:22:56.162: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 05:22:58.167: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008935358s
    Jan 19 05:22:58.167: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 05:23:00.166: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.008446443s
    Jan 19 05:23:00.166: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Jan 19 05:23:00.166: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 01/19/23 05:23:00.169
    Jan 19 05:23:00.177: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan 19 05:23:00.179: INFO: Pod pod-with-prestop-http-hook still exists
    Jan 19 05:23:02.180: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan 19 05:23:02.184: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 01/19/23 05:23:02.184
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jan 19 05:23:02.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-7578" for this suite. 01/19/23 05:23:02.193
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:23:02.201
Jan 19 05:23:02.201: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename containers 01/19/23 05:23:02.202
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:23:02.214
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:23:02.218
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
STEP: Creating a pod to test override all 01/19/23 05:23:02.22
Jan 19 05:23:02.227: INFO: Waiting up to 5m0s for pod "client-containers-62b9df32-02de-457c-9cc4-f11c72095239" in namespace "containers-6887" to be "Succeeded or Failed"
Jan 19 05:23:02.231: INFO: Pod "client-containers-62b9df32-02de-457c-9cc4-f11c72095239": Phase="Pending", Reason="", readiness=false. Elapsed: 3.407713ms
Jan 19 05:23:04.236: INFO: Pod "client-containers-62b9df32-02de-457c-9cc4-f11c72095239": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008557486s
Jan 19 05:23:06.234: INFO: Pod "client-containers-62b9df32-02de-457c-9cc4-f11c72095239": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006957209s
Jan 19 05:23:08.234: INFO: Pod "client-containers-62b9df32-02de-457c-9cc4-f11c72095239": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006824798s
STEP: Saw pod success 01/19/23 05:23:08.234
Jan 19 05:23:08.234: INFO: Pod "client-containers-62b9df32-02de-457c-9cc4-f11c72095239" satisfied condition "Succeeded or Failed"
Jan 19 05:23:08.237: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod client-containers-62b9df32-02de-457c-9cc4-f11c72095239 container agnhost-container: <nil>
STEP: delete the pod 01/19/23 05:23:08.242
Jan 19 05:23:08.251: INFO: Waiting for pod client-containers-62b9df32-02de-457c-9cc4-f11c72095239 to disappear
Jan 19 05:23:08.252: INFO: Pod client-containers-62b9df32-02de-457c-9cc4-f11c72095239 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jan 19 05:23:08.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6887" for this suite. 01/19/23 05:23:08.255
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","completed":248,"skipped":4587,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.058 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:23:02.201
    Jan 19 05:23:02.201: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename containers 01/19/23 05:23:02.202
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:23:02.214
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:23:02.218
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:86
    STEP: Creating a pod to test override all 01/19/23 05:23:02.22
    Jan 19 05:23:02.227: INFO: Waiting up to 5m0s for pod "client-containers-62b9df32-02de-457c-9cc4-f11c72095239" in namespace "containers-6887" to be "Succeeded or Failed"
    Jan 19 05:23:02.231: INFO: Pod "client-containers-62b9df32-02de-457c-9cc4-f11c72095239": Phase="Pending", Reason="", readiness=false. Elapsed: 3.407713ms
    Jan 19 05:23:04.236: INFO: Pod "client-containers-62b9df32-02de-457c-9cc4-f11c72095239": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008557486s
    Jan 19 05:23:06.234: INFO: Pod "client-containers-62b9df32-02de-457c-9cc4-f11c72095239": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006957209s
    Jan 19 05:23:08.234: INFO: Pod "client-containers-62b9df32-02de-457c-9cc4-f11c72095239": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006824798s
    STEP: Saw pod success 01/19/23 05:23:08.234
    Jan 19 05:23:08.234: INFO: Pod "client-containers-62b9df32-02de-457c-9cc4-f11c72095239" satisfied condition "Succeeded or Failed"
    Jan 19 05:23:08.237: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod client-containers-62b9df32-02de-457c-9cc4-f11c72095239 container agnhost-container: <nil>
    STEP: delete the pod 01/19/23 05:23:08.242
    Jan 19 05:23:08.251: INFO: Waiting for pod client-containers-62b9df32-02de-457c-9cc4-f11c72095239 to disappear
    Jan 19 05:23:08.252: INFO: Pod client-containers-62b9df32-02de-457c-9cc4-f11c72095239 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jan 19 05:23:08.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-6887" for this suite. 01/19/23 05:23:08.255
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:23:08.26
Jan 19 05:23:08.261: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename projected 01/19/23 05:23:08.261
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:23:08.273
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:23:08.274
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
STEP: Creating configMap with name cm-test-opt-del-953eeee4-8d1f-4e2e-b972-5f4798e79627 01/19/23 05:23:08.279
STEP: Creating configMap with name cm-test-opt-upd-c0450fcf-791b-4648-a20d-8d0a5c916ed6 01/19/23 05:23:08.283
STEP: Creating the pod 01/19/23 05:23:08.287
Jan 19 05:23:08.296: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d67878f6-190f-4c70-aa8a-6a1e0b6e43af" in namespace "projected-8661" to be "running and ready"
Jan 19 05:23:08.302: INFO: Pod "pod-projected-configmaps-d67878f6-190f-4c70-aa8a-6a1e0b6e43af": Phase="Pending", Reason="", readiness=false. Elapsed: 5.894557ms
Jan 19 05:23:08.302: INFO: The phase of Pod pod-projected-configmaps-d67878f6-190f-4c70-aa8a-6a1e0b6e43af is Pending, waiting for it to be Running (with Ready = true)
Jan 19 05:23:10.307: INFO: Pod "pod-projected-configmaps-d67878f6-190f-4c70-aa8a-6a1e0b6e43af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010165841s
Jan 19 05:23:10.307: INFO: The phase of Pod pod-projected-configmaps-d67878f6-190f-4c70-aa8a-6a1e0b6e43af is Pending, waiting for it to be Running (with Ready = true)
Jan 19 05:23:12.307: INFO: Pod "pod-projected-configmaps-d67878f6-190f-4c70-aa8a-6a1e0b6e43af": Phase="Running", Reason="", readiness=true. Elapsed: 4.010119396s
Jan 19 05:23:12.307: INFO: The phase of Pod pod-projected-configmaps-d67878f6-190f-4c70-aa8a-6a1e0b6e43af is Running (Ready = true)
Jan 19 05:23:12.307: INFO: Pod "pod-projected-configmaps-d67878f6-190f-4c70-aa8a-6a1e0b6e43af" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-953eeee4-8d1f-4e2e-b972-5f4798e79627 01/19/23 05:23:12.324
STEP: Updating configmap cm-test-opt-upd-c0450fcf-791b-4648-a20d-8d0a5c916ed6 01/19/23 05:23:12.33
STEP: Creating configMap with name cm-test-opt-create-fa8d6c27-b0f9-46f7-8e34-c866fa965a4a 01/19/23 05:23:12.334
STEP: waiting to observe update in volume 01/19/23 05:23:12.338
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 19 05:23:16.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8661" for this suite. 01/19/23 05:23:16.375
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":249,"skipped":4647,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.120 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:23:08.26
    Jan 19 05:23:08.261: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename projected 01/19/23 05:23:08.261
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:23:08.273
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:23:08.274
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:173
    STEP: Creating configMap with name cm-test-opt-del-953eeee4-8d1f-4e2e-b972-5f4798e79627 01/19/23 05:23:08.279
    STEP: Creating configMap with name cm-test-opt-upd-c0450fcf-791b-4648-a20d-8d0a5c916ed6 01/19/23 05:23:08.283
    STEP: Creating the pod 01/19/23 05:23:08.287
    Jan 19 05:23:08.296: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d67878f6-190f-4c70-aa8a-6a1e0b6e43af" in namespace "projected-8661" to be "running and ready"
    Jan 19 05:23:08.302: INFO: Pod "pod-projected-configmaps-d67878f6-190f-4c70-aa8a-6a1e0b6e43af": Phase="Pending", Reason="", readiness=false. Elapsed: 5.894557ms
    Jan 19 05:23:08.302: INFO: The phase of Pod pod-projected-configmaps-d67878f6-190f-4c70-aa8a-6a1e0b6e43af is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 05:23:10.307: INFO: Pod "pod-projected-configmaps-d67878f6-190f-4c70-aa8a-6a1e0b6e43af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010165841s
    Jan 19 05:23:10.307: INFO: The phase of Pod pod-projected-configmaps-d67878f6-190f-4c70-aa8a-6a1e0b6e43af is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 05:23:12.307: INFO: Pod "pod-projected-configmaps-d67878f6-190f-4c70-aa8a-6a1e0b6e43af": Phase="Running", Reason="", readiness=true. Elapsed: 4.010119396s
    Jan 19 05:23:12.307: INFO: The phase of Pod pod-projected-configmaps-d67878f6-190f-4c70-aa8a-6a1e0b6e43af is Running (Ready = true)
    Jan 19 05:23:12.307: INFO: Pod "pod-projected-configmaps-d67878f6-190f-4c70-aa8a-6a1e0b6e43af" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-953eeee4-8d1f-4e2e-b972-5f4798e79627 01/19/23 05:23:12.324
    STEP: Updating configmap cm-test-opt-upd-c0450fcf-791b-4648-a20d-8d0a5c916ed6 01/19/23 05:23:12.33
    STEP: Creating configMap with name cm-test-opt-create-fa8d6c27-b0f9-46f7-8e34-c866fa965a4a 01/19/23 05:23:12.334
    STEP: waiting to observe update in volume 01/19/23 05:23:12.338
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 19 05:23:16.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8661" for this suite. 01/19/23 05:23:16.375
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:23:16.38
Jan 19 05:23:16.381: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename cronjob 01/19/23 05:23:16.381
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:23:16.395
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:23:16.398
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 01/19/23 05:23:16.4
STEP: Ensuring a job is scheduled 01/19/23 05:23:16.407
STEP: Ensuring exactly one is scheduled 01/19/23 05:24:00.41
STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/19/23 05:24:00.414
STEP: Ensuring the job is replaced with a new one 01/19/23 05:24:00.416
STEP: Removing cronjob 01/19/23 05:25:00.42
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan 19 05:25:00.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8386" for this suite. 01/19/23 05:25:00.429
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","completed":250,"skipped":4654,"failed":0}
------------------------------
â€¢ [SLOW TEST] [104.061 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:23:16.38
    Jan 19 05:23:16.381: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename cronjob 01/19/23 05:23:16.381
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:23:16.395
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:23:16.398
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 01/19/23 05:23:16.4
    STEP: Ensuring a job is scheduled 01/19/23 05:23:16.407
    STEP: Ensuring exactly one is scheduled 01/19/23 05:24:00.41
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/19/23 05:24:00.414
    STEP: Ensuring the job is replaced with a new one 01/19/23 05:24:00.416
    STEP: Removing cronjob 01/19/23 05:25:00.42
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan 19 05:25:00.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-8386" for this suite. 01/19/23 05:25:00.429
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:25:00.442
Jan 19 05:25:00.442: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename kubectl 01/19/23 05:25:00.443
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:25:00.457
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:25:00.459
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1698
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/19/23 05:25:00.462
Jan 19 05:25:00.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-8858 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
Jan 19 05:25:00.571: INFO: stderr: ""
Jan 19 05:25:00.571: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 01/19/23 05:25:00.571
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1702
Jan 19 05:25:00.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-8858 delete pods e2e-test-httpd-pod'
Jan 19 05:25:03.765: INFO: stderr: ""
Jan 19 05:25:03.765: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 19 05:25:03.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8858" for this suite. 01/19/23 05:25:03.768
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","completed":251,"skipped":4665,"failed":0}
------------------------------
â€¢ [3.332 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1695
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1711

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:25:00.442
    Jan 19 05:25:00.442: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename kubectl 01/19/23 05:25:00.443
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:25:00.457
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:25:00.459
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1698
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1711
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/19/23 05:25:00.462
    Jan 19 05:25:00.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-8858 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
    Jan 19 05:25:00.571: INFO: stderr: ""
    Jan 19 05:25:00.571: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 01/19/23 05:25:00.571
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1702
    Jan 19 05:25:00.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-8858 delete pods e2e-test-httpd-pod'
    Jan 19 05:25:03.765: INFO: stderr: ""
    Jan 19 05:25:03.765: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 19 05:25:03.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8858" for this suite. 01/19/23 05:25:03.768
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:25:03.775
Jan 19 05:25:03.775: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename sched-pred 01/19/23 05:25:03.776
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:25:03.836
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:25:03.839
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 19 05:25:03.841: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 19 05:25:03.847: INFO: Waiting for terminating namespaces to be deleted...
Jan 19 05:25:03.849: INFO: 
Logging pods the apiserver thinks is on node ckcp-nks-default-worker-node-0 before test
Jan 19 05:25:03.854: INFO: kube-flannel-ds-amd64-djm45 from kube-flannel started at 2023-01-19 04:04:25 +0000 UTC (1 container statuses recorded)
Jan 19 05:25:03.854: INFO: 	Container kube-flannel ready: true, restart count 0
Jan 19 05:25:03.854: INFO: coredns-557b76dc76-cg7b6 from kube-system started at 2023-01-19 04:04:25 +0000 UTC (1 container statuses recorded)
Jan 19 05:25:03.854: INFO: 	Container coredns ready: true, restart count 0
Jan 19 05:25:03.854: INFO: coredns-557b76dc76-ncmw9 from kube-system started at 2023-01-19 04:04:25 +0000 UTC (1 container statuses recorded)
Jan 19 05:25:03.854: INFO: 	Container coredns ready: true, restart count 0
Jan 19 05:25:03.854: INFO: csi-cinder-controllerplugin-0 from kube-system started at 2023-01-19 04:04:35 +0000 UTC (5 container statuses recorded)
Jan 19 05:25:03.854: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 19 05:25:03.854: INFO: 	Container csi-attacher ready: true, restart count 0
Jan 19 05:25:03.854: INFO: 	Container csi-provisioner ready: true, restart count 0
Jan 19 05:25:03.854: INFO: 	Container csi-resizer ready: true, restart count 0
Jan 19 05:25:03.854: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jan 19 05:25:03.854: INFO: csi-cinder-nodeplugin-r2d6v from kube-system started at 2023-01-19 04:04:35 +0000 UTC (2 container statuses recorded)
Jan 19 05:25:03.854: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 19 05:25:03.854: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 19 05:25:03.854: INFO: dashboard-metrics-scraper-7cc7856cfb-p9hnn from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
Jan 19 05:25:03.854: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jan 19 05:25:03.854: INFO: kube-dns-autoscaler-7986c68b66-q7lq5 from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
Jan 19 05:25:03.854: INFO: 	Container autoscaler ready: true, restart count 0
Jan 19 05:25:03.854: INFO: kubernetes-dashboard-d7b78f849-zfz2z from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
Jan 19 05:25:03.854: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jan 19 05:25:03.854: INFO: metrics-server-c494c94fc-lkqhw from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
Jan 19 05:25:03.854: INFO: 	Container metrics-server ready: true, restart count 0
Jan 19 05:25:03.854: INFO: npd-428hd from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
Jan 19 05:25:03.854: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 19 05:25:03.854: INFO: snapshot-controller-6bd8bc5484-94b94 from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
Jan 19 05:25:03.854: INFO: 	Container snapshot-controller ready: true, restart count 0
Jan 19 05:25:03.854: INFO: snapshot-controller-6bd8bc5484-nh4pv from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
Jan 19 05:25:03.854: INFO: 	Container snapshot-controller ready: true, restart count 0
Jan 19 05:25:03.854: INFO: sonobuoy-systemd-logs-daemon-set-dbd8990eaea742d4-9s2t2 from sonobuoy started at 2023-01-19 04:19:31 +0000 UTC (2 container statuses recorded)
Jan 19 05:25:03.854: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 05:25:03.854: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 19 05:25:03.854: INFO: 
Logging pods the apiserver thinks is on node ckcp-nks-default-worker-node-1 before test
Jan 19 05:25:03.859: INFO: replace-27901764-5f2wq from cronjob-8386 started at 2023-01-19 05:24:00 +0000 UTC (1 container statuses recorded)
Jan 19 05:25:03.859: INFO: 	Container c ready: true, restart count 0
Jan 19 05:25:03.859: INFO: replace-27901765-zshr6 from cronjob-8386 started at 2023-01-19 05:25:00 +0000 UTC (1 container statuses recorded)
Jan 19 05:25:03.859: INFO: 	Container c ready: true, restart count 0
Jan 19 05:25:03.859: INFO: kube-flannel-ds-amd64-jqq8g from kube-flannel started at 2023-01-19 04:04:25 +0000 UTC (1 container statuses recorded)
Jan 19 05:25:03.859: INFO: 	Container kube-flannel ready: true, restart count 0
Jan 19 05:25:03.859: INFO: csi-cinder-nodeplugin-nrrr6 from kube-system started at 2023-01-19 04:04:35 +0000 UTC (2 container statuses recorded)
Jan 19 05:25:03.859: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 19 05:25:03.859: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 19 05:25:03.859: INFO: npd-wz6bt from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
Jan 19 05:25:03.859: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 19 05:25:03.859: INFO: sonobuoy from sonobuoy started at 2023-01-19 04:19:22 +0000 UTC (1 container statuses recorded)
Jan 19 05:25:03.859: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 19 05:25:03.859: INFO: sonobuoy-e2e-job-35712e4fe0b540a2 from sonobuoy started at 2023-01-19 04:19:31 +0000 UTC (2 container statuses recorded)
Jan 19 05:25:03.859: INFO: 	Container e2e ready: true, restart count 0
Jan 19 05:25:03.859: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 05:25:03.859: INFO: sonobuoy-systemd-logs-daemon-set-dbd8990eaea742d4-7w8p4 from sonobuoy started at 2023-01-19 04:19:31 +0000 UTC (2 container statuses recorded)
Jan 19 05:25:03.859: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 05:25:03.859: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/19/23 05:25:03.859
Jan 19 05:25:03.870: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6923" to be "running"
Jan 19 05:25:03.872: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.261623ms
Jan 19 05:25:05.875: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005817049s
Jan 19 05:25:07.876: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.006214701s
Jan 19 05:25:07.876: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/19/23 05:25:07.878
STEP: Trying to apply a random label on the found node. 01/19/23 05:25:07.892
STEP: verifying the node has the label kubernetes.io/e2e-87dc14d4-1d0c-481a-b86f-9b2e9d72577d 95 01/19/23 05:25:07.9
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 01/19/23 05:25:07.903
Jan 19 05:25:07.908: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-6923" to be "not pending"
Jan 19 05:25:07.915: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.547608ms
Jan 19 05:25:09.920: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01159721s
Jan 19 05:25:11.918: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 4.009791975s
Jan 19 05:25:11.918: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.0.78 on the node which pod4 resides and expect not scheduled 01/19/23 05:25:11.918
Jan 19 05:25:11.925: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-6923" to be "not pending"
Jan 19 05:25:11.930: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.359251ms
Jan 19 05:25:13.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008183204s
Jan 19 05:25:15.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007835341s
Jan 19 05:25:17.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009088268s
Jan 19 05:25:19.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007609511s
Jan 19 05:25:21.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.007798307s
Jan 19 05:25:23.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.008228397s
Jan 19 05:25:25.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.009198633s
Jan 19 05:25:27.935: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.009421506s
Jan 19 05:25:29.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.009178814s
Jan 19 05:25:31.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.008396146s
Jan 19 05:25:33.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.007604638s
Jan 19 05:25:35.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.007720324s
Jan 19 05:25:37.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.00743416s
Jan 19 05:25:39.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.008487545s
Jan 19 05:25:41.935: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.009939385s
Jan 19 05:25:43.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.007512834s
Jan 19 05:25:45.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.008814995s
Jan 19 05:25:47.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.007578118s
Jan 19 05:25:49.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.00916778s
Jan 19 05:25:51.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.007847963s
Jan 19 05:25:53.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.007668004s
Jan 19 05:25:55.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.007684342s
Jan 19 05:25:57.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.007499915s
Jan 19 05:25:59.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.008296851s
Jan 19 05:26:01.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.007971103s
Jan 19 05:26:03.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.008401624s
Jan 19 05:26:05.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.008068865s
Jan 19 05:26:07.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.00771519s
Jan 19 05:26:09.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.007706876s
Jan 19 05:26:11.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.008488788s
Jan 19 05:26:13.935: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.009499952s
Jan 19 05:26:15.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.007881166s
Jan 19 05:26:17.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.007785571s
Jan 19 05:26:19.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.007973943s
Jan 19 05:26:21.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.008277202s
Jan 19 05:26:23.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.00793422s
Jan 19 05:26:25.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.008479666s
Jan 19 05:26:27.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.007768142s
Jan 19 05:26:29.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.008254771s
Jan 19 05:26:31.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.008349416s
Jan 19 05:26:33.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.00798462s
Jan 19 05:26:35.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.008889429s
Jan 19 05:26:37.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.00887579s
Jan 19 05:26:39.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.008811407s
Jan 19 05:26:41.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.007777963s
Jan 19 05:26:43.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.007608578s
Jan 19 05:26:45.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.008089455s
Jan 19 05:26:47.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.008569273s
Jan 19 05:26:49.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.008395569s
Jan 19 05:26:51.936: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.010607013s
Jan 19 05:26:53.935: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.010241324s
Jan 19 05:26:55.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.008919611s
Jan 19 05:26:57.935: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.009855129s
Jan 19 05:26:59.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.008509953s
Jan 19 05:27:01.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.008870729s
Jan 19 05:27:03.935: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.009585755s
Jan 19 05:27:05.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.009260924s
Jan 19 05:27:07.935: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.009519943s
Jan 19 05:27:09.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.008744949s
Jan 19 05:27:11.935: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.009469016s
Jan 19 05:27:13.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.009209191s
Jan 19 05:27:15.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.008678758s
Jan 19 05:27:17.935: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.010130247s
Jan 19 05:27:19.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.008774463s
Jan 19 05:27:21.935: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.009643437s
Jan 19 05:27:23.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.009277918s
Jan 19 05:27:25.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.007768808s
Jan 19 05:27:27.932: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.007273993s
Jan 19 05:27:29.935: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.009742926s
Jan 19 05:27:31.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.007706965s
Jan 19 05:27:33.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.008469552s
Jan 19 05:27:35.932: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.007346499s
Jan 19 05:27:37.935: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.009742888s
Jan 19 05:27:39.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.008967111s
Jan 19 05:27:41.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.007835702s
Jan 19 05:27:43.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.008242636s
Jan 19 05:27:45.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.00882693s
Jan 19 05:27:47.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.007782422s
Jan 19 05:27:49.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.009001837s
Jan 19 05:27:51.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.008356913s
Jan 19 05:27:53.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.008084067s
Jan 19 05:27:55.935: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.009828596s
Jan 19 05:27:57.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.007723996s
Jan 19 05:27:59.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.008913473s
Jan 19 05:28:01.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.008861369s
Jan 19 05:28:03.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.007705044s
Jan 19 05:28:05.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.008078683s
Jan 19 05:28:07.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.007755936s
Jan 19 05:28:09.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.008713631s
Jan 19 05:28:11.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.007775703s
Jan 19 05:28:13.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.008215017s
Jan 19 05:28:15.936: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.010670923s
Jan 19 05:28:17.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.009254278s
Jan 19 05:28:19.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.008467976s
Jan 19 05:28:21.935: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.009434162s
Jan 19 05:28:23.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.009107367s
Jan 19 05:28:25.935: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.010018853s
Jan 19 05:28:27.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.007643138s
Jan 19 05:28:29.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.008055343s
Jan 19 05:28:31.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.007741229s
Jan 19 05:28:33.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.00852766s
Jan 19 05:28:35.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.007698604s
Jan 19 05:28:37.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.008818993s
Jan 19 05:28:39.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.008881817s
Jan 19 05:28:41.935: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.009461511s
Jan 19 05:28:43.935: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.010015879s
Jan 19 05:28:45.935: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.009453159s
Jan 19 05:28:47.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.008774066s
Jan 19 05:28:49.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.009060976s
Jan 19 05:28:51.935: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.009716785s
Jan 19 05:28:53.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.008800779s
Jan 19 05:28:55.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.007808958s
Jan 19 05:28:57.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.008109725s
Jan 19 05:28:59.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.00838714s
Jan 19 05:29:01.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.007908139s
Jan 19 05:29:03.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.008835987s
Jan 19 05:29:05.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.008001201s
Jan 19 05:29:07.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.008487467s
Jan 19 05:29:09.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.007850318s
Jan 19 05:29:11.935: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.009588164s
Jan 19 05:29:13.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.00742229s
Jan 19 05:29:15.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.008763292s
Jan 19 05:29:17.932: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.007257335s
Jan 19 05:29:19.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.008173806s
Jan 19 05:29:21.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.008552664s
Jan 19 05:29:23.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.008605309s
Jan 19 05:29:25.932: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.007165395s
Jan 19 05:29:27.935: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.009666514s
Jan 19 05:29:29.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.008525971s
Jan 19 05:29:31.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.008587283s
Jan 19 05:29:33.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.007431007s
Jan 19 05:29:35.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.008950873s
Jan 19 05:29:37.932: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.007113332s
Jan 19 05:29:39.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.008411126s
Jan 19 05:29:41.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.007962357s
Jan 19 05:29:43.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.007493412s
Jan 19 05:29:45.935: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.010294412s
Jan 19 05:29:47.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.008485575s
Jan 19 05:29:49.940: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.014430365s
Jan 19 05:29:51.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.008543506s
Jan 19 05:29:53.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.008457263s
Jan 19 05:29:55.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.007906454s
Jan 19 05:29:57.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.008493943s
Jan 19 05:29:59.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.00857851s
Jan 19 05:30:01.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.008892465s
Jan 19 05:30:03.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.007666542s
Jan 19 05:30:05.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.007860199s
Jan 19 05:30:07.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.008582979s
Jan 19 05:30:09.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.007712135s
Jan 19 05:30:11.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.008128551s
Jan 19 05:30:11.937: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.011388027s
STEP: removing the label kubernetes.io/e2e-87dc14d4-1d0c-481a-b86f-9b2e9d72577d off the node ckcp-nks-default-worker-node-1 01/19/23 05:30:11.937
STEP: verifying the node doesn't have the label kubernetes.io/e2e-87dc14d4-1d0c-481a-b86f-9b2e9d72577d 01/19/23 05:30:11.95
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jan 19 05:30:11.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6923" for this suite. 01/19/23 05:30:11.955
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","completed":252,"skipped":4683,"failed":0}
------------------------------
â€¢ [SLOW TEST] [308.185 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:25:03.775
    Jan 19 05:25:03.775: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename sched-pred 01/19/23 05:25:03.776
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:25:03.836
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:25:03.839
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jan 19 05:25:03.841: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 19 05:25:03.847: INFO: Waiting for terminating namespaces to be deleted...
    Jan 19 05:25:03.849: INFO: 
    Logging pods the apiserver thinks is on node ckcp-nks-default-worker-node-0 before test
    Jan 19 05:25:03.854: INFO: kube-flannel-ds-amd64-djm45 from kube-flannel started at 2023-01-19 04:04:25 +0000 UTC (1 container statuses recorded)
    Jan 19 05:25:03.854: INFO: 	Container kube-flannel ready: true, restart count 0
    Jan 19 05:25:03.854: INFO: coredns-557b76dc76-cg7b6 from kube-system started at 2023-01-19 04:04:25 +0000 UTC (1 container statuses recorded)
    Jan 19 05:25:03.854: INFO: 	Container coredns ready: true, restart count 0
    Jan 19 05:25:03.854: INFO: coredns-557b76dc76-ncmw9 from kube-system started at 2023-01-19 04:04:25 +0000 UTC (1 container statuses recorded)
    Jan 19 05:25:03.854: INFO: 	Container coredns ready: true, restart count 0
    Jan 19 05:25:03.854: INFO: csi-cinder-controllerplugin-0 from kube-system started at 2023-01-19 04:04:35 +0000 UTC (5 container statuses recorded)
    Jan 19 05:25:03.854: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Jan 19 05:25:03.854: INFO: 	Container csi-attacher ready: true, restart count 0
    Jan 19 05:25:03.854: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jan 19 05:25:03.854: INFO: 	Container csi-resizer ready: true, restart count 0
    Jan 19 05:25:03.854: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jan 19 05:25:03.854: INFO: csi-cinder-nodeplugin-r2d6v from kube-system started at 2023-01-19 04:04:35 +0000 UTC (2 container statuses recorded)
    Jan 19 05:25:03.854: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Jan 19 05:25:03.854: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jan 19 05:25:03.854: INFO: dashboard-metrics-scraper-7cc7856cfb-p9hnn from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
    Jan 19 05:25:03.854: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    Jan 19 05:25:03.854: INFO: kube-dns-autoscaler-7986c68b66-q7lq5 from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
    Jan 19 05:25:03.854: INFO: 	Container autoscaler ready: true, restart count 0
    Jan 19 05:25:03.854: INFO: kubernetes-dashboard-d7b78f849-zfz2z from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
    Jan 19 05:25:03.854: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    Jan 19 05:25:03.854: INFO: metrics-server-c494c94fc-lkqhw from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
    Jan 19 05:25:03.854: INFO: 	Container metrics-server ready: true, restart count 0
    Jan 19 05:25:03.854: INFO: npd-428hd from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
    Jan 19 05:25:03.854: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan 19 05:25:03.854: INFO: snapshot-controller-6bd8bc5484-94b94 from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
    Jan 19 05:25:03.854: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jan 19 05:25:03.854: INFO: snapshot-controller-6bd8bc5484-nh4pv from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
    Jan 19 05:25:03.854: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jan 19 05:25:03.854: INFO: sonobuoy-systemd-logs-daemon-set-dbd8990eaea742d4-9s2t2 from sonobuoy started at 2023-01-19 04:19:31 +0000 UTC (2 container statuses recorded)
    Jan 19 05:25:03.854: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 19 05:25:03.854: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 19 05:25:03.854: INFO: 
    Logging pods the apiserver thinks is on node ckcp-nks-default-worker-node-1 before test
    Jan 19 05:25:03.859: INFO: replace-27901764-5f2wq from cronjob-8386 started at 2023-01-19 05:24:00 +0000 UTC (1 container statuses recorded)
    Jan 19 05:25:03.859: INFO: 	Container c ready: true, restart count 0
    Jan 19 05:25:03.859: INFO: replace-27901765-zshr6 from cronjob-8386 started at 2023-01-19 05:25:00 +0000 UTC (1 container statuses recorded)
    Jan 19 05:25:03.859: INFO: 	Container c ready: true, restart count 0
    Jan 19 05:25:03.859: INFO: kube-flannel-ds-amd64-jqq8g from kube-flannel started at 2023-01-19 04:04:25 +0000 UTC (1 container statuses recorded)
    Jan 19 05:25:03.859: INFO: 	Container kube-flannel ready: true, restart count 0
    Jan 19 05:25:03.859: INFO: csi-cinder-nodeplugin-nrrr6 from kube-system started at 2023-01-19 04:04:35 +0000 UTC (2 container statuses recorded)
    Jan 19 05:25:03.859: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Jan 19 05:25:03.859: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jan 19 05:25:03.859: INFO: npd-wz6bt from kube-system started at 2023-01-19 04:04:35 +0000 UTC (1 container statuses recorded)
    Jan 19 05:25:03.859: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan 19 05:25:03.859: INFO: sonobuoy from sonobuoy started at 2023-01-19 04:19:22 +0000 UTC (1 container statuses recorded)
    Jan 19 05:25:03.859: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 19 05:25:03.859: INFO: sonobuoy-e2e-job-35712e4fe0b540a2 from sonobuoy started at 2023-01-19 04:19:31 +0000 UTC (2 container statuses recorded)
    Jan 19 05:25:03.859: INFO: 	Container e2e ready: true, restart count 0
    Jan 19 05:25:03.859: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 19 05:25:03.859: INFO: sonobuoy-systemd-logs-daemon-set-dbd8990eaea742d4-7w8p4 from sonobuoy started at 2023-01-19 04:19:31 +0000 UTC (2 container statuses recorded)
    Jan 19 05:25:03.859: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 19 05:25:03.859: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:699
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/19/23 05:25:03.859
    Jan 19 05:25:03.870: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6923" to be "running"
    Jan 19 05:25:03.872: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.261623ms
    Jan 19 05:25:05.875: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005817049s
    Jan 19 05:25:07.876: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.006214701s
    Jan 19 05:25:07.876: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/19/23 05:25:07.878
    STEP: Trying to apply a random label on the found node. 01/19/23 05:25:07.892
    STEP: verifying the node has the label kubernetes.io/e2e-87dc14d4-1d0c-481a-b86f-9b2e9d72577d 95 01/19/23 05:25:07.9
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 01/19/23 05:25:07.903
    Jan 19 05:25:07.908: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-6923" to be "not pending"
    Jan 19 05:25:07.915: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.547608ms
    Jan 19 05:25:09.920: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01159721s
    Jan 19 05:25:11.918: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 4.009791975s
    Jan 19 05:25:11.918: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.0.78 on the node which pod4 resides and expect not scheduled 01/19/23 05:25:11.918
    Jan 19 05:25:11.925: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-6923" to be "not pending"
    Jan 19 05:25:11.930: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.359251ms
    Jan 19 05:25:13.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008183204s
    Jan 19 05:25:15.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007835341s
    Jan 19 05:25:17.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009088268s
    Jan 19 05:25:19.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007609511s
    Jan 19 05:25:21.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.007798307s
    Jan 19 05:25:23.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.008228397s
    Jan 19 05:25:25.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.009198633s
    Jan 19 05:25:27.935: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.009421506s
    Jan 19 05:25:29.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.009178814s
    Jan 19 05:25:31.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.008396146s
    Jan 19 05:25:33.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.007604638s
    Jan 19 05:25:35.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.007720324s
    Jan 19 05:25:37.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.00743416s
    Jan 19 05:25:39.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.008487545s
    Jan 19 05:25:41.935: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.009939385s
    Jan 19 05:25:43.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.007512834s
    Jan 19 05:25:45.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.008814995s
    Jan 19 05:25:47.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.007578118s
    Jan 19 05:25:49.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.00916778s
    Jan 19 05:25:51.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.007847963s
    Jan 19 05:25:53.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.007668004s
    Jan 19 05:25:55.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.007684342s
    Jan 19 05:25:57.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.007499915s
    Jan 19 05:25:59.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.008296851s
    Jan 19 05:26:01.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.007971103s
    Jan 19 05:26:03.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.008401624s
    Jan 19 05:26:05.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.008068865s
    Jan 19 05:26:07.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.00771519s
    Jan 19 05:26:09.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.007706876s
    Jan 19 05:26:11.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.008488788s
    Jan 19 05:26:13.935: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.009499952s
    Jan 19 05:26:15.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.007881166s
    Jan 19 05:26:17.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.007785571s
    Jan 19 05:26:19.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.007973943s
    Jan 19 05:26:21.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.008277202s
    Jan 19 05:26:23.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.00793422s
    Jan 19 05:26:25.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.008479666s
    Jan 19 05:26:27.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.007768142s
    Jan 19 05:26:29.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.008254771s
    Jan 19 05:26:31.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.008349416s
    Jan 19 05:26:33.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.00798462s
    Jan 19 05:26:35.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.008889429s
    Jan 19 05:26:37.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.00887579s
    Jan 19 05:26:39.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.008811407s
    Jan 19 05:26:41.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.007777963s
    Jan 19 05:26:43.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.007608578s
    Jan 19 05:26:45.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.008089455s
    Jan 19 05:26:47.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.008569273s
    Jan 19 05:26:49.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.008395569s
    Jan 19 05:26:51.936: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.010607013s
    Jan 19 05:26:53.935: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.010241324s
    Jan 19 05:26:55.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.008919611s
    Jan 19 05:26:57.935: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.009855129s
    Jan 19 05:26:59.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.008509953s
    Jan 19 05:27:01.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.008870729s
    Jan 19 05:27:03.935: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.009585755s
    Jan 19 05:27:05.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.009260924s
    Jan 19 05:27:07.935: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.009519943s
    Jan 19 05:27:09.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.008744949s
    Jan 19 05:27:11.935: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.009469016s
    Jan 19 05:27:13.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.009209191s
    Jan 19 05:27:15.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.008678758s
    Jan 19 05:27:17.935: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.010130247s
    Jan 19 05:27:19.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.008774463s
    Jan 19 05:27:21.935: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.009643437s
    Jan 19 05:27:23.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.009277918s
    Jan 19 05:27:25.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.007768808s
    Jan 19 05:27:27.932: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.007273993s
    Jan 19 05:27:29.935: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.009742926s
    Jan 19 05:27:31.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.007706965s
    Jan 19 05:27:33.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.008469552s
    Jan 19 05:27:35.932: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.007346499s
    Jan 19 05:27:37.935: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.009742888s
    Jan 19 05:27:39.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.008967111s
    Jan 19 05:27:41.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.007835702s
    Jan 19 05:27:43.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.008242636s
    Jan 19 05:27:45.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.00882693s
    Jan 19 05:27:47.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.007782422s
    Jan 19 05:27:49.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.009001837s
    Jan 19 05:27:51.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.008356913s
    Jan 19 05:27:53.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.008084067s
    Jan 19 05:27:55.935: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.009828596s
    Jan 19 05:27:57.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.007723996s
    Jan 19 05:27:59.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.008913473s
    Jan 19 05:28:01.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.008861369s
    Jan 19 05:28:03.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.007705044s
    Jan 19 05:28:05.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.008078683s
    Jan 19 05:28:07.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.007755936s
    Jan 19 05:28:09.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.008713631s
    Jan 19 05:28:11.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.007775703s
    Jan 19 05:28:13.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.008215017s
    Jan 19 05:28:15.936: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.010670923s
    Jan 19 05:28:17.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.009254278s
    Jan 19 05:28:19.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.008467976s
    Jan 19 05:28:21.935: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.009434162s
    Jan 19 05:28:23.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.009107367s
    Jan 19 05:28:25.935: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.010018853s
    Jan 19 05:28:27.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.007643138s
    Jan 19 05:28:29.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.008055343s
    Jan 19 05:28:31.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.007741229s
    Jan 19 05:28:33.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.00852766s
    Jan 19 05:28:35.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.007698604s
    Jan 19 05:28:37.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.008818993s
    Jan 19 05:28:39.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.008881817s
    Jan 19 05:28:41.935: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.009461511s
    Jan 19 05:28:43.935: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.010015879s
    Jan 19 05:28:45.935: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.009453159s
    Jan 19 05:28:47.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.008774066s
    Jan 19 05:28:49.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.009060976s
    Jan 19 05:28:51.935: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.009716785s
    Jan 19 05:28:53.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.008800779s
    Jan 19 05:28:55.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.007808958s
    Jan 19 05:28:57.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.008109725s
    Jan 19 05:28:59.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.00838714s
    Jan 19 05:29:01.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.007908139s
    Jan 19 05:29:03.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.008835987s
    Jan 19 05:29:05.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.008001201s
    Jan 19 05:29:07.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.008487467s
    Jan 19 05:29:09.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.007850318s
    Jan 19 05:29:11.935: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.009588164s
    Jan 19 05:29:13.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.00742229s
    Jan 19 05:29:15.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.008763292s
    Jan 19 05:29:17.932: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.007257335s
    Jan 19 05:29:19.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.008173806s
    Jan 19 05:29:21.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.008552664s
    Jan 19 05:29:23.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.008605309s
    Jan 19 05:29:25.932: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.007165395s
    Jan 19 05:29:27.935: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.009666514s
    Jan 19 05:29:29.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.008525971s
    Jan 19 05:29:31.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.008587283s
    Jan 19 05:29:33.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.007431007s
    Jan 19 05:29:35.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.008950873s
    Jan 19 05:29:37.932: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.007113332s
    Jan 19 05:29:39.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.008411126s
    Jan 19 05:29:41.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.007962357s
    Jan 19 05:29:43.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.007493412s
    Jan 19 05:29:45.935: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.010294412s
    Jan 19 05:29:47.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.008485575s
    Jan 19 05:29:49.940: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.014430365s
    Jan 19 05:29:51.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.008543506s
    Jan 19 05:29:53.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.008457263s
    Jan 19 05:29:55.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.007906454s
    Jan 19 05:29:57.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.008493943s
    Jan 19 05:29:59.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.00857851s
    Jan 19 05:30:01.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.008892465s
    Jan 19 05:30:03.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.007666542s
    Jan 19 05:30:05.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.007860199s
    Jan 19 05:30:07.934: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.008582979s
    Jan 19 05:30:09.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.007712135s
    Jan 19 05:30:11.933: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.008128551s
    Jan 19 05:30:11.937: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.011388027s
    STEP: removing the label kubernetes.io/e2e-87dc14d4-1d0c-481a-b86f-9b2e9d72577d off the node ckcp-nks-default-worker-node-1 01/19/23 05:30:11.937
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-87dc14d4-1d0c-481a-b86f-9b2e9d72577d 01/19/23 05:30:11.95
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jan 19 05:30:11.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-6923" for this suite. 01/19/23 05:30:11.955
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:30:11.96
Jan 19 05:30:11.961: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename statefulset 01/19/23 05:30:11.961
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:30:11.972
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:30:11.975
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1544 01/19/23 05:30:11.976
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
STEP: Creating a new StatefulSet 01/19/23 05:30:11.981
Jan 19 05:30:11.995: INFO: Found 0 stateful pods, waiting for 3
Jan 19 05:30:22.000: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 05:30:22.000: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 05:30:22.000: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 05:30:22.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=statefulset-1544 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 19 05:30:22.188: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 19 05:30:22.188: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 19 05:30:22.188: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 01/19/23 05:30:32.205
Jan 19 05:30:32.223: INFO: Updating stateful set ss2
STEP: Creating a new revision 01/19/23 05:30:32.223
STEP: Updating Pods in reverse ordinal order 01/19/23 05:30:42.236
Jan 19 05:30:42.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=statefulset-1544 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 19 05:30:42.401: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 19 05:30:42.401: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 19 05:30:42.401: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 19 05:30:52.418: INFO: Waiting for StatefulSet statefulset-1544/ss2 to complete update
STEP: Rolling back to a previous revision 01/19/23 05:31:02.426
Jan 19 05:31:02.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=statefulset-1544 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 19 05:31:02.593: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 19 05:31:02.593: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 19 05:31:02.593: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 19 05:31:12.628: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 01/19/23 05:31:22.643
Jan 19 05:31:22.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=statefulset-1544 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 19 05:31:22.804: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 19 05:31:22.804: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 19 05:31:22.804: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 19 05:31:32.826: INFO: Waiting for StatefulSet statefulset-1544/ss2 to complete update
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 19 05:31:42.833: INFO: Deleting all statefulset in ns statefulset-1544
Jan 19 05:31:42.835: INFO: Scaling statefulset ss2 to 0
Jan 19 05:31:52.851: INFO: Waiting for statefulset status.replicas updated to 0
Jan 19 05:31:52.854: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 19 05:31:52.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1544" for this suite. 01/19/23 05:31:52.879
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","completed":253,"skipped":4683,"failed":0}
------------------------------
â€¢ [SLOW TEST] [100.928 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:304

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:30:11.96
    Jan 19 05:30:11.961: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename statefulset 01/19/23 05:30:11.961
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:30:11.972
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:30:11.975
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-1544 01/19/23 05:30:11.976
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:304
    STEP: Creating a new StatefulSet 01/19/23 05:30:11.981
    Jan 19 05:30:11.995: INFO: Found 0 stateful pods, waiting for 3
    Jan 19 05:30:22.000: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 19 05:30:22.000: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 19 05:30:22.000: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Jan 19 05:30:22.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=statefulset-1544 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 19 05:30:22.188: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 19 05:30:22.188: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 19 05:30:22.188: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 01/19/23 05:30:32.205
    Jan 19 05:30:32.223: INFO: Updating stateful set ss2
    STEP: Creating a new revision 01/19/23 05:30:32.223
    STEP: Updating Pods in reverse ordinal order 01/19/23 05:30:42.236
    Jan 19 05:30:42.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=statefulset-1544 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 19 05:30:42.401: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 19 05:30:42.401: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 19 05:30:42.401: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 19 05:30:52.418: INFO: Waiting for StatefulSet statefulset-1544/ss2 to complete update
    STEP: Rolling back to a previous revision 01/19/23 05:31:02.426
    Jan 19 05:31:02.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=statefulset-1544 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 19 05:31:02.593: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 19 05:31:02.593: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 19 05:31:02.593: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 19 05:31:12.628: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 01/19/23 05:31:22.643
    Jan 19 05:31:22.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=statefulset-1544 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 19 05:31:22.804: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 19 05:31:22.804: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 19 05:31:22.804: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 19 05:31:32.826: INFO: Waiting for StatefulSet statefulset-1544/ss2 to complete update
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 19 05:31:42.833: INFO: Deleting all statefulset in ns statefulset-1544
    Jan 19 05:31:42.835: INFO: Scaling statefulset ss2 to 0
    Jan 19 05:31:52.851: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 19 05:31:52.854: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 19 05:31:52.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-1544" for this suite. 01/19/23 05:31:52.879
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:31:52.889
Jan 19 05:31:52.889: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename dns 01/19/23 05:31:52.89
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:31:52.901
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:31:52.904
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 01/19/23 05:31:52.906
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7572.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7572.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7572.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7572.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7572.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7572.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7572.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7572.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7572.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7572.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7572.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7572.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 226.112.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.112.226_udp@PTR;check="$$(dig +tcp +noall +answer +search 226.112.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.112.226_tcp@PTR;sleep 1; done
 01/19/23 05:31:52.941
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7572.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7572.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7572.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7572.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7572.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7572.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7572.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7572.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7572.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7572.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7572.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7572.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 226.112.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.112.226_udp@PTR;check="$$(dig +tcp +noall +answer +search 226.112.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.112.226_tcp@PTR;sleep 1; done
 01/19/23 05:31:52.941
STEP: creating a pod to probe DNS 01/19/23 05:31:52.941
STEP: submitting the pod to kubernetes 01/19/23 05:31:52.941
Jan 19 05:31:52.957: INFO: Waiting up to 15m0s for pod "dns-test-419919a8-32ee-4d0d-aece-9df94116e779" in namespace "dns-7572" to be "running"
Jan 19 05:31:52.962: INFO: Pod "dns-test-419919a8-32ee-4d0d-aece-9df94116e779": Phase="Pending", Reason="", readiness=false. Elapsed: 4.38575ms
Jan 19 05:31:54.965: INFO: Pod "dns-test-419919a8-32ee-4d0d-aece-9df94116e779": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007997091s
Jan 19 05:31:56.966: INFO: Pod "dns-test-419919a8-32ee-4d0d-aece-9df94116e779": Phase="Running", Reason="", readiness=true. Elapsed: 4.00832613s
Jan 19 05:31:56.966: INFO: Pod "dns-test-419919a8-32ee-4d0d-aece-9df94116e779" satisfied condition "running"
STEP: retrieving the pod 01/19/23 05:31:56.966
STEP: looking for the results for each expected name from probers 01/19/23 05:31:56.968
Jan 19 05:31:56.972: INFO: Unable to read wheezy_udp@dns-test-service.dns-7572.svc.cluster.local from pod dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779: the server could not find the requested resource (get pods dns-test-419919a8-32ee-4d0d-aece-9df94116e779)
Jan 19 05:31:56.975: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7572.svc.cluster.local from pod dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779: the server could not find the requested resource (get pods dns-test-419919a8-32ee-4d0d-aece-9df94116e779)
Jan 19 05:31:56.977: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7572.svc.cluster.local from pod dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779: the server could not find the requested resource (get pods dns-test-419919a8-32ee-4d0d-aece-9df94116e779)
Jan 19 05:31:56.980: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7572.svc.cluster.local from pod dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779: the server could not find the requested resource (get pods dns-test-419919a8-32ee-4d0d-aece-9df94116e779)
Jan 19 05:31:56.983: INFO: Unable to read wheezy_udp@_http._tcp.test-service-2.dns-7572.svc.cluster.local from pod dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779: the server could not find the requested resource (get pods dns-test-419919a8-32ee-4d0d-aece-9df94116e779)
Jan 19 05:31:56.986: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.dns-7572.svc.cluster.local from pod dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779: the server could not find the requested resource (get pods dns-test-419919a8-32ee-4d0d-aece-9df94116e779)
Jan 19 05:31:56.989: INFO: Unable to read 10.254.112.226_udp@PTR from pod dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779: the server could not find the requested resource (get pods dns-test-419919a8-32ee-4d0d-aece-9df94116e779)
Jan 19 05:31:56.991: INFO: Unable to read 10.254.112.226_tcp@PTR from pod dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779: the server could not find the requested resource (get pods dns-test-419919a8-32ee-4d0d-aece-9df94116e779)
Jan 19 05:31:56.994: INFO: Unable to read jessie_udp@dns-test-service.dns-7572.svc.cluster.local from pod dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779: the server could not find the requested resource (get pods dns-test-419919a8-32ee-4d0d-aece-9df94116e779)
Jan 19 05:31:56.996: INFO: Unable to read jessie_tcp@dns-test-service.dns-7572.svc.cluster.local from pod dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779: the server could not find the requested resource (get pods dns-test-419919a8-32ee-4d0d-aece-9df94116e779)
Jan 19 05:31:56.999: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7572.svc.cluster.local from pod dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779: the server could not find the requested resource (get pods dns-test-419919a8-32ee-4d0d-aece-9df94116e779)
Jan 19 05:31:57.002: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7572.svc.cluster.local from pod dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779: the server could not find the requested resource (get pods dns-test-419919a8-32ee-4d0d-aece-9df94116e779)
Jan 19 05:31:57.005: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.dns-7572.svc.cluster.local from pod dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779: the server could not find the requested resource (get pods dns-test-419919a8-32ee-4d0d-aece-9df94116e779)
Jan 19 05:31:57.007: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.dns-7572.svc.cluster.local from pod dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779: the server could not find the requested resource (get pods dns-test-419919a8-32ee-4d0d-aece-9df94116e779)
Jan 19 05:31:57.010: INFO: Unable to read 10.254.112.226_udp@PTR from pod dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779: the server could not find the requested resource (get pods dns-test-419919a8-32ee-4d0d-aece-9df94116e779)
Jan 19 05:31:57.012: INFO: Unable to read 10.254.112.226_tcp@PTR from pod dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779: the server could not find the requested resource (get pods dns-test-419919a8-32ee-4d0d-aece-9df94116e779)
Jan 19 05:31:57.012: INFO: Lookups using dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779 failed for: [wheezy_udp@dns-test-service.dns-7572.svc.cluster.local wheezy_tcp@dns-test-service.dns-7572.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7572.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7572.svc.cluster.local wheezy_udp@_http._tcp.test-service-2.dns-7572.svc.cluster.local wheezy_tcp@_http._tcp.test-service-2.dns-7572.svc.cluster.local 10.254.112.226_udp@PTR 10.254.112.226_tcp@PTR jessie_udp@dns-test-service.dns-7572.svc.cluster.local jessie_tcp@dns-test-service.dns-7572.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7572.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7572.svc.cluster.local jessie_udp@_http._tcp.test-service-2.dns-7572.svc.cluster.local jessie_tcp@_http._tcp.test-service-2.dns-7572.svc.cluster.local 10.254.112.226_udp@PTR 10.254.112.226_tcp@PTR]

Jan 19 05:32:02.017: INFO: Unable to read wheezy_udp@dns-test-service.dns-7572.svc.cluster.local from pod dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779: the server could not find the requested resource (get pods dns-test-419919a8-32ee-4d0d-aece-9df94116e779)
Jan 19 05:32:02.020: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7572.svc.cluster.local from pod dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779: the server could not find the requested resource (get pods dns-test-419919a8-32ee-4d0d-aece-9df94116e779)
Jan 19 05:32:02.023: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7572.svc.cluster.local from pod dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779: the server could not find the requested resource (get pods dns-test-419919a8-32ee-4d0d-aece-9df94116e779)
Jan 19 05:32:02.027: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7572.svc.cluster.local from pod dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779: the server could not find the requested resource (get pods dns-test-419919a8-32ee-4d0d-aece-9df94116e779)
Jan 19 05:32:02.043: INFO: Unable to read wheezy_udp@_http._tcp.test-service-2.dns-7572.svc.cluster.local from pod dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779: the server could not find the requested resource (get pods dns-test-419919a8-32ee-4d0d-aece-9df94116e779)
Jan 19 05:32:02.048: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.dns-7572.svc.cluster.local from pod dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779: the server could not find the requested resource (get pods dns-test-419919a8-32ee-4d0d-aece-9df94116e779)
Jan 19 05:32:02.086: INFO: Lookups using dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779 failed for: [wheezy_udp@dns-test-service.dns-7572.svc.cluster.local wheezy_tcp@dns-test-service.dns-7572.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7572.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7572.svc.cluster.local wheezy_udp@_http._tcp.test-service-2.dns-7572.svc.cluster.local wheezy_tcp@_http._tcp.test-service-2.dns-7572.svc.cluster.local]

Jan 19 05:32:07.055: INFO: DNS probes using dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779 succeeded

STEP: deleting the pod 01/19/23 05:32:07.055
STEP: deleting the test service 01/19/23 05:32:07.073
STEP: deleting the test headless service 01/19/23 05:32:07.096
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 19 05:32:07.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7572" for this suite. 01/19/23 05:32:07.115
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","completed":254,"skipped":4683,"failed":0}
------------------------------
â€¢ [SLOW TEST] [14.231 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:31:52.889
    Jan 19 05:31:52.889: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename dns 01/19/23 05:31:52.89
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:31:52.901
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:31:52.904
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 01/19/23 05:31:52.906
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7572.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7572.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7572.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7572.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7572.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7572.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7572.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7572.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7572.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7572.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7572.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7572.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 226.112.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.112.226_udp@PTR;check="$$(dig +tcp +noall +answer +search 226.112.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.112.226_tcp@PTR;sleep 1; done
     01/19/23 05:31:52.941
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7572.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7572.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7572.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7572.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7572.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7572.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7572.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7572.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7572.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7572.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7572.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7572.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 226.112.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.112.226_udp@PTR;check="$$(dig +tcp +noall +answer +search 226.112.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.112.226_tcp@PTR;sleep 1; done
     01/19/23 05:31:52.941
    STEP: creating a pod to probe DNS 01/19/23 05:31:52.941
    STEP: submitting the pod to kubernetes 01/19/23 05:31:52.941
    Jan 19 05:31:52.957: INFO: Waiting up to 15m0s for pod "dns-test-419919a8-32ee-4d0d-aece-9df94116e779" in namespace "dns-7572" to be "running"
    Jan 19 05:31:52.962: INFO: Pod "dns-test-419919a8-32ee-4d0d-aece-9df94116e779": Phase="Pending", Reason="", readiness=false. Elapsed: 4.38575ms
    Jan 19 05:31:54.965: INFO: Pod "dns-test-419919a8-32ee-4d0d-aece-9df94116e779": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007997091s
    Jan 19 05:31:56.966: INFO: Pod "dns-test-419919a8-32ee-4d0d-aece-9df94116e779": Phase="Running", Reason="", readiness=true. Elapsed: 4.00832613s
    Jan 19 05:31:56.966: INFO: Pod "dns-test-419919a8-32ee-4d0d-aece-9df94116e779" satisfied condition "running"
    STEP: retrieving the pod 01/19/23 05:31:56.966
    STEP: looking for the results for each expected name from probers 01/19/23 05:31:56.968
    Jan 19 05:31:56.972: INFO: Unable to read wheezy_udp@dns-test-service.dns-7572.svc.cluster.local from pod dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779: the server could not find the requested resource (get pods dns-test-419919a8-32ee-4d0d-aece-9df94116e779)
    Jan 19 05:31:56.975: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7572.svc.cluster.local from pod dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779: the server could not find the requested resource (get pods dns-test-419919a8-32ee-4d0d-aece-9df94116e779)
    Jan 19 05:31:56.977: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7572.svc.cluster.local from pod dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779: the server could not find the requested resource (get pods dns-test-419919a8-32ee-4d0d-aece-9df94116e779)
    Jan 19 05:31:56.980: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7572.svc.cluster.local from pod dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779: the server could not find the requested resource (get pods dns-test-419919a8-32ee-4d0d-aece-9df94116e779)
    Jan 19 05:31:56.983: INFO: Unable to read wheezy_udp@_http._tcp.test-service-2.dns-7572.svc.cluster.local from pod dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779: the server could not find the requested resource (get pods dns-test-419919a8-32ee-4d0d-aece-9df94116e779)
    Jan 19 05:31:56.986: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.dns-7572.svc.cluster.local from pod dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779: the server could not find the requested resource (get pods dns-test-419919a8-32ee-4d0d-aece-9df94116e779)
    Jan 19 05:31:56.989: INFO: Unable to read 10.254.112.226_udp@PTR from pod dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779: the server could not find the requested resource (get pods dns-test-419919a8-32ee-4d0d-aece-9df94116e779)
    Jan 19 05:31:56.991: INFO: Unable to read 10.254.112.226_tcp@PTR from pod dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779: the server could not find the requested resource (get pods dns-test-419919a8-32ee-4d0d-aece-9df94116e779)
    Jan 19 05:31:56.994: INFO: Unable to read jessie_udp@dns-test-service.dns-7572.svc.cluster.local from pod dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779: the server could not find the requested resource (get pods dns-test-419919a8-32ee-4d0d-aece-9df94116e779)
    Jan 19 05:31:56.996: INFO: Unable to read jessie_tcp@dns-test-service.dns-7572.svc.cluster.local from pod dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779: the server could not find the requested resource (get pods dns-test-419919a8-32ee-4d0d-aece-9df94116e779)
    Jan 19 05:31:56.999: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7572.svc.cluster.local from pod dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779: the server could not find the requested resource (get pods dns-test-419919a8-32ee-4d0d-aece-9df94116e779)
    Jan 19 05:31:57.002: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7572.svc.cluster.local from pod dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779: the server could not find the requested resource (get pods dns-test-419919a8-32ee-4d0d-aece-9df94116e779)
    Jan 19 05:31:57.005: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.dns-7572.svc.cluster.local from pod dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779: the server could not find the requested resource (get pods dns-test-419919a8-32ee-4d0d-aece-9df94116e779)
    Jan 19 05:31:57.007: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.dns-7572.svc.cluster.local from pod dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779: the server could not find the requested resource (get pods dns-test-419919a8-32ee-4d0d-aece-9df94116e779)
    Jan 19 05:31:57.010: INFO: Unable to read 10.254.112.226_udp@PTR from pod dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779: the server could not find the requested resource (get pods dns-test-419919a8-32ee-4d0d-aece-9df94116e779)
    Jan 19 05:31:57.012: INFO: Unable to read 10.254.112.226_tcp@PTR from pod dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779: the server could not find the requested resource (get pods dns-test-419919a8-32ee-4d0d-aece-9df94116e779)
    Jan 19 05:31:57.012: INFO: Lookups using dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779 failed for: [wheezy_udp@dns-test-service.dns-7572.svc.cluster.local wheezy_tcp@dns-test-service.dns-7572.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7572.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7572.svc.cluster.local wheezy_udp@_http._tcp.test-service-2.dns-7572.svc.cluster.local wheezy_tcp@_http._tcp.test-service-2.dns-7572.svc.cluster.local 10.254.112.226_udp@PTR 10.254.112.226_tcp@PTR jessie_udp@dns-test-service.dns-7572.svc.cluster.local jessie_tcp@dns-test-service.dns-7572.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7572.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7572.svc.cluster.local jessie_udp@_http._tcp.test-service-2.dns-7572.svc.cluster.local jessie_tcp@_http._tcp.test-service-2.dns-7572.svc.cluster.local 10.254.112.226_udp@PTR 10.254.112.226_tcp@PTR]

    Jan 19 05:32:02.017: INFO: Unable to read wheezy_udp@dns-test-service.dns-7572.svc.cluster.local from pod dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779: the server could not find the requested resource (get pods dns-test-419919a8-32ee-4d0d-aece-9df94116e779)
    Jan 19 05:32:02.020: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7572.svc.cluster.local from pod dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779: the server could not find the requested resource (get pods dns-test-419919a8-32ee-4d0d-aece-9df94116e779)
    Jan 19 05:32:02.023: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7572.svc.cluster.local from pod dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779: the server could not find the requested resource (get pods dns-test-419919a8-32ee-4d0d-aece-9df94116e779)
    Jan 19 05:32:02.027: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7572.svc.cluster.local from pod dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779: the server could not find the requested resource (get pods dns-test-419919a8-32ee-4d0d-aece-9df94116e779)
    Jan 19 05:32:02.043: INFO: Unable to read wheezy_udp@_http._tcp.test-service-2.dns-7572.svc.cluster.local from pod dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779: the server could not find the requested resource (get pods dns-test-419919a8-32ee-4d0d-aece-9df94116e779)
    Jan 19 05:32:02.048: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.dns-7572.svc.cluster.local from pod dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779: the server could not find the requested resource (get pods dns-test-419919a8-32ee-4d0d-aece-9df94116e779)
    Jan 19 05:32:02.086: INFO: Lookups using dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779 failed for: [wheezy_udp@dns-test-service.dns-7572.svc.cluster.local wheezy_tcp@dns-test-service.dns-7572.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7572.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7572.svc.cluster.local wheezy_udp@_http._tcp.test-service-2.dns-7572.svc.cluster.local wheezy_tcp@_http._tcp.test-service-2.dns-7572.svc.cluster.local]

    Jan 19 05:32:07.055: INFO: DNS probes using dns-7572/dns-test-419919a8-32ee-4d0d-aece-9df94116e779 succeeded

    STEP: deleting the pod 01/19/23 05:32:07.055
    STEP: deleting the test service 01/19/23 05:32:07.073
    STEP: deleting the test headless service 01/19/23 05:32:07.096
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 19 05:32:07.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-7572" for this suite. 01/19/23 05:32:07.115
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:32:07.121
Jan 19 05:32:07.121: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename downward-api 01/19/23 05:32:07.121
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:32:07.136
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:32:07.138
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
STEP: Creating a pod to test downward api env vars 01/19/23 05:32:07.141
Jan 19 05:32:07.149: INFO: Waiting up to 5m0s for pod "downward-api-f2fc014e-8da1-409c-8f7f-6c25644bce6d" in namespace "downward-api-1817" to be "Succeeded or Failed"
Jan 19 05:32:07.157: INFO: Pod "downward-api-f2fc014e-8da1-409c-8f7f-6c25644bce6d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.183079ms
Jan 19 05:32:09.162: INFO: Pod "downward-api-f2fc014e-8da1-409c-8f7f-6c25644bce6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013203206s
Jan 19 05:32:11.161: INFO: Pod "downward-api-f2fc014e-8da1-409c-8f7f-6c25644bce6d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01257194s
Jan 19 05:32:13.161: INFO: Pod "downward-api-f2fc014e-8da1-409c-8f7f-6c25644bce6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012044191s
STEP: Saw pod success 01/19/23 05:32:13.161
Jan 19 05:32:13.161: INFO: Pod "downward-api-f2fc014e-8da1-409c-8f7f-6c25644bce6d" satisfied condition "Succeeded or Failed"
Jan 19 05:32:13.163: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod downward-api-f2fc014e-8da1-409c-8f7f-6c25644bce6d container dapi-container: <nil>
STEP: delete the pod 01/19/23 05:32:13.199
Jan 19 05:32:13.208: INFO: Waiting for pod downward-api-f2fc014e-8da1-409c-8f7f-6c25644bce6d to disappear
Jan 19 05:32:13.211: INFO: Pod downward-api-f2fc014e-8da1-409c-8f7f-6c25644bce6d no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan 19 05:32:13.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1817" for this suite. 01/19/23 05:32:13.214
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","completed":255,"skipped":4685,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.098 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:32:07.121
    Jan 19 05:32:07.121: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename downward-api 01/19/23 05:32:07.121
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:32:07.136
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:32:07.138
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:216
    STEP: Creating a pod to test downward api env vars 01/19/23 05:32:07.141
    Jan 19 05:32:07.149: INFO: Waiting up to 5m0s for pod "downward-api-f2fc014e-8da1-409c-8f7f-6c25644bce6d" in namespace "downward-api-1817" to be "Succeeded or Failed"
    Jan 19 05:32:07.157: INFO: Pod "downward-api-f2fc014e-8da1-409c-8f7f-6c25644bce6d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.183079ms
    Jan 19 05:32:09.162: INFO: Pod "downward-api-f2fc014e-8da1-409c-8f7f-6c25644bce6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013203206s
    Jan 19 05:32:11.161: INFO: Pod "downward-api-f2fc014e-8da1-409c-8f7f-6c25644bce6d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01257194s
    Jan 19 05:32:13.161: INFO: Pod "downward-api-f2fc014e-8da1-409c-8f7f-6c25644bce6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012044191s
    STEP: Saw pod success 01/19/23 05:32:13.161
    Jan 19 05:32:13.161: INFO: Pod "downward-api-f2fc014e-8da1-409c-8f7f-6c25644bce6d" satisfied condition "Succeeded or Failed"
    Jan 19 05:32:13.163: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod downward-api-f2fc014e-8da1-409c-8f7f-6c25644bce6d container dapi-container: <nil>
    STEP: delete the pod 01/19/23 05:32:13.199
    Jan 19 05:32:13.208: INFO: Waiting for pod downward-api-f2fc014e-8da1-409c-8f7f-6c25644bce6d to disappear
    Jan 19 05:32:13.211: INFO: Pod downward-api-f2fc014e-8da1-409c-8f7f-6c25644bce6d no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan 19 05:32:13.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1817" for this suite. 01/19/23 05:32:13.214
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:32:13.218
Jan 19 05:32:13.219: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename security-context 01/19/23 05:32:13.219
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:32:13.233
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:32:13.236
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/19/23 05:32:13.238
Jan 19 05:32:13.246: INFO: Waiting up to 5m0s for pod "security-context-96ff41c7-5fd0-4169-9109-a28910ecf589" in namespace "security-context-5027" to be "Succeeded or Failed"
Jan 19 05:32:13.253: INFO: Pod "security-context-96ff41c7-5fd0-4169-9109-a28910ecf589": Phase="Pending", Reason="", readiness=false. Elapsed: 7.411998ms
Jan 19 05:32:15.257: INFO: Pod "security-context-96ff41c7-5fd0-4169-9109-a28910ecf589": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011250301s
Jan 19 05:32:17.258: INFO: Pod "security-context-96ff41c7-5fd0-4169-9109-a28910ecf589": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012057133s
Jan 19 05:32:19.258: INFO: Pod "security-context-96ff41c7-5fd0-4169-9109-a28910ecf589": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012317481s
STEP: Saw pod success 01/19/23 05:32:19.258
Jan 19 05:32:19.258: INFO: Pod "security-context-96ff41c7-5fd0-4169-9109-a28910ecf589" satisfied condition "Succeeded or Failed"
Jan 19 05:32:19.261: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod security-context-96ff41c7-5fd0-4169-9109-a28910ecf589 container test-container: <nil>
STEP: delete the pod 01/19/23 05:32:19.268
Jan 19 05:32:19.283: INFO: Waiting for pod security-context-96ff41c7-5fd0-4169-9109-a28910ecf589 to disappear
Jan 19 05:32:19.286: INFO: Pod security-context-96ff41c7-5fd0-4169-9109-a28910ecf589 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 19 05:32:19.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-5027" for this suite. 01/19/23 05:32:19.289
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":256,"skipped":4686,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.077 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:32:13.218
    Jan 19 05:32:13.219: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename security-context 01/19/23 05:32:13.219
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:32:13.233
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:32:13.236
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:97
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/19/23 05:32:13.238
    Jan 19 05:32:13.246: INFO: Waiting up to 5m0s for pod "security-context-96ff41c7-5fd0-4169-9109-a28910ecf589" in namespace "security-context-5027" to be "Succeeded or Failed"
    Jan 19 05:32:13.253: INFO: Pod "security-context-96ff41c7-5fd0-4169-9109-a28910ecf589": Phase="Pending", Reason="", readiness=false. Elapsed: 7.411998ms
    Jan 19 05:32:15.257: INFO: Pod "security-context-96ff41c7-5fd0-4169-9109-a28910ecf589": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011250301s
    Jan 19 05:32:17.258: INFO: Pod "security-context-96ff41c7-5fd0-4169-9109-a28910ecf589": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012057133s
    Jan 19 05:32:19.258: INFO: Pod "security-context-96ff41c7-5fd0-4169-9109-a28910ecf589": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012317481s
    STEP: Saw pod success 01/19/23 05:32:19.258
    Jan 19 05:32:19.258: INFO: Pod "security-context-96ff41c7-5fd0-4169-9109-a28910ecf589" satisfied condition "Succeeded or Failed"
    Jan 19 05:32:19.261: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod security-context-96ff41c7-5fd0-4169-9109-a28910ecf589 container test-container: <nil>
    STEP: delete the pod 01/19/23 05:32:19.268
    Jan 19 05:32:19.283: INFO: Waiting for pod security-context-96ff41c7-5fd0-4169-9109-a28910ecf589 to disappear
    Jan 19 05:32:19.286: INFO: Pod security-context-96ff41c7-5fd0-4169-9109-a28910ecf589 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 19 05:32:19.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-5027" for this suite. 01/19/23 05:32:19.289
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:32:19.296
Jan 19 05:32:19.296: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename sched-preemption 01/19/23 05:32:19.297
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:32:19.311
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:32:19.313
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 19 05:32:19.326: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 19 05:33:19.343: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:33:19.346
Jan 19 05:33:19.346: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename sched-preemption-path 01/19/23 05:33:19.347
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:33:19.358
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:33:19.361
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
Jan 19 05:33:19.373: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Jan 19 05:33:19.376: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
Jan 19 05:33:19.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-6521" for this suite. 01/19/23 05:33:19.394
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jan 19 05:33:19.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7423" for this suite. 01/19/23 05:33:19.411
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","completed":257,"skipped":4708,"failed":0}
------------------------------
â€¢ [SLOW TEST] [60.144 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:733

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:32:19.296
    Jan 19 05:32:19.296: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename sched-preemption 01/19/23 05:32:19.297
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:32:19.311
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:32:19.313
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jan 19 05:32:19.326: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 19 05:33:19.343: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:33:19.346
    Jan 19 05:33:19.346: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename sched-preemption-path 01/19/23 05:33:19.347
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:33:19.358
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:33:19.361
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:690
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:733
    Jan 19 05:33:19.373: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Jan 19 05:33:19.376: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/framework.go:187
    Jan 19 05:33:19.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-6521" for this suite. 01/19/23 05:33:19.394
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:706
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jan 19 05:33:19.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-7423" for this suite. 01/19/23 05:33:19.411
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:33:19.441
Jan 19 05:33:19.441: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename containers 01/19/23 05:33:19.441
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:33:19.452
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:33:19.455
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
Jan 19 05:33:19.464: INFO: Waiting up to 5m0s for pod "client-containers-ee125f09-a7a6-4d17-af73-bed3098435dc" in namespace "containers-3329" to be "running"
Jan 19 05:33:19.467: INFO: Pod "client-containers-ee125f09-a7a6-4d17-af73-bed3098435dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.270073ms
Jan 19 05:33:21.472: INFO: Pod "client-containers-ee125f09-a7a6-4d17-af73-bed3098435dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007917983s
Jan 19 05:33:23.472: INFO: Pod "client-containers-ee125f09-a7a6-4d17-af73-bed3098435dc": Phase="Running", Reason="", readiness=true. Elapsed: 4.007793752s
Jan 19 05:33:23.472: INFO: Pod "client-containers-ee125f09-a7a6-4d17-af73-bed3098435dc" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jan 19 05:33:23.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3329" for this suite. 01/19/23 05:33:23.48
{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","completed":258,"skipped":4730,"failed":0}
------------------------------
â€¢ [4.045 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:33:19.441
    Jan 19 05:33:19.441: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename containers 01/19/23 05:33:19.441
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:33:19.452
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:33:19.455
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:38
    Jan 19 05:33:19.464: INFO: Waiting up to 5m0s for pod "client-containers-ee125f09-a7a6-4d17-af73-bed3098435dc" in namespace "containers-3329" to be "running"
    Jan 19 05:33:19.467: INFO: Pod "client-containers-ee125f09-a7a6-4d17-af73-bed3098435dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.270073ms
    Jan 19 05:33:21.472: INFO: Pod "client-containers-ee125f09-a7a6-4d17-af73-bed3098435dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007917983s
    Jan 19 05:33:23.472: INFO: Pod "client-containers-ee125f09-a7a6-4d17-af73-bed3098435dc": Phase="Running", Reason="", readiness=true. Elapsed: 4.007793752s
    Jan 19 05:33:23.472: INFO: Pod "client-containers-ee125f09-a7a6-4d17-af73-bed3098435dc" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jan 19 05:33:23.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-3329" for this suite. 01/19/23 05:33:23.48
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:33:23.488
Jan 19 05:33:23.488: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename gc 01/19/23 05:33:23.488
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:33:23.516
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:33:23.519
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 01/19/23 05:33:23.525
STEP: delete the rc 01/19/23 05:33:28.538
STEP: wait for the rc to be deleted 01/19/23 05:33:28.547
Jan 19 05:33:29.560: INFO: 80 pods remaining
Jan 19 05:33:29.560: INFO: 80 pods has nil DeletionTimestamp
Jan 19 05:33:29.560: INFO: 
Jan 19 05:33:30.578: INFO: 71 pods remaining
Jan 19 05:33:30.578: INFO: 71 pods has nil DeletionTimestamp
Jan 19 05:33:30.578: INFO: 
Jan 19 05:33:31.565: INFO: 60 pods remaining
Jan 19 05:33:31.565: INFO: 60 pods has nil DeletionTimestamp
Jan 19 05:33:31.565: INFO: 
Jan 19 05:33:32.558: INFO: 40 pods remaining
Jan 19 05:33:32.558: INFO: 40 pods has nil DeletionTimestamp
Jan 19 05:33:32.558: INFO: 
Jan 19 05:33:33.559: INFO: 32 pods remaining
Jan 19 05:33:33.559: INFO: 32 pods has nil DeletionTimestamp
Jan 19 05:33:33.559: INFO: 
Jan 19 05:33:34.555: INFO: 20 pods remaining
Jan 19 05:33:34.555: INFO: 20 pods has nil DeletionTimestamp
Jan 19 05:33:34.555: INFO: 
STEP: Gathering metrics 01/19/23 05:33:35.565
W0119 05:33:35.570929      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 19 05:33:35.570: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 19 05:33:35.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7754" for this suite. 01/19/23 05:33:35.574
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","completed":259,"skipped":4807,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.092 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:33:23.488
    Jan 19 05:33:23.488: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename gc 01/19/23 05:33:23.488
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:33:23.516
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:33:23.519
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 01/19/23 05:33:23.525
    STEP: delete the rc 01/19/23 05:33:28.538
    STEP: wait for the rc to be deleted 01/19/23 05:33:28.547
    Jan 19 05:33:29.560: INFO: 80 pods remaining
    Jan 19 05:33:29.560: INFO: 80 pods has nil DeletionTimestamp
    Jan 19 05:33:29.560: INFO: 
    Jan 19 05:33:30.578: INFO: 71 pods remaining
    Jan 19 05:33:30.578: INFO: 71 pods has nil DeletionTimestamp
    Jan 19 05:33:30.578: INFO: 
    Jan 19 05:33:31.565: INFO: 60 pods remaining
    Jan 19 05:33:31.565: INFO: 60 pods has nil DeletionTimestamp
    Jan 19 05:33:31.565: INFO: 
    Jan 19 05:33:32.558: INFO: 40 pods remaining
    Jan 19 05:33:32.558: INFO: 40 pods has nil DeletionTimestamp
    Jan 19 05:33:32.558: INFO: 
    Jan 19 05:33:33.559: INFO: 32 pods remaining
    Jan 19 05:33:33.559: INFO: 32 pods has nil DeletionTimestamp
    Jan 19 05:33:33.559: INFO: 
    Jan 19 05:33:34.555: INFO: 20 pods remaining
    Jan 19 05:33:34.555: INFO: 20 pods has nil DeletionTimestamp
    Jan 19 05:33:34.555: INFO: 
    STEP: Gathering metrics 01/19/23 05:33:35.565
    W0119 05:33:35.570929      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 19 05:33:35.570: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 19 05:33:35.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-7754" for this suite. 01/19/23 05:33:35.574
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:33:35.58
Jan 19 05:33:35.580: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename disruption 01/19/23 05:33:35.581
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:33:35.593
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:33:35.597
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
STEP: Creating a pdb that targets all three pods in a test replica set 01/19/23 05:33:35.602
STEP: Waiting for the pdb to be processed 01/19/23 05:33:35.611
STEP: First trying to evict a pod which shouldn't be evictable 01/19/23 05:33:37.626
STEP: Waiting for all pods to be running 01/19/23 05:33:37.626
Jan 19 05:33:37.628: INFO: pods: 0 < 3
Jan 19 05:33:39.633: INFO: running pods: 0 < 3
Jan 19 05:33:41.639: INFO: running pods: 0 < 3
Jan 19 05:33:43.633: INFO: running pods: 0 < 3
Jan 19 05:33:45.633: INFO: running pods: 0 < 3
Jan 19 05:33:47.632: INFO: running pods: 0 < 3
Jan 19 05:33:49.633: INFO: running pods: 0 < 3
Jan 19 05:33:51.633: INFO: running pods: 0 < 3
Jan 19 05:33:53.634: INFO: running pods: 1 < 3
STEP: locating a running pod 01/19/23 05:33:55.632
STEP: Updating the pdb to allow a pod to be evicted 01/19/23 05:33:55.642
STEP: Waiting for the pdb to be processed 01/19/23 05:33:55.65
STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/19/23 05:33:57.656
STEP: Waiting for all pods to be running 01/19/23 05:33:57.656
STEP: Waiting for the pdb to observed all healthy pods 01/19/23 05:33:57.66
STEP: Patching the pdb to disallow a pod to be evicted 01/19/23 05:33:57.682
STEP: Waiting for the pdb to be processed 01/19/23 05:33:57.717
STEP: Waiting for all pods to be running 01/19/23 05:33:57.723
Jan 19 05:33:57.728: INFO: running pods: 2 < 3
Jan 19 05:33:59.733: INFO: running pods: 2 < 3
STEP: locating a running pod 01/19/23 05:34:01.735
STEP: Deleting the pdb to allow a pod to be evicted 01/19/23 05:34:01.744
STEP: Waiting for the pdb to be deleted 01/19/23 05:34:01.749
STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/19/23 05:34:01.751
STEP: Waiting for all pods to be running 01/19/23 05:34:01.751
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan 19 05:34:01.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9510" for this suite. 01/19/23 05:34:01.769
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","completed":260,"skipped":4822,"failed":0}
------------------------------
â€¢ [SLOW TEST] [26.200 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:33:35.58
    Jan 19 05:33:35.580: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename disruption 01/19/23 05:33:35.581
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:33:35.593
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:33:35.597
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:346
    STEP: Creating a pdb that targets all three pods in a test replica set 01/19/23 05:33:35.602
    STEP: Waiting for the pdb to be processed 01/19/23 05:33:35.611
    STEP: First trying to evict a pod which shouldn't be evictable 01/19/23 05:33:37.626
    STEP: Waiting for all pods to be running 01/19/23 05:33:37.626
    Jan 19 05:33:37.628: INFO: pods: 0 < 3
    Jan 19 05:33:39.633: INFO: running pods: 0 < 3
    Jan 19 05:33:41.639: INFO: running pods: 0 < 3
    Jan 19 05:33:43.633: INFO: running pods: 0 < 3
    Jan 19 05:33:45.633: INFO: running pods: 0 < 3
    Jan 19 05:33:47.632: INFO: running pods: 0 < 3
    Jan 19 05:33:49.633: INFO: running pods: 0 < 3
    Jan 19 05:33:51.633: INFO: running pods: 0 < 3
    Jan 19 05:33:53.634: INFO: running pods: 1 < 3
    STEP: locating a running pod 01/19/23 05:33:55.632
    STEP: Updating the pdb to allow a pod to be evicted 01/19/23 05:33:55.642
    STEP: Waiting for the pdb to be processed 01/19/23 05:33:55.65
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/19/23 05:33:57.656
    STEP: Waiting for all pods to be running 01/19/23 05:33:57.656
    STEP: Waiting for the pdb to observed all healthy pods 01/19/23 05:33:57.66
    STEP: Patching the pdb to disallow a pod to be evicted 01/19/23 05:33:57.682
    STEP: Waiting for the pdb to be processed 01/19/23 05:33:57.717
    STEP: Waiting for all pods to be running 01/19/23 05:33:57.723
    Jan 19 05:33:57.728: INFO: running pods: 2 < 3
    Jan 19 05:33:59.733: INFO: running pods: 2 < 3
    STEP: locating a running pod 01/19/23 05:34:01.735
    STEP: Deleting the pdb to allow a pod to be evicted 01/19/23 05:34:01.744
    STEP: Waiting for the pdb to be deleted 01/19/23 05:34:01.749
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/19/23 05:34:01.751
    STEP: Waiting for all pods to be running 01/19/23 05:34:01.751
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan 19 05:34:01.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-9510" for this suite. 01/19/23 05:34:01.769
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:34:01.782
Jan 19 05:34:01.782: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename configmap 01/19/23 05:34:01.782
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:34:01.8
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:34:01.803
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
STEP: Creating configMap configmap-7852/configmap-test-c06f2cbd-0c1d-4a6d-9566-100bac403746 01/19/23 05:34:01.806
STEP: Creating a pod to test consume configMaps 01/19/23 05:34:01.812
Jan 19 05:34:01.823: INFO: Waiting up to 5m0s for pod "pod-configmaps-acab84e3-8ff1-4a59-b068-72f3ff74e245" in namespace "configmap-7852" to be "Succeeded or Failed"
Jan 19 05:34:01.829: INFO: Pod "pod-configmaps-acab84e3-8ff1-4a59-b068-72f3ff74e245": Phase="Pending", Reason="", readiness=false. Elapsed: 5.430264ms
Jan 19 05:34:03.834: INFO: Pod "pod-configmaps-acab84e3-8ff1-4a59-b068-72f3ff74e245": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010292273s
Jan 19 05:34:05.834: INFO: Pod "pod-configmaps-acab84e3-8ff1-4a59-b068-72f3ff74e245": Phase="Running", Reason="", readiness=false. Elapsed: 4.010286957s
Jan 19 05:34:07.833: INFO: Pod "pod-configmaps-acab84e3-8ff1-4a59-b068-72f3ff74e245": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009177658s
STEP: Saw pod success 01/19/23 05:34:07.833
Jan 19 05:34:07.833: INFO: Pod "pod-configmaps-acab84e3-8ff1-4a59-b068-72f3ff74e245" satisfied condition "Succeeded or Failed"
Jan 19 05:34:07.836: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-configmaps-acab84e3-8ff1-4a59-b068-72f3ff74e245 container env-test: <nil>
STEP: delete the pod 01/19/23 05:34:07.842
Jan 19 05:34:07.852: INFO: Waiting for pod pod-configmaps-acab84e3-8ff1-4a59-b068-72f3ff74e245 to disappear
Jan 19 05:34:07.854: INFO: Pod pod-configmaps-acab84e3-8ff1-4a59-b068-72f3ff74e245 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jan 19 05:34:07.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7852" for this suite. 01/19/23 05:34:07.856
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","completed":261,"skipped":4874,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.079 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:34:01.782
    Jan 19 05:34:01.782: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename configmap 01/19/23 05:34:01.782
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:34:01.8
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:34:01.803
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:92
    STEP: Creating configMap configmap-7852/configmap-test-c06f2cbd-0c1d-4a6d-9566-100bac403746 01/19/23 05:34:01.806
    STEP: Creating a pod to test consume configMaps 01/19/23 05:34:01.812
    Jan 19 05:34:01.823: INFO: Waiting up to 5m0s for pod "pod-configmaps-acab84e3-8ff1-4a59-b068-72f3ff74e245" in namespace "configmap-7852" to be "Succeeded or Failed"
    Jan 19 05:34:01.829: INFO: Pod "pod-configmaps-acab84e3-8ff1-4a59-b068-72f3ff74e245": Phase="Pending", Reason="", readiness=false. Elapsed: 5.430264ms
    Jan 19 05:34:03.834: INFO: Pod "pod-configmaps-acab84e3-8ff1-4a59-b068-72f3ff74e245": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010292273s
    Jan 19 05:34:05.834: INFO: Pod "pod-configmaps-acab84e3-8ff1-4a59-b068-72f3ff74e245": Phase="Running", Reason="", readiness=false. Elapsed: 4.010286957s
    Jan 19 05:34:07.833: INFO: Pod "pod-configmaps-acab84e3-8ff1-4a59-b068-72f3ff74e245": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009177658s
    STEP: Saw pod success 01/19/23 05:34:07.833
    Jan 19 05:34:07.833: INFO: Pod "pod-configmaps-acab84e3-8ff1-4a59-b068-72f3ff74e245" satisfied condition "Succeeded or Failed"
    Jan 19 05:34:07.836: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-configmaps-acab84e3-8ff1-4a59-b068-72f3ff74e245 container env-test: <nil>
    STEP: delete the pod 01/19/23 05:34:07.842
    Jan 19 05:34:07.852: INFO: Waiting for pod pod-configmaps-acab84e3-8ff1-4a59-b068-72f3ff74e245 to disappear
    Jan 19 05:34:07.854: INFO: Pod pod-configmaps-acab84e3-8ff1-4a59-b068-72f3ff74e245 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 19 05:34:07.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7852" for this suite. 01/19/23 05:34:07.856
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:34:07.861
Jan 19 05:34:07.861: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename kubectl 01/19/23 05:34:07.862
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:34:07.873
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:34:07.875
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
Jan 19 05:34:07.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-2379 version'
Jan 19 05:34:07.948: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Jan 19 05:34:07.948: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.4\", GitCommit:\"872a965c6c6526caa949f0c6ac028ef7aff3fb78\", GitTreeState:\"clean\", BuildDate:\"2022-11-09T13:36:36Z\", GoVersion:\"go1.19.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.4\", GitCommit:\"872a965c6c6526caa949f0c6ac028ef7aff3fb78\", GitTreeState:\"clean\", BuildDate:\"2022-11-09T13:29:58Z\", GoVersion:\"go1.19.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 19 05:34:07.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2379" for this suite. 01/19/23 05:34:07.951
{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","completed":262,"skipped":4897,"failed":0}
------------------------------
â€¢ [0.097 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1677
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1683

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:34:07.861
    Jan 19 05:34:07.861: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename kubectl 01/19/23 05:34:07.862
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:34:07.873
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:34:07.875
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1683
    Jan 19 05:34:07.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-2379 version'
    Jan 19 05:34:07.948: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Jan 19 05:34:07.948: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.4\", GitCommit:\"872a965c6c6526caa949f0c6ac028ef7aff3fb78\", GitTreeState:\"clean\", BuildDate:\"2022-11-09T13:36:36Z\", GoVersion:\"go1.19.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.4\", GitCommit:\"872a965c6c6526caa949f0c6ac028ef7aff3fb78\", GitTreeState:\"clean\", BuildDate:\"2022-11-09T13:29:58Z\", GoVersion:\"go1.19.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 19 05:34:07.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2379" for this suite. 01/19/23 05:34:07.951
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:34:07.959
Jan 19 05:34:07.959: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename kubectl 01/19/23 05:34:07.959
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:34:07.989
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:34:07.991
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
STEP: creating a replication controller 01/19/23 05:34:07.993
Jan 19 05:34:07.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-958 create -f -'
Jan 19 05:34:08.633: INFO: stderr: ""
Jan 19 05:34:08.633: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/19/23 05:34:08.633
Jan 19 05:34:08.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-958 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 19 05:34:08.752: INFO: stderr: ""
Jan 19 05:34:08.753: INFO: stdout: "update-demo-nautilus-fdwpf update-demo-nautilus-ms8pr "
Jan 19 05:34:08.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-958 get pods update-demo-nautilus-fdwpf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 19 05:34:08.844: INFO: stderr: ""
Jan 19 05:34:08.844: INFO: stdout: ""
Jan 19 05:34:08.844: INFO: update-demo-nautilus-fdwpf is created but not running
Jan 19 05:34:13.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-958 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 19 05:34:13.935: INFO: stderr: ""
Jan 19 05:34:13.935: INFO: stdout: "update-demo-nautilus-fdwpf update-demo-nautilus-ms8pr "
Jan 19 05:34:13.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-958 get pods update-demo-nautilus-fdwpf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 19 05:34:14.017: INFO: stderr: ""
Jan 19 05:34:14.017: INFO: stdout: ""
Jan 19 05:34:14.017: INFO: update-demo-nautilus-fdwpf is created but not running
Jan 19 05:34:19.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-958 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 19 05:34:19.099: INFO: stderr: ""
Jan 19 05:34:19.099: INFO: stdout: "update-demo-nautilus-fdwpf update-demo-nautilus-ms8pr "
Jan 19 05:34:19.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-958 get pods update-demo-nautilus-fdwpf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 19 05:34:19.181: INFO: stderr: ""
Jan 19 05:34:19.181: INFO: stdout: ""
Jan 19 05:34:19.181: INFO: update-demo-nautilus-fdwpf is created but not running
Jan 19 05:34:24.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-958 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 19 05:34:24.267: INFO: stderr: ""
Jan 19 05:34:24.267: INFO: stdout: "update-demo-nautilus-fdwpf update-demo-nautilus-ms8pr "
Jan 19 05:34:24.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-958 get pods update-demo-nautilus-fdwpf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 19 05:34:24.345: INFO: stderr: ""
Jan 19 05:34:24.345: INFO: stdout: "true"
Jan 19 05:34:24.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-958 get pods update-demo-nautilus-fdwpf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 19 05:34:24.419: INFO: stderr: ""
Jan 19 05:34:24.419: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 19 05:34:24.419: INFO: validating pod update-demo-nautilus-fdwpf
Jan 19 05:34:24.426: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 19 05:34:24.426: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 19 05:34:24.426: INFO: update-demo-nautilus-fdwpf is verified up and running
Jan 19 05:34:24.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-958 get pods update-demo-nautilus-ms8pr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 19 05:34:24.507: INFO: stderr: ""
Jan 19 05:34:24.507: INFO: stdout: "true"
Jan 19 05:34:24.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-958 get pods update-demo-nautilus-ms8pr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 19 05:34:24.585: INFO: stderr: ""
Jan 19 05:34:24.585: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 19 05:34:24.585: INFO: validating pod update-demo-nautilus-ms8pr
Jan 19 05:34:24.590: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 19 05:34:24.590: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 19 05:34:24.590: INFO: update-demo-nautilus-ms8pr is verified up and running
STEP: using delete to clean up resources 01/19/23 05:34:24.59
Jan 19 05:34:24.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-958 delete --grace-period=0 --force -f -'
Jan 19 05:34:24.674: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 19 05:34:24.674: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 19 05:34:24.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-958 get rc,svc -l name=update-demo --no-headers'
Jan 19 05:34:24.773: INFO: stderr: "No resources found in kubectl-958 namespace.\n"
Jan 19 05:34:24.773: INFO: stdout: ""
Jan 19 05:34:24.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-958 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 19 05:34:24.855: INFO: stderr: ""
Jan 19 05:34:24.855: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 19 05:34:24.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-958" for this suite. 01/19/23 05:34:24.859
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","completed":263,"skipped":4908,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.906 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:337

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:34:07.959
    Jan 19 05:34:07.959: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename kubectl 01/19/23 05:34:07.959
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:34:07.989
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:34:07.991
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:337
    STEP: creating a replication controller 01/19/23 05:34:07.993
    Jan 19 05:34:07.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-958 create -f -'
    Jan 19 05:34:08.633: INFO: stderr: ""
    Jan 19 05:34:08.633: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/19/23 05:34:08.633
    Jan 19 05:34:08.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-958 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 19 05:34:08.752: INFO: stderr: ""
    Jan 19 05:34:08.753: INFO: stdout: "update-demo-nautilus-fdwpf update-demo-nautilus-ms8pr "
    Jan 19 05:34:08.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-958 get pods update-demo-nautilus-fdwpf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 19 05:34:08.844: INFO: stderr: ""
    Jan 19 05:34:08.844: INFO: stdout: ""
    Jan 19 05:34:08.844: INFO: update-demo-nautilus-fdwpf is created but not running
    Jan 19 05:34:13.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-958 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 19 05:34:13.935: INFO: stderr: ""
    Jan 19 05:34:13.935: INFO: stdout: "update-demo-nautilus-fdwpf update-demo-nautilus-ms8pr "
    Jan 19 05:34:13.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-958 get pods update-demo-nautilus-fdwpf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 19 05:34:14.017: INFO: stderr: ""
    Jan 19 05:34:14.017: INFO: stdout: ""
    Jan 19 05:34:14.017: INFO: update-demo-nautilus-fdwpf is created but not running
    Jan 19 05:34:19.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-958 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 19 05:34:19.099: INFO: stderr: ""
    Jan 19 05:34:19.099: INFO: stdout: "update-demo-nautilus-fdwpf update-demo-nautilus-ms8pr "
    Jan 19 05:34:19.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-958 get pods update-demo-nautilus-fdwpf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 19 05:34:19.181: INFO: stderr: ""
    Jan 19 05:34:19.181: INFO: stdout: ""
    Jan 19 05:34:19.181: INFO: update-demo-nautilus-fdwpf is created but not running
    Jan 19 05:34:24.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-958 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 19 05:34:24.267: INFO: stderr: ""
    Jan 19 05:34:24.267: INFO: stdout: "update-demo-nautilus-fdwpf update-demo-nautilus-ms8pr "
    Jan 19 05:34:24.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-958 get pods update-demo-nautilus-fdwpf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 19 05:34:24.345: INFO: stderr: ""
    Jan 19 05:34:24.345: INFO: stdout: "true"
    Jan 19 05:34:24.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-958 get pods update-demo-nautilus-fdwpf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 19 05:34:24.419: INFO: stderr: ""
    Jan 19 05:34:24.419: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 19 05:34:24.419: INFO: validating pod update-demo-nautilus-fdwpf
    Jan 19 05:34:24.426: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 19 05:34:24.426: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 19 05:34:24.426: INFO: update-demo-nautilus-fdwpf is verified up and running
    Jan 19 05:34:24.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-958 get pods update-demo-nautilus-ms8pr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 19 05:34:24.507: INFO: stderr: ""
    Jan 19 05:34:24.507: INFO: stdout: "true"
    Jan 19 05:34:24.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-958 get pods update-demo-nautilus-ms8pr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 19 05:34:24.585: INFO: stderr: ""
    Jan 19 05:34:24.585: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 19 05:34:24.585: INFO: validating pod update-demo-nautilus-ms8pr
    Jan 19 05:34:24.590: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 19 05:34:24.590: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 19 05:34:24.590: INFO: update-demo-nautilus-ms8pr is verified up and running
    STEP: using delete to clean up resources 01/19/23 05:34:24.59
    Jan 19 05:34:24.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-958 delete --grace-period=0 --force -f -'
    Jan 19 05:34:24.674: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 19 05:34:24.674: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jan 19 05:34:24.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-958 get rc,svc -l name=update-demo --no-headers'
    Jan 19 05:34:24.773: INFO: stderr: "No resources found in kubectl-958 namespace.\n"
    Jan 19 05:34:24.773: INFO: stdout: ""
    Jan 19 05:34:24.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-958 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 19 05:34:24.855: INFO: stderr: ""
    Jan 19 05:34:24.855: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 19 05:34:24.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-958" for this suite. 01/19/23 05:34:24.859
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:34:24.865
Jan 19 05:34:24.865: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename deployment 01/19/23 05:34:24.865
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:34:24.878
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:34:24.88
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 01/19/23 05:34:24.885
Jan 19 05:34:24.886: INFO: Creating simple deployment test-deployment-d5n4c
Jan 19 05:34:24.904: INFO: deployment "test-deployment-d5n4c" doesn't have the required revision set
Jan 19 05:34:26.913: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 5, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 34, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 34, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-d5n4c-777898ffcc\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Getting /status 01/19/23 05:34:28.92
Jan 19 05:34:28.923: INFO: Deployment test-deployment-d5n4c has Conditions: [{Available True 2023-01-19 05:34:28 +0000 UTC 2023-01-19 05:34:28 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-01-19 05:34:28 +0000 UTC 2023-01-19 05:34:24 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-d5n4c-777898ffcc" has successfully progressed.}]
STEP: updating Deployment Status 01/19/23 05:34:28.923
Jan 19 05:34:28.936: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 34, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 34, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 34, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 34, 24, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-d5n4c-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 01/19/23 05:34:28.936
Jan 19 05:34:28.941: INFO: Observed &Deployment event: ADDED
Jan 19 05:34:28.941: INFO: Observed Deployment test-deployment-d5n4c in namespace deployment-4963 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-19 05:34:24 +0000 UTC 2023-01-19 05:34:24 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-d5n4c-777898ffcc"}
Jan 19 05:34:28.941: INFO: Observed &Deployment event: MODIFIED
Jan 19 05:34:28.941: INFO: Observed Deployment test-deployment-d5n4c in namespace deployment-4963 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-19 05:34:24 +0000 UTC 2023-01-19 05:34:24 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 19 05:34:28.941: INFO: Observed Deployment test-deployment-d5n4c in namespace deployment-4963 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-19 05:34:24 +0000 UTC 2023-01-19 05:34:24 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-d5n4c-777898ffcc" is progressing.}
Jan 19 05:34:28.941: INFO: Observed &Deployment event: MODIFIED
Jan 19 05:34:28.941: INFO: Observed Deployment test-deployment-d5n4c in namespace deployment-4963 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-19 05:34:28 +0000 UTC 2023-01-19 05:34:28 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 19 05:34:28.941: INFO: Observed Deployment test-deployment-d5n4c in namespace deployment-4963 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-19 05:34:28 +0000 UTC 2023-01-19 05:34:24 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-d5n4c-777898ffcc" has successfully progressed.}
Jan 19 05:34:28.941: INFO: Observed &Deployment event: MODIFIED
Jan 19 05:34:28.941: INFO: Observed Deployment test-deployment-d5n4c in namespace deployment-4963 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-19 05:34:28 +0000 UTC 2023-01-19 05:34:28 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 19 05:34:28.941: INFO: Observed Deployment test-deployment-d5n4c in namespace deployment-4963 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-19 05:34:28 +0000 UTC 2023-01-19 05:34:24 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-d5n4c-777898ffcc" has successfully progressed.}
Jan 19 05:34:28.941: INFO: Found Deployment test-deployment-d5n4c in namespace deployment-4963 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 19 05:34:28.941: INFO: Deployment test-deployment-d5n4c has an updated status
STEP: patching the Statefulset Status 01/19/23 05:34:28.941
Jan 19 05:34:28.941: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 19 05:34:28.948: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 01/19/23 05:34:28.948
Jan 19 05:34:28.951: INFO: Observed &Deployment event: ADDED
Jan 19 05:34:28.951: INFO: Observed deployment test-deployment-d5n4c in namespace deployment-4963 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-19 05:34:24 +0000 UTC 2023-01-19 05:34:24 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-d5n4c-777898ffcc"}
Jan 19 05:34:28.952: INFO: Observed &Deployment event: MODIFIED
Jan 19 05:34:28.952: INFO: Observed deployment test-deployment-d5n4c in namespace deployment-4963 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-19 05:34:24 +0000 UTC 2023-01-19 05:34:24 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 19 05:34:28.952: INFO: Observed deployment test-deployment-d5n4c in namespace deployment-4963 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-19 05:34:24 +0000 UTC 2023-01-19 05:34:24 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-d5n4c-777898ffcc" is progressing.}
Jan 19 05:34:28.952: INFO: Observed &Deployment event: MODIFIED
Jan 19 05:34:28.952: INFO: Observed deployment test-deployment-d5n4c in namespace deployment-4963 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-19 05:34:28 +0000 UTC 2023-01-19 05:34:28 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 19 05:34:28.952: INFO: Observed deployment test-deployment-d5n4c in namespace deployment-4963 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-19 05:34:28 +0000 UTC 2023-01-19 05:34:24 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-d5n4c-777898ffcc" has successfully progressed.}
Jan 19 05:34:28.952: INFO: Observed &Deployment event: MODIFIED
Jan 19 05:34:28.952: INFO: Observed deployment test-deployment-d5n4c in namespace deployment-4963 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-19 05:34:28 +0000 UTC 2023-01-19 05:34:28 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 19 05:34:28.952: INFO: Observed deployment test-deployment-d5n4c in namespace deployment-4963 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-19 05:34:28 +0000 UTC 2023-01-19 05:34:24 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-d5n4c-777898ffcc" has successfully progressed.}
Jan 19 05:34:28.952: INFO: Observed deployment test-deployment-d5n4c in namespace deployment-4963 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 19 05:34:28.952: INFO: Observed &Deployment event: MODIFIED
Jan 19 05:34:28.952: INFO: Found deployment test-deployment-d5n4c in namespace deployment-4963 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Jan 19 05:34:28.952: INFO: Deployment test-deployment-d5n4c has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 19 05:34:28.955: INFO: Deployment "test-deployment-d5n4c":
&Deployment{ObjectMeta:{test-deployment-d5n4c  deployment-4963  eafc8301-dfa1-4259-9614-2e29d275fcff 31108 1 2023-01-19 05:34:24 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-01-19 05:34:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-01-19 05:34:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-01-19 05:34:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002a69b78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 19 05:34:28.959: INFO: New ReplicaSet "test-deployment-d5n4c-777898ffcc" of Deployment "test-deployment-d5n4c":
&ReplicaSet{ObjectMeta:{test-deployment-d5n4c-777898ffcc  deployment-4963  2094d084-c082-446d-939e-c916268f6f74 31105 1 2023-01-19 05:34:24 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-d5n4c eafc8301-dfa1-4259-9614-2e29d275fcff 0xc002a69f60 0xc002a69f61}] [] [{kube-controller-manager Update apps/v1 2023-01-19 05:34:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eafc8301-dfa1-4259-9614-2e29d275fcff\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 05:34:27 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045d4008 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 19 05:34:28.968: INFO: Pod "test-deployment-d5n4c-777898ffcc-ctdrh" is available:
&Pod{ObjectMeta:{test-deployment-d5n4c-777898ffcc-ctdrh test-deployment-d5n4c-777898ffcc- deployment-4963  79e3dc05-6d3f-4f93-88f4-66a2683d926e 31104 0 2023-01-19 05:34:24 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [{apps/v1 ReplicaSet test-deployment-d5n4c-777898ffcc 2094d084-c082-446d-939e-c916268f6f74 0xc0019b9530 0xc0019b9531}] [] [{kube-controller-manager Update v1 2023-01-19 05:34:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2094d084-c082-446d-939e-c916268f6f74\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 05:34:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.70.192\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mk9hx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mk9hx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:34:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:34:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:34:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:34:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.78,PodIP:10.100.70.192,StartTime:2023-01-19 05:34:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 05:34:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://a7abe705ded645096f46c0403184f5d7ae146b9f29a56254f536eda883ac2cbd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.70.192,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 19 05:34:28.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4963" for this suite. 01/19/23 05:34:28.97
{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","completed":264,"skipped":4908,"failed":0}
------------------------------
â€¢ [4.109 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:34:24.865
    Jan 19 05:34:24.865: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename deployment 01/19/23 05:34:24.865
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:34:24.878
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:34:24.88
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 01/19/23 05:34:24.885
    Jan 19 05:34:24.886: INFO: Creating simple deployment test-deployment-d5n4c
    Jan 19 05:34:24.904: INFO: deployment "test-deployment-d5n4c" doesn't have the required revision set
    Jan 19 05:34:26.913: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 5, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 34, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 34, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-d5n4c-777898ffcc\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Getting /status 01/19/23 05:34:28.92
    Jan 19 05:34:28.923: INFO: Deployment test-deployment-d5n4c has Conditions: [{Available True 2023-01-19 05:34:28 +0000 UTC 2023-01-19 05:34:28 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-01-19 05:34:28 +0000 UTC 2023-01-19 05:34:24 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-d5n4c-777898ffcc" has successfully progressed.}]
    STEP: updating Deployment Status 01/19/23 05:34:28.923
    Jan 19 05:34:28.936: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 34, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 34, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 34, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 34, 24, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-d5n4c-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 01/19/23 05:34:28.936
    Jan 19 05:34:28.941: INFO: Observed &Deployment event: ADDED
    Jan 19 05:34:28.941: INFO: Observed Deployment test-deployment-d5n4c in namespace deployment-4963 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-19 05:34:24 +0000 UTC 2023-01-19 05:34:24 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-d5n4c-777898ffcc"}
    Jan 19 05:34:28.941: INFO: Observed &Deployment event: MODIFIED
    Jan 19 05:34:28.941: INFO: Observed Deployment test-deployment-d5n4c in namespace deployment-4963 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-19 05:34:24 +0000 UTC 2023-01-19 05:34:24 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 19 05:34:28.941: INFO: Observed Deployment test-deployment-d5n4c in namespace deployment-4963 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-19 05:34:24 +0000 UTC 2023-01-19 05:34:24 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-d5n4c-777898ffcc" is progressing.}
    Jan 19 05:34:28.941: INFO: Observed &Deployment event: MODIFIED
    Jan 19 05:34:28.941: INFO: Observed Deployment test-deployment-d5n4c in namespace deployment-4963 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-19 05:34:28 +0000 UTC 2023-01-19 05:34:28 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 19 05:34:28.941: INFO: Observed Deployment test-deployment-d5n4c in namespace deployment-4963 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-19 05:34:28 +0000 UTC 2023-01-19 05:34:24 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-d5n4c-777898ffcc" has successfully progressed.}
    Jan 19 05:34:28.941: INFO: Observed &Deployment event: MODIFIED
    Jan 19 05:34:28.941: INFO: Observed Deployment test-deployment-d5n4c in namespace deployment-4963 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-19 05:34:28 +0000 UTC 2023-01-19 05:34:28 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 19 05:34:28.941: INFO: Observed Deployment test-deployment-d5n4c in namespace deployment-4963 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-19 05:34:28 +0000 UTC 2023-01-19 05:34:24 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-d5n4c-777898ffcc" has successfully progressed.}
    Jan 19 05:34:28.941: INFO: Found Deployment test-deployment-d5n4c in namespace deployment-4963 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 19 05:34:28.941: INFO: Deployment test-deployment-d5n4c has an updated status
    STEP: patching the Statefulset Status 01/19/23 05:34:28.941
    Jan 19 05:34:28.941: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 19 05:34:28.948: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 01/19/23 05:34:28.948
    Jan 19 05:34:28.951: INFO: Observed &Deployment event: ADDED
    Jan 19 05:34:28.951: INFO: Observed deployment test-deployment-d5n4c in namespace deployment-4963 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-19 05:34:24 +0000 UTC 2023-01-19 05:34:24 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-d5n4c-777898ffcc"}
    Jan 19 05:34:28.952: INFO: Observed &Deployment event: MODIFIED
    Jan 19 05:34:28.952: INFO: Observed deployment test-deployment-d5n4c in namespace deployment-4963 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-19 05:34:24 +0000 UTC 2023-01-19 05:34:24 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 19 05:34:28.952: INFO: Observed deployment test-deployment-d5n4c in namespace deployment-4963 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-19 05:34:24 +0000 UTC 2023-01-19 05:34:24 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-d5n4c-777898ffcc" is progressing.}
    Jan 19 05:34:28.952: INFO: Observed &Deployment event: MODIFIED
    Jan 19 05:34:28.952: INFO: Observed deployment test-deployment-d5n4c in namespace deployment-4963 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-19 05:34:28 +0000 UTC 2023-01-19 05:34:28 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 19 05:34:28.952: INFO: Observed deployment test-deployment-d5n4c in namespace deployment-4963 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-19 05:34:28 +0000 UTC 2023-01-19 05:34:24 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-d5n4c-777898ffcc" has successfully progressed.}
    Jan 19 05:34:28.952: INFO: Observed &Deployment event: MODIFIED
    Jan 19 05:34:28.952: INFO: Observed deployment test-deployment-d5n4c in namespace deployment-4963 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-19 05:34:28 +0000 UTC 2023-01-19 05:34:28 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 19 05:34:28.952: INFO: Observed deployment test-deployment-d5n4c in namespace deployment-4963 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-19 05:34:28 +0000 UTC 2023-01-19 05:34:24 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-d5n4c-777898ffcc" has successfully progressed.}
    Jan 19 05:34:28.952: INFO: Observed deployment test-deployment-d5n4c in namespace deployment-4963 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 19 05:34:28.952: INFO: Observed &Deployment event: MODIFIED
    Jan 19 05:34:28.952: INFO: Found deployment test-deployment-d5n4c in namespace deployment-4963 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Jan 19 05:34:28.952: INFO: Deployment test-deployment-d5n4c has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 19 05:34:28.955: INFO: Deployment "test-deployment-d5n4c":
    &Deployment{ObjectMeta:{test-deployment-d5n4c  deployment-4963  eafc8301-dfa1-4259-9614-2e29d275fcff 31108 1 2023-01-19 05:34:24 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-01-19 05:34:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-01-19 05:34:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-01-19 05:34:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002a69b78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 19 05:34:28.959: INFO: New ReplicaSet "test-deployment-d5n4c-777898ffcc" of Deployment "test-deployment-d5n4c":
    &ReplicaSet{ObjectMeta:{test-deployment-d5n4c-777898ffcc  deployment-4963  2094d084-c082-446d-939e-c916268f6f74 31105 1 2023-01-19 05:34:24 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-d5n4c eafc8301-dfa1-4259-9614-2e29d275fcff 0xc002a69f60 0xc002a69f61}] [] [{kube-controller-manager Update apps/v1 2023-01-19 05:34:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eafc8301-dfa1-4259-9614-2e29d275fcff\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 05:34:27 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045d4008 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 19 05:34:28.968: INFO: Pod "test-deployment-d5n4c-777898ffcc-ctdrh" is available:
    &Pod{ObjectMeta:{test-deployment-d5n4c-777898ffcc-ctdrh test-deployment-d5n4c-777898ffcc- deployment-4963  79e3dc05-6d3f-4f93-88f4-66a2683d926e 31104 0 2023-01-19 05:34:24 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [{apps/v1 ReplicaSet test-deployment-d5n4c-777898ffcc 2094d084-c082-446d-939e-c916268f6f74 0xc0019b9530 0xc0019b9531}] [] [{kube-controller-manager Update v1 2023-01-19 05:34:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2094d084-c082-446d-939e-c916268f6f74\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 05:34:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.70.192\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mk9hx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mk9hx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:34:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:34:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:34:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:34:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.78,PodIP:10.100.70.192,StartTime:2023-01-19 05:34:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 05:34:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://a7abe705ded645096f46c0403184f5d7ae146b9f29a56254f536eda883ac2cbd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.70.192,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 19 05:34:28.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-4963" for this suite. 01/19/23 05:34:28.97
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:34:28.976
Jan 19 05:34:28.976: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename replication-controller 01/19/23 05:34:28.976
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:34:28.99
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:34:28.994
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
STEP: creating a ReplicationController 01/19/23 05:34:28.998
STEP: waiting for RC to be added 01/19/23 05:34:29.008
STEP: waiting for available Replicas 01/19/23 05:34:29.008
STEP: patching ReplicationController 01/19/23 05:34:32.028
STEP: waiting for RC to be modified 01/19/23 05:34:32.037
STEP: patching ReplicationController status 01/19/23 05:34:32.037
STEP: waiting for RC to be modified 01/19/23 05:34:32.042
STEP: waiting for available Replicas 01/19/23 05:34:32.042
STEP: fetching ReplicationController status 01/19/23 05:34:32.047
STEP: patching ReplicationController scale 01/19/23 05:34:32.049
STEP: waiting for RC to be modified 01/19/23 05:34:32.058
STEP: waiting for ReplicationController's scale to be the max amount 01/19/23 05:34:32.058
STEP: fetching ReplicationController; ensuring that it's patched 01/19/23 05:34:34.028
STEP: updating ReplicationController status 01/19/23 05:34:34.038
STEP: waiting for RC to be modified 01/19/23 05:34:34.049
STEP: listing all ReplicationControllers 01/19/23 05:34:34.049
STEP: checking that ReplicationController has expected values 01/19/23 05:34:34.056
STEP: deleting ReplicationControllers by collection 01/19/23 05:34:34.056
STEP: waiting for ReplicationController to have a DELETED watchEvent 01/19/23 05:34:34.068
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan 19 05:34:34.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2478" for this suite. 01/19/23 05:34:34.104
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","completed":265,"skipped":4958,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.136 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:34:28.976
    Jan 19 05:34:28.976: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename replication-controller 01/19/23 05:34:28.976
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:34:28.99
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:34:28.994
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:109
    STEP: creating a ReplicationController 01/19/23 05:34:28.998
    STEP: waiting for RC to be added 01/19/23 05:34:29.008
    STEP: waiting for available Replicas 01/19/23 05:34:29.008
    STEP: patching ReplicationController 01/19/23 05:34:32.028
    STEP: waiting for RC to be modified 01/19/23 05:34:32.037
    STEP: patching ReplicationController status 01/19/23 05:34:32.037
    STEP: waiting for RC to be modified 01/19/23 05:34:32.042
    STEP: waiting for available Replicas 01/19/23 05:34:32.042
    STEP: fetching ReplicationController status 01/19/23 05:34:32.047
    STEP: patching ReplicationController scale 01/19/23 05:34:32.049
    STEP: waiting for RC to be modified 01/19/23 05:34:32.058
    STEP: waiting for ReplicationController's scale to be the max amount 01/19/23 05:34:32.058
    STEP: fetching ReplicationController; ensuring that it's patched 01/19/23 05:34:34.028
    STEP: updating ReplicationController status 01/19/23 05:34:34.038
    STEP: waiting for RC to be modified 01/19/23 05:34:34.049
    STEP: listing all ReplicationControllers 01/19/23 05:34:34.049
    STEP: checking that ReplicationController has expected values 01/19/23 05:34:34.056
    STEP: deleting ReplicationControllers by collection 01/19/23 05:34:34.056
    STEP: waiting for ReplicationController to have a DELETED watchEvent 01/19/23 05:34:34.068
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan 19 05:34:34.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-2478" for this suite. 01/19/23 05:34:34.104
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:34:34.112
Jan 19 05:34:34.113: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename var-expansion 01/19/23 05:34:34.113
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:34:34.129
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:34:34.131
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
STEP: Creating a pod to test substitution in container's args 01/19/23 05:34:34.136
Jan 19 05:34:34.148: INFO: Waiting up to 5m0s for pod "var-expansion-17410161-4057-4863-a9df-f55de8416eb1" in namespace "var-expansion-1684" to be "Succeeded or Failed"
Jan 19 05:34:34.158: INFO: Pod "var-expansion-17410161-4057-4863-a9df-f55de8416eb1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.139974ms
Jan 19 05:34:36.160: INFO: Pod "var-expansion-17410161-4057-4863-a9df-f55de8416eb1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012860044s
Jan 19 05:34:38.164: INFO: Pod "var-expansion-17410161-4057-4863-a9df-f55de8416eb1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016338051s
Jan 19 05:34:40.161: INFO: Pod "var-expansion-17410161-4057-4863-a9df-f55de8416eb1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013198975s
STEP: Saw pod success 01/19/23 05:34:40.161
Jan 19 05:34:40.161: INFO: Pod "var-expansion-17410161-4057-4863-a9df-f55de8416eb1" satisfied condition "Succeeded or Failed"
Jan 19 05:34:40.163: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod var-expansion-17410161-4057-4863-a9df-f55de8416eb1 container dapi-container: <nil>
STEP: delete the pod 01/19/23 05:34:40.169
Jan 19 05:34:40.180: INFO: Waiting for pod var-expansion-17410161-4057-4863-a9df-f55de8416eb1 to disappear
Jan 19 05:34:40.182: INFO: Pod var-expansion-17410161-4057-4863-a9df-f55de8416eb1 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 19 05:34:40.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1684" for this suite. 01/19/23 05:34:40.184
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","completed":266,"skipped":4959,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.076 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:34:34.112
    Jan 19 05:34:34.113: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename var-expansion 01/19/23 05:34:34.113
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:34:34.129
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:34:34.131
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:91
    STEP: Creating a pod to test substitution in container's args 01/19/23 05:34:34.136
    Jan 19 05:34:34.148: INFO: Waiting up to 5m0s for pod "var-expansion-17410161-4057-4863-a9df-f55de8416eb1" in namespace "var-expansion-1684" to be "Succeeded or Failed"
    Jan 19 05:34:34.158: INFO: Pod "var-expansion-17410161-4057-4863-a9df-f55de8416eb1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.139974ms
    Jan 19 05:34:36.160: INFO: Pod "var-expansion-17410161-4057-4863-a9df-f55de8416eb1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012860044s
    Jan 19 05:34:38.164: INFO: Pod "var-expansion-17410161-4057-4863-a9df-f55de8416eb1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016338051s
    Jan 19 05:34:40.161: INFO: Pod "var-expansion-17410161-4057-4863-a9df-f55de8416eb1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013198975s
    STEP: Saw pod success 01/19/23 05:34:40.161
    Jan 19 05:34:40.161: INFO: Pod "var-expansion-17410161-4057-4863-a9df-f55de8416eb1" satisfied condition "Succeeded or Failed"
    Jan 19 05:34:40.163: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod var-expansion-17410161-4057-4863-a9df-f55de8416eb1 container dapi-container: <nil>
    STEP: delete the pod 01/19/23 05:34:40.169
    Jan 19 05:34:40.180: INFO: Waiting for pod var-expansion-17410161-4057-4863-a9df-f55de8416eb1 to disappear
    Jan 19 05:34:40.182: INFO: Pod var-expansion-17410161-4057-4863-a9df-f55de8416eb1 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 19 05:34:40.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-1684" for this suite. 01/19/23 05:34:40.184
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:34:40.189
Jan 19 05:34:40.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename resourcequota 01/19/23 05:34:40.19
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:34:40.199
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:34:40.201
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
STEP: Counting existing ResourceQuota 01/19/23 05:34:40.206
STEP: Creating a ResourceQuota 01/19/23 05:34:45.21
STEP: Ensuring resource quota status is calculated 01/19/23 05:34:45.216
STEP: Creating a ReplicaSet 01/19/23 05:34:47.22
STEP: Ensuring resource quota status captures replicaset creation 01/19/23 05:34:47.236
STEP: Deleting a ReplicaSet 01/19/23 05:34:49.241
STEP: Ensuring resource quota status released usage 01/19/23 05:34:49.248
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 19 05:34:51.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6909" for this suite. 01/19/23 05:34:51.254
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","completed":267,"skipped":4967,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.070 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:34:40.189
    Jan 19 05:34:40.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename resourcequota 01/19/23 05:34:40.19
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:34:40.199
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:34:40.201
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:438
    STEP: Counting existing ResourceQuota 01/19/23 05:34:40.206
    STEP: Creating a ResourceQuota 01/19/23 05:34:45.21
    STEP: Ensuring resource quota status is calculated 01/19/23 05:34:45.216
    STEP: Creating a ReplicaSet 01/19/23 05:34:47.22
    STEP: Ensuring resource quota status captures replicaset creation 01/19/23 05:34:47.236
    STEP: Deleting a ReplicaSet 01/19/23 05:34:49.241
    STEP: Ensuring resource quota status released usage 01/19/23 05:34:49.248
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 19 05:34:51.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-6909" for this suite. 01/19/23 05:34:51.254
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:34:51.259
Jan 19 05:34:51.259: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename kubectl 01/19/23 05:34:51.26
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:34:51.273
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:34:51.276
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
STEP: creating a replication controller 01/19/23 05:34:51.278
Jan 19 05:34:51.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 create -f -'
Jan 19 05:34:51.807: INFO: stderr: ""
Jan 19 05:34:51.807: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/19/23 05:34:51.807
Jan 19 05:34:51.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 19 05:34:51.890: INFO: stderr: ""
Jan 19 05:34:51.890: INFO: stdout: "update-demo-nautilus-5s88k update-demo-nautilus-dzbw6 "
Jan 19 05:34:51.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 get pods update-demo-nautilus-5s88k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 19 05:34:51.968: INFO: stderr: ""
Jan 19 05:34:51.968: INFO: stdout: ""
Jan 19 05:34:51.968: INFO: update-demo-nautilus-5s88k is created but not running
Jan 19 05:34:56.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 19 05:34:57.049: INFO: stderr: ""
Jan 19 05:34:57.049: INFO: stdout: "update-demo-nautilus-5s88k update-demo-nautilus-dzbw6 "
Jan 19 05:34:57.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 get pods update-demo-nautilus-5s88k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 19 05:34:57.127: INFO: stderr: ""
Jan 19 05:34:57.127: INFO: stdout: "true"
Jan 19 05:34:57.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 get pods update-demo-nautilus-5s88k -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 19 05:34:57.206: INFO: stderr: ""
Jan 19 05:34:57.206: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 19 05:34:57.206: INFO: validating pod update-demo-nautilus-5s88k
Jan 19 05:34:57.212: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 19 05:34:57.212: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 19 05:34:57.212: INFO: update-demo-nautilus-5s88k is verified up and running
Jan 19 05:34:57.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 get pods update-demo-nautilus-dzbw6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 19 05:34:57.286: INFO: stderr: ""
Jan 19 05:34:57.286: INFO: stdout: "true"
Jan 19 05:34:57.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 get pods update-demo-nautilus-dzbw6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 19 05:34:57.364: INFO: stderr: ""
Jan 19 05:34:57.364: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 19 05:34:57.364: INFO: validating pod update-demo-nautilus-dzbw6
Jan 19 05:34:57.368: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 19 05:34:57.368: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 19 05:34:57.368: INFO: update-demo-nautilus-dzbw6 is verified up and running
STEP: scaling down the replication controller 01/19/23 05:34:57.368
Jan 19 05:34:57.369: INFO: scanned /root for discovery docs: <nil>
Jan 19 05:34:57.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jan 19 05:34:57.500: INFO: stderr: ""
Jan 19 05:34:57.500: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/19/23 05:34:57.5
Jan 19 05:34:57.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 19 05:34:57.587: INFO: stderr: ""
Jan 19 05:34:57.587: INFO: stdout: "update-demo-nautilus-5s88k update-demo-nautilus-dzbw6 "
STEP: Replicas for name=update-demo: expected=1 actual=2 01/19/23 05:34:57.587
Jan 19 05:35:02.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 19 05:35:02.666: INFO: stderr: ""
Jan 19 05:35:02.666: INFO: stdout: "update-demo-nautilus-dzbw6 "
Jan 19 05:35:02.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 get pods update-demo-nautilus-dzbw6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 19 05:35:02.744: INFO: stderr: ""
Jan 19 05:35:02.744: INFO: stdout: "true"
Jan 19 05:35:02.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 get pods update-demo-nautilus-dzbw6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 19 05:35:02.823: INFO: stderr: ""
Jan 19 05:35:02.823: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 19 05:35:02.823: INFO: validating pod update-demo-nautilus-dzbw6
Jan 19 05:35:02.827: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 19 05:35:02.827: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 19 05:35:02.827: INFO: update-demo-nautilus-dzbw6 is verified up and running
STEP: scaling up the replication controller 01/19/23 05:35:02.827
Jan 19 05:35:02.828: INFO: scanned /root for discovery docs: <nil>
Jan 19 05:35:02.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jan 19 05:35:03.928: INFO: stderr: ""
Jan 19 05:35:03.928: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/19/23 05:35:03.928
Jan 19 05:35:03.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 19 05:35:04.006: INFO: stderr: ""
Jan 19 05:35:04.006: INFO: stdout: "update-demo-nautilus-dzbw6 update-demo-nautilus-zdqpp "
Jan 19 05:35:04.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 get pods update-demo-nautilus-dzbw6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 19 05:35:04.085: INFO: stderr: ""
Jan 19 05:35:04.085: INFO: stdout: "true"
Jan 19 05:35:04.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 get pods update-demo-nautilus-dzbw6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 19 05:35:04.166: INFO: stderr: ""
Jan 19 05:35:04.166: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 19 05:35:04.166: INFO: validating pod update-demo-nautilus-dzbw6
Jan 19 05:35:04.170: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 19 05:35:04.170: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 19 05:35:04.170: INFO: update-demo-nautilus-dzbw6 is verified up and running
Jan 19 05:35:04.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 get pods update-demo-nautilus-zdqpp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 19 05:35:04.246: INFO: stderr: ""
Jan 19 05:35:04.246: INFO: stdout: ""
Jan 19 05:35:04.246: INFO: update-demo-nautilus-zdqpp is created but not running
Jan 19 05:35:09.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 19 05:35:09.328: INFO: stderr: ""
Jan 19 05:35:09.328: INFO: stdout: "update-demo-nautilus-dzbw6 update-demo-nautilus-zdqpp "
Jan 19 05:35:09.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 get pods update-demo-nautilus-dzbw6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 19 05:35:09.409: INFO: stderr: ""
Jan 19 05:35:09.409: INFO: stdout: "true"
Jan 19 05:35:09.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 get pods update-demo-nautilus-dzbw6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 19 05:35:09.486: INFO: stderr: ""
Jan 19 05:35:09.486: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 19 05:35:09.486: INFO: validating pod update-demo-nautilus-dzbw6
Jan 19 05:35:09.489: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 19 05:35:09.489: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 19 05:35:09.489: INFO: update-demo-nautilus-dzbw6 is verified up and running
Jan 19 05:35:09.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 get pods update-demo-nautilus-zdqpp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 19 05:35:09.566: INFO: stderr: ""
Jan 19 05:35:09.566: INFO: stdout: "true"
Jan 19 05:35:09.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 get pods update-demo-nautilus-zdqpp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 19 05:35:09.640: INFO: stderr: ""
Jan 19 05:35:09.640: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 19 05:35:09.640: INFO: validating pod update-demo-nautilus-zdqpp
Jan 19 05:35:09.644: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 19 05:35:09.644: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 19 05:35:09.644: INFO: update-demo-nautilus-zdqpp is verified up and running
STEP: using delete to clean up resources 01/19/23 05:35:09.644
Jan 19 05:35:09.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 delete --grace-period=0 --force -f -'
Jan 19 05:35:09.720: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 19 05:35:09.720: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 19 05:35:09.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 get rc,svc -l name=update-demo --no-headers'
Jan 19 05:35:09.817: INFO: stderr: "No resources found in kubectl-1065 namespace.\n"
Jan 19 05:35:09.817: INFO: stdout: ""
Jan 19 05:35:09.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 19 05:35:09.904: INFO: stderr: ""
Jan 19 05:35:09.904: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 19 05:35:09.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1065" for this suite. 01/19/23 05:35:09.907
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","completed":268,"skipped":4977,"failed":0}
------------------------------
â€¢ [SLOW TEST] [18.653 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:350

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:34:51.259
    Jan 19 05:34:51.259: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename kubectl 01/19/23 05:34:51.26
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:34:51.273
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:34:51.276
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:350
    STEP: creating a replication controller 01/19/23 05:34:51.278
    Jan 19 05:34:51.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 create -f -'
    Jan 19 05:34:51.807: INFO: stderr: ""
    Jan 19 05:34:51.807: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/19/23 05:34:51.807
    Jan 19 05:34:51.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 19 05:34:51.890: INFO: stderr: ""
    Jan 19 05:34:51.890: INFO: stdout: "update-demo-nautilus-5s88k update-demo-nautilus-dzbw6 "
    Jan 19 05:34:51.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 get pods update-demo-nautilus-5s88k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 19 05:34:51.968: INFO: stderr: ""
    Jan 19 05:34:51.968: INFO: stdout: ""
    Jan 19 05:34:51.968: INFO: update-demo-nautilus-5s88k is created but not running
    Jan 19 05:34:56.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 19 05:34:57.049: INFO: stderr: ""
    Jan 19 05:34:57.049: INFO: stdout: "update-demo-nautilus-5s88k update-demo-nautilus-dzbw6 "
    Jan 19 05:34:57.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 get pods update-demo-nautilus-5s88k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 19 05:34:57.127: INFO: stderr: ""
    Jan 19 05:34:57.127: INFO: stdout: "true"
    Jan 19 05:34:57.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 get pods update-demo-nautilus-5s88k -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 19 05:34:57.206: INFO: stderr: ""
    Jan 19 05:34:57.206: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 19 05:34:57.206: INFO: validating pod update-demo-nautilus-5s88k
    Jan 19 05:34:57.212: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 19 05:34:57.212: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 19 05:34:57.212: INFO: update-demo-nautilus-5s88k is verified up and running
    Jan 19 05:34:57.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 get pods update-demo-nautilus-dzbw6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 19 05:34:57.286: INFO: stderr: ""
    Jan 19 05:34:57.286: INFO: stdout: "true"
    Jan 19 05:34:57.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 get pods update-demo-nautilus-dzbw6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 19 05:34:57.364: INFO: stderr: ""
    Jan 19 05:34:57.364: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 19 05:34:57.364: INFO: validating pod update-demo-nautilus-dzbw6
    Jan 19 05:34:57.368: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 19 05:34:57.368: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 19 05:34:57.368: INFO: update-demo-nautilus-dzbw6 is verified up and running
    STEP: scaling down the replication controller 01/19/23 05:34:57.368
    Jan 19 05:34:57.369: INFO: scanned /root for discovery docs: <nil>
    Jan 19 05:34:57.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Jan 19 05:34:57.500: INFO: stderr: ""
    Jan 19 05:34:57.500: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/19/23 05:34:57.5
    Jan 19 05:34:57.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 19 05:34:57.587: INFO: stderr: ""
    Jan 19 05:34:57.587: INFO: stdout: "update-demo-nautilus-5s88k update-demo-nautilus-dzbw6 "
    STEP: Replicas for name=update-demo: expected=1 actual=2 01/19/23 05:34:57.587
    Jan 19 05:35:02.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 19 05:35:02.666: INFO: stderr: ""
    Jan 19 05:35:02.666: INFO: stdout: "update-demo-nautilus-dzbw6 "
    Jan 19 05:35:02.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 get pods update-demo-nautilus-dzbw6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 19 05:35:02.744: INFO: stderr: ""
    Jan 19 05:35:02.744: INFO: stdout: "true"
    Jan 19 05:35:02.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 get pods update-demo-nautilus-dzbw6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 19 05:35:02.823: INFO: stderr: ""
    Jan 19 05:35:02.823: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 19 05:35:02.823: INFO: validating pod update-demo-nautilus-dzbw6
    Jan 19 05:35:02.827: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 19 05:35:02.827: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 19 05:35:02.827: INFO: update-demo-nautilus-dzbw6 is verified up and running
    STEP: scaling up the replication controller 01/19/23 05:35:02.827
    Jan 19 05:35:02.828: INFO: scanned /root for discovery docs: <nil>
    Jan 19 05:35:02.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Jan 19 05:35:03.928: INFO: stderr: ""
    Jan 19 05:35:03.928: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/19/23 05:35:03.928
    Jan 19 05:35:03.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 19 05:35:04.006: INFO: stderr: ""
    Jan 19 05:35:04.006: INFO: stdout: "update-demo-nautilus-dzbw6 update-demo-nautilus-zdqpp "
    Jan 19 05:35:04.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 get pods update-demo-nautilus-dzbw6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 19 05:35:04.085: INFO: stderr: ""
    Jan 19 05:35:04.085: INFO: stdout: "true"
    Jan 19 05:35:04.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 get pods update-demo-nautilus-dzbw6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 19 05:35:04.166: INFO: stderr: ""
    Jan 19 05:35:04.166: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 19 05:35:04.166: INFO: validating pod update-demo-nautilus-dzbw6
    Jan 19 05:35:04.170: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 19 05:35:04.170: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 19 05:35:04.170: INFO: update-demo-nautilus-dzbw6 is verified up and running
    Jan 19 05:35:04.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 get pods update-demo-nautilus-zdqpp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 19 05:35:04.246: INFO: stderr: ""
    Jan 19 05:35:04.246: INFO: stdout: ""
    Jan 19 05:35:04.246: INFO: update-demo-nautilus-zdqpp is created but not running
    Jan 19 05:35:09.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 19 05:35:09.328: INFO: stderr: ""
    Jan 19 05:35:09.328: INFO: stdout: "update-demo-nautilus-dzbw6 update-demo-nautilus-zdqpp "
    Jan 19 05:35:09.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 get pods update-demo-nautilus-dzbw6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 19 05:35:09.409: INFO: stderr: ""
    Jan 19 05:35:09.409: INFO: stdout: "true"
    Jan 19 05:35:09.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 get pods update-demo-nautilus-dzbw6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 19 05:35:09.486: INFO: stderr: ""
    Jan 19 05:35:09.486: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 19 05:35:09.486: INFO: validating pod update-demo-nautilus-dzbw6
    Jan 19 05:35:09.489: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 19 05:35:09.489: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 19 05:35:09.489: INFO: update-demo-nautilus-dzbw6 is verified up and running
    Jan 19 05:35:09.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 get pods update-demo-nautilus-zdqpp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 19 05:35:09.566: INFO: stderr: ""
    Jan 19 05:35:09.566: INFO: stdout: "true"
    Jan 19 05:35:09.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 get pods update-demo-nautilus-zdqpp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 19 05:35:09.640: INFO: stderr: ""
    Jan 19 05:35:09.640: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 19 05:35:09.640: INFO: validating pod update-demo-nautilus-zdqpp
    Jan 19 05:35:09.644: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 19 05:35:09.644: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 19 05:35:09.644: INFO: update-demo-nautilus-zdqpp is verified up and running
    STEP: using delete to clean up resources 01/19/23 05:35:09.644
    Jan 19 05:35:09.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 delete --grace-period=0 --force -f -'
    Jan 19 05:35:09.720: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 19 05:35:09.720: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jan 19 05:35:09.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 get rc,svc -l name=update-demo --no-headers'
    Jan 19 05:35:09.817: INFO: stderr: "No resources found in kubectl-1065 namespace.\n"
    Jan 19 05:35:09.817: INFO: stdout: ""
    Jan 19 05:35:09.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-1065 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 19 05:35:09.904: INFO: stderr: ""
    Jan 19 05:35:09.904: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 19 05:35:09.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1065" for this suite. 01/19/23 05:35:09.907
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:35:09.913
Jan 19 05:35:09.913: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename pods 01/19/23 05:35:09.914
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:35:09.925
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:35:09.927
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
STEP: Create set of pods 01/19/23 05:35:09.93
Jan 19 05:35:09.939: INFO: created test-pod-1
Jan 19 05:35:09.946: INFO: created test-pod-2
Jan 19 05:35:09.952: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 01/19/23 05:35:09.952
Jan 19 05:35:09.952: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-871' to be running and ready
Jan 19 05:35:09.967: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 19 05:35:09.967: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 19 05:35:09.967: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 19 05:35:09.967: INFO: 0 / 3 pods in namespace 'pods-871' are running and ready (0 seconds elapsed)
Jan 19 05:35:09.967: INFO: expected 0 pod replicas in namespace 'pods-871', 0 are Running and Ready.
Jan 19 05:35:09.967: INFO: POD         NODE                            PHASE    GRACE  CONDITIONS
Jan 19 05:35:09.967: INFO: test-pod-1  ckcp-nks-default-worker-node-1  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:35:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:35:09 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:35:09 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:35:09 +0000 UTC  }]
Jan 19 05:35:09.967: INFO: test-pod-2  ckcp-nks-default-worker-node-1  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:35:09 +0000 UTC  }]
Jan 19 05:35:09.967: INFO: test-pod-3  ckcp-nks-default-worker-node-1  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:35:09 +0000 UTC  }]
Jan 19 05:35:09.967: INFO: 
Jan 19 05:35:11.976: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 19 05:35:11.976: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 19 05:35:11.976: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 19 05:35:11.976: INFO: 0 / 3 pods in namespace 'pods-871' are running and ready (2 seconds elapsed)
Jan 19 05:35:11.976: INFO: expected 0 pod replicas in namespace 'pods-871', 0 are Running and Ready.
Jan 19 05:35:11.976: INFO: POD         NODE                            PHASE    GRACE  CONDITIONS
Jan 19 05:35:11.976: INFO: test-pod-1  ckcp-nks-default-worker-node-1  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:35:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:35:09 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:35:09 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:35:09 +0000 UTC  }]
Jan 19 05:35:11.976: INFO: test-pod-2  ckcp-nks-default-worker-node-1  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:35:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:35:09 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:35:09 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:35:09 +0000 UTC  }]
Jan 19 05:35:11.976: INFO: test-pod-3  ckcp-nks-default-worker-node-1  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:35:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:35:09 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:35:09 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:35:09 +0000 UTC  }]
Jan 19 05:35:11.976: INFO: 
Jan 19 05:35:13.977: INFO: 3 / 3 pods in namespace 'pods-871' are running and ready (4 seconds elapsed)
Jan 19 05:35:13.977: INFO: expected 0 pod replicas in namespace 'pods-871', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 01/19/23 05:35:13.995
Jan 19 05:35:13.998: INFO: Pod quantity 3 is different from expected quantity 0
Jan 19 05:35:15.003: INFO: Pod quantity 3 is different from expected quantity 0
Jan 19 05:35:16.003: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 19 05:35:17.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-871" for this suite. 01/19/23 05:35:17.005
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","completed":269,"skipped":4995,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.098 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:35:09.913
    Jan 19 05:35:09.913: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename pods 01/19/23 05:35:09.914
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:35:09.925
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:35:09.927
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:844
    STEP: Create set of pods 01/19/23 05:35:09.93
    Jan 19 05:35:09.939: INFO: created test-pod-1
    Jan 19 05:35:09.946: INFO: created test-pod-2
    Jan 19 05:35:09.952: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 01/19/23 05:35:09.952
    Jan 19 05:35:09.952: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-871' to be running and ready
    Jan 19 05:35:09.967: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 19 05:35:09.967: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 19 05:35:09.967: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 19 05:35:09.967: INFO: 0 / 3 pods in namespace 'pods-871' are running and ready (0 seconds elapsed)
    Jan 19 05:35:09.967: INFO: expected 0 pod replicas in namespace 'pods-871', 0 are Running and Ready.
    Jan 19 05:35:09.967: INFO: POD         NODE                            PHASE    GRACE  CONDITIONS
    Jan 19 05:35:09.967: INFO: test-pod-1  ckcp-nks-default-worker-node-1  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:35:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:35:09 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:35:09 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:35:09 +0000 UTC  }]
    Jan 19 05:35:09.967: INFO: test-pod-2  ckcp-nks-default-worker-node-1  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:35:09 +0000 UTC  }]
    Jan 19 05:35:09.967: INFO: test-pod-3  ckcp-nks-default-worker-node-1  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:35:09 +0000 UTC  }]
    Jan 19 05:35:09.967: INFO: 
    Jan 19 05:35:11.976: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 19 05:35:11.976: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 19 05:35:11.976: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 19 05:35:11.976: INFO: 0 / 3 pods in namespace 'pods-871' are running and ready (2 seconds elapsed)
    Jan 19 05:35:11.976: INFO: expected 0 pod replicas in namespace 'pods-871', 0 are Running and Ready.
    Jan 19 05:35:11.976: INFO: POD         NODE                            PHASE    GRACE  CONDITIONS
    Jan 19 05:35:11.976: INFO: test-pod-1  ckcp-nks-default-worker-node-1  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:35:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:35:09 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:35:09 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:35:09 +0000 UTC  }]
    Jan 19 05:35:11.976: INFO: test-pod-2  ckcp-nks-default-worker-node-1  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:35:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:35:09 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:35:09 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:35:09 +0000 UTC  }]
    Jan 19 05:35:11.976: INFO: test-pod-3  ckcp-nks-default-worker-node-1  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:35:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:35:09 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:35:09 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:35:09 +0000 UTC  }]
    Jan 19 05:35:11.976: INFO: 
    Jan 19 05:35:13.977: INFO: 3 / 3 pods in namespace 'pods-871' are running and ready (4 seconds elapsed)
    Jan 19 05:35:13.977: INFO: expected 0 pod replicas in namespace 'pods-871', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 01/19/23 05:35:13.995
    Jan 19 05:35:13.998: INFO: Pod quantity 3 is different from expected quantity 0
    Jan 19 05:35:15.003: INFO: Pod quantity 3 is different from expected quantity 0
    Jan 19 05:35:16.003: INFO: Pod quantity 3 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 19 05:35:17.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-871" for this suite. 01/19/23 05:35:17.005
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:35:17.012
Jan 19 05:35:17.012: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename taint-single-pod 01/19/23 05:35:17.012
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:35:17.024
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:35:17.026
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
Jan 19 05:35:17.028: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 19 05:36:17.043: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
Jan 19 05:36:17.046: INFO: Starting informer...
STEP: Starting pod... 01/19/23 05:36:17.046
Jan 19 05:36:17.262: INFO: Pod is running on ckcp-nks-default-worker-node-1. Tainting Node
STEP: Trying to apply a taint on the Node 01/19/23 05:36:17.262
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/19/23 05:36:17.274
STEP: Waiting short time to make sure Pod is queued for deletion 01/19/23 05:36:17.277
Jan 19 05:36:17.277: INFO: Pod wasn't evicted. Proceeding
Jan 19 05:36:17.277: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/19/23 05:36:17.289
STEP: Waiting some time to make sure that toleration time passed. 01/19/23 05:36:17.386
Jan 19 05:37:32.386: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
Jan 19 05:37:32.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-4211" for this suite. 01/19/23 05:37:32.391
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","completed":270,"skipped":5008,"failed":0}
------------------------------
â€¢ [SLOW TEST] [135.386 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:35:17.012
    Jan 19 05:35:17.012: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename taint-single-pod 01/19/23 05:35:17.012
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:35:17.024
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:35:17.026
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:166
    Jan 19 05:35:17.028: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 19 05:36:17.043: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:289
    Jan 19 05:36:17.046: INFO: Starting informer...
    STEP: Starting pod... 01/19/23 05:36:17.046
    Jan 19 05:36:17.262: INFO: Pod is running on ckcp-nks-default-worker-node-1. Tainting Node
    STEP: Trying to apply a taint on the Node 01/19/23 05:36:17.262
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/19/23 05:36:17.274
    STEP: Waiting short time to make sure Pod is queued for deletion 01/19/23 05:36:17.277
    Jan 19 05:36:17.277: INFO: Pod wasn't evicted. Proceeding
    Jan 19 05:36:17.277: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/19/23 05:36:17.289
    STEP: Waiting some time to make sure that toleration time passed. 01/19/23 05:36:17.386
    Jan 19 05:37:32.386: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:187
    Jan 19 05:37:32.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-single-pod-4211" for this suite. 01/19/23 05:37:32.391
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:37:32.398
Jan 19 05:37:32.399: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename configmap 01/19/23 05:37:32.399
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:37:32.412
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:37:32.414
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
STEP: Creating configMap with name configmap-test-volume-c80a8f86-197e-487f-b55f-ac8ff6954644 01/19/23 05:37:32.416
STEP: Creating a pod to test consume configMaps 01/19/23 05:37:32.42
Jan 19 05:37:32.430: INFO: Waiting up to 5m0s for pod "pod-configmaps-80efa2c0-2324-467a-b5e0-cdcf53b49ae3" in namespace "configmap-478" to be "Succeeded or Failed"
Jan 19 05:37:32.434: INFO: Pod "pod-configmaps-80efa2c0-2324-467a-b5e0-cdcf53b49ae3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.491284ms
Jan 19 05:37:34.438: INFO: Pod "pod-configmaps-80efa2c0-2324-467a-b5e0-cdcf53b49ae3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008728028s
Jan 19 05:37:36.438: INFO: Pod "pod-configmaps-80efa2c0-2324-467a-b5e0-cdcf53b49ae3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008249297s
Jan 19 05:37:38.438: INFO: Pod "pod-configmaps-80efa2c0-2324-467a-b5e0-cdcf53b49ae3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008780043s
STEP: Saw pod success 01/19/23 05:37:38.439
Jan 19 05:37:38.439: INFO: Pod "pod-configmaps-80efa2c0-2324-467a-b5e0-cdcf53b49ae3" satisfied condition "Succeeded or Failed"
Jan 19 05:37:38.441: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-configmaps-80efa2c0-2324-467a-b5e0-cdcf53b49ae3 container agnhost-container: <nil>
STEP: delete the pod 01/19/23 05:37:38.476
Jan 19 05:37:38.487: INFO: Waiting for pod pod-configmaps-80efa2c0-2324-467a-b5e0-cdcf53b49ae3 to disappear
Jan 19 05:37:38.490: INFO: Pod pod-configmaps-80efa2c0-2324-467a-b5e0-cdcf53b49ae3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 19 05:37:38.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-478" for this suite. 01/19/23 05:37:38.493
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":271,"skipped":5030,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.099 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:37:32.398
    Jan 19 05:37:32.399: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename configmap 01/19/23 05:37:32.399
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:37:32.412
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:37:32.414
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:73
    STEP: Creating configMap with name configmap-test-volume-c80a8f86-197e-487f-b55f-ac8ff6954644 01/19/23 05:37:32.416
    STEP: Creating a pod to test consume configMaps 01/19/23 05:37:32.42
    Jan 19 05:37:32.430: INFO: Waiting up to 5m0s for pod "pod-configmaps-80efa2c0-2324-467a-b5e0-cdcf53b49ae3" in namespace "configmap-478" to be "Succeeded or Failed"
    Jan 19 05:37:32.434: INFO: Pod "pod-configmaps-80efa2c0-2324-467a-b5e0-cdcf53b49ae3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.491284ms
    Jan 19 05:37:34.438: INFO: Pod "pod-configmaps-80efa2c0-2324-467a-b5e0-cdcf53b49ae3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008728028s
    Jan 19 05:37:36.438: INFO: Pod "pod-configmaps-80efa2c0-2324-467a-b5e0-cdcf53b49ae3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008249297s
    Jan 19 05:37:38.438: INFO: Pod "pod-configmaps-80efa2c0-2324-467a-b5e0-cdcf53b49ae3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008780043s
    STEP: Saw pod success 01/19/23 05:37:38.439
    Jan 19 05:37:38.439: INFO: Pod "pod-configmaps-80efa2c0-2324-467a-b5e0-cdcf53b49ae3" satisfied condition "Succeeded or Failed"
    Jan 19 05:37:38.441: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-configmaps-80efa2c0-2324-467a-b5e0-cdcf53b49ae3 container agnhost-container: <nil>
    STEP: delete the pod 01/19/23 05:37:38.476
    Jan 19 05:37:38.487: INFO: Waiting for pod pod-configmaps-80efa2c0-2324-467a-b5e0-cdcf53b49ae3 to disappear
    Jan 19 05:37:38.490: INFO: Pod pod-configmaps-80efa2c0-2324-467a-b5e0-cdcf53b49ae3 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 19 05:37:38.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-478" for this suite. 01/19/23 05:37:38.493
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:37:38.497
Jan 19 05:37:38.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename custom-resource-definition 01/19/23 05:37:38.498
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:37:38.507
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:37:38.509
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Jan 19 05:37:38.512: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 05:37:41.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2288" for this suite. 01/19/23 05:37:41.694
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","completed":272,"skipped":5042,"failed":0}
------------------------------
â€¢ [3.201 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:37:38.497
    Jan 19 05:37:38.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename custom-resource-definition 01/19/23 05:37:38.498
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:37:38.507
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:37:38.509
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Jan 19 05:37:38.512: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 05:37:41.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-2288" for this suite. 01/19/23 05:37:41.694
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:37:41.699
Jan 19 05:37:41.699: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename sched-preemption 01/19/23 05:37:41.7
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:37:41.71
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:37:41.713
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 19 05:37:41.726: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 19 05:38:41.744: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:38:41.747
Jan 19 05:38:41.747: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename sched-preemption-path 01/19/23 05:38:41.747
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:38:41.758
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:38:41.761
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node 01/19/23 05:38:41.763
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/19/23 05:38:41.763
Jan 19 05:38:41.770: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-355" to be "running"
Jan 19 05:38:41.773: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.165938ms
Jan 19 05:38:43.778: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007938121s
Jan 19 05:38:45.777: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.007518298s
Jan 19 05:38:45.777: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/19/23 05:38:45.78
Jan 19 05:38:45.791: INFO: found a healthy node: ckcp-nks-default-worker-node-1
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
Jan 19 05:38:59.874: INFO: pods created so far: [1 1 1]
Jan 19 05:38:59.874: INFO: length of pods created so far: 3
Jan 19 05:39:01.885: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
Jan 19 05:39:08.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-355" for this suite. 01/19/23 05:39:08.893
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jan 19 05:39:08.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7548" for this suite. 01/19/23 05:39:08.929
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","completed":273,"skipped":5045,"failed":0}
------------------------------
â€¢ [SLOW TEST] [87.268 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:543

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:37:41.699
    Jan 19 05:37:41.699: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename sched-preemption 01/19/23 05:37:41.7
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:37:41.71
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:37:41.713
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jan 19 05:37:41.726: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 19 05:38:41.744: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:38:41.747
    Jan 19 05:38:41.747: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename sched-preemption-path 01/19/23 05:38:41.747
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:38:41.758
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:38:41.761
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:496
    STEP: Finding an available node 01/19/23 05:38:41.763
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/19/23 05:38:41.763
    Jan 19 05:38:41.770: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-355" to be "running"
    Jan 19 05:38:41.773: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.165938ms
    Jan 19 05:38:43.778: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007938121s
    Jan 19 05:38:45.777: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.007518298s
    Jan 19 05:38:45.777: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/19/23 05:38:45.78
    Jan 19 05:38:45.791: INFO: found a healthy node: ckcp-nks-default-worker-node-1
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:543
    Jan 19 05:38:59.874: INFO: pods created so far: [1 1 1]
    Jan 19 05:38:59.874: INFO: length of pods created so far: 3
    Jan 19 05:39:01.885: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:187
    Jan 19 05:39:08.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-355" for this suite. 01/19/23 05:39:08.893
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:470
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jan 19 05:39:08.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-7548" for this suite. 01/19/23 05:39:08.929
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:39:08.968
Jan 19 05:39:08.968: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename deployment 01/19/23 05:39:08.968
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:39:08.982
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:39:08.984
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Jan 19 05:39:08.997: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jan 19 05:39:14.000: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/19/23 05:39:14
Jan 19 05:39:14.000: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jan 19 05:39:16.004: INFO: Creating deployment "test-rollover-deployment"
Jan 19 05:39:16.013: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jan 19 05:39:18.018: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jan 19 05:39:18.023: INFO: Ensure that both replica sets have 1 created replica
Jan 19 05:39:18.028: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jan 19 05:39:18.035: INFO: Updating deployment test-rollover-deployment
Jan 19 05:39:18.035: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jan 19 05:39:20.043: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jan 19 05:39:20.049: INFO: Make sure deployment "test-rollover-deployment" is complete
Jan 19 05:39:20.057: INFO: all replica sets need to contain the pod-template-hash label
Jan 19 05:39:20.057: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 39, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 39, 16, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 39, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 39, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 05:39:22.063: INFO: all replica sets need to contain the pod-template-hash label
Jan 19 05:39:22.063: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 39, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 39, 16, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 39, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 39, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 05:39:24.064: INFO: all replica sets need to contain the pod-template-hash label
Jan 19 05:39:24.064: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 39, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 39, 16, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 39, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 39, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 05:39:26.063: INFO: all replica sets need to contain the pod-template-hash label
Jan 19 05:39:26.063: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 39, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 39, 16, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 39, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 39, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 05:39:28.064: INFO: all replica sets need to contain the pod-template-hash label
Jan 19 05:39:28.064: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 39, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 39, 16, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 39, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 39, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 05:39:30.066: INFO: all replica sets need to contain the pod-template-hash label
Jan 19 05:39:30.066: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 39, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 39, 16, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 39, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 39, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 05:39:32.064: INFO: 
Jan 19 05:39:32.064: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 19 05:39:32.071: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-2445  fb815b8d-219d-427b-ad39-8525eb12c1f1 32343 2 2023-01-19 05:39:16 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-19 05:39:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 05:39:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037f13f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-19 05:39:16 +0000 UTC,LastTransitionTime:2023-01-19 05:39:16 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-01-19 05:39:30 +0000 UTC,LastTransitionTime:2023-01-19 05:39:16 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 19 05:39:32.073: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-2445  eb131f44-1217-444d-a038-ae66ce21dbaa 32333 2 2023-01-19 05:39:18 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment fb815b8d-219d-427b-ad39-8525eb12c1f1 0xc00356b0f7 0xc00356b0f8}] [] [{kube-controller-manager Update apps/v1 2023-01-19 05:39:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb815b8d-219d-427b-ad39-8525eb12c1f1\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 05:39:30 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00356b1a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 19 05:39:32.073: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jan 19 05:39:32.073: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-2445  9a3b679e-f1d0-4e97-bb1d-402018e49c43 32342 2 2023-01-19 05:39:08 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment fb815b8d-219d-427b-ad39-8525eb12c1f1 0xc00356aea7 0xc00356aea8}] [] [{e2e.test Update apps/v1 2023-01-19 05:39:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 05:39:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb815b8d-219d-427b-ad39-8525eb12c1f1\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-19 05:39:30 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00356af68 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 19 05:39:32.073: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-2445  b7ea004c-7bd1-4a04-872b-5b9b518cd385 32293 2 2023-01-19 05:39:16 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment fb815b8d-219d-427b-ad39-8525eb12c1f1 0xc00356afd7 0xc00356afd8}] [] [{kube-controller-manager Update apps/v1 2023-01-19 05:39:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb815b8d-219d-427b-ad39-8525eb12c1f1\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 05:39:18 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00356b088 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 19 05:39:32.076: INFO: Pod "test-rollover-deployment-6d45fd857b-wzbwn" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-wzbwn test-rollover-deployment-6d45fd857b- deployment-2445  a6718a3e-43a8-478a-8460-8da0828141a5 32309 0 2023-01-19 05:39:18 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b eb131f44-1217-444d-a038-ae66ce21dbaa 0xc0037f17f7 0xc0037f17f8}] [] [{kube-controller-manager Update v1 2023-01-19 05:39:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eb131f44-1217-444d-a038-ae66ce21dbaa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 05:39:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.70.211\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ql5ms,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ql5ms,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:39:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:39:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:39:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:39:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.78,PodIP:10.100.70.211,StartTime:2023-01-19 05:39:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 05:39:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://cce214bb52fb0cfe660f73af802e6d5fd7654893da237ea872764a0d7492be8a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.70.211,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 19 05:39:32.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2445" for this suite. 01/19/23 05:39:32.079
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","completed":274,"skipped":5077,"failed":0}
------------------------------
â€¢ [SLOW TEST] [23.117 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:39:08.968
    Jan 19 05:39:08.968: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename deployment 01/19/23 05:39:08.968
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:39:08.982
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:39:08.984
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Jan 19 05:39:08.997: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Jan 19 05:39:14.000: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/19/23 05:39:14
    Jan 19 05:39:14.000: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Jan 19 05:39:16.004: INFO: Creating deployment "test-rollover-deployment"
    Jan 19 05:39:16.013: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Jan 19 05:39:18.018: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Jan 19 05:39:18.023: INFO: Ensure that both replica sets have 1 created replica
    Jan 19 05:39:18.028: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Jan 19 05:39:18.035: INFO: Updating deployment test-rollover-deployment
    Jan 19 05:39:18.035: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Jan 19 05:39:20.043: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Jan 19 05:39:20.049: INFO: Make sure deployment "test-rollover-deployment" is complete
    Jan 19 05:39:20.057: INFO: all replica sets need to contain the pod-template-hash label
    Jan 19 05:39:20.057: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 39, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 39, 16, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 39, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 39, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 19 05:39:22.063: INFO: all replica sets need to contain the pod-template-hash label
    Jan 19 05:39:22.063: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 39, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 39, 16, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 39, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 39, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 19 05:39:24.064: INFO: all replica sets need to contain the pod-template-hash label
    Jan 19 05:39:24.064: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 39, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 39, 16, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 39, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 39, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 19 05:39:26.063: INFO: all replica sets need to contain the pod-template-hash label
    Jan 19 05:39:26.063: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 39, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 39, 16, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 39, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 39, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 19 05:39:28.064: INFO: all replica sets need to contain the pod-template-hash label
    Jan 19 05:39:28.064: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 39, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 39, 16, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 39, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 39, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 19 05:39:30.066: INFO: all replica sets need to contain the pod-template-hash label
    Jan 19 05:39:30.066: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 39, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 39, 16, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 39, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 39, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 19 05:39:32.064: INFO: 
    Jan 19 05:39:32.064: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 19 05:39:32.071: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-2445  fb815b8d-219d-427b-ad39-8525eb12c1f1 32343 2 2023-01-19 05:39:16 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-19 05:39:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 05:39:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037f13f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-19 05:39:16 +0000 UTC,LastTransitionTime:2023-01-19 05:39:16 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-01-19 05:39:30 +0000 UTC,LastTransitionTime:2023-01-19 05:39:16 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 19 05:39:32.073: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-2445  eb131f44-1217-444d-a038-ae66ce21dbaa 32333 2 2023-01-19 05:39:18 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment fb815b8d-219d-427b-ad39-8525eb12c1f1 0xc00356b0f7 0xc00356b0f8}] [] [{kube-controller-manager Update apps/v1 2023-01-19 05:39:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb815b8d-219d-427b-ad39-8525eb12c1f1\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 05:39:30 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00356b1a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 19 05:39:32.073: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Jan 19 05:39:32.073: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-2445  9a3b679e-f1d0-4e97-bb1d-402018e49c43 32342 2 2023-01-19 05:39:08 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment fb815b8d-219d-427b-ad39-8525eb12c1f1 0xc00356aea7 0xc00356aea8}] [] [{e2e.test Update apps/v1 2023-01-19 05:39:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 05:39:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb815b8d-219d-427b-ad39-8525eb12c1f1\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-19 05:39:30 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00356af68 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 19 05:39:32.073: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-2445  b7ea004c-7bd1-4a04-872b-5b9b518cd385 32293 2 2023-01-19 05:39:16 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment fb815b8d-219d-427b-ad39-8525eb12c1f1 0xc00356afd7 0xc00356afd8}] [] [{kube-controller-manager Update apps/v1 2023-01-19 05:39:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb815b8d-219d-427b-ad39-8525eb12c1f1\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 05:39:18 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00356b088 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 19 05:39:32.076: INFO: Pod "test-rollover-deployment-6d45fd857b-wzbwn" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-wzbwn test-rollover-deployment-6d45fd857b- deployment-2445  a6718a3e-43a8-478a-8460-8da0828141a5 32309 0 2023-01-19 05:39:18 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b eb131f44-1217-444d-a038-ae66ce21dbaa 0xc0037f17f7 0xc0037f17f8}] [] [{kube-controller-manager Update v1 2023-01-19 05:39:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eb131f44-1217-444d-a038-ae66ce21dbaa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 05:39:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.70.211\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ql5ms,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ql5ms,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ckcp-nks-default-worker-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:39:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:39:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:39:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 05:39:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.78,PodIP:10.100.70.211,StartTime:2023-01-19 05:39:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 05:39:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://cce214bb52fb0cfe660f73af802e6d5fd7654893da237ea872764a0d7492be8a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.70.211,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 19 05:39:32.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-2445" for this suite. 01/19/23 05:39:32.079
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:39:32.085
Jan 19 05:39:32.085: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename webhook 01/19/23 05:39:32.085
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:39:32.099
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:39:32.103
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/19/23 05:39:32.118
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 05:39:32.494
STEP: Deploying the webhook pod 01/19/23 05:39:32.503
STEP: Wait for the deployment to be ready 01/19/23 05:39:32.527
Jan 19 05:39:32.536: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 19 05:39:34.548: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 5, 39, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 39, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 39, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 39, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/19/23 05:39:36.552
STEP: Verifying the service has paired with the endpoint 01/19/23 05:39:36.565
Jan 19 05:39:37.565: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
STEP: Listing all of the created validation webhooks 01/19/23 05:39:37.618
STEP: Creating a configMap that should be mutated 01/19/23 05:39:37.629
STEP: Deleting the collection of validation webhooks 01/19/23 05:39:37.652
STEP: Creating a configMap that should not be mutated 01/19/23 05:39:37.684
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 05:39:37.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8791" for this suite. 01/19/23 05:39:37.696
STEP: Destroying namespace "webhook-8791-markers" for this suite. 01/19/23 05:39:37.7
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","completed":275,"skipped":5097,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.657 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:39:32.085
    Jan 19 05:39:32.085: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename webhook 01/19/23 05:39:32.085
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:39:32.099
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:39:32.103
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/19/23 05:39:32.118
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 05:39:32.494
    STEP: Deploying the webhook pod 01/19/23 05:39:32.503
    STEP: Wait for the deployment to be ready 01/19/23 05:39:32.527
    Jan 19 05:39:32.536: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 19 05:39:34.548: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 5, 39, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 39, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 39, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 39, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/19/23 05:39:36.552
    STEP: Verifying the service has paired with the endpoint 01/19/23 05:39:36.565
    Jan 19 05:39:37.565: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:655
    STEP: Listing all of the created validation webhooks 01/19/23 05:39:37.618
    STEP: Creating a configMap that should be mutated 01/19/23 05:39:37.629
    STEP: Deleting the collection of validation webhooks 01/19/23 05:39:37.652
    STEP: Creating a configMap that should not be mutated 01/19/23 05:39:37.684
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 05:39:37.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8791" for this suite. 01/19/23 05:39:37.696
    STEP: Destroying namespace "webhook-8791-markers" for this suite. 01/19/23 05:39:37.7
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:39:37.742
Jan 19 05:39:37.742: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename downward-api 01/19/23 05:39:37.743
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:39:37.762
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:39:37.765
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
STEP: Creating a pod to test downward API volume plugin 01/19/23 05:39:37.769
Jan 19 05:39:37.777: INFO: Waiting up to 5m0s for pod "downwardapi-volume-995d7704-92c4-4479-acc1-2e94af39e11c" in namespace "downward-api-2283" to be "Succeeded or Failed"
Jan 19 05:39:37.785: INFO: Pod "downwardapi-volume-995d7704-92c4-4479-acc1-2e94af39e11c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.740643ms
Jan 19 05:39:39.791: INFO: Pod "downwardapi-volume-995d7704-92c4-4479-acc1-2e94af39e11c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013821246s
Jan 19 05:39:41.791: INFO: Pod "downwardapi-volume-995d7704-92c4-4479-acc1-2e94af39e11c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01405425s
Jan 19 05:39:43.789: INFO: Pod "downwardapi-volume-995d7704-92c4-4479-acc1-2e94af39e11c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012453524s
STEP: Saw pod success 01/19/23 05:39:43.789
Jan 19 05:39:43.789: INFO: Pod "downwardapi-volume-995d7704-92c4-4479-acc1-2e94af39e11c" satisfied condition "Succeeded or Failed"
Jan 19 05:39:43.792: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod downwardapi-volume-995d7704-92c4-4479-acc1-2e94af39e11c container client-container: <nil>
STEP: delete the pod 01/19/23 05:39:43.826
Jan 19 05:39:43.839: INFO: Waiting for pod downwardapi-volume-995d7704-92c4-4479-acc1-2e94af39e11c to disappear
Jan 19 05:39:43.842: INFO: Pod downwardapi-volume-995d7704-92c4-4479-acc1-2e94af39e11c no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 19 05:39:43.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2283" for this suite. 01/19/23 05:39:43.844
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","completed":276,"skipped":5104,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.106 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:39:37.742
    Jan 19 05:39:37.742: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename downward-api 01/19/23 05:39:37.743
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:39:37.762
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:39:37.765
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:52
    STEP: Creating a pod to test downward API volume plugin 01/19/23 05:39:37.769
    Jan 19 05:39:37.777: INFO: Waiting up to 5m0s for pod "downwardapi-volume-995d7704-92c4-4479-acc1-2e94af39e11c" in namespace "downward-api-2283" to be "Succeeded or Failed"
    Jan 19 05:39:37.785: INFO: Pod "downwardapi-volume-995d7704-92c4-4479-acc1-2e94af39e11c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.740643ms
    Jan 19 05:39:39.791: INFO: Pod "downwardapi-volume-995d7704-92c4-4479-acc1-2e94af39e11c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013821246s
    Jan 19 05:39:41.791: INFO: Pod "downwardapi-volume-995d7704-92c4-4479-acc1-2e94af39e11c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01405425s
    Jan 19 05:39:43.789: INFO: Pod "downwardapi-volume-995d7704-92c4-4479-acc1-2e94af39e11c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012453524s
    STEP: Saw pod success 01/19/23 05:39:43.789
    Jan 19 05:39:43.789: INFO: Pod "downwardapi-volume-995d7704-92c4-4479-acc1-2e94af39e11c" satisfied condition "Succeeded or Failed"
    Jan 19 05:39:43.792: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod downwardapi-volume-995d7704-92c4-4479-acc1-2e94af39e11c container client-container: <nil>
    STEP: delete the pod 01/19/23 05:39:43.826
    Jan 19 05:39:43.839: INFO: Waiting for pod downwardapi-volume-995d7704-92c4-4479-acc1-2e94af39e11c to disappear
    Jan 19 05:39:43.842: INFO: Pod downwardapi-volume-995d7704-92c4-4479-acc1-2e94af39e11c no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 19 05:39:43.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2283" for this suite. 01/19/23 05:39:43.844
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:39:43.848
Jan 19 05:39:43.849: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename secrets 01/19/23 05:39:43.849
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:39:43.862
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:39:43.865
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
STEP: Creating projection with secret that has name secret-emptykey-test-98955e7c-d81b-463d-a448-9b698120ff02 01/19/23 05:39:43.867
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jan 19 05:39:43.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5060" for this suite. 01/19/23 05:39:43.872
{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","completed":277,"skipped":5114,"failed":0}
------------------------------
â€¢ [0.028 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:39:43.848
    Jan 19 05:39:43.849: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename secrets 01/19/23 05:39:43.849
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:39:43.862
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:39:43.865
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:139
    STEP: Creating projection with secret that has name secret-emptykey-test-98955e7c-d81b-463d-a448-9b698120ff02 01/19/23 05:39:43.867
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jan 19 05:39:43.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5060" for this suite. 01/19/23 05:39:43.872
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:39:43.878
Jan 19 05:39:43.878: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename projected 01/19/23 05:39:43.879
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:39:43.89
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:39:43.893
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
STEP: Creating a pod to test downward API volume plugin 01/19/23 05:39:43.896
Jan 19 05:39:43.905: INFO: Waiting up to 5m0s for pod "downwardapi-volume-639aa957-66af-4816-b31b-b9f0cbd56846" in namespace "projected-7680" to be "Succeeded or Failed"
Jan 19 05:39:43.914: INFO: Pod "downwardapi-volume-639aa957-66af-4816-b31b-b9f0cbd56846": Phase="Pending", Reason="", readiness=false. Elapsed: 9.412122ms
Jan 19 05:39:45.918: INFO: Pod "downwardapi-volume-639aa957-66af-4816-b31b-b9f0cbd56846": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012883283s
Jan 19 05:39:47.918: INFO: Pod "downwardapi-volume-639aa957-66af-4816-b31b-b9f0cbd56846": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013597634s
Jan 19 05:39:49.918: INFO: Pod "downwardapi-volume-639aa957-66af-4816-b31b-b9f0cbd56846": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013496526s
STEP: Saw pod success 01/19/23 05:39:49.918
Jan 19 05:39:49.918: INFO: Pod "downwardapi-volume-639aa957-66af-4816-b31b-b9f0cbd56846" satisfied condition "Succeeded or Failed"
Jan 19 05:39:49.920: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod downwardapi-volume-639aa957-66af-4816-b31b-b9f0cbd56846 container client-container: <nil>
STEP: delete the pod 01/19/23 05:39:49.927
Jan 19 05:39:49.938: INFO: Waiting for pod downwardapi-volume-639aa957-66af-4816-b31b-b9f0cbd56846 to disappear
Jan 19 05:39:49.940: INFO: Pod downwardapi-volume-639aa957-66af-4816-b31b-b9f0cbd56846 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 19 05:39:49.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7680" for this suite. 01/19/23 05:39:49.942
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":278,"skipped":5155,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.070 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:39:43.878
    Jan 19 05:39:43.878: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename projected 01/19/23 05:39:43.879
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:39:43.89
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:39:43.893
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:83
    STEP: Creating a pod to test downward API volume plugin 01/19/23 05:39:43.896
    Jan 19 05:39:43.905: INFO: Waiting up to 5m0s for pod "downwardapi-volume-639aa957-66af-4816-b31b-b9f0cbd56846" in namespace "projected-7680" to be "Succeeded or Failed"
    Jan 19 05:39:43.914: INFO: Pod "downwardapi-volume-639aa957-66af-4816-b31b-b9f0cbd56846": Phase="Pending", Reason="", readiness=false. Elapsed: 9.412122ms
    Jan 19 05:39:45.918: INFO: Pod "downwardapi-volume-639aa957-66af-4816-b31b-b9f0cbd56846": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012883283s
    Jan 19 05:39:47.918: INFO: Pod "downwardapi-volume-639aa957-66af-4816-b31b-b9f0cbd56846": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013597634s
    Jan 19 05:39:49.918: INFO: Pod "downwardapi-volume-639aa957-66af-4816-b31b-b9f0cbd56846": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013496526s
    STEP: Saw pod success 01/19/23 05:39:49.918
    Jan 19 05:39:49.918: INFO: Pod "downwardapi-volume-639aa957-66af-4816-b31b-b9f0cbd56846" satisfied condition "Succeeded or Failed"
    Jan 19 05:39:49.920: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod downwardapi-volume-639aa957-66af-4816-b31b-b9f0cbd56846 container client-container: <nil>
    STEP: delete the pod 01/19/23 05:39:49.927
    Jan 19 05:39:49.938: INFO: Waiting for pod downwardapi-volume-639aa957-66af-4816-b31b-b9f0cbd56846 to disappear
    Jan 19 05:39:49.940: INFO: Pod downwardapi-volume-639aa957-66af-4816-b31b-b9f0cbd56846 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 19 05:39:49.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7680" for this suite. 01/19/23 05:39:49.942
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:39:49.948
Jan 19 05:39:49.948: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename podtemplate 01/19/23 05:39:49.949
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:39:49.959
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:39:49.961
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 01/19/23 05:39:49.965
STEP: Replace a pod template 01/19/23 05:39:49.969
Jan 19 05:39:49.975: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Jan 19 05:39:49.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-6193" for this suite. 01/19/23 05:39:49.978
{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","completed":279,"skipped":5157,"failed":0}
------------------------------
â€¢ [0.036 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:39:49.948
    Jan 19 05:39:49.948: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename podtemplate 01/19/23 05:39:49.949
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:39:49.959
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:39:49.961
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 01/19/23 05:39:49.965
    STEP: Replace a pod template 01/19/23 05:39:49.969
    Jan 19 05:39:49.975: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Jan 19 05:39:49.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-6193" for this suite. 01/19/23 05:39:49.978
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:39:49.984
Jan 19 05:39:49.984: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename gc 01/19/23 05:39:49.985
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:39:49.996
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:39:49.999
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Jan 19 05:39:50.034: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"8244426c-62b5-472e-8a54-a4d7cb1a9a8d", Controller:(*bool)(0xc0044bb696), BlockOwnerDeletion:(*bool)(0xc0044bb697)}}
Jan 19 05:39:50.043: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"b11bd885-c982-4ea4-9155-eb787795274e", Controller:(*bool)(0xc00432409e), BlockOwnerDeletion:(*bool)(0xc00432409f)}}
Jan 19 05:39:50.053: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"2b4063b0-acee-41ba-8e89-78f6af85d6cf", Controller:(*bool)(0xc0043242ce), BlockOwnerDeletion:(*bool)(0xc0043242cf)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 19 05:39:55.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5895" for this suite. 01/19/23 05:39:55.068
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","completed":280,"skipped":5158,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.090 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:39:49.984
    Jan 19 05:39:49.984: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename gc 01/19/23 05:39:49.985
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:39:49.996
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:39:49.999
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Jan 19 05:39:50.034: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"8244426c-62b5-472e-8a54-a4d7cb1a9a8d", Controller:(*bool)(0xc0044bb696), BlockOwnerDeletion:(*bool)(0xc0044bb697)}}
    Jan 19 05:39:50.043: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"b11bd885-c982-4ea4-9155-eb787795274e", Controller:(*bool)(0xc00432409e), BlockOwnerDeletion:(*bool)(0xc00432409f)}}
    Jan 19 05:39:50.053: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"2b4063b0-acee-41ba-8e89-78f6af85d6cf", Controller:(*bool)(0xc0043242ce), BlockOwnerDeletion:(*bool)(0xc0043242cf)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 19 05:39:55.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-5895" for this suite. 01/19/23 05:39:55.068
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:39:55.074
Jan 19 05:39:55.074: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename projected 01/19/23 05:39:55.076
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:39:55.087
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:39:55.093
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
STEP: Creating the pod 01/19/23 05:39:55.095
Jan 19 05:39:55.103: INFO: Waiting up to 5m0s for pod "labelsupdateff4ab74e-e7cc-4ced-87d5-1c2e2b1aeba1" in namespace "projected-5106" to be "running and ready"
Jan 19 05:39:55.107: INFO: Pod "labelsupdateff4ab74e-e7cc-4ced-87d5-1c2e2b1aeba1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.200152ms
Jan 19 05:39:55.107: INFO: The phase of Pod labelsupdateff4ab74e-e7cc-4ced-87d5-1c2e2b1aeba1 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 05:39:57.111: INFO: Pod "labelsupdateff4ab74e-e7cc-4ced-87d5-1c2e2b1aeba1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008053158s
Jan 19 05:39:57.111: INFO: The phase of Pod labelsupdateff4ab74e-e7cc-4ced-87d5-1c2e2b1aeba1 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 05:39:59.113: INFO: Pod "labelsupdateff4ab74e-e7cc-4ced-87d5-1c2e2b1aeba1": Phase="Running", Reason="", readiness=true. Elapsed: 4.009757351s
Jan 19 05:39:59.113: INFO: The phase of Pod labelsupdateff4ab74e-e7cc-4ced-87d5-1c2e2b1aeba1 is Running (Ready = true)
Jan 19 05:39:59.113: INFO: Pod "labelsupdateff4ab74e-e7cc-4ced-87d5-1c2e2b1aeba1" satisfied condition "running and ready"
Jan 19 05:39:59.635: INFO: Successfully updated pod "labelsupdateff4ab74e-e7cc-4ced-87d5-1c2e2b1aeba1"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 19 05:40:01.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5106" for this suite. 01/19/23 05:40:01.653
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","completed":281,"skipped":5160,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.591 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:39:55.074
    Jan 19 05:39:55.074: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename projected 01/19/23 05:39:55.076
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:39:55.087
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:39:55.093
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:129
    STEP: Creating the pod 01/19/23 05:39:55.095
    Jan 19 05:39:55.103: INFO: Waiting up to 5m0s for pod "labelsupdateff4ab74e-e7cc-4ced-87d5-1c2e2b1aeba1" in namespace "projected-5106" to be "running and ready"
    Jan 19 05:39:55.107: INFO: Pod "labelsupdateff4ab74e-e7cc-4ced-87d5-1c2e2b1aeba1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.200152ms
    Jan 19 05:39:55.107: INFO: The phase of Pod labelsupdateff4ab74e-e7cc-4ced-87d5-1c2e2b1aeba1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 05:39:57.111: INFO: Pod "labelsupdateff4ab74e-e7cc-4ced-87d5-1c2e2b1aeba1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008053158s
    Jan 19 05:39:57.111: INFO: The phase of Pod labelsupdateff4ab74e-e7cc-4ced-87d5-1c2e2b1aeba1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 05:39:59.113: INFO: Pod "labelsupdateff4ab74e-e7cc-4ced-87d5-1c2e2b1aeba1": Phase="Running", Reason="", readiness=true. Elapsed: 4.009757351s
    Jan 19 05:39:59.113: INFO: The phase of Pod labelsupdateff4ab74e-e7cc-4ced-87d5-1c2e2b1aeba1 is Running (Ready = true)
    Jan 19 05:39:59.113: INFO: Pod "labelsupdateff4ab74e-e7cc-4ced-87d5-1c2e2b1aeba1" satisfied condition "running and ready"
    Jan 19 05:39:59.635: INFO: Successfully updated pod "labelsupdateff4ab74e-e7cc-4ced-87d5-1c2e2b1aeba1"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 19 05:40:01.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5106" for this suite. 01/19/23 05:40:01.653
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:40:01.666
Jan 19 05:40:01.666: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename podtemplate 01/19/23 05:40:01.666
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:40:01.686
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:40:01.688
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Jan 19 05:40:01.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-7096" for this suite. 01/19/23 05:40:01.726
{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","completed":282,"skipped":5166,"failed":0}
------------------------------
â€¢ [0.069 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:40:01.666
    Jan 19 05:40:01.666: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename podtemplate 01/19/23 05:40:01.666
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:40:01.686
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:40:01.688
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Jan 19 05:40:01.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-7096" for this suite. 01/19/23 05:40:01.726
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:40:01.735
Jan 19 05:40:01.735: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename configmap 01/19/23 05:40:01.736
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:40:01.746
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:40:01.749
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
STEP: Creating configMap configmap-6664/configmap-test-2217d076-ba6e-408d-9262-e88c995b886c 01/19/23 05:40:01.751
STEP: Creating a pod to test consume configMaps 01/19/23 05:40:01.755
Jan 19 05:40:01.761: INFO: Waiting up to 5m0s for pod "pod-configmaps-58a23161-9109-4003-818d-9dc4a28b6a6b" in namespace "configmap-6664" to be "Succeeded or Failed"
Jan 19 05:40:01.765: INFO: Pod "pod-configmaps-58a23161-9109-4003-818d-9dc4a28b6a6b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.297431ms
Jan 19 05:40:03.770: INFO: Pod "pod-configmaps-58a23161-9109-4003-818d-9dc4a28b6a6b": Phase="Running", Reason="", readiness=true. Elapsed: 2.00814639s
Jan 19 05:40:05.770: INFO: Pod "pod-configmaps-58a23161-9109-4003-818d-9dc4a28b6a6b": Phase="Running", Reason="", readiness=false. Elapsed: 4.008600446s
Jan 19 05:40:07.769: INFO: Pod "pod-configmaps-58a23161-9109-4003-818d-9dc4a28b6a6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007728472s
STEP: Saw pod success 01/19/23 05:40:07.769
Jan 19 05:40:07.769: INFO: Pod "pod-configmaps-58a23161-9109-4003-818d-9dc4a28b6a6b" satisfied condition "Succeeded or Failed"
Jan 19 05:40:07.772: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-configmaps-58a23161-9109-4003-818d-9dc4a28b6a6b container env-test: <nil>
STEP: delete the pod 01/19/23 05:40:07.778
Jan 19 05:40:07.787: INFO: Waiting for pod pod-configmaps-58a23161-9109-4003-818d-9dc4a28b6a6b to disappear
Jan 19 05:40:07.790: INFO: Pod pod-configmaps-58a23161-9109-4003-818d-9dc4a28b6a6b no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jan 19 05:40:07.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6664" for this suite. 01/19/23 05:40:07.793
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","completed":283,"skipped":5176,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.064 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:40:01.735
    Jan 19 05:40:01.735: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename configmap 01/19/23 05:40:01.736
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:40:01.746
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:40:01.749
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:44
    STEP: Creating configMap configmap-6664/configmap-test-2217d076-ba6e-408d-9262-e88c995b886c 01/19/23 05:40:01.751
    STEP: Creating a pod to test consume configMaps 01/19/23 05:40:01.755
    Jan 19 05:40:01.761: INFO: Waiting up to 5m0s for pod "pod-configmaps-58a23161-9109-4003-818d-9dc4a28b6a6b" in namespace "configmap-6664" to be "Succeeded or Failed"
    Jan 19 05:40:01.765: INFO: Pod "pod-configmaps-58a23161-9109-4003-818d-9dc4a28b6a6b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.297431ms
    Jan 19 05:40:03.770: INFO: Pod "pod-configmaps-58a23161-9109-4003-818d-9dc4a28b6a6b": Phase="Running", Reason="", readiness=true. Elapsed: 2.00814639s
    Jan 19 05:40:05.770: INFO: Pod "pod-configmaps-58a23161-9109-4003-818d-9dc4a28b6a6b": Phase="Running", Reason="", readiness=false. Elapsed: 4.008600446s
    Jan 19 05:40:07.769: INFO: Pod "pod-configmaps-58a23161-9109-4003-818d-9dc4a28b6a6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007728472s
    STEP: Saw pod success 01/19/23 05:40:07.769
    Jan 19 05:40:07.769: INFO: Pod "pod-configmaps-58a23161-9109-4003-818d-9dc4a28b6a6b" satisfied condition "Succeeded or Failed"
    Jan 19 05:40:07.772: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-configmaps-58a23161-9109-4003-818d-9dc4a28b6a6b container env-test: <nil>
    STEP: delete the pod 01/19/23 05:40:07.778
    Jan 19 05:40:07.787: INFO: Waiting for pod pod-configmaps-58a23161-9109-4003-818d-9dc4a28b6a6b to disappear
    Jan 19 05:40:07.790: INFO: Pod pod-configmaps-58a23161-9109-4003-818d-9dc4a28b6a6b no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 19 05:40:07.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6664" for this suite. 01/19/23 05:40:07.793
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:40:07.799
Jan 19 05:40:07.799: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename pods 01/19/23 05:40:07.8
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:40:07.811
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:40:07.814
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
STEP: creating the pod 01/19/23 05:40:07.816
STEP: setting up watch 01/19/23 05:40:07.816
STEP: submitting the pod to kubernetes 01/19/23 05:40:07.919
STEP: verifying the pod is in kubernetes 01/19/23 05:40:07.927
STEP: verifying pod creation was observed 01/19/23 05:40:07.931
Jan 19 05:40:07.932: INFO: Waiting up to 5m0s for pod "pod-submit-remove-69fafc11-1f77-454d-9be7-efea92b408b9" in namespace "pods-7983" to be "running"
Jan 19 05:40:07.936: INFO: Pod "pod-submit-remove-69fafc11-1f77-454d-9be7-efea92b408b9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.191543ms
Jan 19 05:40:09.940: INFO: Pod "pod-submit-remove-69fafc11-1f77-454d-9be7-efea92b408b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008501898s
Jan 19 05:40:11.939: INFO: Pod "pod-submit-remove-69fafc11-1f77-454d-9be7-efea92b408b9": Phase="Running", Reason="", readiness=true. Elapsed: 4.007176006s
Jan 19 05:40:11.939: INFO: Pod "pod-submit-remove-69fafc11-1f77-454d-9be7-efea92b408b9" satisfied condition "running"
STEP: deleting the pod gracefully 01/19/23 05:40:11.942
STEP: verifying pod deletion was observed 01/19/23 05:40:11.949
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 19 05:40:13.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7983" for this suite. 01/19/23 05:40:13.773
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","completed":284,"skipped":5192,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.979 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:40:07.799
    Jan 19 05:40:07.799: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename pods 01/19/23 05:40:07.8
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:40:07.811
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:40:07.814
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:225
    STEP: creating the pod 01/19/23 05:40:07.816
    STEP: setting up watch 01/19/23 05:40:07.816
    STEP: submitting the pod to kubernetes 01/19/23 05:40:07.919
    STEP: verifying the pod is in kubernetes 01/19/23 05:40:07.927
    STEP: verifying pod creation was observed 01/19/23 05:40:07.931
    Jan 19 05:40:07.932: INFO: Waiting up to 5m0s for pod "pod-submit-remove-69fafc11-1f77-454d-9be7-efea92b408b9" in namespace "pods-7983" to be "running"
    Jan 19 05:40:07.936: INFO: Pod "pod-submit-remove-69fafc11-1f77-454d-9be7-efea92b408b9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.191543ms
    Jan 19 05:40:09.940: INFO: Pod "pod-submit-remove-69fafc11-1f77-454d-9be7-efea92b408b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008501898s
    Jan 19 05:40:11.939: INFO: Pod "pod-submit-remove-69fafc11-1f77-454d-9be7-efea92b408b9": Phase="Running", Reason="", readiness=true. Elapsed: 4.007176006s
    Jan 19 05:40:11.939: INFO: Pod "pod-submit-remove-69fafc11-1f77-454d-9be7-efea92b408b9" satisfied condition "running"
    STEP: deleting the pod gracefully 01/19/23 05:40:11.942
    STEP: verifying pod deletion was observed 01/19/23 05:40:11.949
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 19 05:40:13.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-7983" for this suite. 01/19/23 05:40:13.773
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:40:13.779
Jan 19 05:40:13.779: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename custom-resource-definition 01/19/23 05:40:13.779
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:40:13.79
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:40:13.793
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Jan 19 05:40:13.796: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 05:40:14.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-450" for this suite. 01/19/23 05:40:14.824
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","completed":285,"skipped":5192,"failed":0}
------------------------------
â€¢ [1.050 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:40:13.779
    Jan 19 05:40:13.779: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename custom-resource-definition 01/19/23 05:40:13.779
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:40:13.79
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:40:13.793
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Jan 19 05:40:13.796: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 05:40:14.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-450" for this suite. 01/19/23 05:40:14.824
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:40:14.829
Jan 19 05:40:14.829: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename container-lifecycle-hook 01/19/23 05:40:14.83
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:40:14.845
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:40:14.851
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 01/19/23 05:40:14.859
Jan 19 05:40:14.866: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-3452" to be "running and ready"
Jan 19 05:40:14.870: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.315715ms
Jan 19 05:40:14.870: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 19 05:40:16.878: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01170597s
Jan 19 05:40:16.878: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 19 05:40:18.876: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.009690912s
Jan 19 05:40:18.876: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 19 05:40:18.876: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
STEP: create the pod with lifecycle hook 01/19/23 05:40:18.879
Jan 19 05:40:18.886: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-3452" to be "running and ready"
Jan 19 05:40:18.890: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.217447ms
Jan 19 05:40:18.890: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 19 05:40:20.894: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.007529375s
Jan 19 05:40:20.894: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Jan 19 05:40:20.894: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 01/19/23 05:40:20.896
Jan 19 05:40:20.902: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 19 05:40:20.906: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 19 05:40:22.907: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 19 05:40:22.910: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 19 05:40:24.907: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 19 05:40:24.910: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 01/19/23 05:40:24.91
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jan 19 05:40:24.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3452" for this suite. 01/19/23 05:40:24.919
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","completed":286,"skipped":5193,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.095 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:114

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:40:14.829
    Jan 19 05:40:14.829: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/19/23 05:40:14.83
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:40:14.845
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:40:14.851
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 01/19/23 05:40:14.859
    Jan 19 05:40:14.866: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-3452" to be "running and ready"
    Jan 19 05:40:14.870: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.315715ms
    Jan 19 05:40:14.870: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 05:40:16.878: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01170597s
    Jan 19 05:40:16.878: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 05:40:18.876: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.009690912s
    Jan 19 05:40:18.876: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 19 05:40:18.876: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:114
    STEP: create the pod with lifecycle hook 01/19/23 05:40:18.879
    Jan 19 05:40:18.886: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-3452" to be "running and ready"
    Jan 19 05:40:18.890: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.217447ms
    Jan 19 05:40:18.890: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 05:40:20.894: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.007529375s
    Jan 19 05:40:20.894: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Jan 19 05:40:20.894: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 01/19/23 05:40:20.896
    Jan 19 05:40:20.902: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan 19 05:40:20.906: INFO: Pod pod-with-prestop-exec-hook still exists
    Jan 19 05:40:22.907: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan 19 05:40:22.910: INFO: Pod pod-with-prestop-exec-hook still exists
    Jan 19 05:40:24.907: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan 19 05:40:24.910: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 01/19/23 05:40:24.91
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jan 19 05:40:24.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-3452" for this suite. 01/19/23 05:40:24.919
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:40:24.924
Jan 19 05:40:24.925: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename replicaset 01/19/23 05:40:24.925
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:40:24.937
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:40:24.939
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Jan 19 05:40:24.942: INFO: Creating ReplicaSet my-hostname-basic-18576250-b91b-4a1b-9bda-16ae0e995954
Jan 19 05:40:24.948: INFO: Pod name my-hostname-basic-18576250-b91b-4a1b-9bda-16ae0e995954: Found 0 pods out of 1
Jan 19 05:40:29.952: INFO: Pod name my-hostname-basic-18576250-b91b-4a1b-9bda-16ae0e995954: Found 1 pods out of 1
Jan 19 05:40:29.952: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-18576250-b91b-4a1b-9bda-16ae0e995954" is running
Jan 19 05:40:29.952: INFO: Waiting up to 5m0s for pod "my-hostname-basic-18576250-b91b-4a1b-9bda-16ae0e995954-klklb" in namespace "replicaset-2025" to be "running"
Jan 19 05:40:29.954: INFO: Pod "my-hostname-basic-18576250-b91b-4a1b-9bda-16ae0e995954-klklb": Phase="Running", Reason="", readiness=true. Elapsed: 2.201916ms
Jan 19 05:40:29.954: INFO: Pod "my-hostname-basic-18576250-b91b-4a1b-9bda-16ae0e995954-klklb" satisfied condition "running"
Jan 19 05:40:29.954: INFO: Pod "my-hostname-basic-18576250-b91b-4a1b-9bda-16ae0e995954-klklb" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-19 05:40:24 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-19 05:40:27 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-19 05:40:27 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-19 05:40:24 +0000 UTC Reason: Message:}])
Jan 19 05:40:29.954: INFO: Trying to dial the pod
Jan 19 05:40:34.964: INFO: Controller my-hostname-basic-18576250-b91b-4a1b-9bda-16ae0e995954: Got expected result from replica 1 [my-hostname-basic-18576250-b91b-4a1b-9bda-16ae0e995954-klklb]: "my-hostname-basic-18576250-b91b-4a1b-9bda-16ae0e995954-klklb", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 19 05:40:34.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2025" for this suite. 01/19/23 05:40:34.967
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","completed":287,"skipped":5193,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.050 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:40:24.924
    Jan 19 05:40:24.925: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename replicaset 01/19/23 05:40:24.925
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:40:24.937
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:40:24.939
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Jan 19 05:40:24.942: INFO: Creating ReplicaSet my-hostname-basic-18576250-b91b-4a1b-9bda-16ae0e995954
    Jan 19 05:40:24.948: INFO: Pod name my-hostname-basic-18576250-b91b-4a1b-9bda-16ae0e995954: Found 0 pods out of 1
    Jan 19 05:40:29.952: INFO: Pod name my-hostname-basic-18576250-b91b-4a1b-9bda-16ae0e995954: Found 1 pods out of 1
    Jan 19 05:40:29.952: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-18576250-b91b-4a1b-9bda-16ae0e995954" is running
    Jan 19 05:40:29.952: INFO: Waiting up to 5m0s for pod "my-hostname-basic-18576250-b91b-4a1b-9bda-16ae0e995954-klklb" in namespace "replicaset-2025" to be "running"
    Jan 19 05:40:29.954: INFO: Pod "my-hostname-basic-18576250-b91b-4a1b-9bda-16ae0e995954-klklb": Phase="Running", Reason="", readiness=true. Elapsed: 2.201916ms
    Jan 19 05:40:29.954: INFO: Pod "my-hostname-basic-18576250-b91b-4a1b-9bda-16ae0e995954-klklb" satisfied condition "running"
    Jan 19 05:40:29.954: INFO: Pod "my-hostname-basic-18576250-b91b-4a1b-9bda-16ae0e995954-klklb" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-19 05:40:24 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-19 05:40:27 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-19 05:40:27 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-19 05:40:24 +0000 UTC Reason: Message:}])
    Jan 19 05:40:29.954: INFO: Trying to dial the pod
    Jan 19 05:40:34.964: INFO: Controller my-hostname-basic-18576250-b91b-4a1b-9bda-16ae0e995954: Got expected result from replica 1 [my-hostname-basic-18576250-b91b-4a1b-9bda-16ae0e995954-klklb]: "my-hostname-basic-18576250-b91b-4a1b-9bda-16ae0e995954-klklb", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 19 05:40:34.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-2025" for this suite. 01/19/23 05:40:34.967
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:40:34.975
Jan 19 05:40:34.975: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename emptydir 01/19/23 05:40:34.976
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:40:34.987
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:40:34.991
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
STEP: Creating a pod to test emptydir 0644 on tmpfs 01/19/23 05:40:34.993
Jan 19 05:40:35.003: INFO: Waiting up to 5m0s for pod "pod-bb266289-6729-472d-bc80-63fd246aa375" in namespace "emptydir-1419" to be "Succeeded or Failed"
Jan 19 05:40:35.006: INFO: Pod "pod-bb266289-6729-472d-bc80-63fd246aa375": Phase="Pending", Reason="", readiness=false. Elapsed: 3.314427ms
Jan 19 05:40:37.009: INFO: Pod "pod-bb266289-6729-472d-bc80-63fd246aa375": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006222037s
Jan 19 05:40:39.009: INFO: Pod "pod-bb266289-6729-472d-bc80-63fd246aa375": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006183023s
Jan 19 05:40:41.010: INFO: Pod "pod-bb266289-6729-472d-bc80-63fd246aa375": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006970259s
STEP: Saw pod success 01/19/23 05:40:41.01
Jan 19 05:40:41.010: INFO: Pod "pod-bb266289-6729-472d-bc80-63fd246aa375" satisfied condition "Succeeded or Failed"
Jan 19 05:40:41.013: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-bb266289-6729-472d-bc80-63fd246aa375 container test-container: <nil>
STEP: delete the pod 01/19/23 05:40:41.018
Jan 19 05:40:41.028: INFO: Waiting for pod pod-bb266289-6729-472d-bc80-63fd246aa375 to disappear
Jan 19 05:40:41.030: INFO: Pod pod-bb266289-6729-472d-bc80-63fd246aa375 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 19 05:40:41.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1419" for this suite. 01/19/23 05:40:41.033
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":288,"skipped":5209,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.062 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:40:34.975
    Jan 19 05:40:34.975: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename emptydir 01/19/23 05:40:34.976
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:40:34.987
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:40:34.991
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:126
    STEP: Creating a pod to test emptydir 0644 on tmpfs 01/19/23 05:40:34.993
    Jan 19 05:40:35.003: INFO: Waiting up to 5m0s for pod "pod-bb266289-6729-472d-bc80-63fd246aa375" in namespace "emptydir-1419" to be "Succeeded or Failed"
    Jan 19 05:40:35.006: INFO: Pod "pod-bb266289-6729-472d-bc80-63fd246aa375": Phase="Pending", Reason="", readiness=false. Elapsed: 3.314427ms
    Jan 19 05:40:37.009: INFO: Pod "pod-bb266289-6729-472d-bc80-63fd246aa375": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006222037s
    Jan 19 05:40:39.009: INFO: Pod "pod-bb266289-6729-472d-bc80-63fd246aa375": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006183023s
    Jan 19 05:40:41.010: INFO: Pod "pod-bb266289-6729-472d-bc80-63fd246aa375": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006970259s
    STEP: Saw pod success 01/19/23 05:40:41.01
    Jan 19 05:40:41.010: INFO: Pod "pod-bb266289-6729-472d-bc80-63fd246aa375" satisfied condition "Succeeded or Failed"
    Jan 19 05:40:41.013: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-bb266289-6729-472d-bc80-63fd246aa375 container test-container: <nil>
    STEP: delete the pod 01/19/23 05:40:41.018
    Jan 19 05:40:41.028: INFO: Waiting for pod pod-bb266289-6729-472d-bc80-63fd246aa375 to disappear
    Jan 19 05:40:41.030: INFO: Pod pod-bb266289-6729-472d-bc80-63fd246aa375 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 19 05:40:41.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1419" for this suite. 01/19/23 05:40:41.033
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:40:41.038
Jan 19 05:40:41.038: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename secrets 01/19/23 05:40:41.038
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:40:41.05
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:40:41.052
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
STEP: Creating secret with name secret-test-09563171-ec1b-42d8-a6a9-65366b63c8a4 01/19/23 05:40:41.054
STEP: Creating a pod to test consume secrets 01/19/23 05:40:41.059
Jan 19 05:40:41.069: INFO: Waiting up to 5m0s for pod "pod-secrets-d0b02279-a614-41ab-be07-49471d972af8" in namespace "secrets-2959" to be "Succeeded or Failed"
Jan 19 05:40:41.076: INFO: Pod "pod-secrets-d0b02279-a614-41ab-be07-49471d972af8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.913908ms
Jan 19 05:40:43.080: INFO: Pod "pod-secrets-d0b02279-a614-41ab-be07-49471d972af8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011281013s
Jan 19 05:40:45.080: INFO: Pod "pod-secrets-d0b02279-a614-41ab-be07-49471d972af8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011601608s
Jan 19 05:40:47.080: INFO: Pod "pod-secrets-d0b02279-a614-41ab-be07-49471d972af8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011507496s
STEP: Saw pod success 01/19/23 05:40:47.08
Jan 19 05:40:47.080: INFO: Pod "pod-secrets-d0b02279-a614-41ab-be07-49471d972af8" satisfied condition "Succeeded or Failed"
Jan 19 05:40:47.084: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-secrets-d0b02279-a614-41ab-be07-49471d972af8 container secret-volume-test: <nil>
STEP: delete the pod 01/19/23 05:40:47.09
Jan 19 05:40:47.101: INFO: Waiting for pod pod-secrets-d0b02279-a614-41ab-be07-49471d972af8 to disappear
Jan 19 05:40:47.104: INFO: Pod pod-secrets-d0b02279-a614-41ab-be07-49471d972af8 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 19 05:40:47.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2959" for this suite. 01/19/23 05:40:47.107
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":289,"skipped":5217,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.075 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:40:41.038
    Jan 19 05:40:41.038: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename secrets 01/19/23 05:40:41.038
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:40:41.05
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:40:41.052
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:124
    STEP: Creating secret with name secret-test-09563171-ec1b-42d8-a6a9-65366b63c8a4 01/19/23 05:40:41.054
    STEP: Creating a pod to test consume secrets 01/19/23 05:40:41.059
    Jan 19 05:40:41.069: INFO: Waiting up to 5m0s for pod "pod-secrets-d0b02279-a614-41ab-be07-49471d972af8" in namespace "secrets-2959" to be "Succeeded or Failed"
    Jan 19 05:40:41.076: INFO: Pod "pod-secrets-d0b02279-a614-41ab-be07-49471d972af8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.913908ms
    Jan 19 05:40:43.080: INFO: Pod "pod-secrets-d0b02279-a614-41ab-be07-49471d972af8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011281013s
    Jan 19 05:40:45.080: INFO: Pod "pod-secrets-d0b02279-a614-41ab-be07-49471d972af8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011601608s
    Jan 19 05:40:47.080: INFO: Pod "pod-secrets-d0b02279-a614-41ab-be07-49471d972af8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011507496s
    STEP: Saw pod success 01/19/23 05:40:47.08
    Jan 19 05:40:47.080: INFO: Pod "pod-secrets-d0b02279-a614-41ab-be07-49471d972af8" satisfied condition "Succeeded or Failed"
    Jan 19 05:40:47.084: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-secrets-d0b02279-a614-41ab-be07-49471d972af8 container secret-volume-test: <nil>
    STEP: delete the pod 01/19/23 05:40:47.09
    Jan 19 05:40:47.101: INFO: Waiting for pod pod-secrets-d0b02279-a614-41ab-be07-49471d972af8 to disappear
    Jan 19 05:40:47.104: INFO: Pod pod-secrets-d0b02279-a614-41ab-be07-49471d972af8 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 19 05:40:47.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-2959" for this suite. 01/19/23 05:40:47.107
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:40:47.113
Jan 19 05:40:47.113: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename kubelet-test 01/19/23 05:40:47.113
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:40:47.124
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:40:47.128
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Jan 19 05:40:47.139: INFO: Waiting up to 5m0s for pod "busybox-scheduling-da3da546-8948-4cc9-812e-e51e180190fc" in namespace "kubelet-test-3255" to be "running and ready"
Jan 19 05:40:47.147: INFO: Pod "busybox-scheduling-da3da546-8948-4cc9-812e-e51e180190fc": Phase="Pending", Reason="", readiness=false. Elapsed: 7.304419ms
Jan 19 05:40:47.147: INFO: The phase of Pod busybox-scheduling-da3da546-8948-4cc9-812e-e51e180190fc is Pending, waiting for it to be Running (with Ready = true)
Jan 19 05:40:49.151: INFO: Pod "busybox-scheduling-da3da546-8948-4cc9-812e-e51e180190fc": Phase="Running", Reason="", readiness=true. Elapsed: 2.011205951s
Jan 19 05:40:49.151: INFO: The phase of Pod busybox-scheduling-da3da546-8948-4cc9-812e-e51e180190fc is Running (Ready = true)
Jan 19 05:40:49.151: INFO: Pod "busybox-scheduling-da3da546-8948-4cc9-812e-e51e180190fc" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan 19 05:40:49.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3255" for this suite. 01/19/23 05:40:49.161
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","completed":290,"skipped":5224,"failed":0}
------------------------------
â€¢ [2.054 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:40:47.113
    Jan 19 05:40:47.113: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename kubelet-test 01/19/23 05:40:47.113
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:40:47.124
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:40:47.128
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Jan 19 05:40:47.139: INFO: Waiting up to 5m0s for pod "busybox-scheduling-da3da546-8948-4cc9-812e-e51e180190fc" in namespace "kubelet-test-3255" to be "running and ready"
    Jan 19 05:40:47.147: INFO: Pod "busybox-scheduling-da3da546-8948-4cc9-812e-e51e180190fc": Phase="Pending", Reason="", readiness=false. Elapsed: 7.304419ms
    Jan 19 05:40:47.147: INFO: The phase of Pod busybox-scheduling-da3da546-8948-4cc9-812e-e51e180190fc is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 05:40:49.151: INFO: Pod "busybox-scheduling-da3da546-8948-4cc9-812e-e51e180190fc": Phase="Running", Reason="", readiness=true. Elapsed: 2.011205951s
    Jan 19 05:40:49.151: INFO: The phase of Pod busybox-scheduling-da3da546-8948-4cc9-812e-e51e180190fc is Running (Ready = true)
    Jan 19 05:40:49.151: INFO: Pod "busybox-scheduling-da3da546-8948-4cc9-812e-e51e180190fc" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan 19 05:40:49.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-3255" for this suite. 01/19/23 05:40:49.161
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:40:49.167
Jan 19 05:40:49.167: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename downward-api 01/19/23 05:40:49.168
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:40:49.183
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:40:49.186
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
STEP: Creating a pod to test downward api env vars 01/19/23 05:40:49.188
Jan 19 05:40:49.199: INFO: Waiting up to 5m0s for pod "downward-api-df1429c8-4d0a-46e2-ab52-0f27d4777219" in namespace "downward-api-9928" to be "Succeeded or Failed"
Jan 19 05:40:49.204: INFO: Pod "downward-api-df1429c8-4d0a-46e2-ab52-0f27d4777219": Phase="Pending", Reason="", readiness=false. Elapsed: 4.58484ms
Jan 19 05:40:51.208: INFO: Pod "downward-api-df1429c8-4d0a-46e2-ab52-0f27d4777219": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008855701s
Jan 19 05:40:53.209: INFO: Pod "downward-api-df1429c8-4d0a-46e2-ab52-0f27d4777219": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009552562s
Jan 19 05:40:55.208: INFO: Pod "downward-api-df1429c8-4d0a-46e2-ab52-0f27d4777219": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009175598s
STEP: Saw pod success 01/19/23 05:40:55.208
Jan 19 05:40:55.208: INFO: Pod "downward-api-df1429c8-4d0a-46e2-ab52-0f27d4777219" satisfied condition "Succeeded or Failed"
Jan 19 05:40:55.211: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod downward-api-df1429c8-4d0a-46e2-ab52-0f27d4777219 container dapi-container: <nil>
STEP: delete the pod 01/19/23 05:40:55.218
Jan 19 05:40:55.229: INFO: Waiting for pod downward-api-df1429c8-4d0a-46e2-ab52-0f27d4777219 to disappear
Jan 19 05:40:55.232: INFO: Pod downward-api-df1429c8-4d0a-46e2-ab52-0f27d4777219 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan 19 05:40:55.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9928" for this suite. 01/19/23 05:40:55.235
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","completed":291,"skipped":5229,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.073 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:40:49.167
    Jan 19 05:40:49.167: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename downward-api 01/19/23 05:40:49.168
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:40:49.183
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:40:49.186
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:89
    STEP: Creating a pod to test downward api env vars 01/19/23 05:40:49.188
    Jan 19 05:40:49.199: INFO: Waiting up to 5m0s for pod "downward-api-df1429c8-4d0a-46e2-ab52-0f27d4777219" in namespace "downward-api-9928" to be "Succeeded or Failed"
    Jan 19 05:40:49.204: INFO: Pod "downward-api-df1429c8-4d0a-46e2-ab52-0f27d4777219": Phase="Pending", Reason="", readiness=false. Elapsed: 4.58484ms
    Jan 19 05:40:51.208: INFO: Pod "downward-api-df1429c8-4d0a-46e2-ab52-0f27d4777219": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008855701s
    Jan 19 05:40:53.209: INFO: Pod "downward-api-df1429c8-4d0a-46e2-ab52-0f27d4777219": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009552562s
    Jan 19 05:40:55.208: INFO: Pod "downward-api-df1429c8-4d0a-46e2-ab52-0f27d4777219": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009175598s
    STEP: Saw pod success 01/19/23 05:40:55.208
    Jan 19 05:40:55.208: INFO: Pod "downward-api-df1429c8-4d0a-46e2-ab52-0f27d4777219" satisfied condition "Succeeded or Failed"
    Jan 19 05:40:55.211: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod downward-api-df1429c8-4d0a-46e2-ab52-0f27d4777219 container dapi-container: <nil>
    STEP: delete the pod 01/19/23 05:40:55.218
    Jan 19 05:40:55.229: INFO: Waiting for pod downward-api-df1429c8-4d0a-46e2-ab52-0f27d4777219 to disappear
    Jan 19 05:40:55.232: INFO: Pod downward-api-df1429c8-4d0a-46e2-ab52-0f27d4777219 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan 19 05:40:55.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9928" for this suite. 01/19/23 05:40:55.235
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:40:55.24
Jan 19 05:40:55.240: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename services 01/19/23 05:40:55.241
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:40:55.251
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:40:55.255
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
STEP: creating a service nodeport-service with the type=NodePort in namespace services-9098 01/19/23 05:40:55.258
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/19/23 05:40:55.269
STEP: creating service externalsvc in namespace services-9098 01/19/23 05:40:55.269
STEP: creating replication controller externalsvc in namespace services-9098 01/19/23 05:40:55.282
I0119 05:40:55.289008      18 runners.go:193] Created replication controller with name: externalsvc, namespace: services-9098, replica count: 2
I0119 05:40:58.340846      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 01/19/23 05:40:58.345
Jan 19 05:40:58.364: INFO: Creating new exec pod
Jan 19 05:40:58.371: INFO: Waiting up to 5m0s for pod "execpodg8sdx" in namespace "services-9098" to be "running"
Jan 19 05:40:58.378: INFO: Pod "execpodg8sdx": Phase="Pending", Reason="", readiness=false. Elapsed: 7.232119ms
Jan 19 05:41:00.381: INFO: Pod "execpodg8sdx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010388464s
Jan 19 05:41:02.383: INFO: Pod "execpodg8sdx": Phase="Running", Reason="", readiness=true. Elapsed: 4.012049573s
Jan 19 05:41:02.383: INFO: Pod "execpodg8sdx" satisfied condition "running"
Jan 19 05:41:02.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-9098 exec execpodg8sdx -- /bin/sh -x -c nslookup nodeport-service.services-9098.svc.cluster.local'
Jan 19 05:41:02.645: INFO: stderr: "+ nslookup nodeport-service.services-9098.svc.cluster.local\n"
Jan 19 05:41:02.645: INFO: stdout: "Server:\t\t10.254.0.10\nAddress:\t10.254.0.10#53\n\nnodeport-service.services-9098.svc.cluster.local\tcanonical name = externalsvc.services-9098.svc.cluster.local.\nName:\texternalsvc.services-9098.svc.cluster.local\nAddress: 10.254.90.32\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-9098, will wait for the garbage collector to delete the pods 01/19/23 05:41:02.645
Jan 19 05:41:02.709: INFO: Deleting ReplicationController externalsvc took: 9.777419ms
Jan 19 05:41:02.810: INFO: Terminating ReplicationController externalsvc pods took: 100.799675ms
Jan 19 05:41:05.025: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 19 05:41:05.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9098" for this suite. 01/19/23 05:41:05.038
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","completed":292,"skipped":5235,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.805 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:40:55.24
    Jan 19 05:40:55.240: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename services 01/19/23 05:40:55.241
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:40:55.251
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:40:55.255
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1523
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-9098 01/19/23 05:40:55.258
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/19/23 05:40:55.269
    STEP: creating service externalsvc in namespace services-9098 01/19/23 05:40:55.269
    STEP: creating replication controller externalsvc in namespace services-9098 01/19/23 05:40:55.282
    I0119 05:40:55.289008      18 runners.go:193] Created replication controller with name: externalsvc, namespace: services-9098, replica count: 2
    I0119 05:40:58.340846      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 01/19/23 05:40:58.345
    Jan 19 05:40:58.364: INFO: Creating new exec pod
    Jan 19 05:40:58.371: INFO: Waiting up to 5m0s for pod "execpodg8sdx" in namespace "services-9098" to be "running"
    Jan 19 05:40:58.378: INFO: Pod "execpodg8sdx": Phase="Pending", Reason="", readiness=false. Elapsed: 7.232119ms
    Jan 19 05:41:00.381: INFO: Pod "execpodg8sdx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010388464s
    Jan 19 05:41:02.383: INFO: Pod "execpodg8sdx": Phase="Running", Reason="", readiness=true. Elapsed: 4.012049573s
    Jan 19 05:41:02.383: INFO: Pod "execpodg8sdx" satisfied condition "running"
    Jan 19 05:41:02.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-9098 exec execpodg8sdx -- /bin/sh -x -c nslookup nodeport-service.services-9098.svc.cluster.local'
    Jan 19 05:41:02.645: INFO: stderr: "+ nslookup nodeport-service.services-9098.svc.cluster.local\n"
    Jan 19 05:41:02.645: INFO: stdout: "Server:\t\t10.254.0.10\nAddress:\t10.254.0.10#53\n\nnodeport-service.services-9098.svc.cluster.local\tcanonical name = externalsvc.services-9098.svc.cluster.local.\nName:\texternalsvc.services-9098.svc.cluster.local\nAddress: 10.254.90.32\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-9098, will wait for the garbage collector to delete the pods 01/19/23 05:41:02.645
    Jan 19 05:41:02.709: INFO: Deleting ReplicationController externalsvc took: 9.777419ms
    Jan 19 05:41:02.810: INFO: Terminating ReplicationController externalsvc pods took: 100.799675ms
    Jan 19 05:41:05.025: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 19 05:41:05.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9098" for this suite. 01/19/23 05:41:05.038
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:41:05.046
Jan 19 05:41:05.046: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename downward-api 01/19/23 05:41:05.047
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:41:05.058
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:41:05.062
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
STEP: Creating the pod 01/19/23 05:41:05.064
Jan 19 05:41:05.074: INFO: Waiting up to 5m0s for pod "annotationupdate75396b68-599b-4787-8b0e-65360458288a" in namespace "downward-api-802" to be "running and ready"
Jan 19 05:41:05.080: INFO: Pod "annotationupdate75396b68-599b-4787-8b0e-65360458288a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.631292ms
Jan 19 05:41:05.080: INFO: The phase of Pod annotationupdate75396b68-599b-4787-8b0e-65360458288a is Pending, waiting for it to be Running (with Ready = true)
Jan 19 05:41:07.084: INFO: Pod "annotationupdate75396b68-599b-4787-8b0e-65360458288a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009978271s
Jan 19 05:41:07.084: INFO: The phase of Pod annotationupdate75396b68-599b-4787-8b0e-65360458288a is Pending, waiting for it to be Running (with Ready = true)
Jan 19 05:41:09.084: INFO: Pod "annotationupdate75396b68-599b-4787-8b0e-65360458288a": Phase="Running", Reason="", readiness=true. Elapsed: 4.009863158s
Jan 19 05:41:09.084: INFO: The phase of Pod annotationupdate75396b68-599b-4787-8b0e-65360458288a is Running (Ready = true)
Jan 19 05:41:09.084: INFO: Pod "annotationupdate75396b68-599b-4787-8b0e-65360458288a" satisfied condition "running and ready"
Jan 19 05:41:09.607: INFO: Successfully updated pod "annotationupdate75396b68-599b-4787-8b0e-65360458288a"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 19 05:41:11.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-802" for this suite. 01/19/23 05:41:11.625
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","completed":293,"skipped":5277,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.585 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:41:05.046
    Jan 19 05:41:05.046: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename downward-api 01/19/23 05:41:05.047
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:41:05.058
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:41:05.062
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:161
    STEP: Creating the pod 01/19/23 05:41:05.064
    Jan 19 05:41:05.074: INFO: Waiting up to 5m0s for pod "annotationupdate75396b68-599b-4787-8b0e-65360458288a" in namespace "downward-api-802" to be "running and ready"
    Jan 19 05:41:05.080: INFO: Pod "annotationupdate75396b68-599b-4787-8b0e-65360458288a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.631292ms
    Jan 19 05:41:05.080: INFO: The phase of Pod annotationupdate75396b68-599b-4787-8b0e-65360458288a is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 05:41:07.084: INFO: Pod "annotationupdate75396b68-599b-4787-8b0e-65360458288a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009978271s
    Jan 19 05:41:07.084: INFO: The phase of Pod annotationupdate75396b68-599b-4787-8b0e-65360458288a is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 05:41:09.084: INFO: Pod "annotationupdate75396b68-599b-4787-8b0e-65360458288a": Phase="Running", Reason="", readiness=true. Elapsed: 4.009863158s
    Jan 19 05:41:09.084: INFO: The phase of Pod annotationupdate75396b68-599b-4787-8b0e-65360458288a is Running (Ready = true)
    Jan 19 05:41:09.084: INFO: Pod "annotationupdate75396b68-599b-4787-8b0e-65360458288a" satisfied condition "running and ready"
    Jan 19 05:41:09.607: INFO: Successfully updated pod "annotationupdate75396b68-599b-4787-8b0e-65360458288a"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 19 05:41:11.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-802" for this suite. 01/19/23 05:41:11.625
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:41:11.636
Jan 19 05:41:11.636: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename resourcequota 01/19/23 05:41:11.636
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:41:11.652
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:41:11.655
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
STEP: Counting existing ResourceQuota 01/19/23 05:41:11.673
STEP: Creating a ResourceQuota 01/19/23 05:41:16.676
STEP: Ensuring resource quota status is calculated 01/19/23 05:41:16.682
STEP: Creating a ReplicationController 01/19/23 05:41:18.685
STEP: Ensuring resource quota status captures replication controller creation 01/19/23 05:41:18.695
STEP: Deleting a ReplicationController 01/19/23 05:41:20.699
STEP: Ensuring resource quota status released usage 01/19/23 05:41:20.707
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 19 05:41:22.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1910" for this suite. 01/19/23 05:41:22.715
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","completed":294,"skipped":5316,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.087 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:41:11.636
    Jan 19 05:41:11.636: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename resourcequota 01/19/23 05:41:11.636
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:41:11.652
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:41:11.655
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:382
    STEP: Counting existing ResourceQuota 01/19/23 05:41:11.673
    STEP: Creating a ResourceQuota 01/19/23 05:41:16.676
    STEP: Ensuring resource quota status is calculated 01/19/23 05:41:16.682
    STEP: Creating a ReplicationController 01/19/23 05:41:18.685
    STEP: Ensuring resource quota status captures replication controller creation 01/19/23 05:41:18.695
    STEP: Deleting a ReplicationController 01/19/23 05:41:20.699
    STEP: Ensuring resource quota status released usage 01/19/23 05:41:20.707
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 19 05:41:22.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-1910" for this suite. 01/19/23 05:41:22.715
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:41:22.723
Jan 19 05:41:22.723: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename webhook 01/19/23 05:41:22.724
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:41:22.737
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:41:22.739
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/19/23 05:41:22.766
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 05:41:23.128
STEP: Deploying the webhook pod 01/19/23 05:41:23.137
STEP: Wait for the deployment to be ready 01/19/23 05:41:23.148
Jan 19 05:41:23.159: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 19 05:41:25.173: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 5, 41, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 41, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 41, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 41, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/19/23 05:41:27.177
STEP: Verifying the service has paired with the endpoint 01/19/23 05:41:27.187
Jan 19 05:41:28.188: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
STEP: Creating a mutating webhook configuration 01/19/23 05:41:28.192
STEP: Updating a mutating webhook configuration's rules to not include the create operation 01/19/23 05:41:28.211
STEP: Creating a configMap that should not be mutated 01/19/23 05:41:28.218
STEP: Patching a mutating webhook configuration's rules to include the create operation 01/19/23 05:41:28.227
STEP: Creating a configMap that should be mutated 01/19/23 05:41:28.235
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 05:41:28.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4539" for this suite. 01/19/23 05:41:28.258
STEP: Destroying namespace "webhook-4539-markers" for this suite. 01/19/23 05:41:28.263
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","completed":295,"skipped":5323,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.604 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:41:22.723
    Jan 19 05:41:22.723: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename webhook 01/19/23 05:41:22.724
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:41:22.737
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:41:22.739
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/19/23 05:41:22.766
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 05:41:23.128
    STEP: Deploying the webhook pod 01/19/23 05:41:23.137
    STEP: Wait for the deployment to be ready 01/19/23 05:41:23.148
    Jan 19 05:41:23.159: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 19 05:41:25.173: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 5, 41, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 41, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 41, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 41, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/19/23 05:41:27.177
    STEP: Verifying the service has paired with the endpoint 01/19/23 05:41:27.187
    Jan 19 05:41:28.188: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:507
    STEP: Creating a mutating webhook configuration 01/19/23 05:41:28.192
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 01/19/23 05:41:28.211
    STEP: Creating a configMap that should not be mutated 01/19/23 05:41:28.218
    STEP: Patching a mutating webhook configuration's rules to include the create operation 01/19/23 05:41:28.227
    STEP: Creating a configMap that should be mutated 01/19/23 05:41:28.235
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 05:41:28.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4539" for this suite. 01/19/23 05:41:28.258
    STEP: Destroying namespace "webhook-4539-markers" for this suite. 01/19/23 05:41:28.263
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:41:28.327
Jan 19 05:41:28.328: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename kubectl 01/19/23 05:41:28.328
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:41:28.342
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:41:28.344
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
STEP: creating Agnhost RC 01/19/23 05:41:28.346
Jan 19 05:41:28.346: INFO: namespace kubectl-2416
Jan 19 05:41:28.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-2416 create -f -'
Jan 19 05:41:29.147: INFO: stderr: ""
Jan 19 05:41:29.147: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/19/23 05:41:29.147
Jan 19 05:41:30.151: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 19 05:41:30.151: INFO: Found 0 / 1
Jan 19 05:41:31.151: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 19 05:41:31.151: INFO: Found 1 / 1
Jan 19 05:41:31.151: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 19 05:41:31.154: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 19 05:41:31.154: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 19 05:41:31.154: INFO: wait on agnhost-primary startup in kubectl-2416 
Jan 19 05:41:31.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-2416 logs agnhost-primary-cr6dg agnhost-primary'
Jan 19 05:41:31.262: INFO: stderr: ""
Jan 19 05:41:31.262: INFO: stdout: "Paused\n"
STEP: exposing RC 01/19/23 05:41:31.262
Jan 19 05:41:31.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-2416 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jan 19 05:41:31.352: INFO: stderr: ""
Jan 19 05:41:31.352: INFO: stdout: "service/rm2 exposed\n"
Jan 19 05:41:31.360: INFO: Service rm2 in namespace kubectl-2416 found.
STEP: exposing service 01/19/23 05:41:33.365
Jan 19 05:41:33.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-2416 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jan 19 05:41:33.471: INFO: stderr: ""
Jan 19 05:41:33.471: INFO: stdout: "service/rm3 exposed\n"
Jan 19 05:41:33.479: INFO: Service rm3 in namespace kubectl-2416 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 19 05:41:35.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2416" for this suite. 01/19/23 05:41:35.489
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","completed":296,"skipped":5331,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.166 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1407
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:41:28.327
    Jan 19 05:41:28.328: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename kubectl 01/19/23 05:41:28.328
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:41:28.342
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:41:28.344
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1413
    STEP: creating Agnhost RC 01/19/23 05:41:28.346
    Jan 19 05:41:28.346: INFO: namespace kubectl-2416
    Jan 19 05:41:28.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-2416 create -f -'
    Jan 19 05:41:29.147: INFO: stderr: ""
    Jan 19 05:41:29.147: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/19/23 05:41:29.147
    Jan 19 05:41:30.151: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 19 05:41:30.151: INFO: Found 0 / 1
    Jan 19 05:41:31.151: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 19 05:41:31.151: INFO: Found 1 / 1
    Jan 19 05:41:31.151: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jan 19 05:41:31.154: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 19 05:41:31.154: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 19 05:41:31.154: INFO: wait on agnhost-primary startup in kubectl-2416 
    Jan 19 05:41:31.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-2416 logs agnhost-primary-cr6dg agnhost-primary'
    Jan 19 05:41:31.262: INFO: stderr: ""
    Jan 19 05:41:31.262: INFO: stdout: "Paused\n"
    STEP: exposing RC 01/19/23 05:41:31.262
    Jan 19 05:41:31.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-2416 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Jan 19 05:41:31.352: INFO: stderr: ""
    Jan 19 05:41:31.352: INFO: stdout: "service/rm2 exposed\n"
    Jan 19 05:41:31.360: INFO: Service rm2 in namespace kubectl-2416 found.
    STEP: exposing service 01/19/23 05:41:33.365
    Jan 19 05:41:33.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-2416 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Jan 19 05:41:33.471: INFO: stderr: ""
    Jan 19 05:41:33.471: INFO: stdout: "service/rm3 exposed\n"
    Jan 19 05:41:33.479: INFO: Service rm3 in namespace kubectl-2416 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 19 05:41:35.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2416" for this suite. 01/19/23 05:41:35.489
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:41:35.493
Jan 19 05:41:35.494: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename dns 01/19/23 05:41:35.494
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:41:35.507
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:41:35.509
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 01/19/23 05:41:35.512
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5176.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-5176.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 01/19/23 05:41:35.517
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5176.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-5176.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 01/19/23 05:41:35.517
STEP: creating a pod to probe DNS 01/19/23 05:41:35.517
STEP: submitting the pod to kubernetes 01/19/23 05:41:35.517
Jan 19 05:41:35.533: INFO: Waiting up to 15m0s for pod "dns-test-bf86679f-06c1-4079-8d09-78ccb2620f36" in namespace "dns-5176" to be "running"
Jan 19 05:41:35.537: INFO: Pod "dns-test-bf86679f-06c1-4079-8d09-78ccb2620f36": Phase="Pending", Reason="", readiness=false. Elapsed: 4.442775ms
Jan 19 05:41:37.541: INFO: Pod "dns-test-bf86679f-06c1-4079-8d09-78ccb2620f36": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008051514s
Jan 19 05:41:39.542: INFO: Pod "dns-test-bf86679f-06c1-4079-8d09-78ccb2620f36": Phase="Running", Reason="", readiness=true. Elapsed: 4.009349515s
Jan 19 05:41:39.542: INFO: Pod "dns-test-bf86679f-06c1-4079-8d09-78ccb2620f36" satisfied condition "running"
STEP: retrieving the pod 01/19/23 05:41:39.542
STEP: looking for the results for each expected name from probers 01/19/23 05:41:39.546
Jan 19 05:41:39.560: INFO: DNS probes using dns-5176/dns-test-bf86679f-06c1-4079-8d09-78ccb2620f36 succeeded

STEP: deleting the pod 01/19/23 05:41:39.56
STEP: deleting the test headless service 01/19/23 05:41:39.572
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 19 05:41:39.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5176" for this suite. 01/19/23 05:41:39.591
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","completed":297,"skipped":5334,"failed":0}
------------------------------
â€¢ [4.103 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:41:35.493
    Jan 19 05:41:35.494: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename dns 01/19/23 05:41:35.494
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:41:35.507
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:41:35.509
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 01/19/23 05:41:35.512
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5176.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-5176.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     01/19/23 05:41:35.517
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5176.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-5176.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     01/19/23 05:41:35.517
    STEP: creating a pod to probe DNS 01/19/23 05:41:35.517
    STEP: submitting the pod to kubernetes 01/19/23 05:41:35.517
    Jan 19 05:41:35.533: INFO: Waiting up to 15m0s for pod "dns-test-bf86679f-06c1-4079-8d09-78ccb2620f36" in namespace "dns-5176" to be "running"
    Jan 19 05:41:35.537: INFO: Pod "dns-test-bf86679f-06c1-4079-8d09-78ccb2620f36": Phase="Pending", Reason="", readiness=false. Elapsed: 4.442775ms
    Jan 19 05:41:37.541: INFO: Pod "dns-test-bf86679f-06c1-4079-8d09-78ccb2620f36": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008051514s
    Jan 19 05:41:39.542: INFO: Pod "dns-test-bf86679f-06c1-4079-8d09-78ccb2620f36": Phase="Running", Reason="", readiness=true. Elapsed: 4.009349515s
    Jan 19 05:41:39.542: INFO: Pod "dns-test-bf86679f-06c1-4079-8d09-78ccb2620f36" satisfied condition "running"
    STEP: retrieving the pod 01/19/23 05:41:39.542
    STEP: looking for the results for each expected name from probers 01/19/23 05:41:39.546
    Jan 19 05:41:39.560: INFO: DNS probes using dns-5176/dns-test-bf86679f-06c1-4079-8d09-78ccb2620f36 succeeded

    STEP: deleting the pod 01/19/23 05:41:39.56
    STEP: deleting the test headless service 01/19/23 05:41:39.572
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 19 05:41:39.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-5176" for this suite. 01/19/23 05:41:39.591
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:41:39.597
Jan 19 05:41:39.597: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename init-container 01/19/23 05:41:39.597
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:41:39.611
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:41:39.614
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
STEP: creating the pod 01/19/23 05:41:39.616
Jan 19 05:41:39.616: INFO: PodSpec: initContainers in spec.initContainers
Jan 19 05:42:28.111: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-acc4ab1c-0f24-40a6-931e-6a32eb6e7cc6", GenerateName:"", Namespace:"init-container-6370", SelfLink:"", UID:"43cc7629-7aa8-428a-a6a4-08c1c5c86ada", ResourceVersion:"33573", Generation:0, CreationTimestamp:time.Date(2023, time.January, 19, 5, 41, 39, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"616633941"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 19, 5, 41, 39, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000e57218), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 19, 5, 42, 28, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000e57248), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-ck42p", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc002a8ae60), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-ck42p", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-ck42p", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-ck42p", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0051a6770), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ckcp-nks-default-worker-node-1", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00047a1c0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0051a6800)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0051a6820)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0051a6828), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0051a682c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc001098ef0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 19, 5, 41, 39, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 19, 5, 41, 39, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 19, 5, 41, 39, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 19, 5, 41, 39, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.0.78", PodIP:"10.100.70.232", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.100.70.232"}}, StartTime:time.Date(2023, time.January, 19, 5, 41, 39, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00047a2a0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00047a310)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://e93a30ce35f4342d3777236cf56ceef1196442e4aae93ba6f07f682de58736e6", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002a8af00), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002a8aec0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc0051a68af)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 19 05:42:28.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6370" for this suite. 01/19/23 05:42:28.119
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","completed":298,"skipped":5335,"failed":0}
------------------------------
â€¢ [SLOW TEST] [48.528 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:41:39.597
    Jan 19 05:41:39.597: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename init-container 01/19/23 05:41:39.597
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:41:39.611
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:41:39.614
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:333
    STEP: creating the pod 01/19/23 05:41:39.616
    Jan 19 05:41:39.616: INFO: PodSpec: initContainers in spec.initContainers
    Jan 19 05:42:28.111: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-acc4ab1c-0f24-40a6-931e-6a32eb6e7cc6", GenerateName:"", Namespace:"init-container-6370", SelfLink:"", UID:"43cc7629-7aa8-428a-a6a4-08c1c5c86ada", ResourceVersion:"33573", Generation:0, CreationTimestamp:time.Date(2023, time.January, 19, 5, 41, 39, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"616633941"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 19, 5, 41, 39, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000e57218), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 19, 5, 42, 28, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000e57248), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-ck42p", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc002a8ae60), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-ck42p", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-ck42p", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-ck42p", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0051a6770), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ckcp-nks-default-worker-node-1", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00047a1c0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0051a6800)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0051a6820)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0051a6828), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0051a682c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc001098ef0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 19, 5, 41, 39, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 19, 5, 41, 39, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 19, 5, 41, 39, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 19, 5, 41, 39, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.0.78", PodIP:"10.100.70.232", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.100.70.232"}}, StartTime:time.Date(2023, time.January, 19, 5, 41, 39, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00047a2a0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00047a310)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://e93a30ce35f4342d3777236cf56ceef1196442e4aae93ba6f07f682de58736e6", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002a8af00), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002a8aec0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc0051a68af)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 19 05:42:28.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-6370" for this suite. 01/19/23 05:42:28.119
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:42:28.127
Jan 19 05:42:28.127: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename secrets 01/19/23 05:42:28.127
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:42:28.15
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:42:28.153
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
STEP: Creating secret with name s-test-opt-del-c2a134ea-db6e-445c-8316-41ab3b8ea4be 01/19/23 05:42:28.158
STEP: Creating secret with name s-test-opt-upd-831303d6-a5f9-4b18-a5c2-d2fd154a41a6 01/19/23 05:42:28.162
STEP: Creating the pod 01/19/23 05:42:28.166
Jan 19 05:42:28.173: INFO: Waiting up to 5m0s for pod "pod-secrets-33c4d97f-be92-4f46-ba6b-d14dc559bc35" in namespace "secrets-4897" to be "running and ready"
Jan 19 05:42:28.175: INFO: Pod "pod-secrets-33c4d97f-be92-4f46-ba6b-d14dc559bc35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.224686ms
Jan 19 05:42:28.175: INFO: The phase of Pod pod-secrets-33c4d97f-be92-4f46-ba6b-d14dc559bc35 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 05:42:30.179: INFO: Pod "pod-secrets-33c4d97f-be92-4f46-ba6b-d14dc559bc35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006107276s
Jan 19 05:42:30.179: INFO: The phase of Pod pod-secrets-33c4d97f-be92-4f46-ba6b-d14dc559bc35 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 05:42:32.179: INFO: Pod "pod-secrets-33c4d97f-be92-4f46-ba6b-d14dc559bc35": Phase="Running", Reason="", readiness=true. Elapsed: 4.006271229s
Jan 19 05:42:32.179: INFO: The phase of Pod pod-secrets-33c4d97f-be92-4f46-ba6b-d14dc559bc35 is Running (Ready = true)
Jan 19 05:42:32.179: INFO: Pod "pod-secrets-33c4d97f-be92-4f46-ba6b-d14dc559bc35" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-c2a134ea-db6e-445c-8316-41ab3b8ea4be 01/19/23 05:42:32.201
STEP: Updating secret s-test-opt-upd-831303d6-a5f9-4b18-a5c2-d2fd154a41a6 01/19/23 05:42:32.207
STEP: Creating secret with name s-test-opt-create-da27a42c-4f87-4c5d-879c-6f63a1c236f2 01/19/23 05:42:32.211
STEP: waiting to observe update in volume 01/19/23 05:42:32.215
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 19 05:42:34.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4897" for this suite. 01/19/23 05:42:34.242
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":299,"skipped":5410,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.121 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:42:28.127
    Jan 19 05:42:28.127: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename secrets 01/19/23 05:42:28.127
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:42:28.15
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:42:28.153
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:204
    STEP: Creating secret with name s-test-opt-del-c2a134ea-db6e-445c-8316-41ab3b8ea4be 01/19/23 05:42:28.158
    STEP: Creating secret with name s-test-opt-upd-831303d6-a5f9-4b18-a5c2-d2fd154a41a6 01/19/23 05:42:28.162
    STEP: Creating the pod 01/19/23 05:42:28.166
    Jan 19 05:42:28.173: INFO: Waiting up to 5m0s for pod "pod-secrets-33c4d97f-be92-4f46-ba6b-d14dc559bc35" in namespace "secrets-4897" to be "running and ready"
    Jan 19 05:42:28.175: INFO: Pod "pod-secrets-33c4d97f-be92-4f46-ba6b-d14dc559bc35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.224686ms
    Jan 19 05:42:28.175: INFO: The phase of Pod pod-secrets-33c4d97f-be92-4f46-ba6b-d14dc559bc35 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 05:42:30.179: INFO: Pod "pod-secrets-33c4d97f-be92-4f46-ba6b-d14dc559bc35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006107276s
    Jan 19 05:42:30.179: INFO: The phase of Pod pod-secrets-33c4d97f-be92-4f46-ba6b-d14dc559bc35 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 05:42:32.179: INFO: Pod "pod-secrets-33c4d97f-be92-4f46-ba6b-d14dc559bc35": Phase="Running", Reason="", readiness=true. Elapsed: 4.006271229s
    Jan 19 05:42:32.179: INFO: The phase of Pod pod-secrets-33c4d97f-be92-4f46-ba6b-d14dc559bc35 is Running (Ready = true)
    Jan 19 05:42:32.179: INFO: Pod "pod-secrets-33c4d97f-be92-4f46-ba6b-d14dc559bc35" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-c2a134ea-db6e-445c-8316-41ab3b8ea4be 01/19/23 05:42:32.201
    STEP: Updating secret s-test-opt-upd-831303d6-a5f9-4b18-a5c2-d2fd154a41a6 01/19/23 05:42:32.207
    STEP: Creating secret with name s-test-opt-create-da27a42c-4f87-4c5d-879c-6f63a1c236f2 01/19/23 05:42:32.211
    STEP: waiting to observe update in volume 01/19/23 05:42:32.215
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 19 05:42:34.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4897" for this suite. 01/19/23 05:42:34.242
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:42:34.248
Jan 19 05:42:34.248: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename kubelet-test 01/19/23 05:42:34.249
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:42:34.261
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:42:34.263
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan 19 05:42:38.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9808" for this suite. 01/19/23 05:42:38.282
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","completed":300,"skipped":5411,"failed":0}
------------------------------
â€¢ [4.041 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:42:34.248
    Jan 19 05:42:34.248: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename kubelet-test 01/19/23 05:42:34.249
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:42:34.261
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:42:34.263
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan 19 05:42:38.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-9808" for this suite. 01/19/23 05:42:38.282
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:42:38.289
Jan 19 05:42:38.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename secrets 01/19/23 05:42:38.29
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:42:38.354
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:42:38.357
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
STEP: Creating secret with name secret-test-map-bebe2dc7-54d9-4f95-9a1d-bf1333122c12 01/19/23 05:42:38.36
STEP: Creating a pod to test consume secrets 01/19/23 05:42:38.366
Jan 19 05:42:38.380: INFO: Waiting up to 5m0s for pod "pod-secrets-9928b8d1-08ba-427d-bb60-c954bc1eaa67" in namespace "secrets-1295" to be "Succeeded or Failed"
Jan 19 05:42:38.389: INFO: Pod "pod-secrets-9928b8d1-08ba-427d-bb60-c954bc1eaa67": Phase="Pending", Reason="", readiness=false. Elapsed: 9.582688ms
Jan 19 05:42:40.393: INFO: Pod "pod-secrets-9928b8d1-08ba-427d-bb60-c954bc1eaa67": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013806081s
Jan 19 05:42:42.393: INFO: Pod "pod-secrets-9928b8d1-08ba-427d-bb60-c954bc1eaa67": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013525983s
Jan 19 05:42:44.393: INFO: Pod "pod-secrets-9928b8d1-08ba-427d-bb60-c954bc1eaa67": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01391064s
STEP: Saw pod success 01/19/23 05:42:44.393
Jan 19 05:42:44.394: INFO: Pod "pod-secrets-9928b8d1-08ba-427d-bb60-c954bc1eaa67" satisfied condition "Succeeded or Failed"
Jan 19 05:42:44.396: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-secrets-9928b8d1-08ba-427d-bb60-c954bc1eaa67 container secret-volume-test: <nil>
STEP: delete the pod 01/19/23 05:42:44.403
Jan 19 05:42:44.414: INFO: Waiting for pod pod-secrets-9928b8d1-08ba-427d-bb60-c954bc1eaa67 to disappear
Jan 19 05:42:44.417: INFO: Pod pod-secrets-9928b8d1-08ba-427d-bb60-c954bc1eaa67 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 19 05:42:44.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1295" for this suite. 01/19/23 05:42:44.42
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":301,"skipped":5431,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.136 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:42:38.289
    Jan 19 05:42:38.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename secrets 01/19/23 05:42:38.29
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:42:38.354
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:42:38.357
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:88
    STEP: Creating secret with name secret-test-map-bebe2dc7-54d9-4f95-9a1d-bf1333122c12 01/19/23 05:42:38.36
    STEP: Creating a pod to test consume secrets 01/19/23 05:42:38.366
    Jan 19 05:42:38.380: INFO: Waiting up to 5m0s for pod "pod-secrets-9928b8d1-08ba-427d-bb60-c954bc1eaa67" in namespace "secrets-1295" to be "Succeeded or Failed"
    Jan 19 05:42:38.389: INFO: Pod "pod-secrets-9928b8d1-08ba-427d-bb60-c954bc1eaa67": Phase="Pending", Reason="", readiness=false. Elapsed: 9.582688ms
    Jan 19 05:42:40.393: INFO: Pod "pod-secrets-9928b8d1-08ba-427d-bb60-c954bc1eaa67": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013806081s
    Jan 19 05:42:42.393: INFO: Pod "pod-secrets-9928b8d1-08ba-427d-bb60-c954bc1eaa67": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013525983s
    Jan 19 05:42:44.393: INFO: Pod "pod-secrets-9928b8d1-08ba-427d-bb60-c954bc1eaa67": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01391064s
    STEP: Saw pod success 01/19/23 05:42:44.393
    Jan 19 05:42:44.394: INFO: Pod "pod-secrets-9928b8d1-08ba-427d-bb60-c954bc1eaa67" satisfied condition "Succeeded or Failed"
    Jan 19 05:42:44.396: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-secrets-9928b8d1-08ba-427d-bb60-c954bc1eaa67 container secret-volume-test: <nil>
    STEP: delete the pod 01/19/23 05:42:44.403
    Jan 19 05:42:44.414: INFO: Waiting for pod pod-secrets-9928b8d1-08ba-427d-bb60-c954bc1eaa67 to disappear
    Jan 19 05:42:44.417: INFO: Pod pod-secrets-9928b8d1-08ba-427d-bb60-c954bc1eaa67 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 19 05:42:44.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-1295" for this suite. 01/19/23 05:42:44.42
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:42:44.426
Jan 19 05:42:44.426: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename cronjob 01/19/23 05:42:44.427
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:42:44.439
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:42:44.443
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 01/19/23 05:42:44.445
STEP: Ensuring a job is scheduled 01/19/23 05:42:44.452
STEP: Ensuring exactly one is scheduled 01/19/23 05:43:00.459
STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/19/23 05:43:00.462
STEP: Ensuring no more jobs are scheduled 01/19/23 05:43:00.467
STEP: Removing cronjob 01/19/23 05:48:00.473
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan 19 05:48:00.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-3374" for this suite. 01/19/23 05:48:00.501
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","completed":302,"skipped":5459,"failed":0}
------------------------------
â€¢ [SLOW TEST] [316.083 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:42:44.426
    Jan 19 05:42:44.426: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename cronjob 01/19/23 05:42:44.427
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:42:44.439
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:42:44.443
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 01/19/23 05:42:44.445
    STEP: Ensuring a job is scheduled 01/19/23 05:42:44.452
    STEP: Ensuring exactly one is scheduled 01/19/23 05:43:00.459
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/19/23 05:43:00.462
    STEP: Ensuring no more jobs are scheduled 01/19/23 05:43:00.467
    STEP: Removing cronjob 01/19/23 05:48:00.473
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan 19 05:48:00.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-3374" for this suite. 01/19/23 05:48:00.501
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:48:00.511
Jan 19 05:48:00.511: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename downward-api 01/19/23 05:48:00.512
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:48:00.535
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:48:00.539
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
STEP: Creating a pod to test downward API volume plugin 01/19/23 05:48:00.542
Jan 19 05:48:00.551: INFO: Waiting up to 5m0s for pod "downwardapi-volume-acaec850-381e-426d-bb3c-87ed92471ccd" in namespace "downward-api-38" to be "Succeeded or Failed"
Jan 19 05:48:00.556: INFO: Pod "downwardapi-volume-acaec850-381e-426d-bb3c-87ed92471ccd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.517555ms
Jan 19 05:48:02.561: INFO: Pod "downwardapi-volume-acaec850-381e-426d-bb3c-87ed92471ccd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010709863s
Jan 19 05:48:04.561: INFO: Pod "downwardapi-volume-acaec850-381e-426d-bb3c-87ed92471ccd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010427711s
Jan 19 05:48:06.560: INFO: Pod "downwardapi-volume-acaec850-381e-426d-bb3c-87ed92471ccd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009266309s
STEP: Saw pod success 01/19/23 05:48:06.56
Jan 19 05:48:06.560: INFO: Pod "downwardapi-volume-acaec850-381e-426d-bb3c-87ed92471ccd" satisfied condition "Succeeded or Failed"
Jan 19 05:48:06.563: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod downwardapi-volume-acaec850-381e-426d-bb3c-87ed92471ccd container client-container: <nil>
STEP: delete the pod 01/19/23 05:48:06.596
Jan 19 05:48:06.607: INFO: Waiting for pod downwardapi-volume-acaec850-381e-426d-bb3c-87ed92471ccd to disappear
Jan 19 05:48:06.610: INFO: Pod downwardapi-volume-acaec850-381e-426d-bb3c-87ed92471ccd no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 19 05:48:06.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-38" for this suite. 01/19/23 05:48:06.613
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":303,"skipped":5504,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.108 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:48:00.511
    Jan 19 05:48:00.511: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename downward-api 01/19/23 05:48:00.512
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:48:00.535
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:48:00.539
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:67
    STEP: Creating a pod to test downward API volume plugin 01/19/23 05:48:00.542
    Jan 19 05:48:00.551: INFO: Waiting up to 5m0s for pod "downwardapi-volume-acaec850-381e-426d-bb3c-87ed92471ccd" in namespace "downward-api-38" to be "Succeeded or Failed"
    Jan 19 05:48:00.556: INFO: Pod "downwardapi-volume-acaec850-381e-426d-bb3c-87ed92471ccd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.517555ms
    Jan 19 05:48:02.561: INFO: Pod "downwardapi-volume-acaec850-381e-426d-bb3c-87ed92471ccd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010709863s
    Jan 19 05:48:04.561: INFO: Pod "downwardapi-volume-acaec850-381e-426d-bb3c-87ed92471ccd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010427711s
    Jan 19 05:48:06.560: INFO: Pod "downwardapi-volume-acaec850-381e-426d-bb3c-87ed92471ccd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009266309s
    STEP: Saw pod success 01/19/23 05:48:06.56
    Jan 19 05:48:06.560: INFO: Pod "downwardapi-volume-acaec850-381e-426d-bb3c-87ed92471ccd" satisfied condition "Succeeded or Failed"
    Jan 19 05:48:06.563: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod downwardapi-volume-acaec850-381e-426d-bb3c-87ed92471ccd container client-container: <nil>
    STEP: delete the pod 01/19/23 05:48:06.596
    Jan 19 05:48:06.607: INFO: Waiting for pod downwardapi-volume-acaec850-381e-426d-bb3c-87ed92471ccd to disappear
    Jan 19 05:48:06.610: INFO: Pod downwardapi-volume-acaec850-381e-426d-bb3c-87ed92471ccd no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 19 05:48:06.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-38" for this suite. 01/19/23 05:48:06.613
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:48:06.619
Jan 19 05:48:06.620: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename resourcequota 01/19/23 05:48:06.62
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:48:06.633
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:48:06.636
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
STEP: Counting existing ResourceQuota 01/19/23 05:48:06.638
STEP: Creating a ResourceQuota 01/19/23 05:48:11.645
STEP: Ensuring resource quota status is calculated 01/19/23 05:48:11.653
STEP: Creating a Pod that fits quota 01/19/23 05:48:13.658
STEP: Ensuring ResourceQuota status captures the pod usage 01/19/23 05:48:13.678
STEP: Not allowing a pod to be created that exceeds remaining quota 01/19/23 05:48:15.684
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 01/19/23 05:48:15.687
STEP: Ensuring a pod cannot update its resource requirements 01/19/23 05:48:15.689
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 01/19/23 05:48:15.693
STEP: Deleting the pod 01/19/23 05:48:17.697
STEP: Ensuring resource quota status released the pod usage 01/19/23 05:48:17.72
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 19 05:48:19.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-566" for this suite. 01/19/23 05:48:19.729
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","completed":304,"skipped":5522,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.115 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:48:06.619
    Jan 19 05:48:06.620: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename resourcequota 01/19/23 05:48:06.62
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:48:06.633
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:48:06.636
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:220
    STEP: Counting existing ResourceQuota 01/19/23 05:48:06.638
    STEP: Creating a ResourceQuota 01/19/23 05:48:11.645
    STEP: Ensuring resource quota status is calculated 01/19/23 05:48:11.653
    STEP: Creating a Pod that fits quota 01/19/23 05:48:13.658
    STEP: Ensuring ResourceQuota status captures the pod usage 01/19/23 05:48:13.678
    STEP: Not allowing a pod to be created that exceeds remaining quota 01/19/23 05:48:15.684
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 01/19/23 05:48:15.687
    STEP: Ensuring a pod cannot update its resource requirements 01/19/23 05:48:15.689
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 01/19/23 05:48:15.693
    STEP: Deleting the pod 01/19/23 05:48:17.697
    STEP: Ensuring resource quota status released the pod usage 01/19/23 05:48:17.72
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 19 05:48:19.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-566" for this suite. 01/19/23 05:48:19.729
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:48:19.735
Jan 19 05:48:19.735: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename runtimeclass 01/19/23 05:48:19.736
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:48:19.748
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:48:19.75
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Jan 19 05:48:19.764: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-7965 to be scheduled
Jan 19 05:48:19.772: INFO: 1 pods are not scheduled: [runtimeclass-7965/test-runtimeclass-runtimeclass-7965-preconfigured-handler-gkgw7(1f6cbaf9-7493-49a5-9460-35e05e56473d)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan 19 05:48:21.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-7965" for this suite. 01/19/23 05:48:21.79
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","completed":305,"skipped":5527,"failed":0}
------------------------------
â€¢ [2.061 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:48:19.735
    Jan 19 05:48:19.735: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename runtimeclass 01/19/23 05:48:19.736
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:48:19.748
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:48:19.75
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Jan 19 05:48:19.764: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-7965 to be scheduled
    Jan 19 05:48:19.772: INFO: 1 pods are not scheduled: [runtimeclass-7965/test-runtimeclass-runtimeclass-7965-preconfigured-handler-gkgw7(1f6cbaf9-7493-49a5-9460-35e05e56473d)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan 19 05:48:21.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-7965" for this suite. 01/19/23 05:48:21.79
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:48:21.797
Jan 19 05:48:21.797: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename services 01/19/23 05:48:21.798
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:48:21.812
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:48:21.816
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
STEP: creating service endpoint-test2 in namespace services-237 01/19/23 05:48:21.818
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-237 to expose endpoints map[] 01/19/23 05:48:21.828
Jan 19 05:48:21.831: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Jan 19 05:48:22.841: INFO: successfully validated that service endpoint-test2 in namespace services-237 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-237 01/19/23 05:48:22.841
Jan 19 05:48:22.850: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-237" to be "running and ready"
Jan 19 05:48:22.857: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.273004ms
Jan 19 05:48:22.857: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 05:48:24.860: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009154s
Jan 19 05:48:24.860: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 05:48:26.860: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.00972934s
Jan 19 05:48:26.860: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 19 05:48:26.860: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-237 to expose endpoints map[pod1:[80]] 01/19/23 05:48:26.862
Jan 19 05:48:26.873: INFO: successfully validated that service endpoint-test2 in namespace services-237 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 01/19/23 05:48:26.873
Jan 19 05:48:26.873: INFO: Creating new exec pod
Jan 19 05:48:26.880: INFO: Waiting up to 5m0s for pod "execpod4zzpf" in namespace "services-237" to be "running"
Jan 19 05:48:26.884: INFO: Pod "execpod4zzpf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.404068ms
Jan 19 05:48:28.888: INFO: Pod "execpod4zzpf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008269895s
Jan 19 05:48:30.889: INFO: Pod "execpod4zzpf": Phase="Running", Reason="", readiness=true. Elapsed: 4.00910409s
Jan 19 05:48:30.889: INFO: Pod "execpod4zzpf" satisfied condition "running"
Jan 19 05:48:31.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-237 exec execpod4zzpf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan 19 05:48:32.080: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 19 05:48:32.080: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 05:48:32.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-237 exec execpod4zzpf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.239.36 80'
Jan 19 05:48:32.235: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.239.36 80\nConnection to 10.254.239.36 80 port [tcp/http] succeeded!\n"
Jan 19 05:48:32.235: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-237 01/19/23 05:48:32.235
Jan 19 05:48:32.243: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-237" to be "running and ready"
Jan 19 05:48:32.249: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.448574ms
Jan 19 05:48:32.249: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 05:48:34.252: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009851788s
Jan 19 05:48:34.253: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 05:48:36.253: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.009898414s
Jan 19 05:48:36.253: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 19 05:48:36.253: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-237 to expose endpoints map[pod1:[80] pod2:[80]] 01/19/23 05:48:36.255
Jan 19 05:48:36.265: INFO: successfully validated that service endpoint-test2 in namespace services-237 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 01/19/23 05:48:36.265
Jan 19 05:48:37.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-237 exec execpod4zzpf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan 19 05:48:37.437: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 19 05:48:37.437: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 05:48:37.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-237 exec execpod4zzpf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.239.36 80'
Jan 19 05:48:37.605: INFO: stderr: "+ nc -v -t -w 2 10.254.239.36 80\n+ echo hostName\nConnection to 10.254.239.36 80 port [tcp/http] succeeded!\n"
Jan 19 05:48:37.605: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-237 01/19/23 05:48:37.605
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-237 to expose endpoints map[pod2:[80]] 01/19/23 05:48:37.62
Jan 19 05:48:38.642: INFO: successfully validated that service endpoint-test2 in namespace services-237 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 01/19/23 05:48:38.642
Jan 19 05:48:39.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-237 exec execpod4zzpf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan 19 05:48:39.806: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 19 05:48:39.806: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 05:48:39.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-237 exec execpod4zzpf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.239.36 80'
Jan 19 05:48:39.959: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.239.36 80\nConnection to 10.254.239.36 80 port [tcp/http] succeeded!\n"
Jan 19 05:48:39.959: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-237 01/19/23 05:48:39.959
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-237 to expose endpoints map[] 01/19/23 05:48:39.972
Jan 19 05:48:39.982: INFO: successfully validated that service endpoint-test2 in namespace services-237 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 19 05:48:40.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-237" for this suite. 01/19/23 05:48:40.014
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","completed":306,"skipped":5556,"failed":0}
------------------------------
â€¢ [SLOW TEST] [18.225 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:48:21.797
    Jan 19 05:48:21.797: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename services 01/19/23 05:48:21.798
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:48:21.812
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:48:21.816
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:791
    STEP: creating service endpoint-test2 in namespace services-237 01/19/23 05:48:21.818
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-237 to expose endpoints map[] 01/19/23 05:48:21.828
    Jan 19 05:48:21.831: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    Jan 19 05:48:22.841: INFO: successfully validated that service endpoint-test2 in namespace services-237 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-237 01/19/23 05:48:22.841
    Jan 19 05:48:22.850: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-237" to be "running and ready"
    Jan 19 05:48:22.857: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.273004ms
    Jan 19 05:48:22.857: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 05:48:24.860: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009154s
    Jan 19 05:48:24.860: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 05:48:26.860: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.00972934s
    Jan 19 05:48:26.860: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 19 05:48:26.860: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-237 to expose endpoints map[pod1:[80]] 01/19/23 05:48:26.862
    Jan 19 05:48:26.873: INFO: successfully validated that service endpoint-test2 in namespace services-237 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 01/19/23 05:48:26.873
    Jan 19 05:48:26.873: INFO: Creating new exec pod
    Jan 19 05:48:26.880: INFO: Waiting up to 5m0s for pod "execpod4zzpf" in namespace "services-237" to be "running"
    Jan 19 05:48:26.884: INFO: Pod "execpod4zzpf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.404068ms
    Jan 19 05:48:28.888: INFO: Pod "execpod4zzpf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008269895s
    Jan 19 05:48:30.889: INFO: Pod "execpod4zzpf": Phase="Running", Reason="", readiness=true. Elapsed: 4.00910409s
    Jan 19 05:48:30.889: INFO: Pod "execpod4zzpf" satisfied condition "running"
    Jan 19 05:48:31.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-237 exec execpod4zzpf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Jan 19 05:48:32.080: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 19 05:48:32.080: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 19 05:48:32.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-237 exec execpod4zzpf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.239.36 80'
    Jan 19 05:48:32.235: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.239.36 80\nConnection to 10.254.239.36 80 port [tcp/http] succeeded!\n"
    Jan 19 05:48:32.235: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Creating pod pod2 in namespace services-237 01/19/23 05:48:32.235
    Jan 19 05:48:32.243: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-237" to be "running and ready"
    Jan 19 05:48:32.249: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.448574ms
    Jan 19 05:48:32.249: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 05:48:34.252: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009851788s
    Jan 19 05:48:34.253: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 05:48:36.253: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.009898414s
    Jan 19 05:48:36.253: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 19 05:48:36.253: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-237 to expose endpoints map[pod1:[80] pod2:[80]] 01/19/23 05:48:36.255
    Jan 19 05:48:36.265: INFO: successfully validated that service endpoint-test2 in namespace services-237 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 01/19/23 05:48:36.265
    Jan 19 05:48:37.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-237 exec execpod4zzpf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Jan 19 05:48:37.437: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 19 05:48:37.437: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 19 05:48:37.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-237 exec execpod4zzpf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.239.36 80'
    Jan 19 05:48:37.605: INFO: stderr: "+ nc -v -t -w 2 10.254.239.36 80\n+ echo hostName\nConnection to 10.254.239.36 80 port [tcp/http] succeeded!\n"
    Jan 19 05:48:37.605: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-237 01/19/23 05:48:37.605
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-237 to expose endpoints map[pod2:[80]] 01/19/23 05:48:37.62
    Jan 19 05:48:38.642: INFO: successfully validated that service endpoint-test2 in namespace services-237 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 01/19/23 05:48:38.642
    Jan 19 05:48:39.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-237 exec execpod4zzpf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Jan 19 05:48:39.806: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 19 05:48:39.806: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 19 05:48:39.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-237 exec execpod4zzpf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.239.36 80'
    Jan 19 05:48:39.959: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.239.36 80\nConnection to 10.254.239.36 80 port [tcp/http] succeeded!\n"
    Jan 19 05:48:39.959: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod2 in namespace services-237 01/19/23 05:48:39.959
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-237 to expose endpoints map[] 01/19/23 05:48:39.972
    Jan 19 05:48:39.982: INFO: successfully validated that service endpoint-test2 in namespace services-237 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 19 05:48:40.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-237" for this suite. 01/19/23 05:48:40.014
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:48:40.023
Jan 19 05:48:40.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename subpath 01/19/23 05:48:40.024
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:48:40.038
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:48:40.042
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/19/23 05:48:40.045
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-pvlp 01/19/23 05:48:40.058
STEP: Creating a pod to test atomic-volume-subpath 01/19/23 05:48:40.058
Jan 19 05:48:40.065: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-pvlp" in namespace "subpath-2931" to be "Succeeded or Failed"
Jan 19 05:48:40.069: INFO: Pod "pod-subpath-test-configmap-pvlp": Phase="Pending", Reason="", readiness=false. Elapsed: 3.579801ms
Jan 19 05:48:42.073: INFO: Pod "pod-subpath-test-configmap-pvlp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008323194s
Jan 19 05:48:44.073: INFO: Pod "pod-subpath-test-configmap-pvlp": Phase="Running", Reason="", readiness=true. Elapsed: 4.00791124s
Jan 19 05:48:46.072: INFO: Pod "pod-subpath-test-configmap-pvlp": Phase="Running", Reason="", readiness=true. Elapsed: 6.006860474s
Jan 19 05:48:48.072: INFO: Pod "pod-subpath-test-configmap-pvlp": Phase="Running", Reason="", readiness=true. Elapsed: 8.006936862s
Jan 19 05:48:50.072: INFO: Pod "pod-subpath-test-configmap-pvlp": Phase="Running", Reason="", readiness=true. Elapsed: 10.00743467s
Jan 19 05:48:52.074: INFO: Pod "pod-subpath-test-configmap-pvlp": Phase="Running", Reason="", readiness=true. Elapsed: 12.008947679s
Jan 19 05:48:54.072: INFO: Pod "pod-subpath-test-configmap-pvlp": Phase="Running", Reason="", readiness=true. Elapsed: 14.007498476s
Jan 19 05:48:56.073: INFO: Pod "pod-subpath-test-configmap-pvlp": Phase="Running", Reason="", readiness=true. Elapsed: 16.008267467s
Jan 19 05:48:58.072: INFO: Pod "pod-subpath-test-configmap-pvlp": Phase="Running", Reason="", readiness=true. Elapsed: 18.007463136s
Jan 19 05:49:00.073: INFO: Pod "pod-subpath-test-configmap-pvlp": Phase="Running", Reason="", readiness=true. Elapsed: 20.008485517s
Jan 19 05:49:02.073: INFO: Pod "pod-subpath-test-configmap-pvlp": Phase="Running", Reason="", readiness=true. Elapsed: 22.007997313s
Jan 19 05:49:04.072: INFO: Pod "pod-subpath-test-configmap-pvlp": Phase="Running", Reason="", readiness=false. Elapsed: 24.007290137s
Jan 19 05:49:06.073: INFO: Pod "pod-subpath-test-configmap-pvlp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.008070638s
STEP: Saw pod success 01/19/23 05:49:06.073
Jan 19 05:49:06.073: INFO: Pod "pod-subpath-test-configmap-pvlp" satisfied condition "Succeeded or Failed"
Jan 19 05:49:06.076: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-subpath-test-configmap-pvlp container test-container-subpath-configmap-pvlp: <nil>
STEP: delete the pod 01/19/23 05:49:06.081
Jan 19 05:49:06.091: INFO: Waiting for pod pod-subpath-test-configmap-pvlp to disappear
Jan 19 05:49:06.093: INFO: Pod pod-subpath-test-configmap-pvlp no longer exists
STEP: Deleting pod pod-subpath-test-configmap-pvlp 01/19/23 05:49:06.093
Jan 19 05:49:06.093: INFO: Deleting pod "pod-subpath-test-configmap-pvlp" in namespace "subpath-2931"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan 19 05:49:06.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2931" for this suite. 01/19/23 05:49:06.098
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","completed":307,"skipped":5589,"failed":0}
------------------------------
â€¢ [SLOW TEST] [26.082 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:48:40.023
    Jan 19 05:48:40.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename subpath 01/19/23 05:48:40.024
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:48:40.038
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:48:40.042
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/19/23 05:48:40.045
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-pvlp 01/19/23 05:48:40.058
    STEP: Creating a pod to test atomic-volume-subpath 01/19/23 05:48:40.058
    Jan 19 05:48:40.065: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-pvlp" in namespace "subpath-2931" to be "Succeeded or Failed"
    Jan 19 05:48:40.069: INFO: Pod "pod-subpath-test-configmap-pvlp": Phase="Pending", Reason="", readiness=false. Elapsed: 3.579801ms
    Jan 19 05:48:42.073: INFO: Pod "pod-subpath-test-configmap-pvlp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008323194s
    Jan 19 05:48:44.073: INFO: Pod "pod-subpath-test-configmap-pvlp": Phase="Running", Reason="", readiness=true. Elapsed: 4.00791124s
    Jan 19 05:48:46.072: INFO: Pod "pod-subpath-test-configmap-pvlp": Phase="Running", Reason="", readiness=true. Elapsed: 6.006860474s
    Jan 19 05:48:48.072: INFO: Pod "pod-subpath-test-configmap-pvlp": Phase="Running", Reason="", readiness=true. Elapsed: 8.006936862s
    Jan 19 05:48:50.072: INFO: Pod "pod-subpath-test-configmap-pvlp": Phase="Running", Reason="", readiness=true. Elapsed: 10.00743467s
    Jan 19 05:48:52.074: INFO: Pod "pod-subpath-test-configmap-pvlp": Phase="Running", Reason="", readiness=true. Elapsed: 12.008947679s
    Jan 19 05:48:54.072: INFO: Pod "pod-subpath-test-configmap-pvlp": Phase="Running", Reason="", readiness=true. Elapsed: 14.007498476s
    Jan 19 05:48:56.073: INFO: Pod "pod-subpath-test-configmap-pvlp": Phase="Running", Reason="", readiness=true. Elapsed: 16.008267467s
    Jan 19 05:48:58.072: INFO: Pod "pod-subpath-test-configmap-pvlp": Phase="Running", Reason="", readiness=true. Elapsed: 18.007463136s
    Jan 19 05:49:00.073: INFO: Pod "pod-subpath-test-configmap-pvlp": Phase="Running", Reason="", readiness=true. Elapsed: 20.008485517s
    Jan 19 05:49:02.073: INFO: Pod "pod-subpath-test-configmap-pvlp": Phase="Running", Reason="", readiness=true. Elapsed: 22.007997313s
    Jan 19 05:49:04.072: INFO: Pod "pod-subpath-test-configmap-pvlp": Phase="Running", Reason="", readiness=false. Elapsed: 24.007290137s
    Jan 19 05:49:06.073: INFO: Pod "pod-subpath-test-configmap-pvlp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.008070638s
    STEP: Saw pod success 01/19/23 05:49:06.073
    Jan 19 05:49:06.073: INFO: Pod "pod-subpath-test-configmap-pvlp" satisfied condition "Succeeded or Failed"
    Jan 19 05:49:06.076: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-subpath-test-configmap-pvlp container test-container-subpath-configmap-pvlp: <nil>
    STEP: delete the pod 01/19/23 05:49:06.081
    Jan 19 05:49:06.091: INFO: Waiting for pod pod-subpath-test-configmap-pvlp to disappear
    Jan 19 05:49:06.093: INFO: Pod pod-subpath-test-configmap-pvlp no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-pvlp 01/19/23 05:49:06.093
    Jan 19 05:49:06.093: INFO: Deleting pod "pod-subpath-test-configmap-pvlp" in namespace "subpath-2931"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan 19 05:49:06.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-2931" for this suite. 01/19/23 05:49:06.098
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:49:06.106
Jan 19 05:49:06.106: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename gc 01/19/23 05:49:06.106
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:49:06.116
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:49:06.12
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 01/19/23 05:49:06.123
STEP: delete the rc 01/19/23 05:49:11.135
STEP: wait for all pods to be garbage collected 01/19/23 05:49:11.147
STEP: Gathering metrics 01/19/23 05:49:16.157
W0119 05:49:16.162637      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 19 05:49:16.162: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 19 05:49:16.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7747" for this suite. 01/19/23 05:49:16.165
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","completed":308,"skipped":5615,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.066 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:49:06.106
    Jan 19 05:49:06.106: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename gc 01/19/23 05:49:06.106
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:49:06.116
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:49:06.12
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 01/19/23 05:49:06.123
    STEP: delete the rc 01/19/23 05:49:11.135
    STEP: wait for all pods to be garbage collected 01/19/23 05:49:11.147
    STEP: Gathering metrics 01/19/23 05:49:16.157
    W0119 05:49:16.162637      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 19 05:49:16.162: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 19 05:49:16.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-7747" for this suite. 01/19/23 05:49:16.165
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:49:16.172
Jan 19 05:49:16.172: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename pods 01/19/23 05:49:16.173
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:49:16.184
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:49:16.187
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
STEP: creating a Pod with a static label 01/19/23 05:49:16.193
STEP: watching for Pod to be ready 01/19/23 05:49:16.201
Jan 19 05:49:16.204: INFO: observed Pod pod-test in namespace pods-3044 in phase Pending with labels: map[test-pod-static:true] & conditions []
Jan 19 05:49:16.206: INFO: observed Pod pod-test in namespace pods-3044 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:49:16 +0000 UTC  }]
Jan 19 05:49:16.221: INFO: observed Pod pod-test in namespace pods-3044 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:49:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:49:16 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:49:16 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:49:16 +0000 UTC  }]
Jan 19 05:49:18.848: INFO: Found Pod pod-test in namespace pods-3044 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:49:16 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:49:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:49:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:49:16 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 01/19/23 05:49:18.853
STEP: getting the Pod and ensuring that it's patched 01/19/23 05:49:18.863
STEP: replacing the Pod's status Ready condition to False 01/19/23 05:49:18.867
STEP: check the Pod again to ensure its Ready conditions are False 01/19/23 05:49:18.877
STEP: deleting the Pod via a Collection with a LabelSelector 01/19/23 05:49:18.877
STEP: watching for the Pod to be deleted 01/19/23 05:49:18.885
Jan 19 05:49:18.886: INFO: observed event type MODIFIED
Jan 19 05:49:20.860: INFO: observed event type MODIFIED
Jan 19 05:49:21.861: INFO: observed event type MODIFIED
Jan 19 05:49:21.869: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 19 05:49:21.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3044" for this suite. 01/19/23 05:49:21.877
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","completed":309,"skipped":5624,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.710 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:49:16.172
    Jan 19 05:49:16.172: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename pods 01/19/23 05:49:16.173
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:49:16.184
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:49:16.187
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:895
    STEP: creating a Pod with a static label 01/19/23 05:49:16.193
    STEP: watching for Pod to be ready 01/19/23 05:49:16.201
    Jan 19 05:49:16.204: INFO: observed Pod pod-test in namespace pods-3044 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Jan 19 05:49:16.206: INFO: observed Pod pod-test in namespace pods-3044 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:49:16 +0000 UTC  }]
    Jan 19 05:49:16.221: INFO: observed Pod pod-test in namespace pods-3044 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:49:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:49:16 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:49:16 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:49:16 +0000 UTC  }]
    Jan 19 05:49:18.848: INFO: Found Pod pod-test in namespace pods-3044 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:49:16 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:49:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:49:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 05:49:16 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 01/19/23 05:49:18.853
    STEP: getting the Pod and ensuring that it's patched 01/19/23 05:49:18.863
    STEP: replacing the Pod's status Ready condition to False 01/19/23 05:49:18.867
    STEP: check the Pod again to ensure its Ready conditions are False 01/19/23 05:49:18.877
    STEP: deleting the Pod via a Collection with a LabelSelector 01/19/23 05:49:18.877
    STEP: watching for the Pod to be deleted 01/19/23 05:49:18.885
    Jan 19 05:49:18.886: INFO: observed event type MODIFIED
    Jan 19 05:49:20.860: INFO: observed event type MODIFIED
    Jan 19 05:49:21.861: INFO: observed event type MODIFIED
    Jan 19 05:49:21.869: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 19 05:49:21.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-3044" for this suite. 01/19/23 05:49:21.877
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:49:21.882
Jan 19 05:49:21.882: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename downward-api 01/19/23 05:49:21.883
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:49:21.895
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:49:21.898
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
STEP: Creating a pod to test downward API volume plugin 01/19/23 05:49:21.901
Jan 19 05:49:21.909: INFO: Waiting up to 5m0s for pod "downwardapi-volume-02145f1e-37fe-468a-8d2b-cb3298640520" in namespace "downward-api-610" to be "Succeeded or Failed"
Jan 19 05:49:21.914: INFO: Pod "downwardapi-volume-02145f1e-37fe-468a-8d2b-cb3298640520": Phase="Pending", Reason="", readiness=false. Elapsed: 5.621979ms
Jan 19 05:49:23.921: INFO: Pod "downwardapi-volume-02145f1e-37fe-468a-8d2b-cb3298640520": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012339404s
Jan 19 05:49:25.918: INFO: Pod "downwardapi-volume-02145f1e-37fe-468a-8d2b-cb3298640520": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008925587s
STEP: Saw pod success 01/19/23 05:49:25.918
Jan 19 05:49:25.918: INFO: Pod "downwardapi-volume-02145f1e-37fe-468a-8d2b-cb3298640520" satisfied condition "Succeeded or Failed"
Jan 19 05:49:25.920: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod downwardapi-volume-02145f1e-37fe-468a-8d2b-cb3298640520 container client-container: <nil>
STEP: delete the pod 01/19/23 05:49:25.925
Jan 19 05:49:25.937: INFO: Waiting for pod downwardapi-volume-02145f1e-37fe-468a-8d2b-cb3298640520 to disappear
Jan 19 05:49:25.939: INFO: Pod downwardapi-volume-02145f1e-37fe-468a-8d2b-cb3298640520 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 19 05:49:25.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-610" for this suite. 01/19/23 05:49:25.942
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","completed":310,"skipped":5627,"failed":0}
------------------------------
â€¢ [4.066 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:49:21.882
    Jan 19 05:49:21.882: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename downward-api 01/19/23 05:49:21.883
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:49:21.895
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:49:21.898
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:206
    STEP: Creating a pod to test downward API volume plugin 01/19/23 05:49:21.901
    Jan 19 05:49:21.909: INFO: Waiting up to 5m0s for pod "downwardapi-volume-02145f1e-37fe-468a-8d2b-cb3298640520" in namespace "downward-api-610" to be "Succeeded or Failed"
    Jan 19 05:49:21.914: INFO: Pod "downwardapi-volume-02145f1e-37fe-468a-8d2b-cb3298640520": Phase="Pending", Reason="", readiness=false. Elapsed: 5.621979ms
    Jan 19 05:49:23.921: INFO: Pod "downwardapi-volume-02145f1e-37fe-468a-8d2b-cb3298640520": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012339404s
    Jan 19 05:49:25.918: INFO: Pod "downwardapi-volume-02145f1e-37fe-468a-8d2b-cb3298640520": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008925587s
    STEP: Saw pod success 01/19/23 05:49:25.918
    Jan 19 05:49:25.918: INFO: Pod "downwardapi-volume-02145f1e-37fe-468a-8d2b-cb3298640520" satisfied condition "Succeeded or Failed"
    Jan 19 05:49:25.920: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod downwardapi-volume-02145f1e-37fe-468a-8d2b-cb3298640520 container client-container: <nil>
    STEP: delete the pod 01/19/23 05:49:25.925
    Jan 19 05:49:25.937: INFO: Waiting for pod downwardapi-volume-02145f1e-37fe-468a-8d2b-cb3298640520 to disappear
    Jan 19 05:49:25.939: INFO: Pod downwardapi-volume-02145f1e-37fe-468a-8d2b-cb3298640520 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 19 05:49:25.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-610" for this suite. 01/19/23 05:49:25.942
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:49:25.95
Jan 19 05:49:25.950: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename init-container 01/19/23 05:49:25.951
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:49:25.962
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:49:25.964
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
STEP: creating the pod 01/19/23 05:49:25.966
Jan 19 05:49:25.966: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 19 05:49:32.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6600" for this suite. 01/19/23 05:49:32.905
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","completed":311,"skipped":5698,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.960 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:49:25.95
    Jan 19 05:49:25.950: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename init-container 01/19/23 05:49:25.951
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:49:25.962
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:49:25.964
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:176
    STEP: creating the pod 01/19/23 05:49:25.966
    Jan 19 05:49:25.966: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 19 05:49:32.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-6600" for this suite. 01/19/23 05:49:32.905
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:49:32.911
Jan 19 05:49:32.911: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename services 01/19/23 05:49:32.912
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:49:32.923
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:49:32.925
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
STEP: creating service nodeport-test with type=NodePort in namespace services-3700 01/19/23 05:49:32.927
STEP: creating replication controller nodeport-test in namespace services-3700 01/19/23 05:49:32.94
I0119 05:49:32.948969      18 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-3700, replica count: 2
I0119 05:49:36.000230      18 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 19 05:49:36.000: INFO: Creating new exec pod
Jan 19 05:49:36.017: INFO: Waiting up to 5m0s for pod "execpod4cpgr" in namespace "services-3700" to be "running"
Jan 19 05:49:36.021: INFO: Pod "execpod4cpgr": Phase="Pending", Reason="", readiness=false. Elapsed: 4.480117ms
Jan 19 05:49:38.025: INFO: Pod "execpod4cpgr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007989829s
Jan 19 05:49:40.025: INFO: Pod "execpod4cpgr": Phase="Running", Reason="", readiness=true. Elapsed: 4.008416559s
Jan 19 05:49:40.025: INFO: Pod "execpod4cpgr" satisfied condition "running"
Jan 19 05:49:41.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:49:41.205: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:49:41.205: INFO: stdout: ""
Jan 19 05:49:42.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:49:42.387: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:49:42.387: INFO: stdout: ""
Jan 19 05:49:43.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:49:43.364: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:49:43.365: INFO: stdout: ""
Jan 19 05:49:44.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:49:44.366: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:49:44.366: INFO: stdout: ""
Jan 19 05:49:45.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:49:45.364: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:49:45.364: INFO: stdout: ""
Jan 19 05:49:46.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:49:46.372: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:49:46.372: INFO: stdout: ""
Jan 19 05:49:47.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:49:47.377: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:49:47.377: INFO: stdout: ""
Jan 19 05:49:48.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:49:48.369: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:49:48.369: INFO: stdout: ""
Jan 19 05:49:49.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:49:49.363: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:49:49.363: INFO: stdout: ""
Jan 19 05:49:50.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:49:50.362: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:49:50.362: INFO: stdout: ""
Jan 19 05:49:51.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:49:51.363: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:49:51.363: INFO: stdout: ""
Jan 19 05:49:52.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:49:52.366: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:49:52.366: INFO: stdout: ""
Jan 19 05:49:53.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:49:53.364: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:49:53.364: INFO: stdout: ""
Jan 19 05:49:54.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:49:54.372: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:49:54.372: INFO: stdout: ""
Jan 19 05:49:55.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:49:55.369: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:49:55.369: INFO: stdout: ""
Jan 19 05:49:56.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:49:56.365: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:49:56.365: INFO: stdout: ""
Jan 19 05:49:57.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:49:57.365: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:49:57.365: INFO: stdout: ""
Jan 19 05:49:58.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:49:58.357: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:49:58.357: INFO: stdout: ""
Jan 19 05:49:59.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:49:59.360: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:49:59.360: INFO: stdout: ""
Jan 19 05:50:00.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:00.363: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:00.363: INFO: stdout: ""
Jan 19 05:50:01.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:01.380: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:01.380: INFO: stdout: ""
Jan 19 05:50:02.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:02.369: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:02.369: INFO: stdout: ""
Jan 19 05:50:03.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:03.400: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:03.400: INFO: stdout: ""
Jan 19 05:50:04.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:04.365: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:04.365: INFO: stdout: ""
Jan 19 05:50:05.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:05.366: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:05.366: INFO: stdout: ""
Jan 19 05:50:06.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:06.359: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:06.359: INFO: stdout: ""
Jan 19 05:50:07.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:07.377: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:07.377: INFO: stdout: ""
Jan 19 05:50:08.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:08.358: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:08.358: INFO: stdout: ""
Jan 19 05:50:09.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:09.358: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:09.358: INFO: stdout: ""
Jan 19 05:50:10.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:10.364: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:10.364: INFO: stdout: ""
Jan 19 05:50:11.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:11.369: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:11.369: INFO: stdout: ""
Jan 19 05:50:12.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:12.363: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:12.363: INFO: stdout: ""
Jan 19 05:50:13.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:13.359: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:13.359: INFO: stdout: ""
Jan 19 05:50:14.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:14.366: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:14.366: INFO: stdout: ""
Jan 19 05:50:15.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:15.363: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:15.363: INFO: stdout: ""
Jan 19 05:50:16.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:16.377: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:16.377: INFO: stdout: ""
Jan 19 05:50:17.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:17.374: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:17.374: INFO: stdout: ""
Jan 19 05:50:18.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:18.385: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:18.385: INFO: stdout: ""
Jan 19 05:50:19.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:19.368: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:19.368: INFO: stdout: ""
Jan 19 05:50:20.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:20.362: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:20.362: INFO: stdout: ""
Jan 19 05:50:21.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:21.375: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:21.375: INFO: stdout: ""
Jan 19 05:50:22.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:22.367: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:22.368: INFO: stdout: ""
Jan 19 05:50:23.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:23.365: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:23.365: INFO: stdout: ""
Jan 19 05:50:24.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:24.365: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:24.365: INFO: stdout: ""
Jan 19 05:50:25.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:25.370: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:25.370: INFO: stdout: ""
Jan 19 05:50:26.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:26.355: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:26.355: INFO: stdout: ""
Jan 19 05:50:27.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:27.364: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:27.364: INFO: stdout: ""
Jan 19 05:50:28.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:28.361: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:28.361: INFO: stdout: ""
Jan 19 05:50:29.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:29.371: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:29.371: INFO: stdout: ""
Jan 19 05:50:30.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:30.369: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:30.369: INFO: stdout: ""
Jan 19 05:50:31.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:31.361: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:31.361: INFO: stdout: ""
Jan 19 05:50:32.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:32.365: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:32.365: INFO: stdout: ""
Jan 19 05:50:33.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:33.370: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:33.370: INFO: stdout: ""
Jan 19 05:50:34.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:34.358: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:34.358: INFO: stdout: ""
Jan 19 05:50:35.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:35.378: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:35.378: INFO: stdout: ""
Jan 19 05:50:36.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:36.360: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:36.360: INFO: stdout: ""
Jan 19 05:50:37.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:37.377: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:37.377: INFO: stdout: ""
Jan 19 05:50:38.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:38.373: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:38.373: INFO: stdout: ""
Jan 19 05:50:39.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:39.366: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:39.366: INFO: stdout: ""
Jan 19 05:50:40.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:40.373: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:40.373: INFO: stdout: ""
Jan 19 05:50:41.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:41.369: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:41.369: INFO: stdout: ""
Jan 19 05:50:42.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:42.362: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:42.362: INFO: stdout: ""
Jan 19 05:50:43.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:43.372: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:43.372: INFO: stdout: ""
Jan 19 05:50:44.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:44.359: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:44.359: INFO: stdout: ""
Jan 19 05:50:45.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:45.371: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:45.371: INFO: stdout: ""
Jan 19 05:50:46.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:46.362: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:46.362: INFO: stdout: ""
Jan 19 05:50:47.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:47.372: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:47.372: INFO: stdout: ""
Jan 19 05:50:48.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:48.360: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:48.360: INFO: stdout: ""
Jan 19 05:50:49.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:49.365: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:49.365: INFO: stdout: ""
Jan 19 05:50:50.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:50.365: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:50.365: INFO: stdout: ""
Jan 19 05:50:51.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:51.374: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:51.374: INFO: stdout: ""
Jan 19 05:50:52.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:52.365: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:52.365: INFO: stdout: ""
Jan 19 05:50:53.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:53.370: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:53.370: INFO: stdout: ""
Jan 19 05:50:54.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:54.363: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:54.363: INFO: stdout: ""
Jan 19 05:50:55.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:55.362: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:55.362: INFO: stdout: ""
Jan 19 05:50:56.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:56.362: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:56.362: INFO: stdout: ""
Jan 19 05:50:57.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:57.368: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:57.368: INFO: stdout: ""
Jan 19 05:50:58.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:58.366: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:58.366: INFO: stdout: ""
Jan 19 05:50:59.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:50:59.365: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:50:59.365: INFO: stdout: ""
Jan 19 05:51:00.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:51:00.361: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:51:00.361: INFO: stdout: ""
Jan 19 05:51:01.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 05:51:01.374: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 05:51:01.374: INFO: stdout: "nodeport-test-2fthk"
Jan 19 05:51:01.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
Jan 19 05:51:01.543: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.243.22 80\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
Jan 19 05:51:01.543: INFO: stdout: ""
Jan 19 05:51:02.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
Jan 19 05:51:02.698: INFO: stderr: "+ nc -v -t -w 2 10.254.243.22 80\n+ echo hostName\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
Jan 19 05:51:02.698: INFO: stdout: ""
Jan 19 05:51:03.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
Jan 19 05:51:03.705: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.243.22 80\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
Jan 19 05:51:03.705: INFO: stdout: ""
Jan 19 05:51:04.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
Jan 19 05:51:04.707: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.243.22 80\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
Jan 19 05:51:04.707: INFO: stdout: ""
Jan 19 05:51:05.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
Jan 19 05:51:05.700: INFO: stderr: "+ nc -v -t -w 2 10.254.243.22 80\n+ echo hostName\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
Jan 19 05:51:05.700: INFO: stdout: ""
Jan 19 05:51:06.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
Jan 19 05:51:06.706: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.243.22 80\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
Jan 19 05:51:06.706: INFO: stdout: ""
Jan 19 05:51:07.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
Jan 19 05:51:07.706: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.243.22 80\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
Jan 19 05:51:07.706: INFO: stdout: ""
Jan 19 05:51:08.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
Jan 19 05:51:08.718: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.243.22 80\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
Jan 19 05:51:08.718: INFO: stdout: ""
Jan 19 05:51:09.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
Jan 19 05:51:09.700: INFO: stderr: "+ nc -v -t -w 2 10.254.243.22 80\n+ echo hostName\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
Jan 19 05:51:09.700: INFO: stdout: ""
Jan 19 05:51:10.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
Jan 19 05:51:10.715: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.243.22 80\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
Jan 19 05:51:10.715: INFO: stdout: ""
Jan 19 05:51:11.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
Jan 19 05:51:11.714: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.243.22 80\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
Jan 19 05:51:11.714: INFO: stdout: ""
Jan 19 05:51:12.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
Jan 19 05:51:12.702: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.243.22 80\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
Jan 19 05:51:12.702: INFO: stdout: ""
Jan 19 05:51:13.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
Jan 19 05:51:13.709: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.243.22 80\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
Jan 19 05:51:13.709: INFO: stdout: ""
Jan 19 05:51:14.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
Jan 19 05:51:14.701: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.243.22 80\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
Jan 19 05:51:14.701: INFO: stdout: ""
Jan 19 05:51:15.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
Jan 19 05:51:15.697: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.243.22 80\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
Jan 19 05:51:15.697: INFO: stdout: ""
Jan 19 05:51:16.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
Jan 19 05:51:16.702: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.243.22 80\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
Jan 19 05:51:16.702: INFO: stdout: ""
Jan 19 05:51:17.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
Jan 19 05:51:17.699: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.243.22 80\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
Jan 19 05:51:17.699: INFO: stdout: ""
Jan 19 05:51:18.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
Jan 19 05:51:18.697: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.243.22 80\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
Jan 19 05:51:18.697: INFO: stdout: ""
Jan 19 05:51:19.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
Jan 19 05:51:19.699: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.243.22 80\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
Jan 19 05:51:19.699: INFO: stdout: ""
Jan 19 05:51:20.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
Jan 19 05:51:20.723: INFO: stderr: "+ echo+  hostName\nnc -v -t -w 2 10.254.243.22 80\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
Jan 19 05:51:20.723: INFO: stdout: ""
Jan 19 05:51:21.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
Jan 19 05:51:21.704: INFO: stderr: "+ nc -v -t -w 2 10.254.243.22 80\n+ echo hostName\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
Jan 19 05:51:21.704: INFO: stdout: ""
Jan 19 05:51:22.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
Jan 19 05:51:22.702: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.243.22 80\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
Jan 19 05:51:22.702: INFO: stdout: ""
Jan 19 05:51:23.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
Jan 19 05:51:23.700: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.243.22 80\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
Jan 19 05:51:23.700: INFO: stdout: ""
Jan 19 05:51:24.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
Jan 19 05:51:24.704: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.243.22 80\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
Jan 19 05:51:24.704: INFO: stdout: ""
Jan 19 05:51:25.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
Jan 19 05:51:25.699: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.243.22 80\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
Jan 19 05:51:25.699: INFO: stdout: ""
Jan 19 05:51:26.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
Jan 19 05:51:26.705: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.243.22 80\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
Jan 19 05:51:26.705: INFO: stdout: ""
Jan 19 05:51:27.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
Jan 19 05:51:27.700: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.243.22 80\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
Jan 19 05:51:27.700: INFO: stdout: "nodeport-test-2fthk"
Jan 19 05:51:27.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.0.99 31392'
Jan 19 05:51:27.856: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.0.99 31392\nConnection to 192.168.0.99 31392 port [tcp/*] succeeded!\n"
Jan 19 05:51:27.856: INFO: stdout: "nodeport-test-gw5gp"
Jan 19 05:51:27.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.0.78 31392'
Jan 19 05:51:28.024: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.0.78 31392\nConnection to 192.168.0.78 31392 port [tcp/*] succeeded!\n"
Jan 19 05:51:28.024: INFO: stdout: ""
Jan 19 05:51:29.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.0.78 31392'
Jan 19 05:51:29.187: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.0.78 31392\nConnection to 192.168.0.78 31392 port [tcp/*] succeeded!\n"
Jan 19 05:51:29.187: INFO: stdout: "nodeport-test-gw5gp"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 19 05:51:29.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3700" for this suite. 01/19/23 05:51:29.191
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","completed":312,"skipped":5711,"failed":0}
------------------------------
â€¢ [SLOW TEST] [116.285 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:49:32.911
    Jan 19 05:49:32.911: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename services 01/19/23 05:49:32.912
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:49:32.923
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:49:32.925
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1268
    STEP: creating service nodeport-test with type=NodePort in namespace services-3700 01/19/23 05:49:32.927
    STEP: creating replication controller nodeport-test in namespace services-3700 01/19/23 05:49:32.94
    I0119 05:49:32.948969      18 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-3700, replica count: 2
    I0119 05:49:36.000230      18 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 19 05:49:36.000: INFO: Creating new exec pod
    Jan 19 05:49:36.017: INFO: Waiting up to 5m0s for pod "execpod4cpgr" in namespace "services-3700" to be "running"
    Jan 19 05:49:36.021: INFO: Pod "execpod4cpgr": Phase="Pending", Reason="", readiness=false. Elapsed: 4.480117ms
    Jan 19 05:49:38.025: INFO: Pod "execpod4cpgr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007989829s
    Jan 19 05:49:40.025: INFO: Pod "execpod4cpgr": Phase="Running", Reason="", readiness=true. Elapsed: 4.008416559s
    Jan 19 05:49:40.025: INFO: Pod "execpod4cpgr" satisfied condition "running"
    Jan 19 05:49:41.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:49:41.205: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:49:41.205: INFO: stdout: ""
    Jan 19 05:49:42.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:49:42.387: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:49:42.387: INFO: stdout: ""
    Jan 19 05:49:43.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:49:43.364: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:49:43.365: INFO: stdout: ""
    Jan 19 05:49:44.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:49:44.366: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:49:44.366: INFO: stdout: ""
    Jan 19 05:49:45.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:49:45.364: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:49:45.364: INFO: stdout: ""
    Jan 19 05:49:46.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:49:46.372: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:49:46.372: INFO: stdout: ""
    Jan 19 05:49:47.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:49:47.377: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:49:47.377: INFO: stdout: ""
    Jan 19 05:49:48.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:49:48.369: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:49:48.369: INFO: stdout: ""
    Jan 19 05:49:49.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:49:49.363: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:49:49.363: INFO: stdout: ""
    Jan 19 05:49:50.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:49:50.362: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:49:50.362: INFO: stdout: ""
    Jan 19 05:49:51.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:49:51.363: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:49:51.363: INFO: stdout: ""
    Jan 19 05:49:52.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:49:52.366: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:49:52.366: INFO: stdout: ""
    Jan 19 05:49:53.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:49:53.364: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:49:53.364: INFO: stdout: ""
    Jan 19 05:49:54.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:49:54.372: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:49:54.372: INFO: stdout: ""
    Jan 19 05:49:55.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:49:55.369: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:49:55.369: INFO: stdout: ""
    Jan 19 05:49:56.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:49:56.365: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:49:56.365: INFO: stdout: ""
    Jan 19 05:49:57.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:49:57.365: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:49:57.365: INFO: stdout: ""
    Jan 19 05:49:58.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:49:58.357: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:49:58.357: INFO: stdout: ""
    Jan 19 05:49:59.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:49:59.360: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:49:59.360: INFO: stdout: ""
    Jan 19 05:50:00.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:00.363: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:00.363: INFO: stdout: ""
    Jan 19 05:50:01.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:01.380: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:01.380: INFO: stdout: ""
    Jan 19 05:50:02.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:02.369: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:02.369: INFO: stdout: ""
    Jan 19 05:50:03.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:03.400: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:03.400: INFO: stdout: ""
    Jan 19 05:50:04.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:04.365: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:04.365: INFO: stdout: ""
    Jan 19 05:50:05.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:05.366: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:05.366: INFO: stdout: ""
    Jan 19 05:50:06.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:06.359: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:06.359: INFO: stdout: ""
    Jan 19 05:50:07.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:07.377: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:07.377: INFO: stdout: ""
    Jan 19 05:50:08.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:08.358: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:08.358: INFO: stdout: ""
    Jan 19 05:50:09.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:09.358: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:09.358: INFO: stdout: ""
    Jan 19 05:50:10.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:10.364: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:10.364: INFO: stdout: ""
    Jan 19 05:50:11.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:11.369: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:11.369: INFO: stdout: ""
    Jan 19 05:50:12.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:12.363: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:12.363: INFO: stdout: ""
    Jan 19 05:50:13.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:13.359: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:13.359: INFO: stdout: ""
    Jan 19 05:50:14.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:14.366: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:14.366: INFO: stdout: ""
    Jan 19 05:50:15.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:15.363: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:15.363: INFO: stdout: ""
    Jan 19 05:50:16.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:16.377: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:16.377: INFO: stdout: ""
    Jan 19 05:50:17.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:17.374: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:17.374: INFO: stdout: ""
    Jan 19 05:50:18.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:18.385: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:18.385: INFO: stdout: ""
    Jan 19 05:50:19.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:19.368: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:19.368: INFO: stdout: ""
    Jan 19 05:50:20.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:20.362: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:20.362: INFO: stdout: ""
    Jan 19 05:50:21.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:21.375: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:21.375: INFO: stdout: ""
    Jan 19 05:50:22.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:22.367: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:22.368: INFO: stdout: ""
    Jan 19 05:50:23.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:23.365: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:23.365: INFO: stdout: ""
    Jan 19 05:50:24.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:24.365: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:24.365: INFO: stdout: ""
    Jan 19 05:50:25.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:25.370: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:25.370: INFO: stdout: ""
    Jan 19 05:50:26.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:26.355: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:26.355: INFO: stdout: ""
    Jan 19 05:50:27.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:27.364: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:27.364: INFO: stdout: ""
    Jan 19 05:50:28.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:28.361: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:28.361: INFO: stdout: ""
    Jan 19 05:50:29.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:29.371: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:29.371: INFO: stdout: ""
    Jan 19 05:50:30.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:30.369: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:30.369: INFO: stdout: ""
    Jan 19 05:50:31.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:31.361: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:31.361: INFO: stdout: ""
    Jan 19 05:50:32.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:32.365: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:32.365: INFO: stdout: ""
    Jan 19 05:50:33.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:33.370: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:33.370: INFO: stdout: ""
    Jan 19 05:50:34.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:34.358: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:34.358: INFO: stdout: ""
    Jan 19 05:50:35.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:35.378: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:35.378: INFO: stdout: ""
    Jan 19 05:50:36.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:36.360: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:36.360: INFO: stdout: ""
    Jan 19 05:50:37.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:37.377: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:37.377: INFO: stdout: ""
    Jan 19 05:50:38.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:38.373: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:38.373: INFO: stdout: ""
    Jan 19 05:50:39.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:39.366: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:39.366: INFO: stdout: ""
    Jan 19 05:50:40.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:40.373: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:40.373: INFO: stdout: ""
    Jan 19 05:50:41.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:41.369: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:41.369: INFO: stdout: ""
    Jan 19 05:50:42.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:42.362: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:42.362: INFO: stdout: ""
    Jan 19 05:50:43.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:43.372: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:43.372: INFO: stdout: ""
    Jan 19 05:50:44.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:44.359: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:44.359: INFO: stdout: ""
    Jan 19 05:50:45.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:45.371: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:45.371: INFO: stdout: ""
    Jan 19 05:50:46.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:46.362: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:46.362: INFO: stdout: ""
    Jan 19 05:50:47.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:47.372: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:47.372: INFO: stdout: ""
    Jan 19 05:50:48.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:48.360: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:48.360: INFO: stdout: ""
    Jan 19 05:50:49.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:49.365: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:49.365: INFO: stdout: ""
    Jan 19 05:50:50.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:50.365: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:50.365: INFO: stdout: ""
    Jan 19 05:50:51.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:51.374: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:51.374: INFO: stdout: ""
    Jan 19 05:50:52.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:52.365: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:52.365: INFO: stdout: ""
    Jan 19 05:50:53.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:53.370: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:53.370: INFO: stdout: ""
    Jan 19 05:50:54.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:54.363: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:54.363: INFO: stdout: ""
    Jan 19 05:50:55.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:55.362: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:55.362: INFO: stdout: ""
    Jan 19 05:50:56.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:56.362: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:56.362: INFO: stdout: ""
    Jan 19 05:50:57.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:57.368: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:57.368: INFO: stdout: ""
    Jan 19 05:50:58.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:58.366: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:58.366: INFO: stdout: ""
    Jan 19 05:50:59.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:50:59.365: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:50:59.365: INFO: stdout: ""
    Jan 19 05:51:00.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:51:00.361: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:51:00.361: INFO: stdout: ""
    Jan 19 05:51:01.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 05:51:01.374: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 05:51:01.374: INFO: stdout: "nodeport-test-2fthk"
    Jan 19 05:51:01.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
    Jan 19 05:51:01.543: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.243.22 80\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
    Jan 19 05:51:01.543: INFO: stdout: ""
    Jan 19 05:51:02.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
    Jan 19 05:51:02.698: INFO: stderr: "+ nc -v -t -w 2 10.254.243.22 80\n+ echo hostName\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
    Jan 19 05:51:02.698: INFO: stdout: ""
    Jan 19 05:51:03.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
    Jan 19 05:51:03.705: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.243.22 80\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
    Jan 19 05:51:03.705: INFO: stdout: ""
    Jan 19 05:51:04.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
    Jan 19 05:51:04.707: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.243.22 80\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
    Jan 19 05:51:04.707: INFO: stdout: ""
    Jan 19 05:51:05.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
    Jan 19 05:51:05.700: INFO: stderr: "+ nc -v -t -w 2 10.254.243.22 80\n+ echo hostName\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
    Jan 19 05:51:05.700: INFO: stdout: ""
    Jan 19 05:51:06.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
    Jan 19 05:51:06.706: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.243.22 80\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
    Jan 19 05:51:06.706: INFO: stdout: ""
    Jan 19 05:51:07.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
    Jan 19 05:51:07.706: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.243.22 80\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
    Jan 19 05:51:07.706: INFO: stdout: ""
    Jan 19 05:51:08.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
    Jan 19 05:51:08.718: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.243.22 80\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
    Jan 19 05:51:08.718: INFO: stdout: ""
    Jan 19 05:51:09.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
    Jan 19 05:51:09.700: INFO: stderr: "+ nc -v -t -w 2 10.254.243.22 80\n+ echo hostName\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
    Jan 19 05:51:09.700: INFO: stdout: ""
    Jan 19 05:51:10.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
    Jan 19 05:51:10.715: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.243.22 80\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
    Jan 19 05:51:10.715: INFO: stdout: ""
    Jan 19 05:51:11.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
    Jan 19 05:51:11.714: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.243.22 80\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
    Jan 19 05:51:11.714: INFO: stdout: ""
    Jan 19 05:51:12.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
    Jan 19 05:51:12.702: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.243.22 80\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
    Jan 19 05:51:12.702: INFO: stdout: ""
    Jan 19 05:51:13.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
    Jan 19 05:51:13.709: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.243.22 80\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
    Jan 19 05:51:13.709: INFO: stdout: ""
    Jan 19 05:51:14.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
    Jan 19 05:51:14.701: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.243.22 80\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
    Jan 19 05:51:14.701: INFO: stdout: ""
    Jan 19 05:51:15.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
    Jan 19 05:51:15.697: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.243.22 80\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
    Jan 19 05:51:15.697: INFO: stdout: ""
    Jan 19 05:51:16.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
    Jan 19 05:51:16.702: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.243.22 80\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
    Jan 19 05:51:16.702: INFO: stdout: ""
    Jan 19 05:51:17.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
    Jan 19 05:51:17.699: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.243.22 80\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
    Jan 19 05:51:17.699: INFO: stdout: ""
    Jan 19 05:51:18.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
    Jan 19 05:51:18.697: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.243.22 80\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
    Jan 19 05:51:18.697: INFO: stdout: ""
    Jan 19 05:51:19.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
    Jan 19 05:51:19.699: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.243.22 80\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
    Jan 19 05:51:19.699: INFO: stdout: ""
    Jan 19 05:51:20.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
    Jan 19 05:51:20.723: INFO: stderr: "+ echo+  hostName\nnc -v -t -w 2 10.254.243.22 80\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
    Jan 19 05:51:20.723: INFO: stdout: ""
    Jan 19 05:51:21.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
    Jan 19 05:51:21.704: INFO: stderr: "+ nc -v -t -w 2 10.254.243.22 80\n+ echo hostName\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
    Jan 19 05:51:21.704: INFO: stdout: ""
    Jan 19 05:51:22.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
    Jan 19 05:51:22.702: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.243.22 80\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
    Jan 19 05:51:22.702: INFO: stdout: ""
    Jan 19 05:51:23.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
    Jan 19 05:51:23.700: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.243.22 80\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
    Jan 19 05:51:23.700: INFO: stdout: ""
    Jan 19 05:51:24.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
    Jan 19 05:51:24.704: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.243.22 80\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
    Jan 19 05:51:24.704: INFO: stdout: ""
    Jan 19 05:51:25.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
    Jan 19 05:51:25.699: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.243.22 80\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
    Jan 19 05:51:25.699: INFO: stdout: ""
    Jan 19 05:51:26.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
    Jan 19 05:51:26.705: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.243.22 80\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
    Jan 19 05:51:26.705: INFO: stdout: ""
    Jan 19 05:51:27.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.243.22 80'
    Jan 19 05:51:27.700: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.243.22 80\nConnection to 10.254.243.22 80 port [tcp/http] succeeded!\n"
    Jan 19 05:51:27.700: INFO: stdout: "nodeport-test-2fthk"
    Jan 19 05:51:27.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.0.99 31392'
    Jan 19 05:51:27.856: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.0.99 31392\nConnection to 192.168.0.99 31392 port [tcp/*] succeeded!\n"
    Jan 19 05:51:27.856: INFO: stdout: "nodeport-test-gw5gp"
    Jan 19 05:51:27.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.0.78 31392'
    Jan 19 05:51:28.024: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.0.78 31392\nConnection to 192.168.0.78 31392 port [tcp/*] succeeded!\n"
    Jan 19 05:51:28.024: INFO: stdout: ""
    Jan 19 05:51:29.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-3700 exec execpod4cpgr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.0.78 31392'
    Jan 19 05:51:29.187: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.0.78 31392\nConnection to 192.168.0.78 31392 port [tcp/*] succeeded!\n"
    Jan 19 05:51:29.187: INFO: stdout: "nodeport-test-gw5gp"
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 19 05:51:29.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3700" for this suite. 01/19/23 05:51:29.191
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:51:29.196
Jan 19 05:51:29.196: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename services 01/19/23 05:51:29.197
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:51:29.207
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:51:29.209
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
STEP: creating service in namespace services-9100 01/19/23 05:51:29.211
STEP: creating service affinity-clusterip-transition in namespace services-9100 01/19/23 05:51:29.212
STEP: creating replication controller affinity-clusterip-transition in namespace services-9100 01/19/23 05:51:29.22
I0119 05:51:29.226789      18 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-9100, replica count: 3
I0119 05:51:32.277876      18 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 19 05:51:32.284: INFO: Creating new exec pod
Jan 19 05:51:32.289: INFO: Waiting up to 5m0s for pod "execpod-affinity9jbl2" in namespace "services-9100" to be "running"
Jan 19 05:51:32.292: INFO: Pod "execpod-affinity9jbl2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.02778ms
Jan 19 05:51:34.296: INFO: Pod "execpod-affinity9jbl2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007250329s
Jan 19 05:51:36.296: INFO: Pod "execpod-affinity9jbl2": Phase="Running", Reason="", readiness=true. Elapsed: 4.006974798s
Jan 19 05:51:36.296: INFO: Pod "execpod-affinity9jbl2" satisfied condition "running"
Jan 19 05:51:37.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-9100 exec execpod-affinity9jbl2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Jan 19 05:51:37.453: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jan 19 05:51:37.453: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 05:51:37.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-9100 exec execpod-affinity9jbl2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.1.67 80'
Jan 19 05:51:37.612: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.1.67 80\nConnection to 10.254.1.67 80 port [tcp/http] succeeded!\n"
Jan 19 05:51:37.612: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 05:51:37.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-9100 exec execpod-affinity9jbl2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.1.67:80/ ; done'
Jan 19 05:51:37.859: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n"
Jan 19 05:51:37.859: INFO: stdout: "\naffinity-clusterip-transition-bkqfq\naffinity-clusterip-transition-rdzjt\naffinity-clusterip-transition-rdzjt\naffinity-clusterip-transition-kxj5n\naffinity-clusterip-transition-bkqfq\naffinity-clusterip-transition-kxj5n\naffinity-clusterip-transition-bkqfq\naffinity-clusterip-transition-rdzjt\naffinity-clusterip-transition-kxj5n\naffinity-clusterip-transition-rdzjt\naffinity-clusterip-transition-kxj5n\naffinity-clusterip-transition-bkqfq\naffinity-clusterip-transition-rdzjt\naffinity-clusterip-transition-bkqfq\naffinity-clusterip-transition-bkqfq\naffinity-clusterip-transition-kxj5n"
Jan 19 05:51:37.859: INFO: Received response from host: affinity-clusterip-transition-bkqfq
Jan 19 05:51:37.860: INFO: Received response from host: affinity-clusterip-transition-rdzjt
Jan 19 05:51:37.860: INFO: Received response from host: affinity-clusterip-transition-rdzjt
Jan 19 05:51:37.860: INFO: Received response from host: affinity-clusterip-transition-kxj5n
Jan 19 05:51:37.860: INFO: Received response from host: affinity-clusterip-transition-bkqfq
Jan 19 05:51:37.860: INFO: Received response from host: affinity-clusterip-transition-kxj5n
Jan 19 05:51:37.860: INFO: Received response from host: affinity-clusterip-transition-bkqfq
Jan 19 05:51:37.860: INFO: Received response from host: affinity-clusterip-transition-rdzjt
Jan 19 05:51:37.860: INFO: Received response from host: affinity-clusterip-transition-kxj5n
Jan 19 05:51:37.860: INFO: Received response from host: affinity-clusterip-transition-rdzjt
Jan 19 05:51:37.860: INFO: Received response from host: affinity-clusterip-transition-kxj5n
Jan 19 05:51:37.860: INFO: Received response from host: affinity-clusterip-transition-bkqfq
Jan 19 05:51:37.860: INFO: Received response from host: affinity-clusterip-transition-rdzjt
Jan 19 05:51:37.860: INFO: Received response from host: affinity-clusterip-transition-bkqfq
Jan 19 05:51:37.860: INFO: Received response from host: affinity-clusterip-transition-bkqfq
Jan 19 05:51:37.860: INFO: Received response from host: affinity-clusterip-transition-kxj5n
Jan 19 05:51:37.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-9100 exec execpod-affinity9jbl2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.1.67:80/ ; done'
Jan 19 05:51:38.099: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n"
Jan 19 05:51:38.099: INFO: stdout: "\naffinity-clusterip-transition-bkqfq\naffinity-clusterip-transition-bkqfq\naffinity-clusterip-transition-bkqfq\naffinity-clusterip-transition-bkqfq\naffinity-clusterip-transition-bkqfq\naffinity-clusterip-transition-bkqfq\naffinity-clusterip-transition-bkqfq\naffinity-clusterip-transition-bkqfq\naffinity-clusterip-transition-bkqfq\naffinity-clusterip-transition-bkqfq\naffinity-clusterip-transition-bkqfq\naffinity-clusterip-transition-bkqfq\naffinity-clusterip-transition-bkqfq\naffinity-clusterip-transition-bkqfq\naffinity-clusterip-transition-bkqfq\naffinity-clusterip-transition-bkqfq"
Jan 19 05:51:38.099: INFO: Received response from host: affinity-clusterip-transition-bkqfq
Jan 19 05:51:38.099: INFO: Received response from host: affinity-clusterip-transition-bkqfq
Jan 19 05:51:38.099: INFO: Received response from host: affinity-clusterip-transition-bkqfq
Jan 19 05:51:38.099: INFO: Received response from host: affinity-clusterip-transition-bkqfq
Jan 19 05:51:38.099: INFO: Received response from host: affinity-clusterip-transition-bkqfq
Jan 19 05:51:38.099: INFO: Received response from host: affinity-clusterip-transition-bkqfq
Jan 19 05:51:38.099: INFO: Received response from host: affinity-clusterip-transition-bkqfq
Jan 19 05:51:38.099: INFO: Received response from host: affinity-clusterip-transition-bkqfq
Jan 19 05:51:38.099: INFO: Received response from host: affinity-clusterip-transition-bkqfq
Jan 19 05:51:38.099: INFO: Received response from host: affinity-clusterip-transition-bkqfq
Jan 19 05:51:38.099: INFO: Received response from host: affinity-clusterip-transition-bkqfq
Jan 19 05:51:38.099: INFO: Received response from host: affinity-clusterip-transition-bkqfq
Jan 19 05:51:38.099: INFO: Received response from host: affinity-clusterip-transition-bkqfq
Jan 19 05:51:38.099: INFO: Received response from host: affinity-clusterip-transition-bkqfq
Jan 19 05:51:38.099: INFO: Received response from host: affinity-clusterip-transition-bkqfq
Jan 19 05:51:38.099: INFO: Received response from host: affinity-clusterip-transition-bkqfq
Jan 19 05:51:38.099: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-9100, will wait for the garbage collector to delete the pods 01/19/23 05:51:38.112
Jan 19 05:51:38.173: INFO: Deleting ReplicationController affinity-clusterip-transition took: 7.327079ms
Jan 19 05:51:38.274: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.058622ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 19 05:51:40.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9100" for this suite. 01/19/23 05:51:40.292
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","completed":313,"skipped":5711,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.102 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:51:29.196
    Jan 19 05:51:29.196: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename services 01/19/23 05:51:29.197
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:51:29.207
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:51:29.209
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2189
    STEP: creating service in namespace services-9100 01/19/23 05:51:29.211
    STEP: creating service affinity-clusterip-transition in namespace services-9100 01/19/23 05:51:29.212
    STEP: creating replication controller affinity-clusterip-transition in namespace services-9100 01/19/23 05:51:29.22
    I0119 05:51:29.226789      18 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-9100, replica count: 3
    I0119 05:51:32.277876      18 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 19 05:51:32.284: INFO: Creating new exec pod
    Jan 19 05:51:32.289: INFO: Waiting up to 5m0s for pod "execpod-affinity9jbl2" in namespace "services-9100" to be "running"
    Jan 19 05:51:32.292: INFO: Pod "execpod-affinity9jbl2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.02778ms
    Jan 19 05:51:34.296: INFO: Pod "execpod-affinity9jbl2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007250329s
    Jan 19 05:51:36.296: INFO: Pod "execpod-affinity9jbl2": Phase="Running", Reason="", readiness=true. Elapsed: 4.006974798s
    Jan 19 05:51:36.296: INFO: Pod "execpod-affinity9jbl2" satisfied condition "running"
    Jan 19 05:51:37.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-9100 exec execpod-affinity9jbl2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
    Jan 19 05:51:37.453: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Jan 19 05:51:37.453: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 19 05:51:37.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-9100 exec execpod-affinity9jbl2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.1.67 80'
    Jan 19 05:51:37.612: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.1.67 80\nConnection to 10.254.1.67 80 port [tcp/http] succeeded!\n"
    Jan 19 05:51:37.612: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 19 05:51:37.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-9100 exec execpod-affinity9jbl2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.1.67:80/ ; done'
    Jan 19 05:51:37.859: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n"
    Jan 19 05:51:37.859: INFO: stdout: "\naffinity-clusterip-transition-bkqfq\naffinity-clusterip-transition-rdzjt\naffinity-clusterip-transition-rdzjt\naffinity-clusterip-transition-kxj5n\naffinity-clusterip-transition-bkqfq\naffinity-clusterip-transition-kxj5n\naffinity-clusterip-transition-bkqfq\naffinity-clusterip-transition-rdzjt\naffinity-clusterip-transition-kxj5n\naffinity-clusterip-transition-rdzjt\naffinity-clusterip-transition-kxj5n\naffinity-clusterip-transition-bkqfq\naffinity-clusterip-transition-rdzjt\naffinity-clusterip-transition-bkqfq\naffinity-clusterip-transition-bkqfq\naffinity-clusterip-transition-kxj5n"
    Jan 19 05:51:37.859: INFO: Received response from host: affinity-clusterip-transition-bkqfq
    Jan 19 05:51:37.860: INFO: Received response from host: affinity-clusterip-transition-rdzjt
    Jan 19 05:51:37.860: INFO: Received response from host: affinity-clusterip-transition-rdzjt
    Jan 19 05:51:37.860: INFO: Received response from host: affinity-clusterip-transition-kxj5n
    Jan 19 05:51:37.860: INFO: Received response from host: affinity-clusterip-transition-bkqfq
    Jan 19 05:51:37.860: INFO: Received response from host: affinity-clusterip-transition-kxj5n
    Jan 19 05:51:37.860: INFO: Received response from host: affinity-clusterip-transition-bkqfq
    Jan 19 05:51:37.860: INFO: Received response from host: affinity-clusterip-transition-rdzjt
    Jan 19 05:51:37.860: INFO: Received response from host: affinity-clusterip-transition-kxj5n
    Jan 19 05:51:37.860: INFO: Received response from host: affinity-clusterip-transition-rdzjt
    Jan 19 05:51:37.860: INFO: Received response from host: affinity-clusterip-transition-kxj5n
    Jan 19 05:51:37.860: INFO: Received response from host: affinity-clusterip-transition-bkqfq
    Jan 19 05:51:37.860: INFO: Received response from host: affinity-clusterip-transition-rdzjt
    Jan 19 05:51:37.860: INFO: Received response from host: affinity-clusterip-transition-bkqfq
    Jan 19 05:51:37.860: INFO: Received response from host: affinity-clusterip-transition-bkqfq
    Jan 19 05:51:37.860: INFO: Received response from host: affinity-clusterip-transition-kxj5n
    Jan 19 05:51:37.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-9100 exec execpod-affinity9jbl2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.1.67:80/ ; done'
    Jan 19 05:51:38.099: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.67:80/\n"
    Jan 19 05:51:38.099: INFO: stdout: "\naffinity-clusterip-transition-bkqfq\naffinity-clusterip-transition-bkqfq\naffinity-clusterip-transition-bkqfq\naffinity-clusterip-transition-bkqfq\naffinity-clusterip-transition-bkqfq\naffinity-clusterip-transition-bkqfq\naffinity-clusterip-transition-bkqfq\naffinity-clusterip-transition-bkqfq\naffinity-clusterip-transition-bkqfq\naffinity-clusterip-transition-bkqfq\naffinity-clusterip-transition-bkqfq\naffinity-clusterip-transition-bkqfq\naffinity-clusterip-transition-bkqfq\naffinity-clusterip-transition-bkqfq\naffinity-clusterip-transition-bkqfq\naffinity-clusterip-transition-bkqfq"
    Jan 19 05:51:38.099: INFO: Received response from host: affinity-clusterip-transition-bkqfq
    Jan 19 05:51:38.099: INFO: Received response from host: affinity-clusterip-transition-bkqfq
    Jan 19 05:51:38.099: INFO: Received response from host: affinity-clusterip-transition-bkqfq
    Jan 19 05:51:38.099: INFO: Received response from host: affinity-clusterip-transition-bkqfq
    Jan 19 05:51:38.099: INFO: Received response from host: affinity-clusterip-transition-bkqfq
    Jan 19 05:51:38.099: INFO: Received response from host: affinity-clusterip-transition-bkqfq
    Jan 19 05:51:38.099: INFO: Received response from host: affinity-clusterip-transition-bkqfq
    Jan 19 05:51:38.099: INFO: Received response from host: affinity-clusterip-transition-bkqfq
    Jan 19 05:51:38.099: INFO: Received response from host: affinity-clusterip-transition-bkqfq
    Jan 19 05:51:38.099: INFO: Received response from host: affinity-clusterip-transition-bkqfq
    Jan 19 05:51:38.099: INFO: Received response from host: affinity-clusterip-transition-bkqfq
    Jan 19 05:51:38.099: INFO: Received response from host: affinity-clusterip-transition-bkqfq
    Jan 19 05:51:38.099: INFO: Received response from host: affinity-clusterip-transition-bkqfq
    Jan 19 05:51:38.099: INFO: Received response from host: affinity-clusterip-transition-bkqfq
    Jan 19 05:51:38.099: INFO: Received response from host: affinity-clusterip-transition-bkqfq
    Jan 19 05:51:38.099: INFO: Received response from host: affinity-clusterip-transition-bkqfq
    Jan 19 05:51:38.099: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-9100, will wait for the garbage collector to delete the pods 01/19/23 05:51:38.112
    Jan 19 05:51:38.173: INFO: Deleting ReplicationController affinity-clusterip-transition took: 7.327079ms
    Jan 19 05:51:38.274: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.058622ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 19 05:51:40.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9100" for this suite. 01/19/23 05:51:40.292
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:51:40.299
Jan 19 05:51:40.299: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename projected 01/19/23 05:51:40.3
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:51:40.314
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:51:40.318
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
STEP: Creating configMap with name projected-configmap-test-volume-map-7d4a5531-3afe-4597-9e88-370229107c1a 01/19/23 05:51:40.32
STEP: Creating a pod to test consume configMaps 01/19/23 05:51:40.324
Jan 19 05:51:40.332: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a78935ab-8211-4bea-bec0-4f54070afe0a" in namespace "projected-3753" to be "Succeeded or Failed"
Jan 19 05:51:40.334: INFO: Pod "pod-projected-configmaps-a78935ab-8211-4bea-bec0-4f54070afe0a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.472822ms
Jan 19 05:51:42.341: INFO: Pod "pod-projected-configmaps-a78935ab-8211-4bea-bec0-4f54070afe0a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008622072s
Jan 19 05:51:44.339: INFO: Pod "pod-projected-configmaps-a78935ab-8211-4bea-bec0-4f54070afe0a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006573279s
Jan 19 05:51:46.338: INFO: Pod "pod-projected-configmaps-a78935ab-8211-4bea-bec0-4f54070afe0a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00624752s
STEP: Saw pod success 01/19/23 05:51:46.338
Jan 19 05:51:46.338: INFO: Pod "pod-projected-configmaps-a78935ab-8211-4bea-bec0-4f54070afe0a" satisfied condition "Succeeded or Failed"
Jan 19 05:51:46.341: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-projected-configmaps-a78935ab-8211-4bea-bec0-4f54070afe0a container agnhost-container: <nil>
STEP: delete the pod 01/19/23 05:51:46.373
Jan 19 05:51:46.383: INFO: Waiting for pod pod-projected-configmaps-a78935ab-8211-4bea-bec0-4f54070afe0a to disappear
Jan 19 05:51:46.386: INFO: Pod pod-projected-configmaps-a78935ab-8211-4bea-bec0-4f54070afe0a no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 19 05:51:46.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3753" for this suite. 01/19/23 05:51:46.388
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":314,"skipped":5715,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.094 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:51:40.299
    Jan 19 05:51:40.299: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename projected 01/19/23 05:51:40.3
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:51:40.314
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:51:40.318
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:108
    STEP: Creating configMap with name projected-configmap-test-volume-map-7d4a5531-3afe-4597-9e88-370229107c1a 01/19/23 05:51:40.32
    STEP: Creating a pod to test consume configMaps 01/19/23 05:51:40.324
    Jan 19 05:51:40.332: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a78935ab-8211-4bea-bec0-4f54070afe0a" in namespace "projected-3753" to be "Succeeded or Failed"
    Jan 19 05:51:40.334: INFO: Pod "pod-projected-configmaps-a78935ab-8211-4bea-bec0-4f54070afe0a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.472822ms
    Jan 19 05:51:42.341: INFO: Pod "pod-projected-configmaps-a78935ab-8211-4bea-bec0-4f54070afe0a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008622072s
    Jan 19 05:51:44.339: INFO: Pod "pod-projected-configmaps-a78935ab-8211-4bea-bec0-4f54070afe0a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006573279s
    Jan 19 05:51:46.338: INFO: Pod "pod-projected-configmaps-a78935ab-8211-4bea-bec0-4f54070afe0a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00624752s
    STEP: Saw pod success 01/19/23 05:51:46.338
    Jan 19 05:51:46.338: INFO: Pod "pod-projected-configmaps-a78935ab-8211-4bea-bec0-4f54070afe0a" satisfied condition "Succeeded or Failed"
    Jan 19 05:51:46.341: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-projected-configmaps-a78935ab-8211-4bea-bec0-4f54070afe0a container agnhost-container: <nil>
    STEP: delete the pod 01/19/23 05:51:46.373
    Jan 19 05:51:46.383: INFO: Waiting for pod pod-projected-configmaps-a78935ab-8211-4bea-bec0-4f54070afe0a to disappear
    Jan 19 05:51:46.386: INFO: Pod pod-projected-configmaps-a78935ab-8211-4bea-bec0-4f54070afe0a no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 19 05:51:46.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3753" for this suite. 01/19/23 05:51:46.388
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:51:46.394
Jan 19 05:51:46.394: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename dns 01/19/23 05:51:46.394
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:51:46.406
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:51:46.408
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 01/19/23 05:51:46.41
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2324 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2324;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2324 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2324;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2324.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2324.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2324.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2324.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2324.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2324.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2324.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2324.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2324.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2324.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2324.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2324.svc;check="$$(dig +notcp +noall +answer +search 187.132.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.132.187_udp@PTR;check="$$(dig +tcp +noall +answer +search 187.132.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.132.187_tcp@PTR;sleep 1; done
 01/19/23 05:51:46.425
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2324 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2324;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2324 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2324;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2324.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2324.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2324.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2324.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2324.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2324.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2324.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2324.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2324.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2324.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2324.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2324.svc;check="$$(dig +notcp +noall +answer +search 187.132.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.132.187_udp@PTR;check="$$(dig +tcp +noall +answer +search 187.132.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.132.187_tcp@PTR;sleep 1; done
 01/19/23 05:51:46.425
STEP: creating a pod to probe DNS 01/19/23 05:51:46.425
STEP: submitting the pod to kubernetes 01/19/23 05:51:46.425
Jan 19 05:51:46.437: INFO: Waiting up to 15m0s for pod "dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40" in namespace "dns-2324" to be "running"
Jan 19 05:51:46.440: INFO: Pod "dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40": Phase="Pending", Reason="", readiness=false. Elapsed: 3.091639ms
Jan 19 05:51:48.445: INFO: Pod "dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007972315s
Jan 19 05:51:50.445: INFO: Pod "dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40": Phase="Running", Reason="", readiness=true. Elapsed: 4.007665764s
Jan 19 05:51:50.445: INFO: Pod "dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40" satisfied condition "running"
STEP: retrieving the pod 01/19/23 05:51:50.445
STEP: looking for the results for each expected name from probers 01/19/23 05:51:50.448
Jan 19 05:51:50.452: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40: the server could not find the requested resource (get pods dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40)
Jan 19 05:51:50.454: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40: the server could not find the requested resource (get pods dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40)
Jan 19 05:51:50.457: INFO: Unable to read wheezy_udp@dns-test-service.dns-2324 from pod dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40: the server could not find the requested resource (get pods dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40)
Jan 19 05:51:50.460: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2324 from pod dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40: the server could not find the requested resource (get pods dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40)
Jan 19 05:51:50.462: INFO: Unable to read wheezy_udp@dns-test-service.dns-2324.svc from pod dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40: the server could not find the requested resource (get pods dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40)
Jan 19 05:51:50.465: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2324.svc from pod dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40: the server could not find the requested resource (get pods dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40)
Jan 19 05:51:50.468: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2324.svc from pod dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40: the server could not find the requested resource (get pods dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40)
Jan 19 05:51:50.470: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2324.svc from pod dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40: the server could not find the requested resource (get pods dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40)
Jan 19 05:51:50.472: INFO: Unable to read wheezy_udp@_http._tcp.test-service-2.dns-2324.svc from pod dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40: the server could not find the requested resource (get pods dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40)
Jan 19 05:51:50.474: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.dns-2324.svc from pod dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40: the server could not find the requested resource (get pods dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40)
Jan 19 05:51:50.477: INFO: Unable to read 10.254.132.187_udp@PTR from pod dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40: the server could not find the requested resource (get pods dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40)
Jan 19 05:51:50.479: INFO: Unable to read 10.254.132.187_tcp@PTR from pod dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40: the server could not find the requested resource (get pods dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40)
Jan 19 05:51:50.481: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40: the server could not find the requested resource (get pods dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40)
Jan 19 05:51:50.484: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40: the server could not find the requested resource (get pods dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40)
Jan 19 05:51:50.486: INFO: Unable to read jessie_udp@dns-test-service.dns-2324 from pod dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40: the server could not find the requested resource (get pods dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40)
Jan 19 05:51:50.488: INFO: Unable to read jessie_tcp@dns-test-service.dns-2324 from pod dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40: the server could not find the requested resource (get pods dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40)
Jan 19 05:51:50.492: INFO: Unable to read jessie_udp@dns-test-service.dns-2324.svc from pod dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40: the server could not find the requested resource (get pods dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40)
Jan 19 05:51:50.500: INFO: Unable to read jessie_tcp@dns-test-service.dns-2324.svc from pod dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40: the server could not find the requested resource (get pods dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40)
Jan 19 05:51:50.504: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2324.svc from pod dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40: the server could not find the requested resource (get pods dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40)
Jan 19 05:51:50.507: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2324.svc from pod dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40: the server could not find the requested resource (get pods dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40)
Jan 19 05:51:50.510: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.dns-2324.svc from pod dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40: the server could not find the requested resource (get pods dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40)
Jan 19 05:51:50.513: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.dns-2324.svc from pod dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40: the server could not find the requested resource (get pods dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40)
Jan 19 05:51:50.516: INFO: Unable to read 10.254.132.187_udp@PTR from pod dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40: the server could not find the requested resource (get pods dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40)
Jan 19 05:51:50.519: INFO: Unable to read 10.254.132.187_tcp@PTR from pod dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40: the server could not find the requested resource (get pods dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40)
Jan 19 05:51:50.519: INFO: Lookups using dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2324 wheezy_tcp@dns-test-service.dns-2324 wheezy_udp@dns-test-service.dns-2324.svc wheezy_tcp@dns-test-service.dns-2324.svc wheezy_udp@_http._tcp.dns-test-service.dns-2324.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2324.svc wheezy_udp@_http._tcp.test-service-2.dns-2324.svc wheezy_tcp@_http._tcp.test-service-2.dns-2324.svc 10.254.132.187_udp@PTR 10.254.132.187_tcp@PTR jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2324 jessie_tcp@dns-test-service.dns-2324 jessie_udp@dns-test-service.dns-2324.svc jessie_tcp@dns-test-service.dns-2324.svc jessie_udp@_http._tcp.dns-test-service.dns-2324.svc jessie_tcp@_http._tcp.dns-test-service.dns-2324.svc jessie_udp@_http._tcp.test-service-2.dns-2324.svc jessie_tcp@_http._tcp.test-service-2.dns-2324.svc 10.254.132.187_udp@PTR 10.254.132.187_tcp@PTR]

Jan 19 05:51:55.607: INFO: DNS probes using dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40 succeeded

STEP: deleting the pod 01/19/23 05:51:55.607
STEP: deleting the test service 01/19/23 05:51:55.627
STEP: deleting the test headless service 01/19/23 05:51:55.649
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 19 05:51:55.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2324" for this suite. 01/19/23 05:51:55.667
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","completed":315,"skipped":5747,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.278 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:51:46.394
    Jan 19 05:51:46.394: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename dns 01/19/23 05:51:46.394
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:51:46.406
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:51:46.408
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 01/19/23 05:51:46.41
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2324 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2324;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2324 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2324;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2324.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2324.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2324.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2324.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2324.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2324.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2324.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2324.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2324.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2324.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2324.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2324.svc;check="$$(dig +notcp +noall +answer +search 187.132.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.132.187_udp@PTR;check="$$(dig +tcp +noall +answer +search 187.132.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.132.187_tcp@PTR;sleep 1; done
     01/19/23 05:51:46.425
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2324 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2324;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2324 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2324;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2324.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2324.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2324.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2324.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2324.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2324.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2324.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2324.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2324.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2324.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2324.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2324.svc;check="$$(dig +notcp +noall +answer +search 187.132.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.132.187_udp@PTR;check="$$(dig +tcp +noall +answer +search 187.132.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.132.187_tcp@PTR;sleep 1; done
     01/19/23 05:51:46.425
    STEP: creating a pod to probe DNS 01/19/23 05:51:46.425
    STEP: submitting the pod to kubernetes 01/19/23 05:51:46.425
    Jan 19 05:51:46.437: INFO: Waiting up to 15m0s for pod "dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40" in namespace "dns-2324" to be "running"
    Jan 19 05:51:46.440: INFO: Pod "dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40": Phase="Pending", Reason="", readiness=false. Elapsed: 3.091639ms
    Jan 19 05:51:48.445: INFO: Pod "dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007972315s
    Jan 19 05:51:50.445: INFO: Pod "dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40": Phase="Running", Reason="", readiness=true. Elapsed: 4.007665764s
    Jan 19 05:51:50.445: INFO: Pod "dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40" satisfied condition "running"
    STEP: retrieving the pod 01/19/23 05:51:50.445
    STEP: looking for the results for each expected name from probers 01/19/23 05:51:50.448
    Jan 19 05:51:50.452: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40: the server could not find the requested resource (get pods dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40)
    Jan 19 05:51:50.454: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40: the server could not find the requested resource (get pods dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40)
    Jan 19 05:51:50.457: INFO: Unable to read wheezy_udp@dns-test-service.dns-2324 from pod dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40: the server could not find the requested resource (get pods dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40)
    Jan 19 05:51:50.460: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2324 from pod dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40: the server could not find the requested resource (get pods dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40)
    Jan 19 05:51:50.462: INFO: Unable to read wheezy_udp@dns-test-service.dns-2324.svc from pod dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40: the server could not find the requested resource (get pods dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40)
    Jan 19 05:51:50.465: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2324.svc from pod dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40: the server could not find the requested resource (get pods dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40)
    Jan 19 05:51:50.468: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2324.svc from pod dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40: the server could not find the requested resource (get pods dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40)
    Jan 19 05:51:50.470: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2324.svc from pod dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40: the server could not find the requested resource (get pods dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40)
    Jan 19 05:51:50.472: INFO: Unable to read wheezy_udp@_http._tcp.test-service-2.dns-2324.svc from pod dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40: the server could not find the requested resource (get pods dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40)
    Jan 19 05:51:50.474: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.dns-2324.svc from pod dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40: the server could not find the requested resource (get pods dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40)
    Jan 19 05:51:50.477: INFO: Unable to read 10.254.132.187_udp@PTR from pod dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40: the server could not find the requested resource (get pods dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40)
    Jan 19 05:51:50.479: INFO: Unable to read 10.254.132.187_tcp@PTR from pod dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40: the server could not find the requested resource (get pods dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40)
    Jan 19 05:51:50.481: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40: the server could not find the requested resource (get pods dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40)
    Jan 19 05:51:50.484: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40: the server could not find the requested resource (get pods dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40)
    Jan 19 05:51:50.486: INFO: Unable to read jessie_udp@dns-test-service.dns-2324 from pod dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40: the server could not find the requested resource (get pods dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40)
    Jan 19 05:51:50.488: INFO: Unable to read jessie_tcp@dns-test-service.dns-2324 from pod dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40: the server could not find the requested resource (get pods dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40)
    Jan 19 05:51:50.492: INFO: Unable to read jessie_udp@dns-test-service.dns-2324.svc from pod dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40: the server could not find the requested resource (get pods dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40)
    Jan 19 05:51:50.500: INFO: Unable to read jessie_tcp@dns-test-service.dns-2324.svc from pod dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40: the server could not find the requested resource (get pods dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40)
    Jan 19 05:51:50.504: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2324.svc from pod dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40: the server could not find the requested resource (get pods dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40)
    Jan 19 05:51:50.507: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2324.svc from pod dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40: the server could not find the requested resource (get pods dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40)
    Jan 19 05:51:50.510: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.dns-2324.svc from pod dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40: the server could not find the requested resource (get pods dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40)
    Jan 19 05:51:50.513: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.dns-2324.svc from pod dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40: the server could not find the requested resource (get pods dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40)
    Jan 19 05:51:50.516: INFO: Unable to read 10.254.132.187_udp@PTR from pod dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40: the server could not find the requested resource (get pods dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40)
    Jan 19 05:51:50.519: INFO: Unable to read 10.254.132.187_tcp@PTR from pod dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40: the server could not find the requested resource (get pods dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40)
    Jan 19 05:51:50.519: INFO: Lookups using dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2324 wheezy_tcp@dns-test-service.dns-2324 wheezy_udp@dns-test-service.dns-2324.svc wheezy_tcp@dns-test-service.dns-2324.svc wheezy_udp@_http._tcp.dns-test-service.dns-2324.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2324.svc wheezy_udp@_http._tcp.test-service-2.dns-2324.svc wheezy_tcp@_http._tcp.test-service-2.dns-2324.svc 10.254.132.187_udp@PTR 10.254.132.187_tcp@PTR jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2324 jessie_tcp@dns-test-service.dns-2324 jessie_udp@dns-test-service.dns-2324.svc jessie_tcp@dns-test-service.dns-2324.svc jessie_udp@_http._tcp.dns-test-service.dns-2324.svc jessie_tcp@_http._tcp.dns-test-service.dns-2324.svc jessie_udp@_http._tcp.test-service-2.dns-2324.svc jessie_tcp@_http._tcp.test-service-2.dns-2324.svc 10.254.132.187_udp@PTR 10.254.132.187_tcp@PTR]

    Jan 19 05:51:55.607: INFO: DNS probes using dns-2324/dns-test-e572cac0-639b-4b5c-830c-aec3a9dc4c40 succeeded

    STEP: deleting the pod 01/19/23 05:51:55.607
    STEP: deleting the test service 01/19/23 05:51:55.627
    STEP: deleting the test headless service 01/19/23 05:51:55.649
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 19 05:51:55.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-2324" for this suite. 01/19/23 05:51:55.667
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:51:55.673
Jan 19 05:51:55.673: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename container-probe 01/19/23 05:51:55.674
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:51:55.687
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:51:55.69
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
STEP: Creating pod liveness-b8492a63-7966-4aef-b6e5-590966b7177b in namespace container-probe-2399 01/19/23 05:51:55.694
Jan 19 05:51:55.704: INFO: Waiting up to 5m0s for pod "liveness-b8492a63-7966-4aef-b6e5-590966b7177b" in namespace "container-probe-2399" to be "not pending"
Jan 19 05:51:55.708: INFO: Pod "liveness-b8492a63-7966-4aef-b6e5-590966b7177b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.363666ms
Jan 19 05:51:57.712: INFO: Pod "liveness-b8492a63-7966-4aef-b6e5-590966b7177b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008637664s
Jan 19 05:51:59.712: INFO: Pod "liveness-b8492a63-7966-4aef-b6e5-590966b7177b": Phase="Running", Reason="", readiness=true. Elapsed: 4.008537592s
Jan 19 05:51:59.712: INFO: Pod "liveness-b8492a63-7966-4aef-b6e5-590966b7177b" satisfied condition "not pending"
Jan 19 05:51:59.712: INFO: Started pod liveness-b8492a63-7966-4aef-b6e5-590966b7177b in namespace container-probe-2399
STEP: checking the pod's current state and verifying that restartCount is present 01/19/23 05:51:59.712
Jan 19 05:51:59.715: INFO: Initial restart count of pod liveness-b8492a63-7966-4aef-b6e5-590966b7177b is 0
Jan 19 05:52:17.760: INFO: Restart count of pod container-probe-2399/liveness-b8492a63-7966-4aef-b6e5-590966b7177b is now 1 (18.045205688s elapsed)
STEP: deleting the pod 01/19/23 05:52:17.76
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 19 05:52:17.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2399" for this suite. 01/19/23 05:52:17.776
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":316,"skipped":5784,"failed":0}
------------------------------
â€¢ [SLOW TEST] [22.109 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:51:55.673
    Jan 19 05:51:55.673: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename container-probe 01/19/23 05:51:55.674
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:51:55.687
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:51:55.69
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:165
    STEP: Creating pod liveness-b8492a63-7966-4aef-b6e5-590966b7177b in namespace container-probe-2399 01/19/23 05:51:55.694
    Jan 19 05:51:55.704: INFO: Waiting up to 5m0s for pod "liveness-b8492a63-7966-4aef-b6e5-590966b7177b" in namespace "container-probe-2399" to be "not pending"
    Jan 19 05:51:55.708: INFO: Pod "liveness-b8492a63-7966-4aef-b6e5-590966b7177b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.363666ms
    Jan 19 05:51:57.712: INFO: Pod "liveness-b8492a63-7966-4aef-b6e5-590966b7177b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008637664s
    Jan 19 05:51:59.712: INFO: Pod "liveness-b8492a63-7966-4aef-b6e5-590966b7177b": Phase="Running", Reason="", readiness=true. Elapsed: 4.008537592s
    Jan 19 05:51:59.712: INFO: Pod "liveness-b8492a63-7966-4aef-b6e5-590966b7177b" satisfied condition "not pending"
    Jan 19 05:51:59.712: INFO: Started pod liveness-b8492a63-7966-4aef-b6e5-590966b7177b in namespace container-probe-2399
    STEP: checking the pod's current state and verifying that restartCount is present 01/19/23 05:51:59.712
    Jan 19 05:51:59.715: INFO: Initial restart count of pod liveness-b8492a63-7966-4aef-b6e5-590966b7177b is 0
    Jan 19 05:52:17.760: INFO: Restart count of pod container-probe-2399/liveness-b8492a63-7966-4aef-b6e5-590966b7177b is now 1 (18.045205688s elapsed)
    STEP: deleting the pod 01/19/23 05:52:17.76
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 19 05:52:17.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-2399" for this suite. 01/19/23 05:52:17.776
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:52:17.783
Jan 19 05:52:17.783: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename custom-resource-definition 01/19/23 05:52:17.784
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:52:17.799
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:52:17.801
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Jan 19 05:52:17.803: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 05:52:18.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-216" for this suite. 01/19/23 05:52:18.428
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","completed":317,"skipped":5827,"failed":0}
------------------------------
â€¢ [0.651 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:52:17.783
    Jan 19 05:52:17.783: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename custom-resource-definition 01/19/23 05:52:17.784
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:52:17.799
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:52:17.801
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Jan 19 05:52:17.803: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 05:52:18.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-216" for this suite. 01/19/23 05:52:18.428
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:52:18.434
Jan 19 05:52:18.434: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename projected 01/19/23 05:52:18.435
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:52:18.448
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:52:18.45
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
STEP: Creating configMap with name projected-configmap-test-volume-map-72696cb9-eb93-410c-ad58-ce81d2f86f93 01/19/23 05:52:18.452
STEP: Creating a pod to test consume configMaps 01/19/23 05:52:18.455
Jan 19 05:52:18.461: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-37c87b6e-ea21-4076-9298-b216303e31c1" in namespace "projected-5882" to be "Succeeded or Failed"
Jan 19 05:52:18.465: INFO: Pod "pod-projected-configmaps-37c87b6e-ea21-4076-9298-b216303e31c1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.477709ms
Jan 19 05:52:20.468: INFO: Pod "pod-projected-configmaps-37c87b6e-ea21-4076-9298-b216303e31c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007199958s
Jan 19 05:52:22.469: INFO: Pod "pod-projected-configmaps-37c87b6e-ea21-4076-9298-b216303e31c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007576186s
STEP: Saw pod success 01/19/23 05:52:22.469
Jan 19 05:52:22.469: INFO: Pod "pod-projected-configmaps-37c87b6e-ea21-4076-9298-b216303e31c1" satisfied condition "Succeeded or Failed"
Jan 19 05:52:22.472: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-projected-configmaps-37c87b6e-ea21-4076-9298-b216303e31c1 container agnhost-container: <nil>
STEP: delete the pod 01/19/23 05:52:22.477
Jan 19 05:52:22.492: INFO: Waiting for pod pod-projected-configmaps-37c87b6e-ea21-4076-9298-b216303e31c1 to disappear
Jan 19 05:52:22.494: INFO: Pod pod-projected-configmaps-37c87b6e-ea21-4076-9298-b216303e31c1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 19 05:52:22.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5882" for this suite. 01/19/23 05:52:22.497
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":318,"skipped":5827,"failed":0}
------------------------------
â€¢ [4.068 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:52:18.434
    Jan 19 05:52:18.434: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename projected 01/19/23 05:52:18.435
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:52:18.448
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:52:18.45
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:98
    STEP: Creating configMap with name projected-configmap-test-volume-map-72696cb9-eb93-410c-ad58-ce81d2f86f93 01/19/23 05:52:18.452
    STEP: Creating a pod to test consume configMaps 01/19/23 05:52:18.455
    Jan 19 05:52:18.461: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-37c87b6e-ea21-4076-9298-b216303e31c1" in namespace "projected-5882" to be "Succeeded or Failed"
    Jan 19 05:52:18.465: INFO: Pod "pod-projected-configmaps-37c87b6e-ea21-4076-9298-b216303e31c1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.477709ms
    Jan 19 05:52:20.468: INFO: Pod "pod-projected-configmaps-37c87b6e-ea21-4076-9298-b216303e31c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007199958s
    Jan 19 05:52:22.469: INFO: Pod "pod-projected-configmaps-37c87b6e-ea21-4076-9298-b216303e31c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007576186s
    STEP: Saw pod success 01/19/23 05:52:22.469
    Jan 19 05:52:22.469: INFO: Pod "pod-projected-configmaps-37c87b6e-ea21-4076-9298-b216303e31c1" satisfied condition "Succeeded or Failed"
    Jan 19 05:52:22.472: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-projected-configmaps-37c87b6e-ea21-4076-9298-b216303e31c1 container agnhost-container: <nil>
    STEP: delete the pod 01/19/23 05:52:22.477
    Jan 19 05:52:22.492: INFO: Waiting for pod pod-projected-configmaps-37c87b6e-ea21-4076-9298-b216303e31c1 to disappear
    Jan 19 05:52:22.494: INFO: Pod pod-projected-configmaps-37c87b6e-ea21-4076-9298-b216303e31c1 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 19 05:52:22.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5882" for this suite. 01/19/23 05:52:22.497
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:52:22.503
Jan 19 05:52:22.503: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename downward-api 01/19/23 05:52:22.504
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:52:22.517
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:52:22.52
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
STEP: Creating a pod to test downward API volume plugin 01/19/23 05:52:22.522
Jan 19 05:52:22.532: INFO: Waiting up to 5m0s for pod "downwardapi-volume-abb709f3-892a-404a-8ec2-a9cf67091505" in namespace "downward-api-5500" to be "Succeeded or Failed"
Jan 19 05:52:22.540: INFO: Pod "downwardapi-volume-abb709f3-892a-404a-8ec2-a9cf67091505": Phase="Pending", Reason="", readiness=false. Elapsed: 8.448222ms
Jan 19 05:52:24.546: INFO: Pod "downwardapi-volume-abb709f3-892a-404a-8ec2-a9cf67091505": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013558804s
Jan 19 05:52:26.546: INFO: Pod "downwardapi-volume-abb709f3-892a-404a-8ec2-a9cf67091505": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014045141s
Jan 19 05:52:28.546: INFO: Pod "downwardapi-volume-abb709f3-892a-404a-8ec2-a9cf67091505": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014091689s
STEP: Saw pod success 01/19/23 05:52:28.546
Jan 19 05:52:28.546: INFO: Pod "downwardapi-volume-abb709f3-892a-404a-8ec2-a9cf67091505" satisfied condition "Succeeded or Failed"
Jan 19 05:52:28.550: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod downwardapi-volume-abb709f3-892a-404a-8ec2-a9cf67091505 container client-container: <nil>
STEP: delete the pod 01/19/23 05:52:28.556
Jan 19 05:52:28.568: INFO: Waiting for pod downwardapi-volume-abb709f3-892a-404a-8ec2-a9cf67091505 to disappear
Jan 19 05:52:28.571: INFO: Pod downwardapi-volume-abb709f3-892a-404a-8ec2-a9cf67091505 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 19 05:52:28.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5500" for this suite. 01/19/23 05:52:28.574
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","completed":319,"skipped":5828,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.077 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:52:22.503
    Jan 19 05:52:22.503: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename downward-api 01/19/23 05:52:22.504
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:52:22.517
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:52:22.52
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:192
    STEP: Creating a pod to test downward API volume plugin 01/19/23 05:52:22.522
    Jan 19 05:52:22.532: INFO: Waiting up to 5m0s for pod "downwardapi-volume-abb709f3-892a-404a-8ec2-a9cf67091505" in namespace "downward-api-5500" to be "Succeeded or Failed"
    Jan 19 05:52:22.540: INFO: Pod "downwardapi-volume-abb709f3-892a-404a-8ec2-a9cf67091505": Phase="Pending", Reason="", readiness=false. Elapsed: 8.448222ms
    Jan 19 05:52:24.546: INFO: Pod "downwardapi-volume-abb709f3-892a-404a-8ec2-a9cf67091505": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013558804s
    Jan 19 05:52:26.546: INFO: Pod "downwardapi-volume-abb709f3-892a-404a-8ec2-a9cf67091505": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014045141s
    Jan 19 05:52:28.546: INFO: Pod "downwardapi-volume-abb709f3-892a-404a-8ec2-a9cf67091505": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014091689s
    STEP: Saw pod success 01/19/23 05:52:28.546
    Jan 19 05:52:28.546: INFO: Pod "downwardapi-volume-abb709f3-892a-404a-8ec2-a9cf67091505" satisfied condition "Succeeded or Failed"
    Jan 19 05:52:28.550: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod downwardapi-volume-abb709f3-892a-404a-8ec2-a9cf67091505 container client-container: <nil>
    STEP: delete the pod 01/19/23 05:52:28.556
    Jan 19 05:52:28.568: INFO: Waiting for pod downwardapi-volume-abb709f3-892a-404a-8ec2-a9cf67091505 to disappear
    Jan 19 05:52:28.571: INFO: Pod downwardapi-volume-abb709f3-892a-404a-8ec2-a9cf67091505 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 19 05:52:28.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5500" for this suite. 01/19/23 05:52:28.574
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:52:28.58
Jan 19 05:52:28.580: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename webhook 01/19/23 05:52:28.581
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:52:28.593
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:52:28.597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/19/23 05:52:28.614
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 05:52:28.97
STEP: Deploying the webhook pod 01/19/23 05:52:28.978
STEP: Wait for the deployment to be ready 01/19/23 05:52:28.993
Jan 19 05:52:29.001: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 19 05:52:31.011: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 5, 52, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 52, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 52, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 52, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/19/23 05:52:33.015
STEP: Verifying the service has paired with the endpoint 01/19/23 05:52:33.025
Jan 19 05:52:34.026: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 01/19/23 05:52:34.03
STEP: create a configmap that should be updated by the webhook 01/19/23 05:52:34.046
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 05:52:34.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7774" for this suite. 01/19/23 05:52:34.065
STEP: Destroying namespace "webhook-7774-markers" for this suite. 01/19/23 05:52:34.071
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","completed":320,"skipped":5838,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.547 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:52:28.58
    Jan 19 05:52:28.580: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename webhook 01/19/23 05:52:28.581
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:52:28.593
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:52:28.597
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/19/23 05:52:28.614
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 05:52:28.97
    STEP: Deploying the webhook pod 01/19/23 05:52:28.978
    STEP: Wait for the deployment to be ready 01/19/23 05:52:28.993
    Jan 19 05:52:29.001: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 19 05:52:31.011: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 5, 52, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 52, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 5, 52, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 5, 52, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/19/23 05:52:33.015
    STEP: Verifying the service has paired with the endpoint 01/19/23 05:52:33.025
    Jan 19 05:52:34.026: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:251
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 01/19/23 05:52:34.03
    STEP: create a configmap that should be updated by the webhook 01/19/23 05:52:34.046
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 05:52:34.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7774" for this suite. 01/19/23 05:52:34.065
    STEP: Destroying namespace "webhook-7774-markers" for this suite. 01/19/23 05:52:34.071
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:52:34.127
Jan 19 05:52:34.127: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename projected 01/19/23 05:52:34.128
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:52:34.147
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:52:34.149
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
STEP: Creating projection with configMap that has name projected-configmap-test-upd-901ba437-9e97-48d4-968b-0c5cab54ffcd 01/19/23 05:52:34.154
STEP: Creating the pod 01/19/23 05:52:34.234
Jan 19 05:52:34.243: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6e8840c3-2c60-4474-8d1a-b2d7408f189a" in namespace "projected-4560" to be "running and ready"
Jan 19 05:52:34.246: INFO: Pod "pod-projected-configmaps-6e8840c3-2c60-4474-8d1a-b2d7408f189a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.236887ms
Jan 19 05:52:34.246: INFO: The phase of Pod pod-projected-configmaps-6e8840c3-2c60-4474-8d1a-b2d7408f189a is Pending, waiting for it to be Running (with Ready = true)
Jan 19 05:52:36.249: INFO: Pod "pod-projected-configmaps-6e8840c3-2c60-4474-8d1a-b2d7408f189a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005855095s
Jan 19 05:52:36.249: INFO: The phase of Pod pod-projected-configmaps-6e8840c3-2c60-4474-8d1a-b2d7408f189a is Pending, waiting for it to be Running (with Ready = true)
Jan 19 05:52:38.251: INFO: Pod "pod-projected-configmaps-6e8840c3-2c60-4474-8d1a-b2d7408f189a": Phase="Running", Reason="", readiness=true. Elapsed: 4.007387283s
Jan 19 05:52:38.251: INFO: The phase of Pod pod-projected-configmaps-6e8840c3-2c60-4474-8d1a-b2d7408f189a is Running (Ready = true)
Jan 19 05:52:38.251: INFO: Pod "pod-projected-configmaps-6e8840c3-2c60-4474-8d1a-b2d7408f189a" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-901ba437-9e97-48d4-968b-0c5cab54ffcd 01/19/23 05:52:38.258
STEP: waiting to observe update in volume 01/19/23 05:52:38.263
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 19 05:53:48.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4560" for this suite. 01/19/23 05:53:48.531
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":321,"skipped":5846,"failed":0}
------------------------------
â€¢ [SLOW TEST] [74.411 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:52:34.127
    Jan 19 05:52:34.127: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename projected 01/19/23 05:52:34.128
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:52:34.147
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:52:34.149
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:123
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-901ba437-9e97-48d4-968b-0c5cab54ffcd 01/19/23 05:52:34.154
    STEP: Creating the pod 01/19/23 05:52:34.234
    Jan 19 05:52:34.243: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6e8840c3-2c60-4474-8d1a-b2d7408f189a" in namespace "projected-4560" to be "running and ready"
    Jan 19 05:52:34.246: INFO: Pod "pod-projected-configmaps-6e8840c3-2c60-4474-8d1a-b2d7408f189a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.236887ms
    Jan 19 05:52:34.246: INFO: The phase of Pod pod-projected-configmaps-6e8840c3-2c60-4474-8d1a-b2d7408f189a is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 05:52:36.249: INFO: Pod "pod-projected-configmaps-6e8840c3-2c60-4474-8d1a-b2d7408f189a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005855095s
    Jan 19 05:52:36.249: INFO: The phase of Pod pod-projected-configmaps-6e8840c3-2c60-4474-8d1a-b2d7408f189a is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 05:52:38.251: INFO: Pod "pod-projected-configmaps-6e8840c3-2c60-4474-8d1a-b2d7408f189a": Phase="Running", Reason="", readiness=true. Elapsed: 4.007387283s
    Jan 19 05:52:38.251: INFO: The phase of Pod pod-projected-configmaps-6e8840c3-2c60-4474-8d1a-b2d7408f189a is Running (Ready = true)
    Jan 19 05:52:38.251: INFO: Pod "pod-projected-configmaps-6e8840c3-2c60-4474-8d1a-b2d7408f189a" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-901ba437-9e97-48d4-968b-0c5cab54ffcd 01/19/23 05:52:38.258
    STEP: waiting to observe update in volume 01/19/23 05:52:38.263
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 19 05:53:48.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4560" for this suite. 01/19/23 05:53:48.531
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:53:48.539
Jan 19 05:53:48.539: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename container-probe 01/19/23 05:53:48.539
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:53:48.554
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:53:48.557
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
Jan 19 05:53:48.571: INFO: Waiting up to 5m0s for pod "test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa" in namespace "container-probe-865" to be "running and ready"
Jan 19 05:53:48.576: INFO: Pod "test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.891865ms
Jan 19 05:53:48.576: INFO: The phase of Pod test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa is Pending, waiting for it to be Running (with Ready = true)
Jan 19 05:53:50.580: INFO: Pod "test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009359736s
Jan 19 05:53:50.580: INFO: The phase of Pod test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa is Pending, waiting for it to be Running (with Ready = true)
Jan 19 05:53:52.581: INFO: Pod "test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa": Phase="Running", Reason="", readiness=false. Elapsed: 4.009988847s
Jan 19 05:53:52.581: INFO: The phase of Pod test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa is Running (Ready = false)
Jan 19 05:53:54.581: INFO: Pod "test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa": Phase="Running", Reason="", readiness=false. Elapsed: 6.009886169s
Jan 19 05:53:54.581: INFO: The phase of Pod test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa is Running (Ready = false)
Jan 19 05:53:56.580: INFO: Pod "test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa": Phase="Running", Reason="", readiness=false. Elapsed: 8.009355923s
Jan 19 05:53:56.580: INFO: The phase of Pod test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa is Running (Ready = false)
Jan 19 05:53:58.581: INFO: Pod "test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa": Phase="Running", Reason="", readiness=false. Elapsed: 10.009621036s
Jan 19 05:53:58.581: INFO: The phase of Pod test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa is Running (Ready = false)
Jan 19 05:54:00.580: INFO: Pod "test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa": Phase="Running", Reason="", readiness=false. Elapsed: 12.008997763s
Jan 19 05:54:00.580: INFO: The phase of Pod test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa is Running (Ready = false)
Jan 19 05:54:02.581: INFO: Pod "test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa": Phase="Running", Reason="", readiness=false. Elapsed: 14.009668451s
Jan 19 05:54:02.581: INFO: The phase of Pod test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa is Running (Ready = false)
Jan 19 05:54:04.581: INFO: Pod "test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa": Phase="Running", Reason="", readiness=false. Elapsed: 16.009688675s
Jan 19 05:54:04.581: INFO: The phase of Pod test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa is Running (Ready = false)
Jan 19 05:54:06.582: INFO: Pod "test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa": Phase="Running", Reason="", readiness=false. Elapsed: 18.010590311s
Jan 19 05:54:06.582: INFO: The phase of Pod test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa is Running (Ready = false)
Jan 19 05:54:08.580: INFO: Pod "test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa": Phase="Running", Reason="", readiness=false. Elapsed: 20.008852203s
Jan 19 05:54:08.580: INFO: The phase of Pod test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa is Running (Ready = false)
Jan 19 05:54:10.580: INFO: Pod "test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa": Phase="Running", Reason="", readiness=true. Elapsed: 22.009368105s
Jan 19 05:54:10.580: INFO: The phase of Pod test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa is Running (Ready = true)
Jan 19 05:54:10.580: INFO: Pod "test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa" satisfied condition "running and ready"
Jan 19 05:54:10.584: INFO: Container started at 2023-01-19 05:53:50 +0000 UTC, pod became ready at 2023-01-19 05:54:08 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 19 05:54:10.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-865" for this suite. 01/19/23 05:54:10.588
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","completed":322,"skipped":5865,"failed":0}
------------------------------
â€¢ [SLOW TEST] [22.055 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:53:48.539
    Jan 19 05:53:48.539: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename container-probe 01/19/23 05:53:48.539
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:53:48.554
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:53:48.557
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:68
    Jan 19 05:53:48.571: INFO: Waiting up to 5m0s for pod "test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa" in namespace "container-probe-865" to be "running and ready"
    Jan 19 05:53:48.576: INFO: Pod "test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.891865ms
    Jan 19 05:53:48.576: INFO: The phase of Pod test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 05:53:50.580: INFO: Pod "test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009359736s
    Jan 19 05:53:50.580: INFO: The phase of Pod test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 05:53:52.581: INFO: Pod "test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa": Phase="Running", Reason="", readiness=false. Elapsed: 4.009988847s
    Jan 19 05:53:52.581: INFO: The phase of Pod test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa is Running (Ready = false)
    Jan 19 05:53:54.581: INFO: Pod "test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa": Phase="Running", Reason="", readiness=false. Elapsed: 6.009886169s
    Jan 19 05:53:54.581: INFO: The phase of Pod test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa is Running (Ready = false)
    Jan 19 05:53:56.580: INFO: Pod "test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa": Phase="Running", Reason="", readiness=false. Elapsed: 8.009355923s
    Jan 19 05:53:56.580: INFO: The phase of Pod test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa is Running (Ready = false)
    Jan 19 05:53:58.581: INFO: Pod "test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa": Phase="Running", Reason="", readiness=false. Elapsed: 10.009621036s
    Jan 19 05:53:58.581: INFO: The phase of Pod test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa is Running (Ready = false)
    Jan 19 05:54:00.580: INFO: Pod "test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa": Phase="Running", Reason="", readiness=false. Elapsed: 12.008997763s
    Jan 19 05:54:00.580: INFO: The phase of Pod test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa is Running (Ready = false)
    Jan 19 05:54:02.581: INFO: Pod "test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa": Phase="Running", Reason="", readiness=false. Elapsed: 14.009668451s
    Jan 19 05:54:02.581: INFO: The phase of Pod test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa is Running (Ready = false)
    Jan 19 05:54:04.581: INFO: Pod "test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa": Phase="Running", Reason="", readiness=false. Elapsed: 16.009688675s
    Jan 19 05:54:04.581: INFO: The phase of Pod test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa is Running (Ready = false)
    Jan 19 05:54:06.582: INFO: Pod "test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa": Phase="Running", Reason="", readiness=false. Elapsed: 18.010590311s
    Jan 19 05:54:06.582: INFO: The phase of Pod test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa is Running (Ready = false)
    Jan 19 05:54:08.580: INFO: Pod "test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa": Phase="Running", Reason="", readiness=false. Elapsed: 20.008852203s
    Jan 19 05:54:08.580: INFO: The phase of Pod test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa is Running (Ready = false)
    Jan 19 05:54:10.580: INFO: Pod "test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa": Phase="Running", Reason="", readiness=true. Elapsed: 22.009368105s
    Jan 19 05:54:10.580: INFO: The phase of Pod test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa is Running (Ready = true)
    Jan 19 05:54:10.580: INFO: Pod "test-webserver-01691f3b-7a72-4fe5-87d4-09a5deb7b8fa" satisfied condition "running and ready"
    Jan 19 05:54:10.584: INFO: Container started at 2023-01-19 05:53:50 +0000 UTC, pod became ready at 2023-01-19 05:54:08 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 19 05:54:10.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-865" for this suite. 01/19/23 05:54:10.588
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:54:10.594
Jan 19 05:54:10.594: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename pods 01/19/23 05:54:10.594
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:54:10.607
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:54:10.609
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
STEP: creating pod 01/19/23 05:54:10.612
Jan 19 05:54:10.622: INFO: Waiting up to 5m0s for pod "pod-hostip-71f4979e-7f88-4853-ae71-1d57810f5841" in namespace "pods-5395" to be "running and ready"
Jan 19 05:54:10.628: INFO: Pod "pod-hostip-71f4979e-7f88-4853-ae71-1d57810f5841": Phase="Pending", Reason="", readiness=false. Elapsed: 6.626489ms
Jan 19 05:54:10.628: INFO: The phase of Pod pod-hostip-71f4979e-7f88-4853-ae71-1d57810f5841 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 05:54:12.633: INFO: Pod "pod-hostip-71f4979e-7f88-4853-ae71-1d57810f5841": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011199942s
Jan 19 05:54:12.633: INFO: The phase of Pod pod-hostip-71f4979e-7f88-4853-ae71-1d57810f5841 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 05:54:14.632: INFO: Pod "pod-hostip-71f4979e-7f88-4853-ae71-1d57810f5841": Phase="Running", Reason="", readiness=true. Elapsed: 4.010470164s
Jan 19 05:54:14.632: INFO: The phase of Pod pod-hostip-71f4979e-7f88-4853-ae71-1d57810f5841 is Running (Ready = true)
Jan 19 05:54:14.632: INFO: Pod "pod-hostip-71f4979e-7f88-4853-ae71-1d57810f5841" satisfied condition "running and ready"
Jan 19 05:54:14.637: INFO: Pod pod-hostip-71f4979e-7f88-4853-ae71-1d57810f5841 has hostIP: 192.168.0.78
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 19 05:54:14.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5395" for this suite. 01/19/23 05:54:14.64
{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","completed":323,"skipped":5873,"failed":0}
------------------------------
â€¢ [4.053 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:54:10.594
    Jan 19 05:54:10.594: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename pods 01/19/23 05:54:10.594
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:54:10.607
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:54:10.609
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:203
    STEP: creating pod 01/19/23 05:54:10.612
    Jan 19 05:54:10.622: INFO: Waiting up to 5m0s for pod "pod-hostip-71f4979e-7f88-4853-ae71-1d57810f5841" in namespace "pods-5395" to be "running and ready"
    Jan 19 05:54:10.628: INFO: Pod "pod-hostip-71f4979e-7f88-4853-ae71-1d57810f5841": Phase="Pending", Reason="", readiness=false. Elapsed: 6.626489ms
    Jan 19 05:54:10.628: INFO: The phase of Pod pod-hostip-71f4979e-7f88-4853-ae71-1d57810f5841 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 05:54:12.633: INFO: Pod "pod-hostip-71f4979e-7f88-4853-ae71-1d57810f5841": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011199942s
    Jan 19 05:54:12.633: INFO: The phase of Pod pod-hostip-71f4979e-7f88-4853-ae71-1d57810f5841 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 05:54:14.632: INFO: Pod "pod-hostip-71f4979e-7f88-4853-ae71-1d57810f5841": Phase="Running", Reason="", readiness=true. Elapsed: 4.010470164s
    Jan 19 05:54:14.632: INFO: The phase of Pod pod-hostip-71f4979e-7f88-4853-ae71-1d57810f5841 is Running (Ready = true)
    Jan 19 05:54:14.632: INFO: Pod "pod-hostip-71f4979e-7f88-4853-ae71-1d57810f5841" satisfied condition "running and ready"
    Jan 19 05:54:14.637: INFO: Pod pod-hostip-71f4979e-7f88-4853-ae71-1d57810f5841 has hostIP: 192.168.0.78
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 19 05:54:14.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-5395" for this suite. 01/19/23 05:54:14.64
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:54:14.647
Jan 19 05:54:14.647: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename projected 01/19/23 05:54:14.648
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:54:14.661
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:54:14.664
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
STEP: Creating projection with secret that has name projected-secret-test-8a693ec3-aade-4873-bca9-ec2a70fefeac 01/19/23 05:54:14.666
STEP: Creating a pod to test consume secrets 01/19/23 05:54:14.67
Jan 19 05:54:14.678: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-97e79f23-d764-449c-9dd5-ff93a41484fe" in namespace "projected-5433" to be "Succeeded or Failed"
Jan 19 05:54:14.681: INFO: Pod "pod-projected-secrets-97e79f23-d764-449c-9dd5-ff93a41484fe": Phase="Pending", Reason="", readiness=false. Elapsed: 3.043283ms
Jan 19 05:54:16.685: INFO: Pod "pod-projected-secrets-97e79f23-d764-449c-9dd5-ff93a41484fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007768943s
Jan 19 05:54:18.685: INFO: Pod "pod-projected-secrets-97e79f23-d764-449c-9dd5-ff93a41484fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006990061s
STEP: Saw pod success 01/19/23 05:54:18.685
Jan 19 05:54:18.685: INFO: Pod "pod-projected-secrets-97e79f23-d764-449c-9dd5-ff93a41484fe" satisfied condition "Succeeded or Failed"
Jan 19 05:54:18.688: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-projected-secrets-97e79f23-d764-449c-9dd5-ff93a41484fe container projected-secret-volume-test: <nil>
STEP: delete the pod 01/19/23 05:54:18.693
Jan 19 05:54:18.703: INFO: Waiting for pod pod-projected-secrets-97e79f23-d764-449c-9dd5-ff93a41484fe to disappear
Jan 19 05:54:18.706: INFO: Pod pod-projected-secrets-97e79f23-d764-449c-9dd5-ff93a41484fe no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 19 05:54:18.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5433" for this suite. 01/19/23 05:54:18.709
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","completed":324,"skipped":5887,"failed":0}
------------------------------
â€¢ [4.068 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:54:14.647
    Jan 19 05:54:14.647: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename projected 01/19/23 05:54:14.648
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:54:14.661
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:54:14.664
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:45
    STEP: Creating projection with secret that has name projected-secret-test-8a693ec3-aade-4873-bca9-ec2a70fefeac 01/19/23 05:54:14.666
    STEP: Creating a pod to test consume secrets 01/19/23 05:54:14.67
    Jan 19 05:54:14.678: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-97e79f23-d764-449c-9dd5-ff93a41484fe" in namespace "projected-5433" to be "Succeeded or Failed"
    Jan 19 05:54:14.681: INFO: Pod "pod-projected-secrets-97e79f23-d764-449c-9dd5-ff93a41484fe": Phase="Pending", Reason="", readiness=false. Elapsed: 3.043283ms
    Jan 19 05:54:16.685: INFO: Pod "pod-projected-secrets-97e79f23-d764-449c-9dd5-ff93a41484fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007768943s
    Jan 19 05:54:18.685: INFO: Pod "pod-projected-secrets-97e79f23-d764-449c-9dd5-ff93a41484fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006990061s
    STEP: Saw pod success 01/19/23 05:54:18.685
    Jan 19 05:54:18.685: INFO: Pod "pod-projected-secrets-97e79f23-d764-449c-9dd5-ff93a41484fe" satisfied condition "Succeeded or Failed"
    Jan 19 05:54:18.688: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-projected-secrets-97e79f23-d764-449c-9dd5-ff93a41484fe container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/19/23 05:54:18.693
    Jan 19 05:54:18.703: INFO: Waiting for pod pod-projected-secrets-97e79f23-d764-449c-9dd5-ff93a41484fe to disappear
    Jan 19 05:54:18.706: INFO: Pod pod-projected-secrets-97e79f23-d764-449c-9dd5-ff93a41484fe no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 19 05:54:18.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5433" for this suite. 01/19/23 05:54:18.709
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:54:18.716
Jan 19 05:54:18.716: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename namespaces 01/19/23 05:54:18.717
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:54:18.731
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:54:18.733
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
STEP: Read namespace status 01/19/23 05:54:18.735
Jan 19 05:54:18.737: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 01/19/23 05:54:18.737
Jan 19 05:54:18.744: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 01/19/23 05:54:18.744
Jan 19 05:54:18.752: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jan 19 05:54:18.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6963" for this suite. 01/19/23 05:54:18.754
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]","completed":325,"skipped":5922,"failed":0}
------------------------------
â€¢ [0.043 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:54:18.716
    Jan 19 05:54:18.716: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename namespaces 01/19/23 05:54:18.717
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:54:18.731
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:54:18.733
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:298
    STEP: Read namespace status 01/19/23 05:54:18.735
    Jan 19 05:54:18.737: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 01/19/23 05:54:18.737
    Jan 19 05:54:18.744: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 01/19/23 05:54:18.744
    Jan 19 05:54:18.752: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jan 19 05:54:18.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-6963" for this suite. 01/19/23 05:54:18.754
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:54:18.76
Jan 19 05:54:18.760: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename containers 01/19/23 05:54:18.761
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:54:18.773
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:54:18.775
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
STEP: Creating a pod to test override command 01/19/23 05:54:18.776
Jan 19 05:54:18.783: INFO: Waiting up to 5m0s for pod "client-containers-a46178a1-e97c-4476-a9cd-ada074072b85" in namespace "containers-4867" to be "Succeeded or Failed"
Jan 19 05:54:18.790: INFO: Pod "client-containers-a46178a1-e97c-4476-a9cd-ada074072b85": Phase="Pending", Reason="", readiness=false. Elapsed: 6.54108ms
Jan 19 05:54:20.794: INFO: Pod "client-containers-a46178a1-e97c-4476-a9cd-ada074072b85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010975258s
Jan 19 05:54:22.794: INFO: Pod "client-containers-a46178a1-e97c-4476-a9cd-ada074072b85": Phase="Running", Reason="", readiness=false. Elapsed: 4.01011057s
Jan 19 05:54:24.795: INFO: Pod "client-containers-a46178a1-e97c-4476-a9cd-ada074072b85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011236256s
STEP: Saw pod success 01/19/23 05:54:24.795
Jan 19 05:54:24.795: INFO: Pod "client-containers-a46178a1-e97c-4476-a9cd-ada074072b85" satisfied condition "Succeeded or Failed"
Jan 19 05:54:24.797: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod client-containers-a46178a1-e97c-4476-a9cd-ada074072b85 container agnhost-container: <nil>
STEP: delete the pod 01/19/23 05:54:24.803
Jan 19 05:54:24.815: INFO: Waiting for pod client-containers-a46178a1-e97c-4476-a9cd-ada074072b85 to disappear
Jan 19 05:54:24.817: INFO: Pod client-containers-a46178a1-e97c-4476-a9cd-ada074072b85 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jan 19 05:54:24.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4867" for this suite. 01/19/23 05:54:24.819
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","completed":326,"skipped":5929,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.064 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:54:18.76
    Jan 19 05:54:18.760: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename containers 01/19/23 05:54:18.761
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:54:18.773
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:54:18.775
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:72
    STEP: Creating a pod to test override command 01/19/23 05:54:18.776
    Jan 19 05:54:18.783: INFO: Waiting up to 5m0s for pod "client-containers-a46178a1-e97c-4476-a9cd-ada074072b85" in namespace "containers-4867" to be "Succeeded or Failed"
    Jan 19 05:54:18.790: INFO: Pod "client-containers-a46178a1-e97c-4476-a9cd-ada074072b85": Phase="Pending", Reason="", readiness=false. Elapsed: 6.54108ms
    Jan 19 05:54:20.794: INFO: Pod "client-containers-a46178a1-e97c-4476-a9cd-ada074072b85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010975258s
    Jan 19 05:54:22.794: INFO: Pod "client-containers-a46178a1-e97c-4476-a9cd-ada074072b85": Phase="Running", Reason="", readiness=false. Elapsed: 4.01011057s
    Jan 19 05:54:24.795: INFO: Pod "client-containers-a46178a1-e97c-4476-a9cd-ada074072b85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011236256s
    STEP: Saw pod success 01/19/23 05:54:24.795
    Jan 19 05:54:24.795: INFO: Pod "client-containers-a46178a1-e97c-4476-a9cd-ada074072b85" satisfied condition "Succeeded or Failed"
    Jan 19 05:54:24.797: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod client-containers-a46178a1-e97c-4476-a9cd-ada074072b85 container agnhost-container: <nil>
    STEP: delete the pod 01/19/23 05:54:24.803
    Jan 19 05:54:24.815: INFO: Waiting for pod client-containers-a46178a1-e97c-4476-a9cd-ada074072b85 to disappear
    Jan 19 05:54:24.817: INFO: Pod client-containers-a46178a1-e97c-4476-a9cd-ada074072b85 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jan 19 05:54:24.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-4867" for this suite. 01/19/23 05:54:24.819
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:54:24.824
Jan 19 05:54:24.824: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename kubectl 01/19/23 05:54:24.825
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:54:24.84
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:54:24.846
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
Jan 19 05:54:24.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-7010 create -f -'
Jan 19 05:54:25.701: INFO: stderr: ""
Jan 19 05:54:25.701: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jan 19 05:54:25.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-7010 create -f -'
Jan 19 05:54:25.926: INFO: stderr: ""
Jan 19 05:54:25.926: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/19/23 05:54:25.926
Jan 19 05:54:26.929: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 19 05:54:26.929: INFO: Found 0 / 1
Jan 19 05:54:27.932: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 19 05:54:27.932: INFO: Found 0 / 1
Jan 19 05:54:28.930: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 19 05:54:28.930: INFO: Found 1 / 1
Jan 19 05:54:28.930: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 19 05:54:28.933: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 19 05:54:28.933: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 19 05:54:28.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-7010 describe pod agnhost-primary-4kv6m'
Jan 19 05:54:29.020: INFO: stderr: ""
Jan 19 05:54:29.020: INFO: stdout: "Name:             agnhost-primary-4kv6m\nNamespace:        kubectl-7010\nPriority:         0\nService Account:  default\nNode:             ckcp-nks-default-worker-node-1/192.168.0.78\nStart Time:       Thu, 19 Jan 2023 05:54:25 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.100.70.14\nIPs:\n  IP:           10.100.70.14\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://8876e34448ab37fd5a50ae0d4c048674e4f0525ac4df92770db430a3856782b7\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 19 Jan 2023 05:54:27 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fwhx7 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-fwhx7:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-7010/agnhost-primary-4kv6m to ckcp-nks-default-worker-node-1\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
Jan 19 05:54:29.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-7010 describe rc agnhost-primary'
Jan 19 05:54:29.106: INFO: stderr: ""
Jan 19 05:54:29.106: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-7010\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: agnhost-primary-4kv6m\n"
Jan 19 05:54:29.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-7010 describe service agnhost-primary'
Jan 19 05:54:29.195: INFO: stderr: ""
Jan 19 05:54:29.195: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-7010\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.254.114.172\nIPs:               10.254.114.172\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.100.70.14:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jan 19 05:54:29.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-7010 describe node ckcp-nks-default-worker-node-0'
Jan 19 05:54:29.301: INFO: stderr: ""
Jan 19 05:54:29.302: INFO: stdout: "Name:               ckcp-nks-default-worker-node-0\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=m2.c2m4\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=RegionOne\n                    failure-domain.beta.kubernetes.io/zone=kr-pub-a\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ckcp-nks-default-worker-node-0\n                    kubernetes.io/os=linux\n                    magnum.openstack.org/nodegroup=default-worker\n                    magnum.openstack.org/role=worker\n                    node.kubernetes.io/instance-type=m2.c2m4\n                    topology.cinder.csi.openstack.org/zone=kr-pub-a\n                    topology.kubernetes.io/region=RegionOne\n                    topology.kubernetes.io/zone=kr-pub-a\nAnnotations:        alpha.kubernetes.io/provided-node-ip: 192.168.0.99\n                    csi.volume.kubernetes.io/nodeid: {\"cinder.csi.openstack.org\":\"80c7b2ff-ed0e-4af8-baad-48a3a3f8c0f0\"}\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 19 Jan 2023 04:04:25 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ckcp-nks-default-worker-node-0\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 19 Jan 2023 05:54:22 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Thu, 19 Jan 2023 05:53:27 +0000   Thu, 19 Jan 2023 04:04:24 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Thu, 19 Jan 2023 05:53:27 +0000   Thu, 19 Jan 2023 04:04:24 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Thu, 19 Jan 2023 05:53:27 +0000   Thu, 19 Jan 2023 04:04:24 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Thu, 19 Jan 2023 05:53:27 +0000   Thu, 19 Jan 2023 04:04:35 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  192.168.0.99\n  Hostname:    ckcp-nks-default-worker-node-0\nCapacity:\n  cpu:                2\n  ephemeral-storage:  20960236Ki\n  hugepages-2Mi:      0\n  memory:             3879964Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  19316953466\n  hugepages-2Mi:      0\n  memory:             3777564Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 80c7b2ffed0e4af8baad48a3a3f8c0f0\n  System UUID:                80C7B2FF-ED0E-4AF8-BAAD-48A3A3F8C0F0\n  Boot ID:                    cff86e29-9a44-4a1f-a987-34519b55d476\n  Kernel Version:             3.10.0-1160.80.1.el7.x86_64\n  OS Image:                   CentOS Linux 7 (Core)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.9\n  Kubelet Version:            v1.25.4\n  Kube-Proxy Version:         v1.25.4\nPodCIDR:                      10.100.0.0/24\nPodCIDRs:                     10.100.0.0/24\nProviderID:                   openstack:///80c7b2ff-ed0e-4af8-baad-48a3a3f8c0f0\nNon-terminated Pods:          (13 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-flannel                kube-flannel-ds-amd64-djm45                                100m (5%)     100m (5%)   50Mi (1%)        50Mi (1%)      110m\n  kube-system                 coredns-557b76dc76-cg7b6                                   100m (5%)     0 (0%)      70Mi (1%)        170Mi (4%)     118m\n  kube-system                 coredns-557b76dc76-ncmw9                                   100m (5%)     0 (0%)      70Mi (1%)        170Mi (4%)     118m\n  kube-system                 csi-cinder-controllerplugin-0                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         118m\n  kube-system                 csi-cinder-nodeplugin-r2d6v                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         109m\n  kube-system                 dashboard-metrics-scraper-7cc7856cfb-p9hnn                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         118m\n  kube-system                 kube-dns-autoscaler-7986c68b66-q7lq5                       20m (1%)      0 (0%)      10Mi (0%)        0 (0%)         118m\n  kube-system                 kubernetes-dashboard-d7b78f849-zfz2z                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         118m\n  kube-system                 metrics-server-c494c94fc-lkqhw                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         118m\n  kube-system                 npd-428hd                                                  20m (1%)      200m (10%)  20Mi (0%)        100Mi (2%)     109m\n  kube-system                 snapshot-controller-6bd8bc5484-94b94                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         118m\n  kube-system                 snapshot-controller-6bd8bc5484-nh4pv                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         118m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-dbd8990eaea742d4-9s2t2    0 (0%)        0 (0%)      0 (0%)           0 (0%)         94m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                340m (17%)  300m (15%)\n  memory             220Mi (5%)  490Mi (13%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
Jan 19 05:54:29.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-7010 describe namespace kubectl-7010'
Jan 19 05:54:29.393: INFO: stderr: ""
Jan 19 05:54:29.393: INFO: stdout: "Name:         kubectl-7010\nLabels:       e2e-framework=kubectl\n              e2e-run=00430eb5-5905-4abe-8db7-178d376ca2a7\n              kubernetes.io/metadata.name=kubectl-7010\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 19 05:54:29.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7010" for this suite. 01/19/23 05:54:29.396
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","completed":327,"skipped":5945,"failed":0}
------------------------------
â€¢ [4.577 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1268
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1274

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:54:24.824
    Jan 19 05:54:24.824: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename kubectl 01/19/23 05:54:24.825
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:54:24.84
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:54:24.846
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1274
    Jan 19 05:54:24.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-7010 create -f -'
    Jan 19 05:54:25.701: INFO: stderr: ""
    Jan 19 05:54:25.701: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Jan 19 05:54:25.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-7010 create -f -'
    Jan 19 05:54:25.926: INFO: stderr: ""
    Jan 19 05:54:25.926: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/19/23 05:54:25.926
    Jan 19 05:54:26.929: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 19 05:54:26.929: INFO: Found 0 / 1
    Jan 19 05:54:27.932: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 19 05:54:27.932: INFO: Found 0 / 1
    Jan 19 05:54:28.930: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 19 05:54:28.930: INFO: Found 1 / 1
    Jan 19 05:54:28.930: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jan 19 05:54:28.933: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 19 05:54:28.933: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 19 05:54:28.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-7010 describe pod agnhost-primary-4kv6m'
    Jan 19 05:54:29.020: INFO: stderr: ""
    Jan 19 05:54:29.020: INFO: stdout: "Name:             agnhost-primary-4kv6m\nNamespace:        kubectl-7010\nPriority:         0\nService Account:  default\nNode:             ckcp-nks-default-worker-node-1/192.168.0.78\nStart Time:       Thu, 19 Jan 2023 05:54:25 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.100.70.14\nIPs:\n  IP:           10.100.70.14\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://8876e34448ab37fd5a50ae0d4c048674e4f0525ac4df92770db430a3856782b7\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 19 Jan 2023 05:54:27 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fwhx7 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-fwhx7:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-7010/agnhost-primary-4kv6m to ckcp-nks-default-worker-node-1\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
    Jan 19 05:54:29.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-7010 describe rc agnhost-primary'
    Jan 19 05:54:29.106: INFO: stderr: ""
    Jan 19 05:54:29.106: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-7010\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: agnhost-primary-4kv6m\n"
    Jan 19 05:54:29.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-7010 describe service agnhost-primary'
    Jan 19 05:54:29.195: INFO: stderr: ""
    Jan 19 05:54:29.195: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-7010\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.254.114.172\nIPs:               10.254.114.172\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.100.70.14:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Jan 19 05:54:29.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-7010 describe node ckcp-nks-default-worker-node-0'
    Jan 19 05:54:29.301: INFO: stderr: ""
    Jan 19 05:54:29.302: INFO: stdout: "Name:               ckcp-nks-default-worker-node-0\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=m2.c2m4\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=RegionOne\n                    failure-domain.beta.kubernetes.io/zone=kr-pub-a\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ckcp-nks-default-worker-node-0\n                    kubernetes.io/os=linux\n                    magnum.openstack.org/nodegroup=default-worker\n                    magnum.openstack.org/role=worker\n                    node.kubernetes.io/instance-type=m2.c2m4\n                    topology.cinder.csi.openstack.org/zone=kr-pub-a\n                    topology.kubernetes.io/region=RegionOne\n                    topology.kubernetes.io/zone=kr-pub-a\nAnnotations:        alpha.kubernetes.io/provided-node-ip: 192.168.0.99\n                    csi.volume.kubernetes.io/nodeid: {\"cinder.csi.openstack.org\":\"80c7b2ff-ed0e-4af8-baad-48a3a3f8c0f0\"}\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 19 Jan 2023 04:04:25 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ckcp-nks-default-worker-node-0\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 19 Jan 2023 05:54:22 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Thu, 19 Jan 2023 05:53:27 +0000   Thu, 19 Jan 2023 04:04:24 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Thu, 19 Jan 2023 05:53:27 +0000   Thu, 19 Jan 2023 04:04:24 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Thu, 19 Jan 2023 05:53:27 +0000   Thu, 19 Jan 2023 04:04:24 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Thu, 19 Jan 2023 05:53:27 +0000   Thu, 19 Jan 2023 04:04:35 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  192.168.0.99\n  Hostname:    ckcp-nks-default-worker-node-0\nCapacity:\n  cpu:                2\n  ephemeral-storage:  20960236Ki\n  hugepages-2Mi:      0\n  memory:             3879964Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  19316953466\n  hugepages-2Mi:      0\n  memory:             3777564Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 80c7b2ffed0e4af8baad48a3a3f8c0f0\n  System UUID:                80C7B2FF-ED0E-4AF8-BAAD-48A3A3F8C0F0\n  Boot ID:                    cff86e29-9a44-4a1f-a987-34519b55d476\n  Kernel Version:             3.10.0-1160.80.1.el7.x86_64\n  OS Image:                   CentOS Linux 7 (Core)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.9\n  Kubelet Version:            v1.25.4\n  Kube-Proxy Version:         v1.25.4\nPodCIDR:                      10.100.0.0/24\nPodCIDRs:                     10.100.0.0/24\nProviderID:                   openstack:///80c7b2ff-ed0e-4af8-baad-48a3a3f8c0f0\nNon-terminated Pods:          (13 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-flannel                kube-flannel-ds-amd64-djm45                                100m (5%)     100m (5%)   50Mi (1%)        50Mi (1%)      110m\n  kube-system                 coredns-557b76dc76-cg7b6                                   100m (5%)     0 (0%)      70Mi (1%)        170Mi (4%)     118m\n  kube-system                 coredns-557b76dc76-ncmw9                                   100m (5%)     0 (0%)      70Mi (1%)        170Mi (4%)     118m\n  kube-system                 csi-cinder-controllerplugin-0                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         118m\n  kube-system                 csi-cinder-nodeplugin-r2d6v                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         109m\n  kube-system                 dashboard-metrics-scraper-7cc7856cfb-p9hnn                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         118m\n  kube-system                 kube-dns-autoscaler-7986c68b66-q7lq5                       20m (1%)      0 (0%)      10Mi (0%)        0 (0%)         118m\n  kube-system                 kubernetes-dashboard-d7b78f849-zfz2z                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         118m\n  kube-system                 metrics-server-c494c94fc-lkqhw                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         118m\n  kube-system                 npd-428hd                                                  20m (1%)      200m (10%)  20Mi (0%)        100Mi (2%)     109m\n  kube-system                 snapshot-controller-6bd8bc5484-94b94                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         118m\n  kube-system                 snapshot-controller-6bd8bc5484-nh4pv                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         118m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-dbd8990eaea742d4-9s2t2    0 (0%)        0 (0%)      0 (0%)           0 (0%)         94m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                340m (17%)  300m (15%)\n  memory             220Mi (5%)  490Mi (13%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
    Jan 19 05:54:29.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=kubectl-7010 describe namespace kubectl-7010'
    Jan 19 05:54:29.393: INFO: stderr: ""
    Jan 19 05:54:29.393: INFO: stdout: "Name:         kubectl-7010\nLabels:       e2e-framework=kubectl\n              e2e-run=00430eb5-5905-4abe-8db7-178d376ca2a7\n              kubernetes.io/metadata.name=kubectl-7010\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 19 05:54:29.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7010" for this suite. 01/19/23 05:54:29.396
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:54:29.401
Jan 19 05:54:29.401: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename pod-network-test 01/19/23 05:54:29.402
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:54:29.414
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:54:29.417
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-9092 01/19/23 05:54:29.419
STEP: creating a selector 01/19/23 05:54:29.419
STEP: Creating the service pods in kubernetes 01/19/23 05:54:29.419
Jan 19 05:54:29.419: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 19 05:54:29.438: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9092" to be "running and ready"
Jan 19 05:54:29.443: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.453271ms
Jan 19 05:54:29.443: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 05:54:31.450: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012138065s
Jan 19 05:54:31.450: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 05:54:33.447: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.0094031s
Jan 19 05:54:33.447: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 05:54:35.448: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.010002331s
Jan 19 05:54:35.448: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 05:54:37.447: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.009243916s
Jan 19 05:54:37.447: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 05:54:39.447: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.009192398s
Jan 19 05:54:39.447: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 05:54:41.446: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.008171754s
Jan 19 05:54:41.446: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 05:54:43.449: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.010745452s
Jan 19 05:54:43.449: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 05:54:45.446: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.008468696s
Jan 19 05:54:45.446: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 05:54:47.448: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.009570747s
Jan 19 05:54:47.448: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 05:54:49.448: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.010267734s
Jan 19 05:54:49.448: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 05:54:51.448: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.009882798s
Jan 19 05:54:51.448: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 19 05:54:51.448: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 19 05:54:51.450: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9092" to be "running and ready"
Jan 19 05:54:51.453: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.414747ms
Jan 19 05:54:51.453: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 19 05:54:51.453: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 01/19/23 05:54:51.456
Jan 19 05:54:51.470: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9092" to be "running"
Jan 19 05:54:51.478: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.804345ms
Jan 19 05:54:53.483: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012767515s
Jan 19 05:54:55.481: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.011313932s
Jan 19 05:54:55.481: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 19 05:54:55.484: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-9092" to be "running"
Jan 19 05:54:55.487: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.867486ms
Jan 19 05:54:55.487: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jan 19 05:54:55.489: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 19 05:54:55.489: INFO: Going to poll 10.100.24.204 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Jan 19 05:54:55.491: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.100.24.204:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9092 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 05:54:55.491: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
Jan 19 05:54:55.491: INFO: ExecWithOptions: Clientset creation
Jan 19 05:54:55.491: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-9092/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.100.24.204%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 19 05:54:55.584: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 19 05:54:55.584: INFO: Going to poll 10.100.70.15 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Jan 19 05:54:55.588: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.100.70.15:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9092 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 05:54:55.588: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
Jan 19 05:54:55.589: INFO: ExecWithOptions: Clientset creation
Jan 19 05:54:55.589: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-9092/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.100.70.15%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 19 05:54:55.667: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jan 19 05:54:55.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9092" for this suite. 01/19/23 05:54:55.671
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","completed":328,"skipped":5945,"failed":0}
------------------------------
â€¢ [SLOW TEST] [26.275 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:54:29.401
    Jan 19 05:54:29.401: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename pod-network-test 01/19/23 05:54:29.402
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:54:29.414
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:54:29.417
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-9092 01/19/23 05:54:29.419
    STEP: creating a selector 01/19/23 05:54:29.419
    STEP: Creating the service pods in kubernetes 01/19/23 05:54:29.419
    Jan 19 05:54:29.419: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 19 05:54:29.438: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9092" to be "running and ready"
    Jan 19 05:54:29.443: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.453271ms
    Jan 19 05:54:29.443: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 05:54:31.450: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012138065s
    Jan 19 05:54:31.450: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 05:54:33.447: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.0094031s
    Jan 19 05:54:33.447: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 05:54:35.448: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.010002331s
    Jan 19 05:54:35.448: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 05:54:37.447: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.009243916s
    Jan 19 05:54:37.447: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 05:54:39.447: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.009192398s
    Jan 19 05:54:39.447: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 05:54:41.446: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.008171754s
    Jan 19 05:54:41.446: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 05:54:43.449: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.010745452s
    Jan 19 05:54:43.449: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 05:54:45.446: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.008468696s
    Jan 19 05:54:45.446: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 05:54:47.448: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.009570747s
    Jan 19 05:54:47.448: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 05:54:49.448: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.010267734s
    Jan 19 05:54:49.448: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 05:54:51.448: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.009882798s
    Jan 19 05:54:51.448: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 19 05:54:51.448: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 19 05:54:51.450: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9092" to be "running and ready"
    Jan 19 05:54:51.453: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.414747ms
    Jan 19 05:54:51.453: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 19 05:54:51.453: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 01/19/23 05:54:51.456
    Jan 19 05:54:51.470: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9092" to be "running"
    Jan 19 05:54:51.478: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.804345ms
    Jan 19 05:54:53.483: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012767515s
    Jan 19 05:54:55.481: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.011313932s
    Jan 19 05:54:55.481: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 19 05:54:55.484: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-9092" to be "running"
    Jan 19 05:54:55.487: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.867486ms
    Jan 19 05:54:55.487: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jan 19 05:54:55.489: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jan 19 05:54:55.489: INFO: Going to poll 10.100.24.204 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Jan 19 05:54:55.491: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.100.24.204:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9092 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 05:54:55.491: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    Jan 19 05:54:55.491: INFO: ExecWithOptions: Clientset creation
    Jan 19 05:54:55.491: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-9092/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.100.24.204%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 19 05:54:55.584: INFO: Found all 1 expected endpoints: [netserver-0]
    Jan 19 05:54:55.584: INFO: Going to poll 10.100.70.15 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Jan 19 05:54:55.588: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.100.70.15:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9092 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 05:54:55.588: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    Jan 19 05:54:55.589: INFO: ExecWithOptions: Clientset creation
    Jan 19 05:54:55.589: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-9092/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.100.70.15%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 19 05:54:55.667: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jan 19 05:54:55.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-9092" for this suite. 01/19/23 05:54:55.671
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:54:55.677
Jan 19 05:54:55.677: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename svcaccounts 01/19/23 05:54:55.677
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:54:55.697
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:54:55.7
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
STEP: creating a ServiceAccount 01/19/23 05:54:55.702
STEP: watching for the ServiceAccount to be added 01/19/23 05:54:55.714
STEP: patching the ServiceAccount 01/19/23 05:54:55.715
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 01/19/23 05:54:55.719
STEP: deleting the ServiceAccount 01/19/23 05:54:55.722
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 19 05:54:55.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7690" for this suite. 01/19/23 05:54:55.735
{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","completed":329,"skipped":5947,"failed":0}
------------------------------
â€¢ [0.063 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:54:55.677
    Jan 19 05:54:55.677: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename svcaccounts 01/19/23 05:54:55.677
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:54:55.697
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:54:55.7
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:646
    STEP: creating a ServiceAccount 01/19/23 05:54:55.702
    STEP: watching for the ServiceAccount to be added 01/19/23 05:54:55.714
    STEP: patching the ServiceAccount 01/19/23 05:54:55.715
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 01/19/23 05:54:55.719
    STEP: deleting the ServiceAccount 01/19/23 05:54:55.722
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 19 05:54:55.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-7690" for this suite. 01/19/23 05:54:55.735
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:54:55.74
Jan 19 05:54:55.740: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename projected 01/19/23 05:54:55.74
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:54:55.752
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:54:55.753
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
STEP: Creating a pod to test downward API volume plugin 01/19/23 05:54:55.756
Jan 19 05:54:55.764: INFO: Waiting up to 5m0s for pod "downwardapi-volume-042181d6-9e4e-4a39-9675-a26b27bf4f75" in namespace "projected-8945" to be "Succeeded or Failed"
Jan 19 05:54:55.769: INFO: Pod "downwardapi-volume-042181d6-9e4e-4a39-9675-a26b27bf4f75": Phase="Pending", Reason="", readiness=false. Elapsed: 5.310766ms
Jan 19 05:54:57.773: INFO: Pod "downwardapi-volume-042181d6-9e4e-4a39-9675-a26b27bf4f75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009305854s
Jan 19 05:54:59.773: INFO: Pod "downwardapi-volume-042181d6-9e4e-4a39-9675-a26b27bf4f75": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009452394s
Jan 19 05:55:01.773: INFO: Pod "downwardapi-volume-042181d6-9e4e-4a39-9675-a26b27bf4f75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009541498s
STEP: Saw pod success 01/19/23 05:55:01.773
Jan 19 05:55:01.774: INFO: Pod "downwardapi-volume-042181d6-9e4e-4a39-9675-a26b27bf4f75" satisfied condition "Succeeded or Failed"
Jan 19 05:55:01.776: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod downwardapi-volume-042181d6-9e4e-4a39-9675-a26b27bf4f75 container client-container: <nil>
STEP: delete the pod 01/19/23 05:55:01.781
Jan 19 05:55:01.788: INFO: Waiting for pod downwardapi-volume-042181d6-9e4e-4a39-9675-a26b27bf4f75 to disappear
Jan 19 05:55:01.790: INFO: Pod downwardapi-volume-042181d6-9e4e-4a39-9675-a26b27bf4f75 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 19 05:55:01.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8945" for this suite. 01/19/23 05:55:01.793
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":330,"skipped":5948,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.058 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:54:55.74
    Jan 19 05:54:55.740: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename projected 01/19/23 05:54:55.74
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:54:55.752
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:54:55.753
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:260
    STEP: Creating a pod to test downward API volume plugin 01/19/23 05:54:55.756
    Jan 19 05:54:55.764: INFO: Waiting up to 5m0s for pod "downwardapi-volume-042181d6-9e4e-4a39-9675-a26b27bf4f75" in namespace "projected-8945" to be "Succeeded or Failed"
    Jan 19 05:54:55.769: INFO: Pod "downwardapi-volume-042181d6-9e4e-4a39-9675-a26b27bf4f75": Phase="Pending", Reason="", readiness=false. Elapsed: 5.310766ms
    Jan 19 05:54:57.773: INFO: Pod "downwardapi-volume-042181d6-9e4e-4a39-9675-a26b27bf4f75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009305854s
    Jan 19 05:54:59.773: INFO: Pod "downwardapi-volume-042181d6-9e4e-4a39-9675-a26b27bf4f75": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009452394s
    Jan 19 05:55:01.773: INFO: Pod "downwardapi-volume-042181d6-9e4e-4a39-9675-a26b27bf4f75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009541498s
    STEP: Saw pod success 01/19/23 05:55:01.773
    Jan 19 05:55:01.774: INFO: Pod "downwardapi-volume-042181d6-9e4e-4a39-9675-a26b27bf4f75" satisfied condition "Succeeded or Failed"
    Jan 19 05:55:01.776: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod downwardapi-volume-042181d6-9e4e-4a39-9675-a26b27bf4f75 container client-container: <nil>
    STEP: delete the pod 01/19/23 05:55:01.781
    Jan 19 05:55:01.788: INFO: Waiting for pod downwardapi-volume-042181d6-9e4e-4a39-9675-a26b27bf4f75 to disappear
    Jan 19 05:55:01.790: INFO: Pod downwardapi-volume-042181d6-9e4e-4a39-9675-a26b27bf4f75 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 19 05:55:01.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8945" for this suite. 01/19/23 05:55:01.793
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:55:01.798
Jan 19 05:55:01.798: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename runtimeclass 01/19/23 05:55:01.799
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:55:01.81
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:55:01.812
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan 19 05:55:01.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-2947" for this suite. 01/19/23 05:55:01.822
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","completed":331,"skipped":5964,"failed":0}
------------------------------
â€¢ [0.029 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:55:01.798
    Jan 19 05:55:01.798: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename runtimeclass 01/19/23 05:55:01.799
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:55:01.81
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:55:01.812
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan 19 05:55:01.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-2947" for this suite. 01/19/23 05:55:01.822
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:55:01.829
Jan 19 05:55:01.829: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename resourcequota 01/19/23 05:55:01.829
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:55:01.84
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:55:01.842
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
STEP: Creating a ResourceQuota 01/19/23 05:55:01.847
STEP: Getting a ResourceQuota 01/19/23 05:55:01.851
STEP: Listing all ResourceQuotas with LabelSelector 01/19/23 05:55:01.853
STEP: Patching the ResourceQuota 01/19/23 05:55:01.856
STEP: Deleting a Collection of ResourceQuotas 01/19/23 05:55:01.862
STEP: Verifying the deleted ResourceQuota 01/19/23 05:55:01.87
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 19 05:55:01.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7911" for this suite. 01/19/23 05:55:01.874
{"msg":"PASSED [sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]","completed":332,"skipped":6005,"failed":0}
------------------------------
â€¢ [0.050 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:55:01.829
    Jan 19 05:55:01.829: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename resourcequota 01/19/23 05:55:01.829
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:55:01.84
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:55:01.842
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:933
    STEP: Creating a ResourceQuota 01/19/23 05:55:01.847
    STEP: Getting a ResourceQuota 01/19/23 05:55:01.851
    STEP: Listing all ResourceQuotas with LabelSelector 01/19/23 05:55:01.853
    STEP: Patching the ResourceQuota 01/19/23 05:55:01.856
    STEP: Deleting a Collection of ResourceQuotas 01/19/23 05:55:01.862
    STEP: Verifying the deleted ResourceQuota 01/19/23 05:55:01.87
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 19 05:55:01.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-7911" for this suite. 01/19/23 05:55:01.874
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:55:01.879
Jan 19 05:55:01.879: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename projected 01/19/23 05:55:01.88
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:55:01.891
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:55:01.893
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
STEP: Creating a pod to test downward API volume plugin 01/19/23 05:55:01.895
Jan 19 05:55:01.902: INFO: Waiting up to 5m0s for pod "downwardapi-volume-03b3cd75-ad4e-40d0-a2a5-aecbfe5ec230" in namespace "projected-6472" to be "Succeeded or Failed"
Jan 19 05:55:01.905: INFO: Pod "downwardapi-volume-03b3cd75-ad4e-40d0-a2a5-aecbfe5ec230": Phase="Pending", Reason="", readiness=false. Elapsed: 2.656953ms
Jan 19 05:55:03.909: INFO: Pod "downwardapi-volume-03b3cd75-ad4e-40d0-a2a5-aecbfe5ec230": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006785472s
Jan 19 05:55:05.910: INFO: Pod "downwardapi-volume-03b3cd75-ad4e-40d0-a2a5-aecbfe5ec230": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007199098s
Jan 19 05:55:07.909: INFO: Pod "downwardapi-volume-03b3cd75-ad4e-40d0-a2a5-aecbfe5ec230": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006819279s
STEP: Saw pod success 01/19/23 05:55:07.909
Jan 19 05:55:07.909: INFO: Pod "downwardapi-volume-03b3cd75-ad4e-40d0-a2a5-aecbfe5ec230" satisfied condition "Succeeded or Failed"
Jan 19 05:55:07.912: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod downwardapi-volume-03b3cd75-ad4e-40d0-a2a5-aecbfe5ec230 container client-container: <nil>
STEP: delete the pod 01/19/23 05:55:07.917
Jan 19 05:55:07.929: INFO: Waiting for pod downwardapi-volume-03b3cd75-ad4e-40d0-a2a5-aecbfe5ec230 to disappear
Jan 19 05:55:07.931: INFO: Pod downwardapi-volume-03b3cd75-ad4e-40d0-a2a5-aecbfe5ec230 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 19 05:55:07.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6472" for this suite. 01/19/23 05:55:07.934
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":333,"skipped":6039,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.060 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:55:01.879
    Jan 19 05:55:01.879: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename projected 01/19/23 05:55:01.88
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:55:01.891
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:55:01.893
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:248
    STEP: Creating a pod to test downward API volume plugin 01/19/23 05:55:01.895
    Jan 19 05:55:01.902: INFO: Waiting up to 5m0s for pod "downwardapi-volume-03b3cd75-ad4e-40d0-a2a5-aecbfe5ec230" in namespace "projected-6472" to be "Succeeded or Failed"
    Jan 19 05:55:01.905: INFO: Pod "downwardapi-volume-03b3cd75-ad4e-40d0-a2a5-aecbfe5ec230": Phase="Pending", Reason="", readiness=false. Elapsed: 2.656953ms
    Jan 19 05:55:03.909: INFO: Pod "downwardapi-volume-03b3cd75-ad4e-40d0-a2a5-aecbfe5ec230": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006785472s
    Jan 19 05:55:05.910: INFO: Pod "downwardapi-volume-03b3cd75-ad4e-40d0-a2a5-aecbfe5ec230": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007199098s
    Jan 19 05:55:07.909: INFO: Pod "downwardapi-volume-03b3cd75-ad4e-40d0-a2a5-aecbfe5ec230": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006819279s
    STEP: Saw pod success 01/19/23 05:55:07.909
    Jan 19 05:55:07.909: INFO: Pod "downwardapi-volume-03b3cd75-ad4e-40d0-a2a5-aecbfe5ec230" satisfied condition "Succeeded or Failed"
    Jan 19 05:55:07.912: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod downwardapi-volume-03b3cd75-ad4e-40d0-a2a5-aecbfe5ec230 container client-container: <nil>
    STEP: delete the pod 01/19/23 05:55:07.917
    Jan 19 05:55:07.929: INFO: Waiting for pod downwardapi-volume-03b3cd75-ad4e-40d0-a2a5-aecbfe5ec230 to disappear
    Jan 19 05:55:07.931: INFO: Pod downwardapi-volume-03b3cd75-ad4e-40d0-a2a5-aecbfe5ec230 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 19 05:55:07.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6472" for this suite. 01/19/23 05:55:07.934
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:55:07.939
Jan 19 05:55:07.939: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename resourcequota 01/19/23 05:55:07.94
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:55:07.95
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:55:07.953
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
STEP: Counting existing ResourceQuota 01/19/23 05:55:24.96
STEP: Creating a ResourceQuota 01/19/23 05:55:29.964
STEP: Ensuring resource quota status is calculated 01/19/23 05:55:29.969
STEP: Creating a ConfigMap 01/19/23 05:55:31.974
STEP: Ensuring resource quota status captures configMap creation 01/19/23 05:55:31.984
STEP: Deleting a ConfigMap 01/19/23 05:55:33.988
STEP: Ensuring resource quota status released usage 01/19/23 05:55:33.995
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 19 05:55:36.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4448" for this suite. 01/19/23 05:55:36.003
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","completed":334,"skipped":6042,"failed":0}
------------------------------
â€¢ [SLOW TEST] [28.071 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:55:07.939
    Jan 19 05:55:07.939: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename resourcequota 01/19/23 05:55:07.94
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:55:07.95
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:55:07.953
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:316
    STEP: Counting existing ResourceQuota 01/19/23 05:55:24.96
    STEP: Creating a ResourceQuota 01/19/23 05:55:29.964
    STEP: Ensuring resource quota status is calculated 01/19/23 05:55:29.969
    STEP: Creating a ConfigMap 01/19/23 05:55:31.974
    STEP: Ensuring resource quota status captures configMap creation 01/19/23 05:55:31.984
    STEP: Deleting a ConfigMap 01/19/23 05:55:33.988
    STEP: Ensuring resource quota status released usage 01/19/23 05:55:33.995
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 19 05:55:36.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-4448" for this suite. 01/19/23 05:55:36.003
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:55:36.01
Jan 19 05:55:36.010: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename discovery 01/19/23 05:55:36.01
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:55:36.022
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:55:36.025
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 01/19/23 05:55:36.028
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Jan 19 05:55:36.330: INFO: Checking APIGroup: apiregistration.k8s.io
Jan 19 05:55:36.331: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jan 19 05:55:36.331: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Jan 19 05:55:36.331: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jan 19 05:55:36.331: INFO: Checking APIGroup: apps
Jan 19 05:55:36.331: INFO: PreferredVersion.GroupVersion: apps/v1
Jan 19 05:55:36.332: INFO: Versions found [{apps/v1 v1}]
Jan 19 05:55:36.332: INFO: apps/v1 matches apps/v1
Jan 19 05:55:36.332: INFO: Checking APIGroup: events.k8s.io
Jan 19 05:55:36.332: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jan 19 05:55:36.332: INFO: Versions found [{events.k8s.io/v1 v1}]
Jan 19 05:55:36.332: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jan 19 05:55:36.332: INFO: Checking APIGroup: authentication.k8s.io
Jan 19 05:55:36.333: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jan 19 05:55:36.333: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Jan 19 05:55:36.333: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jan 19 05:55:36.333: INFO: Checking APIGroup: authorization.k8s.io
Jan 19 05:55:36.334: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jan 19 05:55:36.334: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Jan 19 05:55:36.334: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jan 19 05:55:36.334: INFO: Checking APIGroup: autoscaling
Jan 19 05:55:36.334: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Jan 19 05:55:36.334: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
Jan 19 05:55:36.334: INFO: autoscaling/v2 matches autoscaling/v2
Jan 19 05:55:36.334: INFO: Checking APIGroup: batch
Jan 19 05:55:36.335: INFO: PreferredVersion.GroupVersion: batch/v1
Jan 19 05:55:36.335: INFO: Versions found [{batch/v1 v1}]
Jan 19 05:55:36.335: INFO: batch/v1 matches batch/v1
Jan 19 05:55:36.335: INFO: Checking APIGroup: certificates.k8s.io
Jan 19 05:55:36.336: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jan 19 05:55:36.336: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Jan 19 05:55:36.336: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jan 19 05:55:36.336: INFO: Checking APIGroup: networking.k8s.io
Jan 19 05:55:36.336: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jan 19 05:55:36.336: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1alpha1 v1alpha1}]
Jan 19 05:55:36.336: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jan 19 05:55:36.336: INFO: Checking APIGroup: policy
Jan 19 05:55:36.337: INFO: PreferredVersion.GroupVersion: policy/v1
Jan 19 05:55:36.337: INFO: Versions found [{policy/v1 v1}]
Jan 19 05:55:36.337: INFO: policy/v1 matches policy/v1
Jan 19 05:55:36.337: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jan 19 05:55:36.338: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jan 19 05:55:36.338: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Jan 19 05:55:36.338: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jan 19 05:55:36.338: INFO: Checking APIGroup: storage.k8s.io
Jan 19 05:55:36.339: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jan 19 05:55:36.339: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jan 19 05:55:36.339: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jan 19 05:55:36.339: INFO: Checking APIGroup: admissionregistration.k8s.io
Jan 19 05:55:36.339: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jan 19 05:55:36.340: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Jan 19 05:55:36.340: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jan 19 05:55:36.340: INFO: Checking APIGroup: apiextensions.k8s.io
Jan 19 05:55:36.340: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jan 19 05:55:36.340: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Jan 19 05:55:36.340: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jan 19 05:55:36.340: INFO: Checking APIGroup: scheduling.k8s.io
Jan 19 05:55:36.341: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jan 19 05:55:36.341: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Jan 19 05:55:36.341: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jan 19 05:55:36.341: INFO: Checking APIGroup: coordination.k8s.io
Jan 19 05:55:36.342: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jan 19 05:55:36.342: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Jan 19 05:55:36.342: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jan 19 05:55:36.342: INFO: Checking APIGroup: node.k8s.io
Jan 19 05:55:36.342: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jan 19 05:55:36.342: INFO: Versions found [{node.k8s.io/v1 v1}]
Jan 19 05:55:36.342: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jan 19 05:55:36.342: INFO: Checking APIGroup: discovery.k8s.io
Jan 19 05:55:36.343: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Jan 19 05:55:36.343: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Jan 19 05:55:36.343: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Jan 19 05:55:36.343: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jan 19 05:55:36.344: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Jan 19 05:55:36.344: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Jan 19 05:55:36.344: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Jan 19 05:55:36.344: INFO: Checking APIGroup: internal.apiserver.k8s.io
Jan 19 05:55:36.344: INFO: PreferredVersion.GroupVersion: internal.apiserver.k8s.io/v1alpha1
Jan 19 05:55:36.344: INFO: Versions found [{internal.apiserver.k8s.io/v1alpha1 v1alpha1}]
Jan 19 05:55:36.344: INFO: internal.apiserver.k8s.io/v1alpha1 matches internal.apiserver.k8s.io/v1alpha1
Jan 19 05:55:36.344: INFO: Checking APIGroup: snapshot.storage.k8s.io
Jan 19 05:55:36.345: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Jan 19 05:55:36.345: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
Jan 19 05:55:36.345: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Jan 19 05:55:36.345: INFO: Checking APIGroup: metrics.k8s.io
Jan 19 05:55:36.346: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Jan 19 05:55:36.346: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Jan 19 05:55:36.346: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
Jan 19 05:55:36.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-7506" for this suite. 01/19/23 05:55:36.349
{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","completed":335,"skipped":6045,"failed":0}
------------------------------
â€¢ [0.344 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:55:36.01
    Jan 19 05:55:36.010: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename discovery 01/19/23 05:55:36.01
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:55:36.022
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:55:36.025
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 01/19/23 05:55:36.028
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Jan 19 05:55:36.330: INFO: Checking APIGroup: apiregistration.k8s.io
    Jan 19 05:55:36.331: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Jan 19 05:55:36.331: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Jan 19 05:55:36.331: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Jan 19 05:55:36.331: INFO: Checking APIGroup: apps
    Jan 19 05:55:36.331: INFO: PreferredVersion.GroupVersion: apps/v1
    Jan 19 05:55:36.332: INFO: Versions found [{apps/v1 v1}]
    Jan 19 05:55:36.332: INFO: apps/v1 matches apps/v1
    Jan 19 05:55:36.332: INFO: Checking APIGroup: events.k8s.io
    Jan 19 05:55:36.332: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Jan 19 05:55:36.332: INFO: Versions found [{events.k8s.io/v1 v1}]
    Jan 19 05:55:36.332: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Jan 19 05:55:36.332: INFO: Checking APIGroup: authentication.k8s.io
    Jan 19 05:55:36.333: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Jan 19 05:55:36.333: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Jan 19 05:55:36.333: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Jan 19 05:55:36.333: INFO: Checking APIGroup: authorization.k8s.io
    Jan 19 05:55:36.334: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Jan 19 05:55:36.334: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Jan 19 05:55:36.334: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Jan 19 05:55:36.334: INFO: Checking APIGroup: autoscaling
    Jan 19 05:55:36.334: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Jan 19 05:55:36.334: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
    Jan 19 05:55:36.334: INFO: autoscaling/v2 matches autoscaling/v2
    Jan 19 05:55:36.334: INFO: Checking APIGroup: batch
    Jan 19 05:55:36.335: INFO: PreferredVersion.GroupVersion: batch/v1
    Jan 19 05:55:36.335: INFO: Versions found [{batch/v1 v1}]
    Jan 19 05:55:36.335: INFO: batch/v1 matches batch/v1
    Jan 19 05:55:36.335: INFO: Checking APIGroup: certificates.k8s.io
    Jan 19 05:55:36.336: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Jan 19 05:55:36.336: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Jan 19 05:55:36.336: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Jan 19 05:55:36.336: INFO: Checking APIGroup: networking.k8s.io
    Jan 19 05:55:36.336: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Jan 19 05:55:36.336: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1alpha1 v1alpha1}]
    Jan 19 05:55:36.336: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Jan 19 05:55:36.336: INFO: Checking APIGroup: policy
    Jan 19 05:55:36.337: INFO: PreferredVersion.GroupVersion: policy/v1
    Jan 19 05:55:36.337: INFO: Versions found [{policy/v1 v1}]
    Jan 19 05:55:36.337: INFO: policy/v1 matches policy/v1
    Jan 19 05:55:36.337: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Jan 19 05:55:36.338: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Jan 19 05:55:36.338: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Jan 19 05:55:36.338: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Jan 19 05:55:36.338: INFO: Checking APIGroup: storage.k8s.io
    Jan 19 05:55:36.339: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Jan 19 05:55:36.339: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Jan 19 05:55:36.339: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Jan 19 05:55:36.339: INFO: Checking APIGroup: admissionregistration.k8s.io
    Jan 19 05:55:36.339: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Jan 19 05:55:36.340: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Jan 19 05:55:36.340: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Jan 19 05:55:36.340: INFO: Checking APIGroup: apiextensions.k8s.io
    Jan 19 05:55:36.340: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Jan 19 05:55:36.340: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Jan 19 05:55:36.340: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Jan 19 05:55:36.340: INFO: Checking APIGroup: scheduling.k8s.io
    Jan 19 05:55:36.341: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Jan 19 05:55:36.341: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Jan 19 05:55:36.341: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Jan 19 05:55:36.341: INFO: Checking APIGroup: coordination.k8s.io
    Jan 19 05:55:36.342: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Jan 19 05:55:36.342: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Jan 19 05:55:36.342: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Jan 19 05:55:36.342: INFO: Checking APIGroup: node.k8s.io
    Jan 19 05:55:36.342: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Jan 19 05:55:36.342: INFO: Versions found [{node.k8s.io/v1 v1}]
    Jan 19 05:55:36.342: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Jan 19 05:55:36.342: INFO: Checking APIGroup: discovery.k8s.io
    Jan 19 05:55:36.343: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Jan 19 05:55:36.343: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Jan 19 05:55:36.343: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Jan 19 05:55:36.343: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Jan 19 05:55:36.344: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
    Jan 19 05:55:36.344: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
    Jan 19 05:55:36.344: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
    Jan 19 05:55:36.344: INFO: Checking APIGroup: internal.apiserver.k8s.io
    Jan 19 05:55:36.344: INFO: PreferredVersion.GroupVersion: internal.apiserver.k8s.io/v1alpha1
    Jan 19 05:55:36.344: INFO: Versions found [{internal.apiserver.k8s.io/v1alpha1 v1alpha1}]
    Jan 19 05:55:36.344: INFO: internal.apiserver.k8s.io/v1alpha1 matches internal.apiserver.k8s.io/v1alpha1
    Jan 19 05:55:36.344: INFO: Checking APIGroup: snapshot.storage.k8s.io
    Jan 19 05:55:36.345: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
    Jan 19 05:55:36.345: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
    Jan 19 05:55:36.345: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
    Jan 19 05:55:36.345: INFO: Checking APIGroup: metrics.k8s.io
    Jan 19 05:55:36.346: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    Jan 19 05:55:36.346: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    Jan 19 05:55:36.346: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:187
    Jan 19 05:55:36.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "discovery-7506" for this suite. 01/19/23 05:55:36.349
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:55:36.354
Jan 19 05:55:36.354: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename replication-controller 01/19/23 05:55:36.355
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:55:36.365
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:55:36.367
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
STEP: Given a Pod with a 'name' label pod-adoption is created 01/19/23 05:55:36.368
Jan 19 05:55:36.378: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-674" to be "running and ready"
Jan 19 05:55:36.384: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 5.466524ms
Jan 19 05:55:36.384: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jan 19 05:55:38.389: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010602127s
Jan 19 05:55:38.389: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jan 19 05:55:40.388: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 4.009930733s
Jan 19 05:55:40.388: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Jan 19 05:55:40.388: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 01/19/23 05:55:40.391
STEP: Then the orphan pod is adopted 01/19/23 05:55:40.395
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan 19 05:55:41.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-674" for this suite. 01/19/23 05:55:41.407
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","completed":336,"skipped":6074,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.058 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:55:36.354
    Jan 19 05:55:36.354: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename replication-controller 01/19/23 05:55:36.355
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:55:36.365
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:55:36.367
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:91
    STEP: Given a Pod with a 'name' label pod-adoption is created 01/19/23 05:55:36.368
    Jan 19 05:55:36.378: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-674" to be "running and ready"
    Jan 19 05:55:36.384: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 5.466524ms
    Jan 19 05:55:36.384: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 05:55:38.389: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010602127s
    Jan 19 05:55:38.389: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 05:55:40.388: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 4.009930733s
    Jan 19 05:55:40.388: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Jan 19 05:55:40.388: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 01/19/23 05:55:40.391
    STEP: Then the orphan pod is adopted 01/19/23 05:55:40.395
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan 19 05:55:41.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-674" for this suite. 01/19/23 05:55:41.407
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:55:41.414
Jan 19 05:55:41.414: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename conformance-tests 01/19/23 05:55:41.414
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:55:41.444
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:55:41.448
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 01/19/23 05:55:41.45
Jan 19 05:55:41.450: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
Jan 19 05:55:41.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-7460" for this suite. 01/19/23 05:55:41.458
{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","completed":337,"skipped":6120,"failed":0}
------------------------------
â€¢ [0.050 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:55:41.414
    Jan 19 05:55:41.414: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename conformance-tests 01/19/23 05:55:41.414
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:55:41.444
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:55:41.448
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 01/19/23 05:55:41.45
    Jan 19 05:55:41.450: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:187
    Jan 19 05:55:41.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "conformance-tests-7460" for this suite. 01/19/23 05:55:41.458
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:55:41.464
Jan 19 05:55:41.464: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename services 01/19/23 05:55:41.465
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:55:41.479
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:55:41.481
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
STEP: creating service in namespace services-2494 01/19/23 05:55:41.484
Jan 19 05:55:41.495: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-2494" to be "running and ready"
Jan 19 05:55:41.500: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 5.91318ms
Jan 19 05:55:41.500: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jan 19 05:55:43.505: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.010107329s
Jan 19 05:55:43.505: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Jan 19 05:55:43.505: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Jan 19 05:55:43.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-2494 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jan 19 05:55:43.718: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jan 19 05:55:43.718: INFO: stdout: "iptables"
Jan 19 05:55:43.718: INFO: proxyMode: iptables
Jan 19 05:55:43.732: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan 19 05:55:43.735: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-2494 01/19/23 05:55:43.735
STEP: creating replication controller affinity-clusterip-timeout in namespace services-2494 01/19/23 05:55:43.743
I0119 05:55:43.752012      18 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-2494, replica count: 3
I0119 05:55:46.803383      18 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 19 05:55:46.810: INFO: Creating new exec pod
Jan 19 05:55:46.817: INFO: Waiting up to 5m0s for pod "execpod-affinityzp9vb" in namespace "services-2494" to be "running"
Jan 19 05:55:46.825: INFO: Pod "execpod-affinityzp9vb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.075302ms
Jan 19 05:55:48.831: INFO: Pod "execpod-affinityzp9vb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013375166s
Jan 19 05:55:50.830: INFO: Pod "execpod-affinityzp9vb": Phase="Running", Reason="", readiness=true. Elapsed: 4.012214341s
Jan 19 05:55:50.830: INFO: Pod "execpod-affinityzp9vb" satisfied condition "running"
Jan 19 05:55:51.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-2494 exec execpod-affinityzp9vb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Jan 19 05:55:52.016: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Jan 19 05:55:52.016: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 05:55:52.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-2494 exec execpod-affinityzp9vb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.238.155 80'
Jan 19 05:55:52.184: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.238.155 80\nConnection to 10.254.238.155 80 port [tcp/http] succeeded!\n"
Jan 19 05:55:52.184: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 05:55:52.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-2494 exec execpod-affinityzp9vb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.238.155:80/ ; done'
Jan 19 05:55:52.411: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.238.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.238.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.238.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.238.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.238.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.238.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.238.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.238.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.238.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.238.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.238.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.238.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.238.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.238.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.238.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.238.155:80/\n"
Jan 19 05:55:52.411: INFO: stdout: "\naffinity-clusterip-timeout-nrvhw\naffinity-clusterip-timeout-nrvhw\naffinity-clusterip-timeout-nrvhw\naffinity-clusterip-timeout-nrvhw\naffinity-clusterip-timeout-nrvhw\naffinity-clusterip-timeout-nrvhw\naffinity-clusterip-timeout-nrvhw\naffinity-clusterip-timeout-nrvhw\naffinity-clusterip-timeout-nrvhw\naffinity-clusterip-timeout-nrvhw\naffinity-clusterip-timeout-nrvhw\naffinity-clusterip-timeout-nrvhw\naffinity-clusterip-timeout-nrvhw\naffinity-clusterip-timeout-nrvhw\naffinity-clusterip-timeout-nrvhw\naffinity-clusterip-timeout-nrvhw"
Jan 19 05:55:52.411: INFO: Received response from host: affinity-clusterip-timeout-nrvhw
Jan 19 05:55:52.411: INFO: Received response from host: affinity-clusterip-timeout-nrvhw
Jan 19 05:55:52.411: INFO: Received response from host: affinity-clusterip-timeout-nrvhw
Jan 19 05:55:52.411: INFO: Received response from host: affinity-clusterip-timeout-nrvhw
Jan 19 05:55:52.411: INFO: Received response from host: affinity-clusterip-timeout-nrvhw
Jan 19 05:55:52.411: INFO: Received response from host: affinity-clusterip-timeout-nrvhw
Jan 19 05:55:52.411: INFO: Received response from host: affinity-clusterip-timeout-nrvhw
Jan 19 05:55:52.411: INFO: Received response from host: affinity-clusterip-timeout-nrvhw
Jan 19 05:55:52.411: INFO: Received response from host: affinity-clusterip-timeout-nrvhw
Jan 19 05:55:52.411: INFO: Received response from host: affinity-clusterip-timeout-nrvhw
Jan 19 05:55:52.411: INFO: Received response from host: affinity-clusterip-timeout-nrvhw
Jan 19 05:55:52.411: INFO: Received response from host: affinity-clusterip-timeout-nrvhw
Jan 19 05:55:52.411: INFO: Received response from host: affinity-clusterip-timeout-nrvhw
Jan 19 05:55:52.411: INFO: Received response from host: affinity-clusterip-timeout-nrvhw
Jan 19 05:55:52.411: INFO: Received response from host: affinity-clusterip-timeout-nrvhw
Jan 19 05:55:52.411: INFO: Received response from host: affinity-clusterip-timeout-nrvhw
Jan 19 05:55:52.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-2494 exec execpod-affinityzp9vb -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.254.238.155:80/'
Jan 19 05:55:52.565: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.254.238.155:80/\n"
Jan 19 05:55:52.565: INFO: stdout: "affinity-clusterip-timeout-nrvhw"
Jan 19 05:56:12.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-2494 exec execpod-affinityzp9vb -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.254.238.155:80/'
Jan 19 05:56:12.723: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.254.238.155:80/\n"
Jan 19 05:56:12.723: INFO: stdout: "affinity-clusterip-timeout-9h7wz"
Jan 19 05:56:12.724: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-2494, will wait for the garbage collector to delete the pods 01/19/23 05:56:12.735
Jan 19 05:56:12.798: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 7.846346ms
Jan 19 05:56:12.900: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 102.072713ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 19 05:56:14.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2494" for this suite. 01/19/23 05:56:14.832
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","completed":338,"skipped":6126,"failed":0}
------------------------------
â€¢ [SLOW TEST] [33.372 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:55:41.464
    Jan 19 05:55:41.464: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename services 01/19/23 05:55:41.465
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:55:41.479
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:55:41.481
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2173
    STEP: creating service in namespace services-2494 01/19/23 05:55:41.484
    Jan 19 05:55:41.495: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-2494" to be "running and ready"
    Jan 19 05:55:41.500: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 5.91318ms
    Jan 19 05:55:41.500: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 05:55:43.505: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.010107329s
    Jan 19 05:55:43.505: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Jan 19 05:55:43.505: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Jan 19 05:55:43.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-2494 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Jan 19 05:55:43.718: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Jan 19 05:55:43.718: INFO: stdout: "iptables"
    Jan 19 05:55:43.718: INFO: proxyMode: iptables
    Jan 19 05:55:43.732: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Jan 19 05:55:43.735: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-clusterip-timeout in namespace services-2494 01/19/23 05:55:43.735
    STEP: creating replication controller affinity-clusterip-timeout in namespace services-2494 01/19/23 05:55:43.743
    I0119 05:55:43.752012      18 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-2494, replica count: 3
    I0119 05:55:46.803383      18 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 19 05:55:46.810: INFO: Creating new exec pod
    Jan 19 05:55:46.817: INFO: Waiting up to 5m0s for pod "execpod-affinityzp9vb" in namespace "services-2494" to be "running"
    Jan 19 05:55:46.825: INFO: Pod "execpod-affinityzp9vb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.075302ms
    Jan 19 05:55:48.831: INFO: Pod "execpod-affinityzp9vb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013375166s
    Jan 19 05:55:50.830: INFO: Pod "execpod-affinityzp9vb": Phase="Running", Reason="", readiness=true. Elapsed: 4.012214341s
    Jan 19 05:55:50.830: INFO: Pod "execpod-affinityzp9vb" satisfied condition "running"
    Jan 19 05:55:51.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-2494 exec execpod-affinityzp9vb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
    Jan 19 05:55:52.016: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
    Jan 19 05:55:52.016: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 19 05:55:52.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-2494 exec execpod-affinityzp9vb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.238.155 80'
    Jan 19 05:55:52.184: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.238.155 80\nConnection to 10.254.238.155 80 port [tcp/http] succeeded!\n"
    Jan 19 05:55:52.184: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 19 05:55:52.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-2494 exec execpod-affinityzp9vb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.238.155:80/ ; done'
    Jan 19 05:55:52.411: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.238.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.238.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.238.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.238.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.238.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.238.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.238.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.238.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.238.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.238.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.238.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.238.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.238.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.238.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.238.155:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.238.155:80/\n"
    Jan 19 05:55:52.411: INFO: stdout: "\naffinity-clusterip-timeout-nrvhw\naffinity-clusterip-timeout-nrvhw\naffinity-clusterip-timeout-nrvhw\naffinity-clusterip-timeout-nrvhw\naffinity-clusterip-timeout-nrvhw\naffinity-clusterip-timeout-nrvhw\naffinity-clusterip-timeout-nrvhw\naffinity-clusterip-timeout-nrvhw\naffinity-clusterip-timeout-nrvhw\naffinity-clusterip-timeout-nrvhw\naffinity-clusterip-timeout-nrvhw\naffinity-clusterip-timeout-nrvhw\naffinity-clusterip-timeout-nrvhw\naffinity-clusterip-timeout-nrvhw\naffinity-clusterip-timeout-nrvhw\naffinity-clusterip-timeout-nrvhw"
    Jan 19 05:55:52.411: INFO: Received response from host: affinity-clusterip-timeout-nrvhw
    Jan 19 05:55:52.411: INFO: Received response from host: affinity-clusterip-timeout-nrvhw
    Jan 19 05:55:52.411: INFO: Received response from host: affinity-clusterip-timeout-nrvhw
    Jan 19 05:55:52.411: INFO: Received response from host: affinity-clusterip-timeout-nrvhw
    Jan 19 05:55:52.411: INFO: Received response from host: affinity-clusterip-timeout-nrvhw
    Jan 19 05:55:52.411: INFO: Received response from host: affinity-clusterip-timeout-nrvhw
    Jan 19 05:55:52.411: INFO: Received response from host: affinity-clusterip-timeout-nrvhw
    Jan 19 05:55:52.411: INFO: Received response from host: affinity-clusterip-timeout-nrvhw
    Jan 19 05:55:52.411: INFO: Received response from host: affinity-clusterip-timeout-nrvhw
    Jan 19 05:55:52.411: INFO: Received response from host: affinity-clusterip-timeout-nrvhw
    Jan 19 05:55:52.411: INFO: Received response from host: affinity-clusterip-timeout-nrvhw
    Jan 19 05:55:52.411: INFO: Received response from host: affinity-clusterip-timeout-nrvhw
    Jan 19 05:55:52.411: INFO: Received response from host: affinity-clusterip-timeout-nrvhw
    Jan 19 05:55:52.411: INFO: Received response from host: affinity-clusterip-timeout-nrvhw
    Jan 19 05:55:52.411: INFO: Received response from host: affinity-clusterip-timeout-nrvhw
    Jan 19 05:55:52.411: INFO: Received response from host: affinity-clusterip-timeout-nrvhw
    Jan 19 05:55:52.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-2494 exec execpod-affinityzp9vb -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.254.238.155:80/'
    Jan 19 05:55:52.565: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.254.238.155:80/\n"
    Jan 19 05:55:52.565: INFO: stdout: "affinity-clusterip-timeout-nrvhw"
    Jan 19 05:56:12.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=services-2494 exec execpod-affinityzp9vb -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.254.238.155:80/'
    Jan 19 05:56:12.723: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.254.238.155:80/\n"
    Jan 19 05:56:12.723: INFO: stdout: "affinity-clusterip-timeout-9h7wz"
    Jan 19 05:56:12.724: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-2494, will wait for the garbage collector to delete the pods 01/19/23 05:56:12.735
    Jan 19 05:56:12.798: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 7.846346ms
    Jan 19 05:56:12.900: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 102.072713ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 19 05:56:14.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2494" for this suite. 01/19/23 05:56:14.832
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:56:14.836
Jan 19 05:56:14.837: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename watch 01/19/23 05:56:14.837
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:56:14.857
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:56:14.859
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 01/19/23 05:56:14.862
STEP: creating a watch on configmaps with label B 01/19/23 05:56:14.863
STEP: creating a watch on configmaps with label A or B 01/19/23 05:56:14.864
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 01/19/23 05:56:14.865
Jan 19 05:56:14.870: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6716  62840c52-1784-4783-a481-df84e4605fb7 36732 0 2023-01-19 05:56:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-19 05:56:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 19 05:56:14.870: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6716  62840c52-1784-4783-a481-df84e4605fb7 36732 0 2023-01-19 05:56:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-19 05:56:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 01/19/23 05:56:14.87
Jan 19 05:56:14.876: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6716  62840c52-1784-4783-a481-df84e4605fb7 36733 0 2023-01-19 05:56:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-19 05:56:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 19 05:56:14.876: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6716  62840c52-1784-4783-a481-df84e4605fb7 36733 0 2023-01-19 05:56:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-19 05:56:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 01/19/23 05:56:14.876
Jan 19 05:56:14.881: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6716  62840c52-1784-4783-a481-df84e4605fb7 36734 0 2023-01-19 05:56:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-19 05:56:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 19 05:56:14.882: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6716  62840c52-1784-4783-a481-df84e4605fb7 36734 0 2023-01-19 05:56:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-19 05:56:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 01/19/23 05:56:14.882
Jan 19 05:56:14.886: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6716  62840c52-1784-4783-a481-df84e4605fb7 36735 0 2023-01-19 05:56:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-19 05:56:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 19 05:56:14.886: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6716  62840c52-1784-4783-a481-df84e4605fb7 36735 0 2023-01-19 05:56:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-19 05:56:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 01/19/23 05:56:14.886
Jan 19 05:56:14.889: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6716  4e2be297-8f09-4022-a706-a4320e1fc600 36736 0 2023-01-19 05:56:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-19 05:56:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 19 05:56:14.889: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6716  4e2be297-8f09-4022-a706-a4320e1fc600 36736 0 2023-01-19 05:56:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-19 05:56:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 01/19/23 05:56:24.89
Jan 19 05:56:24.898: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6716  4e2be297-8f09-4022-a706-a4320e1fc600 36793 0 2023-01-19 05:56:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-19 05:56:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 19 05:56:24.898: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6716  4e2be297-8f09-4022-a706-a4320e1fc600 36793 0 2023-01-19 05:56:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-19 05:56:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan 19 05:56:34.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6716" for this suite. 01/19/23 05:56:34.904
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","completed":339,"skipped":6132,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.074 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:56:14.836
    Jan 19 05:56:14.837: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename watch 01/19/23 05:56:14.837
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:56:14.857
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:56:14.859
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 01/19/23 05:56:14.862
    STEP: creating a watch on configmaps with label B 01/19/23 05:56:14.863
    STEP: creating a watch on configmaps with label A or B 01/19/23 05:56:14.864
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 01/19/23 05:56:14.865
    Jan 19 05:56:14.870: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6716  62840c52-1784-4783-a481-df84e4605fb7 36732 0 2023-01-19 05:56:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-19 05:56:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 19 05:56:14.870: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6716  62840c52-1784-4783-a481-df84e4605fb7 36732 0 2023-01-19 05:56:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-19 05:56:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 01/19/23 05:56:14.87
    Jan 19 05:56:14.876: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6716  62840c52-1784-4783-a481-df84e4605fb7 36733 0 2023-01-19 05:56:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-19 05:56:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 19 05:56:14.876: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6716  62840c52-1784-4783-a481-df84e4605fb7 36733 0 2023-01-19 05:56:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-19 05:56:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 01/19/23 05:56:14.876
    Jan 19 05:56:14.881: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6716  62840c52-1784-4783-a481-df84e4605fb7 36734 0 2023-01-19 05:56:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-19 05:56:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 19 05:56:14.882: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6716  62840c52-1784-4783-a481-df84e4605fb7 36734 0 2023-01-19 05:56:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-19 05:56:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 01/19/23 05:56:14.882
    Jan 19 05:56:14.886: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6716  62840c52-1784-4783-a481-df84e4605fb7 36735 0 2023-01-19 05:56:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-19 05:56:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 19 05:56:14.886: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6716  62840c52-1784-4783-a481-df84e4605fb7 36735 0 2023-01-19 05:56:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-19 05:56:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 01/19/23 05:56:14.886
    Jan 19 05:56:14.889: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6716  4e2be297-8f09-4022-a706-a4320e1fc600 36736 0 2023-01-19 05:56:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-19 05:56:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 19 05:56:14.889: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6716  4e2be297-8f09-4022-a706-a4320e1fc600 36736 0 2023-01-19 05:56:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-19 05:56:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 01/19/23 05:56:24.89
    Jan 19 05:56:24.898: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6716  4e2be297-8f09-4022-a706-a4320e1fc600 36793 0 2023-01-19 05:56:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-19 05:56:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 19 05:56:24.898: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6716  4e2be297-8f09-4022-a706-a4320e1fc600 36793 0 2023-01-19 05:56:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-19 05:56:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan 19 05:56:34.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-6716" for this suite. 01/19/23 05:56:34.904
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:56:34.911
Jan 19 05:56:34.911: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename kubelet-test 01/19/23 05:56:34.912
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:56:34.926
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:56:34.928
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Jan 19 05:56:34.937: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs613769ac-53d0-4139-b557-fc0688e4abf1" in namespace "kubelet-test-9474" to be "running and ready"
Jan 19 05:56:34.942: INFO: Pod "busybox-readonly-fs613769ac-53d0-4139-b557-fc0688e4abf1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.061646ms
Jan 19 05:56:34.942: INFO: The phase of Pod busybox-readonly-fs613769ac-53d0-4139-b557-fc0688e4abf1 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 05:56:36.948: INFO: Pod "busybox-readonly-fs613769ac-53d0-4139-b557-fc0688e4abf1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01013553s
Jan 19 05:56:36.948: INFO: The phase of Pod busybox-readonly-fs613769ac-53d0-4139-b557-fc0688e4abf1 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 05:56:38.947: INFO: Pod "busybox-readonly-fs613769ac-53d0-4139-b557-fc0688e4abf1": Phase="Running", Reason="", readiness=true. Elapsed: 4.009487866s
Jan 19 05:56:38.947: INFO: The phase of Pod busybox-readonly-fs613769ac-53d0-4139-b557-fc0688e4abf1 is Running (Ready = true)
Jan 19 05:56:38.947: INFO: Pod "busybox-readonly-fs613769ac-53d0-4139-b557-fc0688e4abf1" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan 19 05:56:38.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9474" for this suite. 01/19/23 05:56:38.983
{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","completed":340,"skipped":6145,"failed":0}
------------------------------
â€¢ [4.077 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:56:34.911
    Jan 19 05:56:34.911: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename kubelet-test 01/19/23 05:56:34.912
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:56:34.926
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:56:34.928
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Jan 19 05:56:34.937: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs613769ac-53d0-4139-b557-fc0688e4abf1" in namespace "kubelet-test-9474" to be "running and ready"
    Jan 19 05:56:34.942: INFO: Pod "busybox-readonly-fs613769ac-53d0-4139-b557-fc0688e4abf1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.061646ms
    Jan 19 05:56:34.942: INFO: The phase of Pod busybox-readonly-fs613769ac-53d0-4139-b557-fc0688e4abf1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 05:56:36.948: INFO: Pod "busybox-readonly-fs613769ac-53d0-4139-b557-fc0688e4abf1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01013553s
    Jan 19 05:56:36.948: INFO: The phase of Pod busybox-readonly-fs613769ac-53d0-4139-b557-fc0688e4abf1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 05:56:38.947: INFO: Pod "busybox-readonly-fs613769ac-53d0-4139-b557-fc0688e4abf1": Phase="Running", Reason="", readiness=true. Elapsed: 4.009487866s
    Jan 19 05:56:38.947: INFO: The phase of Pod busybox-readonly-fs613769ac-53d0-4139-b557-fc0688e4abf1 is Running (Ready = true)
    Jan 19 05:56:38.947: INFO: Pod "busybox-readonly-fs613769ac-53d0-4139-b557-fc0688e4abf1" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan 19 05:56:38.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-9474" for this suite. 01/19/23 05:56:38.983
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:56:38.989
Jan 19 05:56:38.989: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename events 01/19/23 05:56:38.99
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:56:39.002
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:56:39.003
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 01/19/23 05:56:39.005
STEP: listing events in all namespaces 01/19/23 05:56:39.009
STEP: listing events in test namespace 01/19/23 05:56:39.011
STEP: listing events with field selection filtering on source 01/19/23 05:56:39.013
STEP: listing events with field selection filtering on reportingController 01/19/23 05:56:39.015
STEP: getting the test event 01/19/23 05:56:39.017
STEP: patching the test event 01/19/23 05:56:39.018
STEP: getting the test event 01/19/23 05:56:39.023
STEP: updating the test event 01/19/23 05:56:39.025
STEP: getting the test event 01/19/23 05:56:39.029
STEP: deleting the test event 01/19/23 05:56:39.03
STEP: listing events in all namespaces 01/19/23 05:56:39.035
STEP: listing events in test namespace 01/19/23 05:56:39.037
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Jan 19 05:56:39.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9747" for this suite. 01/19/23 05:56:39.041
{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","completed":341,"skipped":6164,"failed":0}
------------------------------
â€¢ [0.057 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:56:38.989
    Jan 19 05:56:38.989: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename events 01/19/23 05:56:38.99
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:56:39.002
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:56:39.003
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 01/19/23 05:56:39.005
    STEP: listing events in all namespaces 01/19/23 05:56:39.009
    STEP: listing events in test namespace 01/19/23 05:56:39.011
    STEP: listing events with field selection filtering on source 01/19/23 05:56:39.013
    STEP: listing events with field selection filtering on reportingController 01/19/23 05:56:39.015
    STEP: getting the test event 01/19/23 05:56:39.017
    STEP: patching the test event 01/19/23 05:56:39.018
    STEP: getting the test event 01/19/23 05:56:39.023
    STEP: updating the test event 01/19/23 05:56:39.025
    STEP: getting the test event 01/19/23 05:56:39.029
    STEP: deleting the test event 01/19/23 05:56:39.03
    STEP: listing events in all namespaces 01/19/23 05:56:39.035
    STEP: listing events in test namespace 01/19/23 05:56:39.037
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Jan 19 05:56:39.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-9747" for this suite. 01/19/23 05:56:39.041
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:56:39.046
Jan 19 05:56:39.046: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename security-context-test 01/19/23 05:56:39.047
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:56:39.057
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:56:39.059
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
Jan 19 05:56:39.069: INFO: Waiting up to 5m0s for pod "busybox-user-65534-ecddf5fe-2893-4710-af8d-2cb4cd813705" in namespace "security-context-test-6414" to be "Succeeded or Failed"
Jan 19 05:56:39.072: INFO: Pod "busybox-user-65534-ecddf5fe-2893-4710-af8d-2cb4cd813705": Phase="Pending", Reason="", readiness=false. Elapsed: 3.306185ms
Jan 19 05:56:41.077: INFO: Pod "busybox-user-65534-ecddf5fe-2893-4710-af8d-2cb4cd813705": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007672557s
Jan 19 05:56:43.077: INFO: Pod "busybox-user-65534-ecddf5fe-2893-4710-af8d-2cb4cd813705": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007811066s
Jan 19 05:56:45.078: INFO: Pod "busybox-user-65534-ecddf5fe-2893-4710-af8d-2cb4cd813705": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009073419s
Jan 19 05:56:45.078: INFO: Pod "busybox-user-65534-ecddf5fe-2893-4710-af8d-2cb4cd813705" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 19 05:56:45.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6414" for this suite. 01/19/23 05:56:45.082
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","completed":342,"skipped":6181,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.042 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:308
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:56:39.046
    Jan 19 05:56:39.046: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename security-context-test 01/19/23 05:56:39.047
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:56:39.057
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:56:39.059
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:346
    Jan 19 05:56:39.069: INFO: Waiting up to 5m0s for pod "busybox-user-65534-ecddf5fe-2893-4710-af8d-2cb4cd813705" in namespace "security-context-test-6414" to be "Succeeded or Failed"
    Jan 19 05:56:39.072: INFO: Pod "busybox-user-65534-ecddf5fe-2893-4710-af8d-2cb4cd813705": Phase="Pending", Reason="", readiness=false. Elapsed: 3.306185ms
    Jan 19 05:56:41.077: INFO: Pod "busybox-user-65534-ecddf5fe-2893-4710-af8d-2cb4cd813705": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007672557s
    Jan 19 05:56:43.077: INFO: Pod "busybox-user-65534-ecddf5fe-2893-4710-af8d-2cb4cd813705": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007811066s
    Jan 19 05:56:45.078: INFO: Pod "busybox-user-65534-ecddf5fe-2893-4710-af8d-2cb4cd813705": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009073419s
    Jan 19 05:56:45.078: INFO: Pod "busybox-user-65534-ecddf5fe-2893-4710-af8d-2cb4cd813705" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 19 05:56:45.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-6414" for this suite. 01/19/23 05:56:45.082
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:56:45.089
Jan 19 05:56:45.089: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename resourcequota 01/19/23 05:56:45.09
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:56:45.105
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:56:45.107
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
STEP: Creating a ResourceQuota with best effort scope 01/19/23 05:56:45.11
STEP: Ensuring ResourceQuota status is calculated 01/19/23 05:56:45.115
STEP: Creating a ResourceQuota with not best effort scope 01/19/23 05:56:47.119
STEP: Ensuring ResourceQuota status is calculated 01/19/23 05:56:47.125
STEP: Creating a best-effort pod 01/19/23 05:56:49.129
STEP: Ensuring resource quota with best effort scope captures the pod usage 01/19/23 05:56:49.141
STEP: Ensuring resource quota with not best effort ignored the pod usage 01/19/23 05:56:51.145
STEP: Deleting the pod 01/19/23 05:56:53.149
STEP: Ensuring resource quota status released the pod usage 01/19/23 05:56:53.161
STEP: Creating a not best-effort pod 01/19/23 05:56:55.166
STEP: Ensuring resource quota with not best effort scope captures the pod usage 01/19/23 05:56:55.18
STEP: Ensuring resource quota with best effort scope ignored the pod usage 01/19/23 05:56:57.185
STEP: Deleting the pod 01/19/23 05:56:59.19
STEP: Ensuring resource quota status released the pod usage 01/19/23 05:56:59.207
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 19 05:57:01.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6029" for this suite. 01/19/23 05:57:01.214
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","completed":343,"skipped":6211,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.130 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:56:45.089
    Jan 19 05:56:45.089: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename resourcequota 01/19/23 05:56:45.09
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:56:45.105
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:56:45.107
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:793
    STEP: Creating a ResourceQuota with best effort scope 01/19/23 05:56:45.11
    STEP: Ensuring ResourceQuota status is calculated 01/19/23 05:56:45.115
    STEP: Creating a ResourceQuota with not best effort scope 01/19/23 05:56:47.119
    STEP: Ensuring ResourceQuota status is calculated 01/19/23 05:56:47.125
    STEP: Creating a best-effort pod 01/19/23 05:56:49.129
    STEP: Ensuring resource quota with best effort scope captures the pod usage 01/19/23 05:56:49.141
    STEP: Ensuring resource quota with not best effort ignored the pod usage 01/19/23 05:56:51.145
    STEP: Deleting the pod 01/19/23 05:56:53.149
    STEP: Ensuring resource quota status released the pod usage 01/19/23 05:56:53.161
    STEP: Creating a not best-effort pod 01/19/23 05:56:55.166
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 01/19/23 05:56:55.18
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 01/19/23 05:56:57.185
    STEP: Deleting the pod 01/19/23 05:56:59.19
    STEP: Ensuring resource quota status released the pod usage 01/19/23 05:56:59.207
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 19 05:57:01.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-6029" for this suite. 01/19/23 05:57:01.214
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:57:01.219
Jan 19 05:57:01.219: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename configmap 01/19/23 05:57:01.22
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:57:01.231
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:57:01.234
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
STEP: Creating configMap with name configmap-test-volume-238e55c5-6dea-42e3-a4c2-cd447435b629 01/19/23 05:57:01.237
STEP: Creating a pod to test consume configMaps 01/19/23 05:57:01.24
Jan 19 05:57:01.248: INFO: Waiting up to 5m0s for pod "pod-configmaps-78b9a452-2bb6-48b8-9ef7-4de1a1a2a51e" in namespace "configmap-6414" to be "Succeeded or Failed"
Jan 19 05:57:01.253: INFO: Pod "pod-configmaps-78b9a452-2bb6-48b8-9ef7-4de1a1a2a51e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.514375ms
Jan 19 05:57:03.258: INFO: Pod "pod-configmaps-78b9a452-2bb6-48b8-9ef7-4de1a1a2a51e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009602522s
Jan 19 05:57:05.258: INFO: Pod "pod-configmaps-78b9a452-2bb6-48b8-9ef7-4de1a1a2a51e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009835719s
Jan 19 05:57:07.257: INFO: Pod "pod-configmaps-78b9a452-2bb6-48b8-9ef7-4de1a1a2a51e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008674879s
STEP: Saw pod success 01/19/23 05:57:07.257
Jan 19 05:57:07.257: INFO: Pod "pod-configmaps-78b9a452-2bb6-48b8-9ef7-4de1a1a2a51e" satisfied condition "Succeeded or Failed"
Jan 19 05:57:07.260: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-configmaps-78b9a452-2bb6-48b8-9ef7-4de1a1a2a51e container configmap-volume-test: <nil>
STEP: delete the pod 01/19/23 05:57:07.266
Jan 19 05:57:07.280: INFO: Waiting for pod pod-configmaps-78b9a452-2bb6-48b8-9ef7-4de1a1a2a51e to disappear
Jan 19 05:57:07.282: INFO: Pod pod-configmaps-78b9a452-2bb6-48b8-9ef7-4de1a1a2a51e no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 19 05:57:07.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6414" for this suite. 01/19/23 05:57:07.284
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":344,"skipped":6212,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.070 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:57:01.219
    Jan 19 05:57:01.219: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename configmap 01/19/23 05:57:01.22
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:57:01.231
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:57:01.234
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:422
    STEP: Creating configMap with name configmap-test-volume-238e55c5-6dea-42e3-a4c2-cd447435b629 01/19/23 05:57:01.237
    STEP: Creating a pod to test consume configMaps 01/19/23 05:57:01.24
    Jan 19 05:57:01.248: INFO: Waiting up to 5m0s for pod "pod-configmaps-78b9a452-2bb6-48b8-9ef7-4de1a1a2a51e" in namespace "configmap-6414" to be "Succeeded or Failed"
    Jan 19 05:57:01.253: INFO: Pod "pod-configmaps-78b9a452-2bb6-48b8-9ef7-4de1a1a2a51e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.514375ms
    Jan 19 05:57:03.258: INFO: Pod "pod-configmaps-78b9a452-2bb6-48b8-9ef7-4de1a1a2a51e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009602522s
    Jan 19 05:57:05.258: INFO: Pod "pod-configmaps-78b9a452-2bb6-48b8-9ef7-4de1a1a2a51e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009835719s
    Jan 19 05:57:07.257: INFO: Pod "pod-configmaps-78b9a452-2bb6-48b8-9ef7-4de1a1a2a51e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008674879s
    STEP: Saw pod success 01/19/23 05:57:07.257
    Jan 19 05:57:07.257: INFO: Pod "pod-configmaps-78b9a452-2bb6-48b8-9ef7-4de1a1a2a51e" satisfied condition "Succeeded or Failed"
    Jan 19 05:57:07.260: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-configmaps-78b9a452-2bb6-48b8-9ef7-4de1a1a2a51e container configmap-volume-test: <nil>
    STEP: delete the pod 01/19/23 05:57:07.266
    Jan 19 05:57:07.280: INFO: Waiting for pod pod-configmaps-78b9a452-2bb6-48b8-9ef7-4de1a1a2a51e to disappear
    Jan 19 05:57:07.282: INFO: Pod pod-configmaps-78b9a452-2bb6-48b8-9ef7-4de1a1a2a51e no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 19 05:57:07.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6414" for this suite. 01/19/23 05:57:07.284
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:57:07.289
Jan 19 05:57:07.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename projected 01/19/23 05:57:07.29
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:57:07.304
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:57:07.306
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
STEP: Creating configMap with name projected-configmap-test-volume-ed0e51c5-f9c7-4272-ba5c-ebc499d87c1b 01/19/23 05:57:07.308
STEP: Creating a pod to test consume configMaps 01/19/23 05:57:07.311
Jan 19 05:57:07.318: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4fcc1441-45a9-4a67-be7a-a3e67efc6284" in namespace "projected-7932" to be "Succeeded or Failed"
Jan 19 05:57:07.322: INFO: Pod "pod-projected-configmaps-4fcc1441-45a9-4a67-be7a-a3e67efc6284": Phase="Pending", Reason="", readiness=false. Elapsed: 3.662783ms
Jan 19 05:57:09.348: INFO: Pod "pod-projected-configmaps-4fcc1441-45a9-4a67-be7a-a3e67efc6284": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029436464s
Jan 19 05:57:11.326: INFO: Pod "pod-projected-configmaps-4fcc1441-45a9-4a67-be7a-a3e67efc6284": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008226505s
Jan 19 05:57:13.326: INFO: Pod "pod-projected-configmaps-4fcc1441-45a9-4a67-be7a-a3e67efc6284": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007738963s
STEP: Saw pod success 01/19/23 05:57:13.326
Jan 19 05:57:13.326: INFO: Pod "pod-projected-configmaps-4fcc1441-45a9-4a67-be7a-a3e67efc6284" satisfied condition "Succeeded or Failed"
Jan 19 05:57:13.331: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-projected-configmaps-4fcc1441-45a9-4a67-be7a-a3e67efc6284 container agnhost-container: <nil>
STEP: delete the pod 01/19/23 05:57:13.339
Jan 19 05:57:13.354: INFO: Waiting for pod pod-projected-configmaps-4fcc1441-45a9-4a67-be7a-a3e67efc6284 to disappear
Jan 19 05:57:13.356: INFO: Pod pod-projected-configmaps-4fcc1441-45a9-4a67-be7a-a3e67efc6284 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 19 05:57:13.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7932" for this suite. 01/19/23 05:57:13.359
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":345,"skipped":6221,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.075 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:57:07.289
    Jan 19 05:57:07.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename projected 01/19/23 05:57:07.29
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:57:07.304
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:57:07.306
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:46
    STEP: Creating configMap with name projected-configmap-test-volume-ed0e51c5-f9c7-4272-ba5c-ebc499d87c1b 01/19/23 05:57:07.308
    STEP: Creating a pod to test consume configMaps 01/19/23 05:57:07.311
    Jan 19 05:57:07.318: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4fcc1441-45a9-4a67-be7a-a3e67efc6284" in namespace "projected-7932" to be "Succeeded or Failed"
    Jan 19 05:57:07.322: INFO: Pod "pod-projected-configmaps-4fcc1441-45a9-4a67-be7a-a3e67efc6284": Phase="Pending", Reason="", readiness=false. Elapsed: 3.662783ms
    Jan 19 05:57:09.348: INFO: Pod "pod-projected-configmaps-4fcc1441-45a9-4a67-be7a-a3e67efc6284": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029436464s
    Jan 19 05:57:11.326: INFO: Pod "pod-projected-configmaps-4fcc1441-45a9-4a67-be7a-a3e67efc6284": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008226505s
    Jan 19 05:57:13.326: INFO: Pod "pod-projected-configmaps-4fcc1441-45a9-4a67-be7a-a3e67efc6284": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007738963s
    STEP: Saw pod success 01/19/23 05:57:13.326
    Jan 19 05:57:13.326: INFO: Pod "pod-projected-configmaps-4fcc1441-45a9-4a67-be7a-a3e67efc6284" satisfied condition "Succeeded or Failed"
    Jan 19 05:57:13.331: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-projected-configmaps-4fcc1441-45a9-4a67-be7a-a3e67efc6284 container agnhost-container: <nil>
    STEP: delete the pod 01/19/23 05:57:13.339
    Jan 19 05:57:13.354: INFO: Waiting for pod pod-projected-configmaps-4fcc1441-45a9-4a67-be7a-a3e67efc6284 to disappear
    Jan 19 05:57:13.356: INFO: Pod pod-projected-configmaps-4fcc1441-45a9-4a67-be7a-a3e67efc6284 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 19 05:57:13.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7932" for this suite. 01/19/23 05:57:13.359
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:57:13.365
Jan 19 05:57:13.365: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename watch 01/19/23 05:57:13.366
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:57:13.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:57:13.394
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 01/19/23 05:57:13.396
STEP: starting a background goroutine to produce watch events 01/19/23 05:57:13.398
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 01/19/23 05:57:13.398
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan 19 05:57:16.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1427" for this suite. 01/19/23 05:57:16.22
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","completed":346,"skipped":6248,"failed":0}
------------------------------
â€¢ [2.907 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:57:13.365
    Jan 19 05:57:13.365: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename watch 01/19/23 05:57:13.366
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:57:13.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:57:13.394
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 01/19/23 05:57:13.396
    STEP: starting a background goroutine to produce watch events 01/19/23 05:57:13.398
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 01/19/23 05:57:13.398
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan 19 05:57:16.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-1427" for this suite. 01/19/23 05:57:16.22
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:57:16.273
Jan 19 05:57:16.273: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename custom-resource-definition 01/19/23 05:57:16.274
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:57:16.288
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:57:16.293
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Jan 19 05:57:16.298: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 05:57:22.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4917" for this suite. 01/19/23 05:57:22.53
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","completed":347,"skipped":6276,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.263 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:57:16.273
    Jan 19 05:57:16.273: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename custom-resource-definition 01/19/23 05:57:16.274
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:57:16.288
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:57:16.293
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Jan 19 05:57:16.298: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 05:57:22.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-4917" for this suite. 01/19/23 05:57:22.53
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:57:22.537
Jan 19 05:57:22.537: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename pod-network-test 01/19/23 05:57:22.538
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:57:22.549
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:57:22.553
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-9382 01/19/23 05:57:22.556
STEP: creating a selector 01/19/23 05:57:22.556
STEP: Creating the service pods in kubernetes 01/19/23 05:57:22.556
Jan 19 05:57:22.556: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 19 05:57:22.578: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9382" to be "running and ready"
Jan 19 05:57:22.585: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.707991ms
Jan 19 05:57:22.585: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 05:57:24.589: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010805227s
Jan 19 05:57:24.589: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 05:57:26.590: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.011863578s
Jan 19 05:57:26.590: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 05:57:28.590: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.011682781s
Jan 19 05:57:28.590: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 05:57:30.590: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.011273152s
Jan 19 05:57:30.590: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 05:57:32.589: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.011126576s
Jan 19 05:57:32.589: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 05:57:34.590: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.011662604s
Jan 19 05:57:34.590: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 05:57:36.589: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.011141332s
Jan 19 05:57:36.589: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 05:57:38.590: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.011868392s
Jan 19 05:57:38.590: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 05:57:40.590: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.011322151s
Jan 19 05:57:40.590: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 05:57:42.590: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.011296799s
Jan 19 05:57:42.590: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 05:57:44.590: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.011415148s
Jan 19 05:57:44.590: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 19 05:57:44.590: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 19 05:57:44.593: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9382" to be "running and ready"
Jan 19 05:57:44.596: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.897704ms
Jan 19 05:57:44.596: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 19 05:57:44.596: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 01/19/23 05:57:44.598
Jan 19 05:57:44.604: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9382" to be "running"
Jan 19 05:57:44.615: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.570515ms
Jan 19 05:57:46.619: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014362552s
Jan 19 05:57:48.619: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.014934388s
Jan 19 05:57:48.619: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 19 05:57:48.622: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 19 05:57:48.622: INFO: Breadth first check of 10.100.24.206 on host 192.168.0.99...
Jan 19 05:57:48.624: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.70.28:9080/dial?request=hostname&protocol=http&host=10.100.24.206&port=8083&tries=1'] Namespace:pod-network-test-9382 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 05:57:48.624: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
Jan 19 05:57:48.625: INFO: ExecWithOptions: Clientset creation
Jan 19 05:57:48.625: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-9382/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.70.28%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.100.24.206%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 19 05:57:48.731: INFO: Waiting for responses: map[]
Jan 19 05:57:48.731: INFO: reached 10.100.24.206 after 0/1 tries
Jan 19 05:57:48.731: INFO: Breadth first check of 10.100.70.27 on host 192.168.0.78...
Jan 19 05:57:48.735: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.70.28:9080/dial?request=hostname&protocol=http&host=10.100.70.27&port=8083&tries=1'] Namespace:pod-network-test-9382 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 05:57:48.735: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
Jan 19 05:57:48.736: INFO: ExecWithOptions: Clientset creation
Jan 19 05:57:48.736: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-9382/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.70.28%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.100.70.27%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 19 05:57:48.825: INFO: Waiting for responses: map[]
Jan 19 05:57:48.825: INFO: reached 10.100.70.27 after 0/1 tries
Jan 19 05:57:48.825: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jan 19 05:57:48.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9382" for this suite. 01/19/23 05:57:48.828
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","completed":348,"skipped":6299,"failed":0}
------------------------------
â€¢ [SLOW TEST] [26.297 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:57:22.537
    Jan 19 05:57:22.537: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename pod-network-test 01/19/23 05:57:22.538
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:57:22.549
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:57:22.553
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-9382 01/19/23 05:57:22.556
    STEP: creating a selector 01/19/23 05:57:22.556
    STEP: Creating the service pods in kubernetes 01/19/23 05:57:22.556
    Jan 19 05:57:22.556: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 19 05:57:22.578: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9382" to be "running and ready"
    Jan 19 05:57:22.585: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.707991ms
    Jan 19 05:57:22.585: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 05:57:24.589: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010805227s
    Jan 19 05:57:24.589: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 05:57:26.590: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.011863578s
    Jan 19 05:57:26.590: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 05:57:28.590: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.011682781s
    Jan 19 05:57:28.590: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 05:57:30.590: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.011273152s
    Jan 19 05:57:30.590: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 05:57:32.589: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.011126576s
    Jan 19 05:57:32.589: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 05:57:34.590: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.011662604s
    Jan 19 05:57:34.590: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 05:57:36.589: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.011141332s
    Jan 19 05:57:36.589: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 05:57:38.590: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.011868392s
    Jan 19 05:57:38.590: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 05:57:40.590: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.011322151s
    Jan 19 05:57:40.590: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 05:57:42.590: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.011296799s
    Jan 19 05:57:42.590: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 05:57:44.590: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.011415148s
    Jan 19 05:57:44.590: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 19 05:57:44.590: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 19 05:57:44.593: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9382" to be "running and ready"
    Jan 19 05:57:44.596: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.897704ms
    Jan 19 05:57:44.596: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 19 05:57:44.596: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 01/19/23 05:57:44.598
    Jan 19 05:57:44.604: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9382" to be "running"
    Jan 19 05:57:44.615: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.570515ms
    Jan 19 05:57:46.619: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014362552s
    Jan 19 05:57:48.619: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.014934388s
    Jan 19 05:57:48.619: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 19 05:57:48.622: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jan 19 05:57:48.622: INFO: Breadth first check of 10.100.24.206 on host 192.168.0.99...
    Jan 19 05:57:48.624: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.70.28:9080/dial?request=hostname&protocol=http&host=10.100.24.206&port=8083&tries=1'] Namespace:pod-network-test-9382 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 05:57:48.624: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    Jan 19 05:57:48.625: INFO: ExecWithOptions: Clientset creation
    Jan 19 05:57:48.625: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-9382/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.70.28%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.100.24.206%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 19 05:57:48.731: INFO: Waiting for responses: map[]
    Jan 19 05:57:48.731: INFO: reached 10.100.24.206 after 0/1 tries
    Jan 19 05:57:48.731: INFO: Breadth first check of 10.100.70.27 on host 192.168.0.78...
    Jan 19 05:57:48.735: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.70.28:9080/dial?request=hostname&protocol=http&host=10.100.70.27&port=8083&tries=1'] Namespace:pod-network-test-9382 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 05:57:48.735: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    Jan 19 05:57:48.736: INFO: ExecWithOptions: Clientset creation
    Jan 19 05:57:48.736: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-9382/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.70.28%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.100.70.27%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 19 05:57:48.825: INFO: Waiting for responses: map[]
    Jan 19 05:57:48.825: INFO: reached 10.100.70.27 after 0/1 tries
    Jan 19 05:57:48.825: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jan 19 05:57:48.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-9382" for this suite. 01/19/23 05:57:48.828
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:57:48.835
Jan 19 05:57:48.835: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename replication-controller 01/19/23 05:57:48.835
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:57:48.852
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:57:48.855
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
Jan 19 05:57:48.857: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 01/19/23 05:57:48.868
STEP: Checking rc "condition-test" has the desired failure condition set 01/19/23 05:57:48.877
STEP: Scaling down rc "condition-test" to satisfy pod quota 01/19/23 05:57:49.886
Jan 19 05:57:49.893: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 01/19/23 05:57:49.893
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan 19 05:57:50.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5395" for this suite. 01/19/23 05:57:50.902
{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","completed":349,"skipped":6305,"failed":0}
------------------------------
â€¢ [2.072 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:57:48.835
    Jan 19 05:57:48.835: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename replication-controller 01/19/23 05:57:48.835
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:57:48.852
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:57:48.855
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:82
    Jan 19 05:57:48.857: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 01/19/23 05:57:48.868
    STEP: Checking rc "condition-test" has the desired failure condition set 01/19/23 05:57:48.877
    STEP: Scaling down rc "condition-test" to satisfy pod quota 01/19/23 05:57:49.886
    Jan 19 05:57:49.893: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 01/19/23 05:57:49.893
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan 19 05:57:50.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-5395" for this suite. 01/19/23 05:57:50.902
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:57:50.908
Jan 19 05:57:50.908: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename emptydir 01/19/23 05:57:50.909
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:57:50.919
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:57:50.923
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
STEP: Creating a pod to test emptydir volume type on node default medium 01/19/23 05:57:50.926
Jan 19 05:57:50.933: INFO: Waiting up to 5m0s for pod "pod-aa6ee771-9fb5-4cfb-b299-75e6e9efc492" in namespace "emptydir-9507" to be "Succeeded or Failed"
Jan 19 05:57:50.936: INFO: Pod "pod-aa6ee771-9fb5-4cfb-b299-75e6e9efc492": Phase="Pending", Reason="", readiness=false. Elapsed: 3.069108ms
Jan 19 05:57:52.940: INFO: Pod "pod-aa6ee771-9fb5-4cfb-b299-75e6e9efc492": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00619263s
Jan 19 05:57:54.940: INFO: Pod "pod-aa6ee771-9fb5-4cfb-b299-75e6e9efc492": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006980051s
Jan 19 05:57:56.940: INFO: Pod "pod-aa6ee771-9fb5-4cfb-b299-75e6e9efc492": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006578756s
STEP: Saw pod success 01/19/23 05:57:56.94
Jan 19 05:57:56.940: INFO: Pod "pod-aa6ee771-9fb5-4cfb-b299-75e6e9efc492" satisfied condition "Succeeded or Failed"
Jan 19 05:57:56.942: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-aa6ee771-9fb5-4cfb-b299-75e6e9efc492 container test-container: <nil>
STEP: delete the pod 01/19/23 05:57:56.948
Jan 19 05:57:56.961: INFO: Waiting for pod pod-aa6ee771-9fb5-4cfb-b299-75e6e9efc492 to disappear
Jan 19 05:57:56.963: INFO: Pod pod-aa6ee771-9fb5-4cfb-b299-75e6e9efc492 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 19 05:57:56.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9507" for this suite. 01/19/23 05:57:56.968
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":350,"skipped":6334,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.066 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:57:50.908
    Jan 19 05:57:50.908: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename emptydir 01/19/23 05:57:50.909
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:57:50.919
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:57:50.923
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:156
    STEP: Creating a pod to test emptydir volume type on node default medium 01/19/23 05:57:50.926
    Jan 19 05:57:50.933: INFO: Waiting up to 5m0s for pod "pod-aa6ee771-9fb5-4cfb-b299-75e6e9efc492" in namespace "emptydir-9507" to be "Succeeded or Failed"
    Jan 19 05:57:50.936: INFO: Pod "pod-aa6ee771-9fb5-4cfb-b299-75e6e9efc492": Phase="Pending", Reason="", readiness=false. Elapsed: 3.069108ms
    Jan 19 05:57:52.940: INFO: Pod "pod-aa6ee771-9fb5-4cfb-b299-75e6e9efc492": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00619263s
    Jan 19 05:57:54.940: INFO: Pod "pod-aa6ee771-9fb5-4cfb-b299-75e6e9efc492": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006980051s
    Jan 19 05:57:56.940: INFO: Pod "pod-aa6ee771-9fb5-4cfb-b299-75e6e9efc492": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006578756s
    STEP: Saw pod success 01/19/23 05:57:56.94
    Jan 19 05:57:56.940: INFO: Pod "pod-aa6ee771-9fb5-4cfb-b299-75e6e9efc492" satisfied condition "Succeeded or Failed"
    Jan 19 05:57:56.942: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-aa6ee771-9fb5-4cfb-b299-75e6e9efc492 container test-container: <nil>
    STEP: delete the pod 01/19/23 05:57:56.948
    Jan 19 05:57:56.961: INFO: Waiting for pod pod-aa6ee771-9fb5-4cfb-b299-75e6e9efc492 to disappear
    Jan 19 05:57:56.963: INFO: Pod pod-aa6ee771-9fb5-4cfb-b299-75e6e9efc492 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 19 05:57:56.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9507" for this suite. 01/19/23 05:57:56.968
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:57:56.975
Jan 19 05:57:56.975: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename services 01/19/23 05:57:56.976
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:57:56.992
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:57:56.995
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 19 05:57:56.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1363" for this suite. 01/19/23 05:57:57.002
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","completed":351,"skipped":6362,"failed":0}
------------------------------
â€¢ [0.032 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:57:56.975
    Jan 19 05:57:56.975: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename services 01/19/23 05:57:56.976
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:57:56.992
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:57:56.995
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:781
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 19 05:57:56.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1363" for this suite. 01/19/23 05:57:57.002
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:57:57.008
Jan 19 05:57:57.008: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename subpath 01/19/23 05:57:57.009
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:57:57.024
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:57:57.026
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/19/23 05:57:57.028
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-sj7v 01/19/23 05:57:57.035
STEP: Creating a pod to test atomic-volume-subpath 01/19/23 05:57:57.035
Jan 19 05:57:57.046: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-sj7v" in namespace "subpath-2110" to be "Succeeded or Failed"
Jan 19 05:57:57.050: INFO: Pod "pod-subpath-test-secret-sj7v": Phase="Pending", Reason="", readiness=false. Elapsed: 4.659164ms
Jan 19 05:57:59.059: INFO: Pod "pod-subpath-test-secret-sj7v": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013072044s
Jan 19 05:58:01.054: INFO: Pod "pod-subpath-test-secret-sj7v": Phase="Running", Reason="", readiness=true. Elapsed: 4.008749755s
Jan 19 05:58:03.054: INFO: Pod "pod-subpath-test-secret-sj7v": Phase="Running", Reason="", readiness=true. Elapsed: 6.008776425s
Jan 19 05:58:05.055: INFO: Pod "pod-subpath-test-secret-sj7v": Phase="Running", Reason="", readiness=true. Elapsed: 8.009463254s
Jan 19 05:58:07.056: INFO: Pod "pod-subpath-test-secret-sj7v": Phase="Running", Reason="", readiness=true. Elapsed: 10.010000332s
Jan 19 05:58:09.056: INFO: Pod "pod-subpath-test-secret-sj7v": Phase="Running", Reason="", readiness=true. Elapsed: 12.01011089s
Jan 19 05:58:11.055: INFO: Pod "pod-subpath-test-secret-sj7v": Phase="Running", Reason="", readiness=true. Elapsed: 14.009799536s
Jan 19 05:58:13.054: INFO: Pod "pod-subpath-test-secret-sj7v": Phase="Running", Reason="", readiness=true. Elapsed: 16.008328998s
Jan 19 05:58:15.054: INFO: Pod "pod-subpath-test-secret-sj7v": Phase="Running", Reason="", readiness=true. Elapsed: 18.008005052s
Jan 19 05:58:17.054: INFO: Pod "pod-subpath-test-secret-sj7v": Phase="Running", Reason="", readiness=true. Elapsed: 20.008883682s
Jan 19 05:58:19.055: INFO: Pod "pod-subpath-test-secret-sj7v": Phase="Running", Reason="", readiness=true. Elapsed: 22.009371588s
Jan 19 05:58:21.055: INFO: Pod "pod-subpath-test-secret-sj7v": Phase="Running", Reason="", readiness=false. Elapsed: 24.009234055s
Jan 19 05:58:23.056: INFO: Pod "pod-subpath-test-secret-sj7v": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.010159106s
STEP: Saw pod success 01/19/23 05:58:23.056
Jan 19 05:58:23.056: INFO: Pod "pod-subpath-test-secret-sj7v" satisfied condition "Succeeded or Failed"
Jan 19 05:58:23.059: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-subpath-test-secret-sj7v container test-container-subpath-secret-sj7v: <nil>
STEP: delete the pod 01/19/23 05:58:23.067
Jan 19 05:58:23.078: INFO: Waiting for pod pod-subpath-test-secret-sj7v to disappear
Jan 19 05:58:23.081: INFO: Pod pod-subpath-test-secret-sj7v no longer exists
STEP: Deleting pod pod-subpath-test-secret-sj7v 01/19/23 05:58:23.081
Jan 19 05:58:23.081: INFO: Deleting pod "pod-subpath-test-secret-sj7v" in namespace "subpath-2110"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan 19 05:58:23.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2110" for this suite. 01/19/23 05:58:23.086
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","completed":352,"skipped":6397,"failed":0}
------------------------------
â€¢ [SLOW TEST] [26.082 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:57:57.008
    Jan 19 05:57:57.008: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename subpath 01/19/23 05:57:57.009
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:57:57.024
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:57:57.026
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/19/23 05:57:57.028
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-sj7v 01/19/23 05:57:57.035
    STEP: Creating a pod to test atomic-volume-subpath 01/19/23 05:57:57.035
    Jan 19 05:57:57.046: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-sj7v" in namespace "subpath-2110" to be "Succeeded or Failed"
    Jan 19 05:57:57.050: INFO: Pod "pod-subpath-test-secret-sj7v": Phase="Pending", Reason="", readiness=false. Elapsed: 4.659164ms
    Jan 19 05:57:59.059: INFO: Pod "pod-subpath-test-secret-sj7v": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013072044s
    Jan 19 05:58:01.054: INFO: Pod "pod-subpath-test-secret-sj7v": Phase="Running", Reason="", readiness=true. Elapsed: 4.008749755s
    Jan 19 05:58:03.054: INFO: Pod "pod-subpath-test-secret-sj7v": Phase="Running", Reason="", readiness=true. Elapsed: 6.008776425s
    Jan 19 05:58:05.055: INFO: Pod "pod-subpath-test-secret-sj7v": Phase="Running", Reason="", readiness=true. Elapsed: 8.009463254s
    Jan 19 05:58:07.056: INFO: Pod "pod-subpath-test-secret-sj7v": Phase="Running", Reason="", readiness=true. Elapsed: 10.010000332s
    Jan 19 05:58:09.056: INFO: Pod "pod-subpath-test-secret-sj7v": Phase="Running", Reason="", readiness=true. Elapsed: 12.01011089s
    Jan 19 05:58:11.055: INFO: Pod "pod-subpath-test-secret-sj7v": Phase="Running", Reason="", readiness=true. Elapsed: 14.009799536s
    Jan 19 05:58:13.054: INFO: Pod "pod-subpath-test-secret-sj7v": Phase="Running", Reason="", readiness=true. Elapsed: 16.008328998s
    Jan 19 05:58:15.054: INFO: Pod "pod-subpath-test-secret-sj7v": Phase="Running", Reason="", readiness=true. Elapsed: 18.008005052s
    Jan 19 05:58:17.054: INFO: Pod "pod-subpath-test-secret-sj7v": Phase="Running", Reason="", readiness=true. Elapsed: 20.008883682s
    Jan 19 05:58:19.055: INFO: Pod "pod-subpath-test-secret-sj7v": Phase="Running", Reason="", readiness=true. Elapsed: 22.009371588s
    Jan 19 05:58:21.055: INFO: Pod "pod-subpath-test-secret-sj7v": Phase="Running", Reason="", readiness=false. Elapsed: 24.009234055s
    Jan 19 05:58:23.056: INFO: Pod "pod-subpath-test-secret-sj7v": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.010159106s
    STEP: Saw pod success 01/19/23 05:58:23.056
    Jan 19 05:58:23.056: INFO: Pod "pod-subpath-test-secret-sj7v" satisfied condition "Succeeded or Failed"
    Jan 19 05:58:23.059: INFO: Trying to get logs from node ckcp-nks-default-worker-node-1 pod pod-subpath-test-secret-sj7v container test-container-subpath-secret-sj7v: <nil>
    STEP: delete the pod 01/19/23 05:58:23.067
    Jan 19 05:58:23.078: INFO: Waiting for pod pod-subpath-test-secret-sj7v to disappear
    Jan 19 05:58:23.081: INFO: Pod pod-subpath-test-secret-sj7v no longer exists
    STEP: Deleting pod pod-subpath-test-secret-sj7v 01/19/23 05:58:23.081
    Jan 19 05:58:23.081: INFO: Deleting pod "pod-subpath-test-secret-sj7v" in namespace "subpath-2110"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan 19 05:58:23.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-2110" for this suite. 01/19/23 05:58:23.086
  << End Captured GinkgoWriter Output
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:58:23.09
Jan 19 05:58:23.091: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename certificates 01/19/23 05:58:23.091
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:58:23.101
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:58:23.103
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 01/19/23 05:58:24.245
STEP: getting /apis/certificates.k8s.io 01/19/23 05:58:24.248
STEP: getting /apis/certificates.k8s.io/v1 01/19/23 05:58:24.248
STEP: creating 01/19/23 05:58:24.25
STEP: getting 01/19/23 05:58:24.267
STEP: listing 01/19/23 05:58:24.269
STEP: watching 01/19/23 05:58:24.272
Jan 19 05:58:24.272: INFO: starting watch
STEP: patching 01/19/23 05:58:24.274
STEP: updating 01/19/23 05:58:24.28
Jan 19 05:58:24.284: INFO: waiting for watch events with expected annotations
Jan 19 05:58:24.284: INFO: saw patched and updated annotations
STEP: getting /approval 01/19/23 05:58:24.284
STEP: patching /approval 01/19/23 05:58:24.287
STEP: updating /approval 01/19/23 05:58:24.293
STEP: getting /status 01/19/23 05:58:24.3
STEP: patching /status 01/19/23 05:58:24.302
STEP: updating /status 01/19/23 05:58:24.308
STEP: deleting 01/19/23 05:58:24.316
STEP: deleting a collection 01/19/23 05:58:24.335
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 05:58:24.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-502" for this suite. 01/19/23 05:58:24.366
{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","completed":353,"skipped":6397,"failed":0}
------------------------------
â€¢ [1.282 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:58:23.09
    Jan 19 05:58:23.091: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename certificates 01/19/23 05:58:23.091
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:58:23.101
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:58:23.103
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 01/19/23 05:58:24.245
    STEP: getting /apis/certificates.k8s.io 01/19/23 05:58:24.248
    STEP: getting /apis/certificates.k8s.io/v1 01/19/23 05:58:24.248
    STEP: creating 01/19/23 05:58:24.25
    STEP: getting 01/19/23 05:58:24.267
    STEP: listing 01/19/23 05:58:24.269
    STEP: watching 01/19/23 05:58:24.272
    Jan 19 05:58:24.272: INFO: starting watch
    STEP: patching 01/19/23 05:58:24.274
    STEP: updating 01/19/23 05:58:24.28
    Jan 19 05:58:24.284: INFO: waiting for watch events with expected annotations
    Jan 19 05:58:24.284: INFO: saw patched and updated annotations
    STEP: getting /approval 01/19/23 05:58:24.284
    STEP: patching /approval 01/19/23 05:58:24.287
    STEP: updating /approval 01/19/23 05:58:24.293
    STEP: getting /status 01/19/23 05:58:24.3
    STEP: patching /status 01/19/23 05:58:24.302
    STEP: updating /status 01/19/23 05:58:24.308
    STEP: deleting 01/19/23 05:58:24.316
    STEP: deleting a collection 01/19/23 05:58:24.335
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 05:58:24.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "certificates-502" for this suite. 01/19/23 05:58:24.366
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:58:24.373
Jan 19 05:58:24.373: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename proxy 01/19/23 05:58:24.373
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:58:24.387
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:58:24.389
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Jan 19 05:58:24.393: INFO: Creating pod...
Jan 19 05:58:24.400: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6008" to be "running"
Jan 19 05:58:24.404: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 3.777268ms
Jan 19 05:58:26.407: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007093833s
Jan 19 05:58:28.408: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.007790473s
Jan 19 05:58:28.408: INFO: Pod "agnhost" satisfied condition "running"
Jan 19 05:58:28.408: INFO: Creating service...
Jan 19 05:58:28.417: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6008/pods/agnhost/proxy?method=DELETE
Jan 19 05:58:28.428: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 19 05:58:28.428: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6008/pods/agnhost/proxy?method=OPTIONS
Jan 19 05:58:28.430: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 19 05:58:28.431: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6008/pods/agnhost/proxy?method=PATCH
Jan 19 05:58:28.434: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 19 05:58:28.434: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6008/pods/agnhost/proxy?method=POST
Jan 19 05:58:28.437: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 19 05:58:28.437: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6008/pods/agnhost/proxy?method=PUT
Jan 19 05:58:28.440: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 19 05:58:28.440: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6008/services/e2e-proxy-test-service/proxy?method=DELETE
Jan 19 05:58:28.446: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 19 05:58:28.446: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6008/services/e2e-proxy-test-service/proxy?method=OPTIONS
Jan 19 05:58:28.450: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 19 05:58:28.450: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6008/services/e2e-proxy-test-service/proxy?method=PATCH
Jan 19 05:58:28.454: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 19 05:58:28.454: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6008/services/e2e-proxy-test-service/proxy?method=POST
Jan 19 05:58:28.458: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 19 05:58:28.458: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6008/services/e2e-proxy-test-service/proxy?method=PUT
Jan 19 05:58:28.463: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 19 05:58:28.463: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6008/pods/agnhost/proxy?method=GET
Jan 19 05:58:28.465: INFO: http.Client request:GET StatusCode:301
Jan 19 05:58:28.465: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6008/services/e2e-proxy-test-service/proxy?method=GET
Jan 19 05:58:28.468: INFO: http.Client request:GET StatusCode:301
Jan 19 05:58:28.468: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6008/pods/agnhost/proxy?method=HEAD
Jan 19 05:58:28.470: INFO: http.Client request:HEAD StatusCode:301
Jan 19 05:58:28.470: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6008/services/e2e-proxy-test-service/proxy?method=HEAD
Jan 19 05:58:28.473: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Jan 19 05:58:28.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6008" for this suite. 01/19/23 05:58:28.476
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","completed":354,"skipped":6405,"failed":0}
------------------------------
â€¢ [4.109 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:58:24.373
    Jan 19 05:58:24.373: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename proxy 01/19/23 05:58:24.373
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:58:24.387
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:58:24.389
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Jan 19 05:58:24.393: INFO: Creating pod...
    Jan 19 05:58:24.400: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6008" to be "running"
    Jan 19 05:58:24.404: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 3.777268ms
    Jan 19 05:58:26.407: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007093833s
    Jan 19 05:58:28.408: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.007790473s
    Jan 19 05:58:28.408: INFO: Pod "agnhost" satisfied condition "running"
    Jan 19 05:58:28.408: INFO: Creating service...
    Jan 19 05:58:28.417: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6008/pods/agnhost/proxy?method=DELETE
    Jan 19 05:58:28.428: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 19 05:58:28.428: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6008/pods/agnhost/proxy?method=OPTIONS
    Jan 19 05:58:28.430: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 19 05:58:28.431: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6008/pods/agnhost/proxy?method=PATCH
    Jan 19 05:58:28.434: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 19 05:58:28.434: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6008/pods/agnhost/proxy?method=POST
    Jan 19 05:58:28.437: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 19 05:58:28.437: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6008/pods/agnhost/proxy?method=PUT
    Jan 19 05:58:28.440: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 19 05:58:28.440: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6008/services/e2e-proxy-test-service/proxy?method=DELETE
    Jan 19 05:58:28.446: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 19 05:58:28.446: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6008/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Jan 19 05:58:28.450: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 19 05:58:28.450: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6008/services/e2e-proxy-test-service/proxy?method=PATCH
    Jan 19 05:58:28.454: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 19 05:58:28.454: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6008/services/e2e-proxy-test-service/proxy?method=POST
    Jan 19 05:58:28.458: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 19 05:58:28.458: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6008/services/e2e-proxy-test-service/proxy?method=PUT
    Jan 19 05:58:28.463: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 19 05:58:28.463: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6008/pods/agnhost/proxy?method=GET
    Jan 19 05:58:28.465: INFO: http.Client request:GET StatusCode:301
    Jan 19 05:58:28.465: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6008/services/e2e-proxy-test-service/proxy?method=GET
    Jan 19 05:58:28.468: INFO: http.Client request:GET StatusCode:301
    Jan 19 05:58:28.468: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6008/pods/agnhost/proxy?method=HEAD
    Jan 19 05:58:28.470: INFO: http.Client request:HEAD StatusCode:301
    Jan 19 05:58:28.470: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6008/services/e2e-proxy-test-service/proxy?method=HEAD
    Jan 19 05:58:28.473: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Jan 19 05:58:28.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-6008" for this suite. 01/19/23 05:58:28.476
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:58:28.482
Jan 19 05:58:28.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename taint-multiple-pods 01/19/23 05:58:28.483
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:58:28.497
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:58:28.5
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
Jan 19 05:58:28.502: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 19 05:59:28.518: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
Jan 19 05:59:28.521: INFO: Starting informer...
STEP: Starting pods... 01/19/23 05:59:28.521
Jan 19 05:59:28.740: INFO: Pod1 is running on ckcp-nks-default-worker-node-1. Tainting Node
Jan 19 05:59:28.949: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-9269" to be "running"
Jan 19 05:59:28.952: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.874133ms
Jan 19 05:59:30.957: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008473554s
Jan 19 05:59:32.955: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 4.006702791s
Jan 19 05:59:32.955: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Jan 19 05:59:32.955: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-9269" to be "running"
Jan 19 05:59:32.958: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.652558ms
Jan 19 05:59:32.958: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Jan 19 05:59:32.958: INFO: Pod2 is running on ckcp-nks-default-worker-node-1. Tainting Node
STEP: Trying to apply a taint on the Node 01/19/23 05:59:32.958
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/19/23 05:59:32.97
STEP: Waiting for Pod1 and Pod2 to be deleted 01/19/23 05:59:32.973
Jan 19 05:59:38.496: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jan 19 05:59:58.537: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/19/23 05:59:58.55
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
Jan 19 05:59:58.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-9269" for this suite. 01/19/23 05:59:58.567
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","completed":355,"skipped":6415,"failed":0}
------------------------------
â€¢ [SLOW TEST] [90.089 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:58:28.482
    Jan 19 05:58:28.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename taint-multiple-pods 01/19/23 05:58:28.483
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:58:28.497
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:58:28.5
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:348
    Jan 19 05:58:28.502: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 19 05:59:28.518: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:420
    Jan 19 05:59:28.521: INFO: Starting informer...
    STEP: Starting pods... 01/19/23 05:59:28.521
    Jan 19 05:59:28.740: INFO: Pod1 is running on ckcp-nks-default-worker-node-1. Tainting Node
    Jan 19 05:59:28.949: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-9269" to be "running"
    Jan 19 05:59:28.952: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.874133ms
    Jan 19 05:59:30.957: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008473554s
    Jan 19 05:59:32.955: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 4.006702791s
    Jan 19 05:59:32.955: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Jan 19 05:59:32.955: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-9269" to be "running"
    Jan 19 05:59:32.958: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.652558ms
    Jan 19 05:59:32.958: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Jan 19 05:59:32.958: INFO: Pod2 is running on ckcp-nks-default-worker-node-1. Tainting Node
    STEP: Trying to apply a taint on the Node 01/19/23 05:59:32.958
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/19/23 05:59:32.97
    STEP: Waiting for Pod1 and Pod2 to be deleted 01/19/23 05:59:32.973
    Jan 19 05:59:38.496: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Jan 19 05:59:58.537: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/19/23 05:59:58.55
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:187
    Jan 19 05:59:58.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-multiple-pods-9269" for this suite. 01/19/23 05:59:58.567
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 05:59:58.573
Jan 19 05:59:58.573: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename endpointslice 01/19/23 05:59:58.574
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:59:58.585
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:59:58.589
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
STEP: referencing a single matching pod 01/19/23 06:00:03.655
STEP: referencing matching pods with named port 01/19/23 06:00:08.664
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 01/19/23 06:00:13.671
STEP: recreating EndpointSlices after they've been deleted 01/19/23 06:00:18.684
Jan 19 06:00:18.702: INFO: EndpointSlice for Service endpointslice-3371/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jan 19 06:00:28.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-3371" for this suite. 01/19/23 06:00:28.718
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","completed":356,"skipped":6458,"failed":0}
------------------------------
â€¢ [SLOW TEST] [30.154 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 05:59:58.573
    Jan 19 05:59:58.573: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename endpointslice 01/19/23 05:59:58.574
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 05:59:58.585
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 05:59:58.589
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:204
    STEP: referencing a single matching pod 01/19/23 06:00:03.655
    STEP: referencing matching pods with named port 01/19/23 06:00:08.664
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 01/19/23 06:00:13.671
    STEP: recreating EndpointSlices after they've been deleted 01/19/23 06:00:18.684
    Jan 19 06:00:18.702: INFO: EndpointSlice for Service endpointslice-3371/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jan 19 06:00:28.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-3371" for this suite. 01/19/23 06:00:28.718
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 06:00:28.727
Jan 19 06:00:28.727: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename container-runtime 01/19/23 06:00:28.728
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 06:00:28.741
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 06:00:28.743
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 01/19/23 06:00:28.753
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 01/19/23 06:00:45.834
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 01/19/23 06:00:45.837
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 01/19/23 06:00:45.842
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 01/19/23 06:00:45.842
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 01/19/23 06:00:45.863
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 01/19/23 06:00:48.882
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 01/19/23 06:00:50.891
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 01/19/23 06:00:50.896
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 01/19/23 06:00:50.896
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 01/19/23 06:00:50.916
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 01/19/23 06:00:51.924
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 01/19/23 06:00:55.943
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 01/19/23 06:00:55.948
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 01/19/23 06:00:55.948
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan 19 06:00:55.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-924" for this suite. 01/19/23 06:00:55.973
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","completed":357,"skipped":6463,"failed":0}
------------------------------
â€¢ [SLOW TEST] [27.251 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 06:00:28.727
    Jan 19 06:00:28.727: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename container-runtime 01/19/23 06:00:28.728
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 06:00:28.741
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 06:00:28.743
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 01/19/23 06:00:28.753
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 01/19/23 06:00:45.834
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 01/19/23 06:00:45.837
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 01/19/23 06:00:45.842
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 01/19/23 06:00:45.842
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 01/19/23 06:00:45.863
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 01/19/23 06:00:48.882
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 01/19/23 06:00:50.891
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 01/19/23 06:00:50.896
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 01/19/23 06:00:50.896
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 01/19/23 06:00:50.916
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 01/19/23 06:00:51.924
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 01/19/23 06:00:55.943
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 01/19/23 06:00:55.948
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 01/19/23 06:00:55.948
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan 19 06:00:55.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-924" for this suite. 01/19/23 06:00:55.973
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 06:00:55.979
Jan 19 06:00:55.979: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename resourcequota 01/19/23 06:00:55.98
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 06:00:55.991
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 06:00:55.994
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
STEP: Counting existing ResourceQuota 01/19/23 06:00:55.996
STEP: Creating a ResourceQuota 01/19/23 06:01:01
STEP: Ensuring resource quota status is calculated 01/19/23 06:01:01.005
STEP: Creating a Service 01/19/23 06:01:03.009
STEP: Creating a NodePort Service 01/19/23 06:01:03.025
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 01/19/23 06:01:03.048
STEP: Ensuring resource quota status captures service creation 01/19/23 06:01:03.068
STEP: Deleting Services 01/19/23 06:01:05.073
STEP: Ensuring resource quota status released usage 01/19/23 06:01:05.113
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 19 06:01:07.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8801" for this suite. 01/19/23 06:01:07.122
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","completed":358,"skipped":6506,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.151 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 06:00:55.979
    Jan 19 06:00:55.979: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename resourcequota 01/19/23 06:00:55.98
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 06:00:55.991
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 06:00:55.994
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:90
    STEP: Counting existing ResourceQuota 01/19/23 06:00:55.996
    STEP: Creating a ResourceQuota 01/19/23 06:01:01
    STEP: Ensuring resource quota status is calculated 01/19/23 06:01:01.005
    STEP: Creating a Service 01/19/23 06:01:03.009
    STEP: Creating a NodePort Service 01/19/23 06:01:03.025
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 01/19/23 06:01:03.048
    STEP: Ensuring resource quota status captures service creation 01/19/23 06:01:03.068
    STEP: Deleting Services 01/19/23 06:01:05.073
    STEP: Ensuring resource quota status released usage 01/19/23 06:01:05.113
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 19 06:01:07.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-8801" for this suite. 01/19/23 06:01:07.122
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 06:01:07.132
Jan 19 06:01:07.132: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename pods 01/19/23 06:01:07.133
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 06:01:07.148
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 06:01:07.151
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
STEP: Create a pod 01/19/23 06:01:07.154
Jan 19 06:01:07.160: INFO: Waiting up to 5m0s for pod "pod-bsml6" in namespace "pods-2380" to be "running"
Jan 19 06:01:07.165: INFO: Pod "pod-bsml6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.704052ms
Jan 19 06:01:09.170: INFO: Pod "pod-bsml6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009866514s
Jan 19 06:01:11.169: INFO: Pod "pod-bsml6": Phase="Running", Reason="", readiness=true. Elapsed: 4.009446092s
Jan 19 06:01:11.169: INFO: Pod "pod-bsml6" satisfied condition "running"
STEP: patching /status 01/19/23 06:01:11.169
Jan 19 06:01:11.175: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 19 06:01:11.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2380" for this suite. 01/19/23 06:01:11.178
{"msg":"PASSED [sig-node] Pods should patch a pod status [Conformance]","completed":359,"skipped":6611,"failed":0}
------------------------------
â€¢ [4.051 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 06:01:07.132
    Jan 19 06:01:07.132: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename pods 01/19/23 06:01:07.133
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 06:01:07.148
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 06:01:07.151
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1082
    STEP: Create a pod 01/19/23 06:01:07.154
    Jan 19 06:01:07.160: INFO: Waiting up to 5m0s for pod "pod-bsml6" in namespace "pods-2380" to be "running"
    Jan 19 06:01:07.165: INFO: Pod "pod-bsml6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.704052ms
    Jan 19 06:01:09.170: INFO: Pod "pod-bsml6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009866514s
    Jan 19 06:01:11.169: INFO: Pod "pod-bsml6": Phase="Running", Reason="", readiness=true. Elapsed: 4.009446092s
    Jan 19 06:01:11.169: INFO: Pod "pod-bsml6" satisfied condition "running"
    STEP: patching /status 01/19/23 06:01:11.169
    Jan 19 06:01:11.175: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 19 06:01:11.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-2380" for this suite. 01/19/23 06:01:11.178
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 06:01:11.184
Jan 19 06:01:11.184: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename projected 01/19/23 06:01:11.185
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 06:01:11.197
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 06:01:11.2
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
STEP: Creating secret with name s-test-opt-del-24ad7871-2952-4701-8cc4-5df2cbf61ea9 01/19/23 06:01:11.205
STEP: Creating secret with name s-test-opt-upd-8802f5a7-ba9d-4110-aef3-40d3682d6615 01/19/23 06:01:11.209
STEP: Creating the pod 01/19/23 06:01:11.213
Jan 19 06:01:11.221: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c4debf7e-1d7a-4b7a-897e-fc0d9c637064" in namespace "projected-1505" to be "running and ready"
Jan 19 06:01:11.227: INFO: Pod "pod-projected-secrets-c4debf7e-1d7a-4b7a-897e-fc0d9c637064": Phase="Pending", Reason="", readiness=false. Elapsed: 5.782398ms
Jan 19 06:01:11.227: INFO: The phase of Pod pod-projected-secrets-c4debf7e-1d7a-4b7a-897e-fc0d9c637064 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 06:01:13.232: INFO: Pod "pod-projected-secrets-c4debf7e-1d7a-4b7a-897e-fc0d9c637064": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010665979s
Jan 19 06:01:13.232: INFO: The phase of Pod pod-projected-secrets-c4debf7e-1d7a-4b7a-897e-fc0d9c637064 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 06:01:15.231: INFO: Pod "pod-projected-secrets-c4debf7e-1d7a-4b7a-897e-fc0d9c637064": Phase="Running", Reason="", readiness=true. Elapsed: 4.009432192s
Jan 19 06:01:15.231: INFO: The phase of Pod pod-projected-secrets-c4debf7e-1d7a-4b7a-897e-fc0d9c637064 is Running (Ready = true)
Jan 19 06:01:15.231: INFO: Pod "pod-projected-secrets-c4debf7e-1d7a-4b7a-897e-fc0d9c637064" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-24ad7871-2952-4701-8cc4-5df2cbf61ea9 01/19/23 06:01:15.275
STEP: Updating secret s-test-opt-upd-8802f5a7-ba9d-4110-aef3-40d3682d6615 01/19/23 06:01:15.28
STEP: Creating secret with name s-test-opt-create-f454f116-949e-42ae-8e9b-5b2b5b07319d 01/19/23 06:01:15.284
STEP: waiting to observe update in volume 01/19/23 06:01:15.288
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 19 06:01:17.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1505" for this suite. 01/19/23 06:01:17.316
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":360,"skipped":6629,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.138 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 06:01:11.184
    Jan 19 06:01:11.184: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename projected 01/19/23 06:01:11.185
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 06:01:11.197
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 06:01:11.2
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:214
    STEP: Creating secret with name s-test-opt-del-24ad7871-2952-4701-8cc4-5df2cbf61ea9 01/19/23 06:01:11.205
    STEP: Creating secret with name s-test-opt-upd-8802f5a7-ba9d-4110-aef3-40d3682d6615 01/19/23 06:01:11.209
    STEP: Creating the pod 01/19/23 06:01:11.213
    Jan 19 06:01:11.221: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c4debf7e-1d7a-4b7a-897e-fc0d9c637064" in namespace "projected-1505" to be "running and ready"
    Jan 19 06:01:11.227: INFO: Pod "pod-projected-secrets-c4debf7e-1d7a-4b7a-897e-fc0d9c637064": Phase="Pending", Reason="", readiness=false. Elapsed: 5.782398ms
    Jan 19 06:01:11.227: INFO: The phase of Pod pod-projected-secrets-c4debf7e-1d7a-4b7a-897e-fc0d9c637064 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 06:01:13.232: INFO: Pod "pod-projected-secrets-c4debf7e-1d7a-4b7a-897e-fc0d9c637064": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010665979s
    Jan 19 06:01:13.232: INFO: The phase of Pod pod-projected-secrets-c4debf7e-1d7a-4b7a-897e-fc0d9c637064 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 06:01:15.231: INFO: Pod "pod-projected-secrets-c4debf7e-1d7a-4b7a-897e-fc0d9c637064": Phase="Running", Reason="", readiness=true. Elapsed: 4.009432192s
    Jan 19 06:01:15.231: INFO: The phase of Pod pod-projected-secrets-c4debf7e-1d7a-4b7a-897e-fc0d9c637064 is Running (Ready = true)
    Jan 19 06:01:15.231: INFO: Pod "pod-projected-secrets-c4debf7e-1d7a-4b7a-897e-fc0d9c637064" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-24ad7871-2952-4701-8cc4-5df2cbf61ea9 01/19/23 06:01:15.275
    STEP: Updating secret s-test-opt-upd-8802f5a7-ba9d-4110-aef3-40d3682d6615 01/19/23 06:01:15.28
    STEP: Creating secret with name s-test-opt-create-f454f116-949e-42ae-8e9b-5b2b5b07319d 01/19/23 06:01:15.284
    STEP: waiting to observe update in volume 01/19/23 06:01:15.288
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 19 06:01:17.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1505" for this suite. 01/19/23 06:01:17.316
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 06:01:17.323
Jan 19 06:01:17.323: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename crd-publish-openapi 01/19/23 06:01:17.324
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 06:01:17.347
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 06:01:17.349
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
Jan 19 06:01:17.352: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/19/23 06:01:19.765
Jan 19 06:01:19.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-713 --namespace=crd-publish-openapi-713 create -f -'
Jan 19 06:01:20.395: INFO: stderr: ""
Jan 19 06:01:20.395: INFO: stdout: "e2e-test-crd-publish-openapi-5478-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 19 06:01:20.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-713 --namespace=crd-publish-openapi-713 delete e2e-test-crd-publish-openapi-5478-crds test-cr'
Jan 19 06:01:20.479: INFO: stderr: ""
Jan 19 06:01:20.479: INFO: stdout: "e2e-test-crd-publish-openapi-5478-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jan 19 06:01:20.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-713 --namespace=crd-publish-openapi-713 apply -f -'
Jan 19 06:01:20.703: INFO: stderr: ""
Jan 19 06:01:20.703: INFO: stdout: "e2e-test-crd-publish-openapi-5478-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 19 06:01:20.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-713 --namespace=crd-publish-openapi-713 delete e2e-test-crd-publish-openapi-5478-crds test-cr'
Jan 19 06:01:20.841: INFO: stderr: ""
Jan 19 06:01:20.841: INFO: stdout: "e2e-test-crd-publish-openapi-5478-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 01/19/23 06:01:20.841
Jan 19 06:01:20.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-713 explain e2e-test-crd-publish-openapi-5478-crds'
Jan 19 06:01:21.445: INFO: stderr: ""
Jan 19 06:01:21.445: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5478-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 06:01:23.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-713" for this suite. 01/19/23 06:01:23.935
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","completed":361,"skipped":6665,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.619 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 06:01:17.323
    Jan 19 06:01:17.323: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename crd-publish-openapi 01/19/23 06:01:17.324
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 06:01:17.347
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 06:01:17.349
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:193
    Jan 19 06:01:17.352: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/19/23 06:01:19.765
    Jan 19 06:01:19.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-713 --namespace=crd-publish-openapi-713 create -f -'
    Jan 19 06:01:20.395: INFO: stderr: ""
    Jan 19 06:01:20.395: INFO: stdout: "e2e-test-crd-publish-openapi-5478-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jan 19 06:01:20.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-713 --namespace=crd-publish-openapi-713 delete e2e-test-crd-publish-openapi-5478-crds test-cr'
    Jan 19 06:01:20.479: INFO: stderr: ""
    Jan 19 06:01:20.479: INFO: stdout: "e2e-test-crd-publish-openapi-5478-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Jan 19 06:01:20.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-713 --namespace=crd-publish-openapi-713 apply -f -'
    Jan 19 06:01:20.703: INFO: stderr: ""
    Jan 19 06:01:20.703: INFO: stdout: "e2e-test-crd-publish-openapi-5478-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jan 19 06:01:20.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-713 --namespace=crd-publish-openapi-713 delete e2e-test-crd-publish-openapi-5478-crds test-cr'
    Jan 19 06:01:20.841: INFO: stderr: ""
    Jan 19 06:01:20.841: INFO: stdout: "e2e-test-crd-publish-openapi-5478-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 01/19/23 06:01:20.841
    Jan 19 06:01:20.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3882518140 --namespace=crd-publish-openapi-713 explain e2e-test-crd-publish-openapi-5478-crds'
    Jan 19 06:01:21.445: INFO: stderr: ""
    Jan 19 06:01:21.445: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5478-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 06:01:23.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-713" for this suite. 01/19/23 06:01:23.935
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 06:01:23.944
Jan 19 06:01:23.944: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
STEP: Building a namespace api object, basename pods 01/19/23 06:01:23.945
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 06:01:23.96
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 06:01:23.962
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
STEP: creating the pod 01/19/23 06:01:23.964
STEP: submitting the pod to kubernetes 01/19/23 06:01:23.964
Jan 19 06:01:23.973: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-1c295d8a-c2e9-4cd4-acb4-cbfd90ab9fbe" in namespace "pods-5600" to be "running and ready"
Jan 19 06:01:23.976: INFO: Pod "pod-update-activedeadlineseconds-1c295d8a-c2e9-4cd4-acb4-cbfd90ab9fbe": Phase="Pending", Reason="", readiness=false. Elapsed: 3.156012ms
Jan 19 06:01:23.976: INFO: The phase of Pod pod-update-activedeadlineseconds-1c295d8a-c2e9-4cd4-acb4-cbfd90ab9fbe is Pending, waiting for it to be Running (with Ready = true)
Jan 19 06:01:25.980: INFO: Pod "pod-update-activedeadlineseconds-1c295d8a-c2e9-4cd4-acb4-cbfd90ab9fbe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007279851s
Jan 19 06:01:25.980: INFO: The phase of Pod pod-update-activedeadlineseconds-1c295d8a-c2e9-4cd4-acb4-cbfd90ab9fbe is Pending, waiting for it to be Running (with Ready = true)
Jan 19 06:01:27.980: INFO: Pod "pod-update-activedeadlineseconds-1c295d8a-c2e9-4cd4-acb4-cbfd90ab9fbe": Phase="Running", Reason="", readiness=true. Elapsed: 4.007804074s
Jan 19 06:01:27.981: INFO: The phase of Pod pod-update-activedeadlineseconds-1c295d8a-c2e9-4cd4-acb4-cbfd90ab9fbe is Running (Ready = true)
Jan 19 06:01:27.981: INFO: Pod "pod-update-activedeadlineseconds-1c295d8a-c2e9-4cd4-acb4-cbfd90ab9fbe" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 01/19/23 06:01:27.984
STEP: updating the pod 01/19/23 06:01:27.988
Jan 19 06:01:28.500: INFO: Successfully updated pod "pod-update-activedeadlineseconds-1c295d8a-c2e9-4cd4-acb4-cbfd90ab9fbe"
Jan 19 06:01:28.500: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-1c295d8a-c2e9-4cd4-acb4-cbfd90ab9fbe" in namespace "pods-5600" to be "terminated with reason DeadlineExceeded"
Jan 19 06:01:28.506: INFO: Pod "pod-update-activedeadlineseconds-1c295d8a-c2e9-4cd4-acb4-cbfd90ab9fbe": Phase="Running", Reason="", readiness=true. Elapsed: 6.005905ms
Jan 19 06:01:30.510: INFO: Pod "pod-update-activedeadlineseconds-1c295d8a-c2e9-4cd4-acb4-cbfd90ab9fbe": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.009878482s
Jan 19 06:01:30.510: INFO: Pod "pod-update-activedeadlineseconds-1c295d8a-c2e9-4cd4-acb4-cbfd90ab9fbe" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 19 06:01:30.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5600" for this suite. 01/19/23 06:01:30.514
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","completed":362,"skipped":6690,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.582 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 06:01:23.944
    Jan 19 06:01:23.944: INFO: >>> kubeConfig: /tmp/kubeconfig-3882518140
    STEP: Building a namespace api object, basename pods 01/19/23 06:01:23.945
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 06:01:23.96
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 06:01:23.962
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:397
    STEP: creating the pod 01/19/23 06:01:23.964
    STEP: submitting the pod to kubernetes 01/19/23 06:01:23.964
    Jan 19 06:01:23.973: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-1c295d8a-c2e9-4cd4-acb4-cbfd90ab9fbe" in namespace "pods-5600" to be "running and ready"
    Jan 19 06:01:23.976: INFO: Pod "pod-update-activedeadlineseconds-1c295d8a-c2e9-4cd4-acb4-cbfd90ab9fbe": Phase="Pending", Reason="", readiness=false. Elapsed: 3.156012ms
    Jan 19 06:01:23.976: INFO: The phase of Pod pod-update-activedeadlineseconds-1c295d8a-c2e9-4cd4-acb4-cbfd90ab9fbe is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 06:01:25.980: INFO: Pod "pod-update-activedeadlineseconds-1c295d8a-c2e9-4cd4-acb4-cbfd90ab9fbe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007279851s
    Jan 19 06:01:25.980: INFO: The phase of Pod pod-update-activedeadlineseconds-1c295d8a-c2e9-4cd4-acb4-cbfd90ab9fbe is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 06:01:27.980: INFO: Pod "pod-update-activedeadlineseconds-1c295d8a-c2e9-4cd4-acb4-cbfd90ab9fbe": Phase="Running", Reason="", readiness=true. Elapsed: 4.007804074s
    Jan 19 06:01:27.981: INFO: The phase of Pod pod-update-activedeadlineseconds-1c295d8a-c2e9-4cd4-acb4-cbfd90ab9fbe is Running (Ready = true)
    Jan 19 06:01:27.981: INFO: Pod "pod-update-activedeadlineseconds-1c295d8a-c2e9-4cd4-acb4-cbfd90ab9fbe" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 01/19/23 06:01:27.984
    STEP: updating the pod 01/19/23 06:01:27.988
    Jan 19 06:01:28.500: INFO: Successfully updated pod "pod-update-activedeadlineseconds-1c295d8a-c2e9-4cd4-acb4-cbfd90ab9fbe"
    Jan 19 06:01:28.500: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-1c295d8a-c2e9-4cd4-acb4-cbfd90ab9fbe" in namespace "pods-5600" to be "terminated with reason DeadlineExceeded"
    Jan 19 06:01:28.506: INFO: Pod "pod-update-activedeadlineseconds-1c295d8a-c2e9-4cd4-acb4-cbfd90ab9fbe": Phase="Running", Reason="", readiness=true. Elapsed: 6.005905ms
    Jan 19 06:01:30.510: INFO: Pod "pod-update-activedeadlineseconds-1c295d8a-c2e9-4cd4-acb4-cbfd90ab9fbe": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.009878482s
    Jan 19 06:01:30.510: INFO: Pod "pod-update-activedeadlineseconds-1c295d8a-c2e9-4cd4-acb4-cbfd90ab9fbe" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 19 06:01:30.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-5600" for this suite. 01/19/23 06:01:30.514
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
{"msg":"Test Suite completed","completed":362,"skipped":6705,"failed":0}
Jan 19 06:01:30.527: INFO: Running AfterSuite actions on all nodes
Jan 19 06:01:30.527: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
Jan 19 06:01:30.527: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
Jan 19 06:01:30.527: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
Jan 19 06:01:30.527: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Jan 19 06:01:30.527: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Jan 19 06:01:30.527: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Jan 19 06:01:30.527: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
Jan 19 06:01:30.527: INFO: Running AfterSuite actions on node 1
Jan 19 06:01:30.527: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Jan 19 06:01:30.527: INFO: Running AfterSuite actions on all nodes
    Jan 19 06:01:30.527: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
    Jan 19 06:01:30.527: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
    Jan 19 06:01:30.527: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
    Jan 19 06:01:30.527: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
    Jan 19 06:01:30.527: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
    Jan 19 06:01:30.527: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
    Jan 19 06:01:30.527: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Jan 19 06:01:30.527: INFO: Running AfterSuite actions on node 1
    Jan 19 06:01:30.527: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:146
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:146
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:559
------------------------------
[ReportAfterSuite] PASSED [0.063 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:559
  << End Captured GinkgoWriter Output
------------------------------

Ran 362 of 7067 Specs in 6100.931 seconds
SUCCESS! -- 362 Passed | 0 Failed | 0 Pending | 6705 Skipped
PASS

Ginkgo ran 1 suite in 1h41m41.299618334s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.1.6[0m

